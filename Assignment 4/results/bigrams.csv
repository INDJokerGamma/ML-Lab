,gram_count,document_count,Average_freq
temraz keane,82,2,41.0
keane counterfactual,96,2,48.0
counterfactual data,98,2,49.0
data augmentation,180,2,90.0
augmentation abstract,2,2,1.0
abstract learning,2,2,1.0
learning from,37,6,6.166666666666667
from class,2,2,1.0
class imbalanced,11,4,2.75
imbalanced datasets,25,6,4.166666666666667
datasets poses,2,2,1.0
poses challenges,2,2,1.0
challenges for,4,4,1.0
for many,4,2,2.0
many machine,2,2,1.0
machine learning,78,6,13.0
learning algorithms,17,5,3.4
algorithms many,2,2,1.0
many domains,2,2,1.0
domains are,2,2,1.0
are by,2,2,1.0
by definition,7,3,2.3333333333333335
definition class,2,2,1.0
imbalanced by,3,3,1.0
by virtue,2,2,1.0
virtue of,2,2,1.0
of having,2,2,1.0
having a,2,2,1.0
a majority,7,3,2.3333333333333335
majority class,114,6,19.0
class that,10,3,3.3333333333333335
that naturally,3,3,1.0
naturally has,2,2,1.0
has many,2,2,1.0
many more,7,3,2.3333333333333335
more instances,4,2,2.0
instances than,4,2,2.0
than its,2,2,1.0
its minority,4,2,2.0
minority class,201,6,33.5
class genuine,2,2,1.0
genuine bank,2,2,1.0
bank transactions,2,2,1.0
transactions occur,2,2,1.0
occur much,2,2,1.0
much more,3,3,1.0
more often,2,2,1.0
often than,2,2,1.0
than fraudulent,4,2,2.0
fraudulent ones,4,2,2.0
ones many,2,2,1.0
many methods,2,2,1.0
methods have,10,5,2.0
have been,40,6,6.666666666666667
been proposed,5,3,1.6666666666666667
proposed to,12,4,3.0
to solve,15,5,3.0
solve the,7,3,2.3333333333333335
the class,103,6,17.166666666666668
class imbalance,82,6,13.666666666666666
imbalance problem,54,6,9.0
problem among,2,2,1.0
among the,10,5,2.0
the most,34,6,5.666666666666667
most popular,5,3,1.6666666666666667
popular being,2,2,1.0
being oversampling,2,2,1.0
oversampling techniques,4,2,2.0
techniques such,3,3,1.0
such as,30,6,5.0
as smote,6,3,2.0
smote these,2,2,1.0
these methods,14,6,2.3333333333333335
methods synthetic,1,1,1.0
synthetic instances,40,4,10.0
instances in,34,2,17.0
in the,334,6,55.666666666666664
the minority,207,6,34.5
class to,10,5,2.0
to balance,6,5,1.2
balance the,5,4,1.25
the dataset,72,6,12.0
dataset performing,2,2,1.0
performing data,2,2,1.0
data augmentations,2,2,1.0
augmentations that,2,2,1.0
that improve,8,2,4.0
improve the,14,6,2.3333333333333335
the performance,27,6,4.5
performance of,33,6,5.5
of predictive,2,2,1.0
predictive machine,2,2,1.0
learning ml,4,2,2.0
ml models,11,2,5.5
models in,2,2,1.0
in this,109,6,18.166666666666668
this paper,45,6,7.5
paper we,11,5,2.2
we advance,2,2,1.0
advance a,2,2,1.0
a novel,13,3,4.333333333333333
novel data,2,2,1.0
augmentation method,18,2,9.0
method adapted,2,2,1.0
adapted from,4,2,2.0
from explainable,2,2,1.0
explainable ai,8,2,4.0
ai that,2,2,1.0
that generates,4,2,2.0
generates synthetic,13,4,3.25
synthetic counterfactual,16,2,8.0
counterfactual instances,8,2,4.0
class unlike,2,2,1.0
unlike other,2,2,1.0
other oversampling,4,2,2.0
techniques this,2,2,1.0
this method,24,4,6.0
method adaptively,2,2,1.0
adaptively combines,2,2,1.0
combines instances,1,1,1.0
instances from,4,2,2.0
from the,105,6,17.5
dataset using,4,3,1.3333333333333333
using actual,2,2,1.0
actual rather,2,2,1.0
rather than,22,6,3.6666666666666665
than interpolating,2,2,1.0
interpolating values,2,2,1.0
values between,6,2,3.0
between instances,9,2,4.5
instances several,2,2,1.0
several experiments,2,2,1.0
experiments using,2,2,1.0
using four,2,2,1.0
four different,8,4,2.0
different classifiers,2,2,1.0
classifiers and,4,2,2.0
and solving,2,2,1.0
solving the,3,3,1.0
problem using,4,2,2.0
using a,21,5,4.2
a counterfactual,14,2,7.0
counterfactual method,33,2,16.5
method for,25,6,4.166666666666667
for data,27,5,5.4
augmentation mohammed,2,2,1.0
mohammed mark,2,2,1.0
mark school,2,2,1.0
school of,5,4,1.25
of computer,17,5,3.4
computer science,14,4,3.5
science university,4,2,2.0
university college,6,2,3.0
college dublin,6,2,3.0
dublin belfield,6,2,3.0
belfield dublin,6,2,3.0
dublin ireland,6,2,3.0
ireland insight,2,2,1.0
insight centre,4,2,2.0
centre for,4,2,2.0
data analytics,5,3,1.6666666666666667
analytics university,2,2,1.0
ireland vistamilk,2,2,1.0
vistamilk sfi,4,2,2.0
sfi research,4,2,2.0
research centre,4,2,2.0
centre university,2,2,1.0
ireland temraz,2,2,1.0
augmentation datasets,4,2,2.0
datasets are,10,4,2.5
are reported,2,2,1.0
reported which,2,2,1.0
which show,4,2,2.0
show that,17,4,4.25
that this,12,4,3.0
this counterfactual,13,2,6.5
counterfactual augmentation,12,2,6.0
method cfa,6,2,3.0
cfa generates,4,2,2.0
generates useful,2,2,1.0
useful synthetic,2,2,1.0
synthetic datapoints,13,2,6.5
datapoints in,2,2,1.0
class the,18,6,3.0
the experiments,4,3,1.3333333333333333
experiments also,1,1,1.0
also show,4,2,2.0
that cfa,22,2,11.0
cfa is,14,2,7.0
is competitive,3,3,1.0
competitive with,2,2,1.0
with many,4,2,2.0
many other,4,2,2.0
oversampling methods,9,2,4.5
methods many,2,2,1.0
many of,8,4,2.0
of which,4,4,1.0
which are,13,6,2.1666666666666665
are variants,2,2,1.0
variants of,4,2,2.0
of smote,29,4,7.25
smote the,7,4,1.75
the basis,4,3,1.3333333333333333
basis for,4,2,2.0
for cfa,12,2,6.0
cfa s,4,2,2.0
s performance,14,4,3.5
performance is,6,4,1.5
is discussed,2,2,1.0
discussed along,2,2,1.0
along with,6,2,3.0
with the,88,6,14.666666666666666
the conditions,4,2,2.0
conditions under,6,2,3.0
under which,6,2,3.0
which it,2,2,1.0
it is,50,6,8.333333333333334
is likely,9,3,3.0
likely to,31,6,5.166666666666667
to perform,10,4,2.5
perform better,5,4,1.25
better or,2,2,1.0
or worse,2,2,1.0
worse in,3,3,1.0
in future,4,3,1.3333333333333333
future tests,2,2,1.0
tests keywords,2,2,1.0
keywords counterfactual,2,2,1.0
counterfactual class,4,2,2.0
problem reasoning,2,2,1.0
reasoning data,2,2,1.0
augmentation explainable,2,2,1.0
ai introduction,2,2,1.0
introduction imbalanced,2,2,1.0
datasets create,2,2,1.0
create significant,2,2,1.0
significant problems,2,2,1.0
problems for,4,4,1.0
for machine,4,3,1.3333333333333333
ml in,2,2,1.0
in classification,4,2,2.0
classification tasks,6,2,3.0
tasks classically,2,2,1.0
classically this,2,2,1.0
this problem,13,4,3.25
problem arises,2,2,1.0
arises in,2,2,1.0
in binary,5,3,1.6666666666666667
binary classification,11,3,3.6666666666666665
tasks when,2,2,1.0
when most,1,1,1.0
most of,18,5,3.6
of data,32,5,6.4
data comes,2,2,1.0
comes from,6,2,3.0
from one,2,2,1.0
one class,8,3,2.6666666666666665
the majority,119,6,19.833333333333332
class and,35,5,7.0
and less,2,2,1.0
less comes,2,2,1.0
the other,25,6,4.166666666666667
other class,4,3,1.3333333333333333
class for,9,4,2.25
for instance,18,3,6.0
instance in,23,3,7.666666666666667
in datasets,7,3,2.3333333333333335
datasets always,2,2,1.0
always have,2,2,1.0
have many,2,2,1.0
ones simply,2,2,1.0
simply because,2,2,1.0
because the,7,4,1.75
the latter,5,3,1.6666666666666667
latter are,2,2,1.0
are rarer,2,2,1.0
rarer than,2,2,1.0
than the,20,6,3.3333333333333335
the former,3,3,1.0
former when,2,2,1.0
when a,5,4,1.25
a given,25,5,5.0
given class,2,2,1.0
class is,24,6,4.0
is in,6,4,1.5
dataset in,12,5,2.4
this way,7,4,1.75
way a,2,2,1.0
a classifier,12,4,3.0
classifier s,5,3,1.6666666666666667
performance can,2,2,1.0
can be,105,6,17.5
be compromised,3,3,1.0
compromised in,2,2,1.0
in several,3,3,1.0
several ways,2,2,1.0
ways for,4,2,2.0
instance it,4,3,1.3333333333333333
it may,8,5,1.6
may show,3,3,1.0
show poor,2,2,1.0
poor accuracy,2,2,1.0
accuracy in,4,4,1.0
in predicting,2,2,1.0
predicting the,3,3,1.0
class or,10,4,2.5
or spuriously,2,2,1.0
spuriously high,4,2,2.0
high accuracy,2,2,1.0
accuracy for,4,4,1.0
for the,166,6,27.666666666666668
the classifier,16,3,5.333333333333333
classifier as,2,2,1.0
as a,43,6,7.166666666666667
a whole,2,2,1.0
whole based,2,2,1.0
based only,2,2,1.0
only on,2,2,1.0
on its,5,3,1.6666666666666667
its success,4,2,2.0
success with,2,2,1.0
class it,5,3,1.6666666666666667
it can,33,6,5.5
can result,2,2,1.0
result in,7,4,1.75
in poor,2,2,1.0
poor rule,2,2,1.0
rule induction,4,3,1.3333333333333333
induction for,2,2,1.0
for decision,2,2,1.0
decision trees,5,3,1.6666666666666667
trees this,2,2,1.0
problem has,5,3,1.6666666666666667
has been,46,6,7.666666666666667
been recognized,4,3,1.3333333333333333
recognized in,3,3,1.0
in many,11,5,2.2
many application,4,4,1.0
application domains,7,4,1.75
domains such,3,3,1.0
as medical,3,3,1.0
medical diagnosis,3,3,1.0
diagnosis fraud,1,1,1.0
fraud detection,10,6,1.6666666666666667
detection text,2,2,1.0
text classification,7,3,2.3333333333333335
classification and,3,3,1.0
and detection,2,2,1.0
detection of,6,4,1.5
of oil,6,4,1.5
oil spills,8,5,1.6
spills in,6,4,1.5
in satellite,6,4,1.5
satellite radar,7,4,1.75
radar images,7,4,1.75
images notably,2,2,1.0
notably recently,2,2,1.0
recently some,2,2,1.0
some of,11,6,1.8333333333333333
of the,551,6,91.83333333333333
the techniques,7,5,1.4
techniques proposed,4,4,1.0
problem have,2,2,1.0
have also,6,4,1.5
also proved,2,2,1.0
proved useful,2,2,1.0
useful in,2,2,1.0
in data,13,3,4.333333333333333
augmentation for,8,2,4.0
for deep,6,2,3.0
deep learning,6,2,3.0
learning models,2,2,1.0
models when,2,2,1.0
when new,2,2,1.0
new synthetic,23,3,7.666666666666667
synthetic keane,2,2,1.0
augmentation points,4,2,2.0
points need,2,2,1.0
need to,15,5,3.0
to be,105,6,17.5
be generated,10,5,2.0
generated to,4,2,2.0
to create,11,3,3.6666666666666665
create the,5,3,1.6666666666666667
the large,2,2,1.0
large labelled,2,2,1.0
labelled datasets,2,2,1.0
datasets required,2,2,1.0
required for,2,2,1.0
for better,3,3,1.0
better performance,9,3,3.0
performance in,5,4,1.25
this literature,2,2,1.0
literature several,2,2,1.0
several important,3,3,1.0
important approaches,2,2,1.0
approaches have,4,3,1.3333333333333333
have emerged,4,2,2.0
emerged to,2,2,1.0
to deal,8,4,2.0
deal with,8,4,2.0
with this,8,4,2.0
problem based,2,2,1.0
based on,90,6,15.0
on solutions,2,2,1.0
solutions at,2,2,1.0
at the,15,6,2.5
the data,58,6,9.666666666666666
data or,5,4,1.25
or algorithm,2,2,1.0
algorithm levels,2,2,1.0
levels data,2,2,1.0
data level,4,2,2.0
level solutions,6,2,3.0
solutions attempt,2,2,1.0
attempt to,2,2,1.0
to change,3,3,1.0
change the,6,2,3.0
the distribution,7,4,1.75
distribution of,11,5,2.2
the imbalanced,19,4,4.75
imbalanced data,102,6,17.0
data by,8,5,1.6
by the,36,6,6.0
the original,49,6,8.166666666666666
original data,6,4,1.5
data typically,1,1,1.0
typically these,4,2,2.0
these techniques,9,4,2.25
techniques either,2,2,1.0
either oversample,2,2,1.0
oversample the,7,3,2.3333333333333335
or undersample,2,2,1.0
undersample the,4,2,2.0
or sample,2,2,1.0
sample using,2,2,1.0
using some,3,3,1.0
some combination,2,2,1.0
combination of,24,6,4.0
of both,6,5,1.2
both of,2,2,1.0
of these,26,6,4.333333333333333
methods specifically,2,2,1.0
specifically the,3,2,1.5
the synthetic,25,5,5.0
synthetic minority,36,6,6.0
minority technique,15,5,3.0
technique smote,9,4,2.25
smote has,3,2,1.5
has become,3,3,1.0
become a,3,3,1.0
a very,8,4,2.0
very popular,4,4,1.0
popular method,4,2,2.0
for solving,2,2,1.0
solving issues,1,1,1.0
issues in,4,4,1.0
in traditional,2,2,1.0
traditional ml,2,2,1.0
ml and,2,2,1.0
and has,2,2,1.0
has also,3,3,1.0
also been,4,4,1.0
been applied,7,3,2.3333333333333335
applied to,20,5,4.0
to the,244,6,40.666666666666664
augmentation problem,1,1,1.0
problem in,12,5,2.4
in deep,2,2,1.0
deep learners,2,2,1.0
learners algorithm,2,2,1.0
algorithm level,2,2,1.0
solutions aim,2,2,1.0
aim to,2,2,1.0
to modify,3,3,1.0
modify the,3,3,1.0
the machine,4,3,1.3333333333333333
algorithms used,2,2,1.0
used to,58,6,9.666666666666666
to mitigate,4,2,2.0
mitigate their,2,2,1.0
their bias,2,2,1.0
bias towards,2,2,1.0
towards majority,2,2,1.0
majority groups,3,3,1.0
groups typically,2,2,1.0
techniques involve,2,2,1.0
involve the,2,2,1.0
the use,18,6,3.0
use of,24,6,4.0
of and,12,4,3.0
and ensemble,4,4,1.0
ensemble methods,3,3,1.0
methods in,19,4,4.75
we explore,2,2,1.0
explore a,2,2,1.0
novel approach,2,2,1.0
approach to,12,5,2.4
to both,2,2,1.0
both the,6,5,1.2
imbalance and,8,5,1.6
and data,25,6,4.166666666666667
augmentation problems,6,2,3.0
problems using,3,3,1.0
using an,2,2,1.0
an counterfactual,2,2,1.0
method that,5,3,1.6666666666666667
synthetic in,6,3,2.0
class interestingly,2,2,1.0
interestingly this,2,2,1.0
method was,3,3,1.0
was previously,2,2,1.0
previously developed,2,2,1.0
developed to,5,3,1.6666666666666667
solve problems,5,2,2.5
problems in,7,4,1.75
in explainable,2,2,1.0
ai xai,6,2,3.0
xai for,2,2,1.0
for reviews,4,2,2.0
reviews see,4,2,2.0
see in,2,2,1.0
in logic,3,3,1.0
logic lewis,2,2,1.0
lewis proposed,2,2,1.0
proposed that,2,2,1.0
that counterfactuals,2,2,1.0
counterfactuals are,4,2,2.0
are the,11,5,2.2
the closest,6,2,3.0
closest possible,2,2,1.0
possible world,2,2,1.0
world to,4,2,2.0
the current,19,6,3.1666666666666665
current world,2,2,1.0
world in,4,2,2.0
in which,25,6,4.166666666666667
which the,15,4,3.75
the outcome,4,2,2.0
outcome is,2,2,1.0
is different,2,2,1.0
different hence,2,2,1.0
hence the,5,3,1.6666666666666667
the intuition,5,3,1.6666666666666667
intuition behind,2,2,1.0
behind the,3,3,1.0
current technique,2,2,1.0
technique is,5,5,1.0
is that,40,6,6.666666666666667
that it,21,6,3.5
it generates,4,3,1.3333333333333333
instances using,6,2,3.0
using the,49,6,8.166666666666666
the actual,2,2,1.0
actual of,2,2,1.0
of instances,14,4,3.5
instances not,2,2,1.0
not interpolated,2,2,1.0
interpolated values,4,2,2.0
values that,4,2,2.0
that are,41,5,8.2
the close,2,2,1.0
close to,27,3,9.0
to existing,8,2,4.0
existing thus,1,1,1.0
thus populating,2,2,1.0
populating the,2,2,1.0
class with,9,4,2.25
with plausible,2,2,1.0
plausible adaptations,2,2,1.0
adaptations of,2,2,1.0
of existing,1,1,1.0
existing data,1,1,1.0
data if,2,2,1.0
if this,4,2,2.0
this intuition,2,2,1.0
intuition is,3,3,1.0
is correct,2,2,1.0
correct then,2,2,1.0
then the,15,5,3.0
instances generated,6,3,2.0
generated by,11,4,2.75
by this,5,4,1.25
method should,4,2,2.0
should improve,2,2,1.0
improve ml,2,2,1.0
ml performance,2,2,1.0
performance perhaps,2,2,1.0
perhaps to,2,2,1.0
to a,29,6,4.833333333333333
a level,2,2,1.0
level that,2,2,1.0
that advances,2,2,1.0
advances current,2,2,1.0
current techniques,2,2,1.0
techniques temraz,2,2,1.0
augmentation related,2,2,1.0
related work,4,2,2.0
work the,4,3,1.3333333333333333
the related,2,2,1.0
work to,2,2,1.0
the present,8,2,4.0
present research,2,2,1.0
research comes,2,2,1.0
from two,2,2,1.0
two different,8,5,1.6
different strands,2,2,1.0
strands of,2,2,1.0
of ai,5,3,1.6666666666666667
ai research,2,2,1.0
research from,2,2,1.0
from i,2,2,1.0
i sampling,2,2,1.0
sampling techniques,2,2,1.0
techniques for,10,6,1.6666666666666667
problem and,15,5,3.0
and ii,4,2,2.0
ii counterfactual,2,2,1.0
counterfactual methods,18,2,9.0
methods for,19,6,3.1666666666666665
for explainable,2,2,1.0
xai data,2,2,1.0
solutions to,5,3,1.6666666666666667
problem are,2,2,1.0
are dominated,2,2,1.0
dominated by,2,2,1.0
by three,2,2,1.0
three main,4,2,2.0
main approaches,2,2,1.0
approaches random,2,2,1.0
random ros,2,2,1.0
ros random,2,2,1.0
random rus,2,2,1.0
rus and,2,2,1.0
and synthetic,4,4,1.0
smote in,10,5,2.0
in ros,2,2,1.0
ros the,2,2,1.0
class distribution,17,6,2.8333333333333335
distribution is,5,5,1.0
is balanced,2,2,1.0
balanced by,2,2,1.0
by randomly,5,3,1.6666666666666667
randomly adding,2,2,1.0
adding multiple,2,2,1.0
multiple copies,2,2,1.0
copies of,3,3,1.0
of some,7,5,1.4
minority classes,14,6,2.3333333333333335
classes to,5,3,1.6666666666666667
the training,58,5,11.6
training data,34,6,5.666666666666667
data whereas,2,2,1.0
whereas with,2,2,1.0
with rus,2,2,1.0
rus a,2,2,1.0
a certain,5,3,1.6666666666666667
certain number,2,2,1.0
number of,99,6,16.5
of examples,2,2,1.0
examples of,16,6,2.6666666666666665
class are,8,5,1.6
are randomly,4,4,1.0
randomly removed,2,2,1.0
removed from,2,2,1.0
original dataset,14,3,4.666666666666667
dataset although,2,2,1.0
although these,2,2,1.0
methods can,5,3,1.6666666666666667
can the,2,2,1.0
dataset they,4,2,2.0
they have,8,2,4.0
have some,2,2,1.0
some since,1,1,1.0
since ros,2,2,1.0
ros merely,2,2,1.0
merely copies,2,2,1.0
copies instances,2,2,1.0
instances no,2,2,1.0
no new,2,2,1.0
new information,2,2,1.0
information is,2,2,1.0
is added,2,2,1.0
added to,8,2,4.0
the and,12,5,2.4
and hence,6,4,1.5
hence it,3,3,1.0
can lead,2,2,1.0
lead to,9,4,2.25
to overfitting,2,2,1.0
overfitting on,2,2,1.0
on the,142,6,23.666666666666668
other hand,8,5,1.6
hand since,2,2,1.0
since rus,2,2,1.0
rus randomly,2,2,1.0
randomly removes,1,1,1.0
removes examples,1,1,1.0
examples from,7,4,1.75
class data,4,4,1.0
data can,3,3,1.0
be discarded,2,2,1.0
discarded that,2,2,1.0
that may,2,2,1.0
may be,14,4,3.5
be important,3,3,1.0
important the,2,2,1.0
the third,9,5,1.8
third option,2,2,1.0
option smote,2,2,1.0
smote adopts,2,2,1.0
adopts a,2,2,1.0
a somewhat,2,2,1.0
somewhat different,2,2,1.0
different approach,5,2,2.5
approach based,5,3,1.6666666666666667
on oversampling,4,2,2.0
oversampling from,2,2,1.0
class as,11,4,2.75
smote is,8,4,2.0
is the,55,6,9.166666666666666
the baseline,6,2,3.0
baseline method,3,3,1.0
method used,4,2,2.0
used for,14,4,3.5
for comparisons,2,2,1.0
comparisons in,2,2,1.0
present experiments,2,2,1.0
experiments we,5,3,1.6666666666666667
we briefly,6,2,3.0
briefly describe,2,2,1.0
describe it,2,2,1.0
it in,3,3,1.0
in more,2,2,1.0
more detail,2,2,1.0
detail here,2,2,1.0
here see,2,2,1.0
see section,12,2,6.0
section along,2,2,1.0
with important,2,2,1.0
important smote,2,2,1.0
smote variants,24,2,12.0
variants see,2,2,1.0
section before,2,2,1.0
before going,2,2,1.0
going on,2,2,1.0
on to,3,2,1.5
to describe,2,2,1.0
describe the,4,2,2.0
the counterfactual,26,2,13.0
method we,2,2,1.0
we have,14,4,3.5
have adapted,2,2,1.0
the xai,12,2,6.0
xai literature,6,2,3.0
literature see,2,2,1.0
section finally,3,3,1.0
finally we,2,2,1.0
briefly sketch,4,2,2.0
sketch the,2,2,1.0
the recent,3,3,1.0
recent and,2,2,1.0
and very,3,3,1.0
very small,3,3,1.0
small literature,4,2,2.0
literature that,2,2,1.0
that has,6,2,3.0
has begun,2,2,1.0
begun to,2,2,1.0
to apply,6,4,1.5
apply these,2,2,1.0
these counterfactual,2,2,1.0
counterfactual xai,2,2,1.0
xai methods,2,2,1.0
methods to,5,4,1.25
to and,4,4,1.0
augmentation see,1,1,1.0
section temraz,2,2,1.0
augmentation data,2,2,1.0
data sampling,3,3,1.0
sampling methods,7,5,1.4
problem smote,2,2,1.0
smote synthetic,11,6,1.8333333333333333
minority oversampling,25,6,4.166666666666667
oversampling technique,13,6,2.1666666666666665
smote oversamples,2,2,1.0
oversamples the,2,2,1.0
class by,4,4,1.0
by creating,4,2,2.0
creating synthetic,3,3,1.0
instances rather,4,2,2.0
than by,2,2,1.0
by oversampling,3,3,1.0
oversampling using,2,2,1.0
using replacement,2,2,1.0
replacement it,2,2,1.0
is one,13,5,2.6
one of,36,6,6.0
most widely,4,3,1.3333333333333333
widely used,4,3,1.3333333333333333
used solutions,2,2,1.0
the problem,42,6,7.0
problem google,2,2,1.0
google scholar,2,2,1.0
scholar lists,2,2,1.0
lists over,2,2,1.0
over citations,2,2,1.0
citations to,2,2,1.0
original paper,2,2,1.0
paper in,3,3,1.0
in smote,5,3,1.6666666666666667
the new,9,4,2.25
new example,2,2,1.0
example in,11,4,2.75
is created,1,1,1.0
created by,4,3,1.3333333333333333
by between,1,1,1.0
between several,3,3,1.0
several minority,3,3,1.0
class instances,8,3,2.6666666666666665
instances by,6,4,1.5
by interpolating,4,3,1.3333333333333333
interpolating instead,3,3,1.0
instead of,14,6,2.3333333333333335
of copying,2,2,1.0
copying instances,2,2,1.0
instances smote,2,2,1.0
smote avoids,3,3,1.0
avoids the,3,3,1.0
and creates,1,1,1.0
creates new,1,1,1.0
in neighborhoods,2,2,1.0
neighborhoods surrounding,2,2,1.0
surrounding instances,2,2,1.0
class briefly,2,2,1.0
briefly the,2,2,1.0
the algorithm,19,5,3.8
algorithm works,2,2,1.0
works as,2,2,1.0
as follows,17,6,2.8333333333333335
follows assume,2,2,1.0
assume that,10,4,2.5
that the,94,6,15.666666666666666
is 𝑃,4,2,2.0
𝑃 and,6,2,3.0
and the,109,6,18.166666666666668
is smote,2,2,1.0
smote starts,2,2,1.0
starts by,2,2,1.0
randomly selecting,3,3,1.0
selecting a,2,2,1.0
a minority,12,3,4.0
minority instance,20,3,6.666666666666667
instance 𝑝,8,2,4.0
𝑝 from,6,2,3.0
class 𝑃,6,2,3.0
and then,28,5,5.6
then determines,1,1,1.0
determines 𝑚,1,1,1.0
𝑚 as,2,2,1.0
as the,64,6,10.666666666666666
the nearest,14,6,2.3333333333333335
nearest neighbors,47,5,9.4
neighbors of,19,4,4.75
of 𝑝,8,2,4.0
𝑝 after,2,2,1.0
after determining,2,2,1.0
determining 𝑚,2,2,1.0
𝑚 nearest,8,2,4.0
𝑝 it,4,2,2.0
it selects,2,2,1.0
selects a,2,2,1.0
a random,12,5,2.4
random neighbor,2,2,1.0
neighbor 𝑚,2,2,1.0
𝑚 where,3,2,1.5
where 𝑚,2,2,1.0
𝑚 smote,1,1,1.0
smote creates,2,2,1.0
creates a,2,2,1.0
a new,28,6,4.666666666666667
new instance,8,2,4.0
𝑝 using,4,2,2.0
the following,12,5,2.4
following formula,2,2,1.0
formula 𝑝,2,2,1.0
𝑝 𝑝,8,2,4.0
𝑝 𝑚,2,2,1.0
where 𝛿,2,2,1.0
𝛿 is,2,2,1.0
is a,65,6,10.833333333333334
random number,7,5,1.4
number between,4,4,1.0
between and,7,5,1.4
and this,7,4,1.75
this new,7,3,2.3333333333333335
instance is,7,3,2.3333333333333335
is then,5,3,1.6666666666666667
then added,4,2,2.0
dataset for,12,3,4.0
class one,2,2,1.0
the potential,6,4,1.5
potential problems,2,2,1.0
problems with,2,2,1.0
with smote,4,3,1.3333333333333333
that its,2,2,1.0
its generation,2,2,1.0
generation of,14,5,2.8
of minority,14,5,2.8
minority instances,50,3,16.666666666666668
instances is,2,2,1.0
is done,7,5,1.4
done without,2,2,1.0
without reference,2,2,1.0
reference to,2,2,1.0
or indeed,2,2,1.0
indeed any,2,2,1.0
any consideration,1,1,1.0
consideration that,1,1,1.0
that some,5,3,1.6666666666666667
some minority,4,2,2.0
instances may,2,2,1.0
be better,4,4,1.0
better than,19,5,3.8
than others,5,3,1.6666666666666667
others to,2,2,1.0
to use,7,5,1.4
use in,8,4,2.0
this process,4,3,1.3333333333333333
process another,2,2,1.0
another issue,2,2,1.0
issue is,2,2,1.0
may introduce,3,3,1.0
introduce noise,2,2,1.0
noise by,2,2,1.0
by generating,2,2,1.0
generating interpolated,2,2,1.0
that do,5,3,1.6666666666666667
do not,9,5,1.8
not exist,1,1,1.0
exist in,5,2,2.5
the domain,4,3,1.3333333333333333
domain the,2,2,1.0
the interpolated,2,2,1.0
interpolated value,2,2,1.0
value could,2,2,1.0
could be,29,6,4.833333333333333
be accordingly,2,2,1.0
accordingly many,2,2,1.0
many extensions,2,2,1.0
extensions have,2,2,1.0
been made,4,4,1.0
made to,4,3,1.3333333333333333
to smote,4,2,2.0
smote that,4,2,2.0
improve on,8,2,4.0
its operation,2,2,1.0
operation in,3,3,1.0
following we,2,2,1.0
we review,2,2,1.0
review the,2,2,1.0
the smote,20,6,3.3333333333333335
variants that,4,2,2.0
are closest,2,2,1.0
closest to,2,2,1.0
current method,2,2,1.0
method proposed,3,3,1.0
to reveal,2,2,1.0
reveal how,2,2,1.0
how it,6,2,3.0
it differs,2,2,1.0
differs temraz,2,2,1.0
augmentation smote,8,2,4.0
variants three,2,2,1.0
three key,4,2,2.0
key insights,4,2,2.0
insights there,2,2,1.0
there are,27,6,4.5
are many,2,2,1.0
many variants,2,2,1.0
original s,2,2,1.0
performance based,2,2,1.0
on several,2,2,1.0
several insights,2,2,1.0
insights about,4,2,2.0
about how,2,2,1.0
how to,6,4,1.5
problem so,2,2,1.0
so these,2,2,1.0
these variants,2,2,1.0
variants often,2,2,1.0
often hinge,2,2,1.0
hinge on,2,2,1.0
on regions,1,1,1.0
regions in,8,2,4.0
class they,4,2,2.0
they emphasise,2,2,1.0
emphasise the,2,2,1.0
the importance,7,2,3.5
importance of,8,3,2.6666666666666665
of focusing,2,2,1.0
focusing on,5,5,1.0
the border,3,3,1.0
border region,2,2,1.0
region between,2,2,1.0
between the,26,6,4.333333333333333
majority and,17,5,3.4
and minority,17,5,3.4
classes and,16,6,2.6666666666666665
and sometimes,4,2,2.0
sometimes analyze,1,1,1.0
analyze the,1,1,1.0
with respect,18,5,3.6
respect to,18,5,3.6
minority to,2,2,1.0
to guide,8,2,4.0
guide smote,6,2,3.0
in selected,2,2,1.0
selected regions,2,2,1.0
regions one,2,2,1.0
one critical,2,2,1.0
critical improvement,2,2,1.0
improvement to,2,2,1.0
original smote,4,2,2.0
smote method,6,3,2.0
method hinges,2,2,1.0
hinges on,4,2,2.0
the insight,4,2,2.0
insight that,2,2,1.0
that not,3,3,1.0
not all,3,3,1.0
all regions,2,2,1.0
are equal,5,5,1.0
equal some,2,2,1.0
some may,2,2,1.0
be more,20,4,5.0
more or,1,1,1.0
or safer,2,2,1.0
safer than,2,2,1.0
others within,2,2,1.0
within which,4,2,2.0
which to,5,3,1.6666666666666667
apply smote,2,2,1.0
smote for,11,3,3.6666666666666665
instance smote,1,1,1.0
smote clusters,2,2,1.0
clusters minority,2,2,1.0
instances into,5,3,1.6666666666666667
into k,2,2,1.0
k clusters,2,2,1.0
clusters and,3,3,1.0
then oversamples,1,1,1.0
oversamples from,1,1,1.0
from clusters,2,2,1.0
clusters with,4,4,1.0
most the,1,1,1.0
the assumption,4,4,1.0
assumption being,2,2,1.0
being that,2,2,1.0
that these,7,3,2.3333333333333335
these are,2,2,1.0
are safer,2,2,1.0
safer regions,2,2,1.0
regions and,2,2,1.0
and are,6,3,2.0
are less,2,2,1.0
less likely,4,2,2.0
to generate,45,6,7.5
generate noise,2,2,1.0
noise see,2,2,1.0
see for,4,2,2.0
for a,46,6,7.666666666666667
a related,4,2,2.0
related solution,2,2,1.0
solution other,2,2,1.0
other versions,2,2,1.0
versions of,3,3,1.0
of this,31,5,6.2
this approach,10,4,2.5
approach have,2,2,1.0
have used,5,3,1.6666666666666667
used dbscan,2,2,1.0
dbscan a,2,2,1.0
a based,3,2,1.5
based clustering,2,2,1.0
clustering algorithm,5,3,1.6666666666666667
algorithm to,11,6,1.8333333333333333
to identify,9,4,2.25
identify safe,2,2,1.0
safe regions,2,2,1.0
regions or,2,2,1.0
or use,2,2,1.0
use representative,2,2,1.0
representative points,2,2,1.0
points within,3,3,1.0
within to,1,1,1.0
smote some,2,2,1.0
some methods,2,2,1.0
methods project,2,2,1.0
project the,2,2,1.0
class into,4,2,2.0
into a,11,4,2.75
a lower,4,2,2.0
lower dimension,4,2,2.0
dimension before,2,2,1.0
before applying,2,2,1.0
applying smote,5,3,1.6666666666666667
smote to,4,2,2.0
the clusters,3,3,1.0
clusters found,2,2,1.0
found for,4,2,2.0
instance somo,2,2,1.0
somo uses,2,2,1.0
uses a,13,4,3.25
a map,2,2,1.0
map to,2,2,1.0
to transform,2,2,1.0
transform datasets,2,2,1.0
datasets into,2,2,1.0
a space,2,2,1.0
space and,8,3,2.6666666666666665
and uses,2,2,1.0
a embedding,2,2,1.0
embedding algorithm,2,2,1.0
to project,2,2,1.0
project into,2,2,1.0
dimension where,2,2,1.0
where the,17,6,2.8333333333333335
the datasets,28,4,7.0
are more,14,5,2.8
more separable,2,2,1.0
separable still,2,2,1.0
still others,2,2,1.0
others such,2,2,1.0
as and,2,2,1.0
and adasyn,13,3,4.333333333333333
adasyn explore,2,2,1.0
explore different,2,2,1.0
different ways,5,3,1.6666666666666667
ways to,2,2,1.0
identify regions,2,2,1.0
regions within,2,2,1.0
generate minority,2,2,1.0
instances defines,2,2,1.0
defines a,2,2,1.0
a geometric,2,2,1.0
geometric region,2,2,1.0
region around,2,2,1.0
around each,2,2,1.0
each minority,27,3,9.0
class instance,3,3,1.0
instance for,3,2,1.5
for generating,13,3,4.333333333333333
generating synthetic,10,4,2.5
datapoints adasyn,2,2,1.0
adasyn proposed,2,2,1.0
proposed by,5,3,1.6666666666666667
by he,2,2,1.0
he et,2,2,1.0
et al,41,4,10.25
al generates,2,2,1.0
generates minority,2,2,1.0
instances according,3,3,1.0
according to,29,5,5.8
to their,13,4,3.25
their generating,1,1,1.0
generating more,2,2,1.0
more synthetic,9,3,3.0
synthetic data,33,5,6.6
data from,2,2,1.0
from minority,2,2,1.0
instances that,19,3,6.333333333333333
are harder,2,2,1.0
harder to,2,2,1.0
to learn,8,5,1.6
learn compared,2,2,1.0
compared to,17,6,2.8333333333333335
to minority,11,6,1.8333333333333333
are easier,4,2,2.0
easier to,4,2,2.0
learn where,2,2,1.0
where is,3,3,1.0
is related,5,4,1.25
related to,26,6,4.333333333333333
the number,67,6,11.166666666666666
of temraz,2,2,1.0
augmentation instances,4,2,2.0
the neighbors,9,4,2.25
neighbors that,3,3,1.0
that belong,3,3,1.0
belong to,8,5,1.6
class however,5,3,1.6666666666666667
however these,2,2,1.0
these solutions,2,2,1.0
solutions owe,2,2,1.0
owe a,2,2,1.0
a lot,5,3,1.6666666666666667
lot to,4,2,2.0
to another,5,3,1.6666666666666667
another key,2,2,1.0
key insight,2,2,1.0
insight namely,2,2,1.0
namely that,4,2,2.0
that regions,2,2,1.0
regions close,4,2,2.0
class boundary,7,4,1.75
boundary are,4,2,2.0
are particularly,4,2,2.0
particularly important,4,2,2.0
important for,3,3,1.0
instance generation,6,2,3.0
generation smote,3,2,1.5
smote on,4,3,1.3333333333333333
the borderline,8,3,2.6666666666666665
borderline the,2,2,1.0
the idea,9,5,1.8
idea that,5,3,1.6666666666666667
that different,2,2,1.0
different regions,2,2,1.0
dataset need,2,2,1.0
be differently,1,1,1.0
differently owes,2,2,1.0
owes a,2,2,1.0
intuition that,2,2,1.0
that minority,2,2,1.0
instances close,4,2,2.0
the decision,39,4,9.75
decision boundary,44,4,11.0
boundary of,3,3,1.0
classifier are,2,2,1.0
important to,7,4,1.75
to successful,2,2,1.0
successful classification,1,1,1.0
classification so,1,1,1.0
so generating,2,2,1.0
generating minority,4,2,2.0
minority in,2,2,1.0
this boundary,2,2,1.0
boundary region,4,3,1.3333333333333333
region should,2,2,1.0
should help,2,2,1.0
help performance,2,2,1.0
performance more,2,2,1.0
more proposed,2,2,1.0
by han,2,2,1.0
han et,3,3,1.0
al realized,2,2,1.0
realized this,2,2,1.0
this idea,6,4,1.5
idea by,2,2,1.0
creating instances,2,2,1.0
using only,3,3,1.0
only minority,2,2,1.0
are close,4,2,2.0
boundary so,4,2,2.0
so again,4,2,2.0
again assume,2,2,1.0
is 𝑋,2,2,1.0
𝑋 and,4,2,2.0
the whole,6,4,1.5
whole training,2,2,1.0
training set,20,4,5.0
set is,9,5,1.8
in for,3,3,1.0
for every,4,2,2.0
every minority,3,3,1.0
𝑝 in,4,2,2.0
𝑃 the,2,2,1.0
the method,20,5,4.0
method calculates,2,2,1.0
calculates its,2,2,1.0
its 𝑚,2,2,1.0
neighbors from,6,4,1.5
set it,2,2,1.0
it should,10,3,3.3333333333333335
should be,30,6,5.0
be noted,8,3,2.6666666666666665
noted that,12,4,3.0
of majority,25,6,4.166666666666667
majority instances,18,2,9.0
instances among,2,2,1.0
the 𝑚,4,2,2.0
neighbors is,5,3,1.6666666666666667
is called,3,3,1.0
called as,2,2,1.0
as 𝑚,2,2,1.0
𝑚 in,2,2,1.0
in step,5,3,1.6666666666666667
step if,4,2,2.0
if all,2,2,1.0
all the,46,6,7.666666666666667
𝑝 are,4,2,2.0
are majority,2,2,1.0
instances 𝑝,1,1,1.0
𝑝 is,12,2,6.0
is considered,4,3,1.3333333333333333
considered as,3,3,1.0
as noise,3,3,1.0
noise and,5,4,1.25
and is,7,4,1.75
is not,36,6,6.0
not used,4,2,2.0
used in,22,5,4.4
the next,16,4,4.0
next step,3,3,1.0
if the,22,6,3.6666666666666665
the set,20,5,4.0
set of,37,6,6.166666666666667
𝑝 s,2,2,1.0
s majority,2,2,1.0
majority nearest,2,2,1.0
is larger,2,2,1.0
larger than,2,2,1.0
than that,2,2,1.0
that of,15,4,3.75
of its,10,4,2.5
minority ones,3,3,1.0
ones 𝑚,2,2,1.0
𝑚 𝑝,4,2,2.0
𝑝 will,2,2,1.0
will be,41,5,8.2
be easily,3,3,1.0
easily and,1,1,1.0
and put,2,2,1.0
put into,2,2,1.0
a danger,2,2,1.0
danger set,6,2,3.0
set if,2,2,1.0
if then,1,1,1.0
then 𝑝,2,2,1.0
is safe,2,2,1.0
safe and,2,2,1.0
and does,5,3,1.6666666666666667
does not,35,6,5.833333333333333
not participate,2,2,1.0
participate in,2,2,1.0
the subsequent,2,2,1.0
subsequent steps,2,2,1.0
steps this,2,2,1.0
this danger,2,2,1.0
set contains,2,2,1.0
contains the,2,2,1.0
borderline instances,2,2,1.0
instances of,3,3,1.0
class finally,2,2,1.0
finally for,4,2,2.0
for each,75,6,12.5
each instance,3,2,1.5
the danger,2,2,1.0
set the,2,2,1.0
from 𝑃,2,2,1.0
𝑃 are,2,2,1.0
are found,3,3,1.0
found and,8,2,4.0
the steps,2,2,1.0
steps from,2,2,1.0
method are,2,2,1.0
are applied,2,2,1.0
to them,3,3,1.0
them to,9,4,2.25
generate synthetic,17,5,3.4
class this,8,3,2.6666666666666665
this insight,2,2,1.0
insight about,2,2,1.0
about the,10,3,3.3333333333333335
the boundary,14,4,3.5
boundary regions,4,2,2.0
regions has,2,2,1.0
been exploited,2,2,1.0
exploited in,2,2,1.0
in different,7,5,1.4
for example,20,5,4.0
example uses,2,2,1.0
uses an,2,2,1.0
an svm,2,2,1.0
svm to,2,2,1.0
to approximate,3,3,1.0
approximate the,3,3,1.0
decision and,1,1,1.0
then generates,2,2,1.0
generates new,2,2,1.0
data along,2,2,1.0
along the,6,4,1.5
the lines,2,2,1.0
lines joining,2,2,1.0
joining each,2,2,1.0
each temraz,2,2,1.0
augmentation with,2,2,1.0
with its,3,3,1.0
its nearest,7,4,1.75
neighbors using,2,2,1.0
using interpolation,2,2,1.0
interpolation or,2,2,1.0
or extrapolation,2,2,1.0
extrapolation techniques,2,2,1.0
techniques in,7,3,2.3333333333333335
in a,43,6,7.166666666666667
a similar,3,3,1.0
similar vein,1,1,1.0
vein divides,1,1,1.0
divides minority,2,2,1.0
into three,4,4,1.0
three groups,2,2,1.0
groups security,2,2,1.0
security instances,2,2,1.0
instances border,2,2,1.0
border instances,2,2,1.0
instances and,7,2,3.5
and latent,2,2,1.0
latent noise,2,2,1.0
noise instances,2,2,1.0
then treats,2,2,1.0
treats these,2,2,1.0
these groups,3,3,1.0
groups differently,2,2,1.0
differently when,2,2,1.0
when generating,2,2,1.0
generating instances,2,2,1.0
instances other,2,2,1.0
other variants,2,2,1.0
variants in,2,2,1.0
this vein,2,2,1.0
vein adjust,2,2,1.0
adjust the,2,2,1.0
the sampling,9,5,1.8
sampling rate,2,2,1.0
rate for,2,2,1.0
for some,6,3,2.0
instances those,2,2,1.0
those close,2,2,1.0
boundary to,4,3,1.3333333333333333
to improve,17,5,3.4
improve these,2,2,1.0
methods further,2,2,1.0
further see,2,2,1.0
see and,4,2,2.0
this use,2,2,1.0
of boundary,2,2,1.0
class also,2,2,1.0
also raises,2,2,1.0
raises questions,2,2,1.0
questions about,2,2,1.0
the relationship,9,3,3.0
relationship of,2,2,1.0
instances to,8,3,2.6666666666666665
instances leading,2,2,1.0
leading to,3,3,1.0
a third,4,2,2.0
third insight,2,2,1.0
insight underlying,2,2,1.0
underlying smote,2,2,1.0
variants namely,2,2,1.0
that oversampling,2,2,1.0
oversampling in,12,3,4.0
class can,8,3,2.6666666666666665
be by,2,2,1.0
by considering,4,4,1.0
considering the,6,4,1.5
class using,14,3,4.666666666666667
class a,3,3,1.0
third important,2,2,1.0
important insight,2,2,1.0
insight in,2,2,1.0
this research,2,2,1.0
research area,2,2,1.0
area which,2,2,1.0
which becomes,2,2,1.0
becomes more,2,2,1.0
more apparent,2,2,1.0
apparent when,2,2,1.0
when borderlines,2,2,1.0
borderlines are,2,2,1.0
are explored,2,2,1.0
explored is,2,2,1.0
relationship between,7,3,2.3333333333333335
can also,8,5,1.6
also help,2,2,1.0
help guide,2,2,1.0
smote earlier,2,2,1.0
earlier we,2,2,1.0
we saw,4,2,2.0
saw that,2,2,1.0
that does,2,2,1.0
not interpolate,2,2,1.0
interpolate instances,2,2,1.0
instances when,2,2,1.0
when the,17,5,3.4
neighbors show,2,2,1.0
show a,2,2,1.0
a preponderance,2,2,1.0
preponderance of,2,2,1.0
majority see,1,1,1.0
see also,2,2,1.0
also adysyn,2,2,1.0
adysyn this,2,2,1.0
this is,30,6,5.0
one way,2,2,1.0
way to,5,4,1.25
to take,4,4,1.0
take the,7,3,2.3333333333333335
into account,8,4,2.0
account other,2,2,1.0
other explore,1,1,1.0
explore the,4,3,1.3333333333333333
between classes,3,3,1.0
to undersample,2,2,1.0
using techniques,3,3,1.0
techniques or,2,2,1.0
or to,2,2,1.0
guide the,4,2,2.0
the oversampling,7,3,2.3333333333333335
oversampling of,2,2,1.0
instance finds,1,1,1.0
finds pairs,2,2,1.0
pairs of,5,3,1.6666666666666667
instances between,2,2,1.0
minority and,11,3,3.6666666666666665
and majority,17,4,4.25
majority classes,5,3,1.6666666666666667
classes that,8,2,4.0
are very,4,4,1.0
very similar,5,3,1.6666666666666667
similar low,2,2,1.0
low euclidean,2,2,1.0
euclidean distance,16,3,5.333333333333333
distance a,3,3,1.0
a tomek,4,3,1.3333333333333333
tomek link,7,3,2.3333333333333335
link and,2,2,1.0
then removes,2,2,1.0
removes the,2,2,1.0
majority instance,8,2,4.0
the pair,6,2,3.0
pair by,2,2,1.0
by removing,2,2,1.0
removing these,2,2,1.0
these and,2,2,1.0
and applying,2,2,1.0
it attempts,2,2,1.0
attempts to,3,3,1.0
the classes,16,6,2.6666666666666665
classes uses,2,2,1.0
related approach,2,2,1.0
approach involving,2,2,1.0
involving the,3,3,1.0
the edited,2,2,1.0
edited nearest,4,4,1.0
nearest neighbour,6,2,3.0
neighbour method,2,2,1.0
method other,2,2,1.0
other methods,12,4,3.0
methods and,4,4,1.0
and swim,2,2,1.0
swim perform,2,2,1.0
perform explicit,2,2,1.0
explicit analyses,2,2,1.0
analyses of,4,2,2.0
and use,6,4,1.5
use this,2,2,1.0
this analysis,2,2,1.0
analysis to,2,2,1.0
generation does,1,1,1.0
does this,3,3,1.0
this by,2,2,1.0
by computing,2,2,1.0
computing a,2,2,1.0
a score,3,3,1.0
score for,4,2,2.0
instance where,2,2,1.0
where safety,2,2,1.0
safety is,2,2,1.0
is based,5,4,1.25
the frequency,3,3,1.0
frequency of,3,3,1.0
the neighbours,9,3,3.0
neighbours and,3,3,1.0
and a,22,5,4.4
a temraz,2,2,1.0
augmentation ratio,2,2,1.0
ratio based,2,2,1.0
the score,4,3,1.3333333333333333
score of,2,2,1.0
of a,41,6,6.833333333333333
instance over,2,2,1.0
over that,2,2,1.0
its neighbours,4,3,1.3333333333333333
neighbours s,1,1,1.0
s finer,2,2,1.0
finer analysis,2,2,1.0
analysis of,16,6,2.6666666666666665
between majority,7,3,2.3333333333333335
minority has,2,2,1.0
been shown,9,3,3.0
shown to,9,4,2.25
to performance,4,3,1.3333333333333333
performance over,4,4,1.0
over sampling,3,3,1.0
sampling with,2,2,1.0
majority swim,2,2,1.0
swim adopt,2,2,1.0
adopt a,2,2,1.0
a different,9,5,1.8
approach leveraging,2,2,1.0
leveraging information,2,2,1.0
information about,7,3,2.3333333333333335
the density,4,3,1.3333333333333333
density of,2,2,1.0
the mahalanobis,2,2,1.0
mahalanobis distances,1,1,1.0
distances requiring,1,1,1.0
requiring generated,2,2,1.0
generated minority,2,2,1.0
to have,4,2,2.0
have similar,2,2,1.0
similar distances,2,2,1.0
distances to,2,2,1.0
their minority,2,2,1.0
minority seeds,2,2,1.0
seeds so,2,2,1.0
so swim,2,2,1.0
swim essentially,2,2,1.0
essentially analyses,2,2,1.0
analyses the,4,3,1.3333333333333333
the topology,4,2,2.0
topology of,4,2,2.0
the generation,10,4,2.5
instances see,2,2,1.0
and for,14,5,2.8
for related,2,2,1.0
related finally,1,1,1.0
finally is,2,2,1.0
is another,6,4,1.5
another method,3,3,1.0
that takes,5,3,1.6666666666666667
takes similarities,2,2,1.0
similarities to,2,2,1.0
to majority,4,2,2.0
majority into,1,1,1.0
account in,2,2,1.0
in computing,4,2,2.0
computing rough,2,2,1.0
rough sets,4,2,2.0
sets over,2,2,1.0
over the,16,5,3.2
class after,2,2,1.0
after smote,2,2,1.0
generate additional,2,2,1.0
additional minority,2,2,1.0
instances this,3,3,1.0
method acts,2,2,1.0
acts like,2,2,1.0
like a,4,2,2.0
a step,4,2,2.0
step to,2,2,1.0
to generated,1,1,1.0
generated instances,2,2,1.0
that might,2,2,1.0
might be,7,5,1.4
be noise,2,2,1.0
noise many,2,2,1.0
methods improve,2,2,1.0
on s,3,3,1.0
performance and,4,4,1.0
and as,4,2,2.0
as such,8,3,2.6666666666666665
such show,2,2,1.0
that paying,2,2,1.0
paying more,2,2,1.0
more attention,4,4,1.0
attention to,4,4,1.0
can play,2,2,1.0
play a,5,3,1.6666666666666667
a key,4,2,2.0
key role,2,2,1.0
role in,5,3,1.6666666666666667
in instance,2,2,1.0
generation in,6,3,2.0
class we,3,3,1.0
we will,17,3,5.666666666666667
will see,2,2,1.0
see later,2,2,1.0
later that,2,2,1.0
that while,3,3,1.0
while the,5,4,1.25
current counterfactual,2,2,1.0
methods reflect,2,2,1.0
reflect these,2,2,1.0
these three,2,2,1.0
about out,2,2,1.0
out to,2,2,1.0
on smote,3,3,1.0
smote it,2,2,1.0
is quite,6,2,3.0
quite different,6,2,3.0
different from,7,4,1.75
from all,2,2,1.0
all of,18,5,3.6
the above,2,2,1.0
above methods,2,2,1.0
in how,2,2,1.0
it operates,2,2,1.0
operates see,2,2,1.0
section but,2,2,1.0
but before,2,2,1.0
before considering,2,2,1.0
considering this,2,2,1.0
method in,8,6,1.3333333333333333
in detail,5,4,1.25
detail we,2,2,1.0
we first,2,2,1.0
first briefly,2,2,1.0
briefly review,2,2,1.0
review how,2,2,1.0
it has,11,4,2.75
has emerged,2,2,1.0
emerged in,2,2,1.0
in xai,22,2,11.0
xai counterfactual,8,2,4.0
counterfactual generation,4,2,2.0
xai in,2,2,1.0
we deploy,2,2,1.0
deploy a,2,2,1.0
method to,22,5,4.4
instances counterfactual,2,2,1.0
been developed,5,3,1.6666666666666667
generate examples,2,2,1.0
examples to,4,3,1.3333333333333333
to explain,3,3,1.0
explain the,2,2,1.0
the predictions,3,3,1.0
predictions of,3,3,1.0
of ml,2,2,1.0
models and,4,2,2.0
and to,12,6,2.0
to provide,11,5,2.2
provide algorithmic,2,2,1.0
algorithmic recourse,4,2,2.0
recourse for,2,2,1.0
for temraz,2,2,1.0
augmentation trying,2,2,1.0
trying to,7,4,1.75
mitigate automated,2,2,1.0
automated decisions,4,2,2.0
decisions for,2,2,1.0
see the,7,3,2.3333333333333335
the classic,4,3,1.3333333333333333
classic counterfactual,2,2,1.0
counterfactual is,1,1,1.0
one that,4,4,1.0
that is,32,6,5.333333333333333
is given,4,4,1.0
given when,2,2,1.0
when an,2,2,1.0
an automated,2,2,1.0
automated system,2,2,1.0
system refuses,2,2,1.0
refuses a,2,2,1.0
a person,2,2,1.0
person on,2,2,1.0
on a,29,6,4.833333333333333
a loan,10,2,5.0
loan application,2,2,1.0
application when,2,2,1.0
the asks,2,2,1.0
asks why,2,2,1.0
why the,7,3,2.3333333333333335
the system,4,2,2.0
system might,2,2,1.0
might counterfactually,2,2,1.0
counterfactually explain,2,2,1.0
explain that,2,2,1.0
that if,8,4,2.0
if you,2,2,1.0
you a,1,1,1.0
loan for,2,2,1.0
for less,2,2,1.0
less over,2,2,1.0
over a,5,5,1.0
a shorter,2,2,1.0
shorter term,2,2,1.0
term then,2,2,1.0
then you,2,2,1.0
you would,2,2,1.0
would have,4,2,2.0
been granted,2,2,1.0
granted the,2,2,1.0
the loan,9,2,4.5
loan that,4,2,2.0
counterfactual explanation,2,2,1.0
explanation tells,2,2,1.0
tells users,2,2,1.0
users about,2,2,1.0
which outcome,1,1,1.0
outcome would,3,2,1.5
would change,2,2,1.0
closest world,2,2,1.0
their world,2,2,1.0
would be,18,5,3.6
be what,2,2,1.0
what they,2,2,1.0
they desire,2,2,1.0
desire counterfactuals,2,2,1.0
counterfactuals have,2,2,1.0
been researched,3,3,1.0
researched for,2,2,1.0
some time,2,2,1.0
time in,5,4,1.25
in ai,5,3,1.6666666666666667
ai under,2,2,1.0
under diverse,2,2,1.0
diverse names,1,1,1.0
names for,1,1,1.0
for in,3,2,1.5
the past,2,2,1.0
past they,2,2,1.0
been called,2,2,1.0
called nearest,2,2,1.0
nearest unlike,2,2,1.0
unlike neighbours,2,2,1.0
neighbours nuns,2,2,1.0
nuns or,2,2,1.0
or inverse,2,2,1.0
inverse classifications,2,2,1.0
classifications recently,2,2,1.0
recently they,2,2,1.0
emerged as,2,2,1.0
a hot,4,3,1.3333333333333333
hot topic,4,3,1.3333333333333333
topic in,3,3,1.0
xai because,1,1,1.0
because they,6,3,2.0
they appear,2,2,1.0
appear to,2,2,1.0
have psychological,2,2,1.0
psychological benefits,2,2,1.0
benefits people,2,2,1.0
people naturally,2,2,1.0
naturally understand,2,2,1.0
understand them,2,2,1.0
them and,2,2,1.0
and legal,2,2,1.0
legal benefits,2,2,1.0
benefits they,2,2,1.0
they are,30,5,6.0
are said,2,2,1.0
said to,4,3,1.3333333333333333
be gdpr,2,2,1.0
gdpr compliant,2,2,1.0
compliant optimization,2,2,1.0
optimization techniques,2,2,1.0
techniques are,5,3,1.6666666666666667
are currently,2,2,1.0
currently the,3,3,1.0
for computing,3,3,1.0
computing counterfactuals,2,2,1.0
counterfactuals given,2,2,1.0
given a,8,5,1.6
a test,5,3,1.6666666666666667
test instance,6,2,3.0
instance one,2,2,1.0
one encoding,2,2,1.0
encoding the,2,2,1.0
loan refusal,2,2,1.0
refusal these,2,2,1.0
these optimization,4,2,2.0
optimization methods,3,2,1.5
methods search,2,2,1.0
search a,2,2,1.0
a sometimes,2,2,1.0
sometimes randomly,1,1,1.0
randomly generated,2,2,1.0
generated space,2,2,1.0
space of,2,2,1.0
of perturbations,2,2,1.0
perturbations of,2,2,1.0
the query,4,2,2.0
query synthetic,2,2,1.0
instances under,2,2,1.0
under a,2,2,1.0
a loss,3,3,1.0
loss function,2,2,1.0
function that,2,2,1.0
that balances,1,1,1.0
balances proximity,1,1,1.0
proximity to,4,2,2.0
the against,2,2,1.0
against proximity,2,2,1.0
boundary for,2,2,1.0
that counters,2,2,1.0
counters that,2,2,1.0
query using,2,2,1.0
a scaled,2,2,1.0
scaled wachter,2,2,1.0
wachter et,6,2,3.0
et s,4,2,2.0
s seminal,2,2,1.0
seminal method,2,2,1.0
method uses,2,2,1.0
uses gradient,2,2,1.0
gradient descent,2,2,1.0
descent to,2,2,1.0
to find,13,4,3.25
find the,6,3,2.0
the best,25,5,5.0
best counterfactual,2,2,1.0
counterfactual instance,11,2,5.5
a query,3,3,1.0
query though,2,2,1.0
though later,2,2,1.0
later models,2,2,1.0
models have,2,2,1.0
used other,2,2,1.0
other techniques,2,2,1.0
techniques genetic,2,2,1.0
genetic algorithms,3,3,1.0
algorithms mothilal,2,2,1.0
mothilal et,6,2,3.0
al proposed,6,3,2.0
proposed the,4,2,2.0
the diverse,2,2,1.0
diverse counterfactual,6,2,3.0
counterfactual explanations,15,2,7.5
explanations dice,2,2,1.0
dice method,4,2,2.0
method as,5,3,1.6666666666666667
as an,6,5,1.2
an extension,3,3,1.0
extension to,3,3,1.0
generate a,11,3,3.6666666666666665
a set,17,5,3.4
of counterfactual,7,2,3.5
counterfactual candidates,2,2,1.0
candidates avoiding,2,2,1.0
avoiding the,2,2,1.0
problem of,16,6,2.6666666666666665
of generating,3,3,1.0
generating sets,2,2,1.0
sets of,10,4,2.5
of candidates,2,2,1.0
candidates that,2,2,1.0
that were,3,3,1.0
were trivial,2,2,1.0
trivial variants,2,2,1.0
variants on,2,2,1.0
on one,3,3,1.0
one another,5,3,1.6666666666666667
another the,2,2,1.0
the main,15,4,3.75
main problem,2,2,1.0
problem with,6,4,1.5
with these,5,3,1.6666666666666667
methods is,1,1,1.0
that given,2,2,1.0
given their,2,2,1.0
their blind,2,2,1.0
blind perturbation,2,2,1.0
perturbation of,2,2,1.0
of they,2,2,1.0
they sometimes,2,2,1.0
sometimes generate,2,2,1.0
generate invalid,2,2,1.0
invalid keane,2,2,1.0
points this,2,2,1.0
this defect,2,2,1.0
defect has,2,2,1.0
has potentially,2,2,1.0
potentially serious,2,2,1.0
serious for,2,2,1.0
for their,2,2,1.0
their use,4,2,2.0
problem as,4,4,1.0
as it,8,4,2.0
it suggests,2,2,1.0
suggests that,6,2,3.0
that they,12,4,3.0
they might,2,2,1.0
might populate,2,2,1.0
populate the,2,2,1.0
with noise,1,1,1.0
noise with,2,2,1.0
with negative,1,1,1.0
negative effects,3,3,1.0
effects on,4,3,1.3333333333333333
performance however,2,2,1.0
however a,2,2,1.0
very different,2,2,1.0
to counterfactual,2,2,1.0
generation has,2,2,1.0
has recently,2,2,1.0
recently been,2,2,1.0
proposed this,2,2,1.0
method finds,2,2,1.0
finds the,4,2,2.0
the s,3,3,1.0
s that,2,2,1.0
takes part,2,2,1.0
part in,8,2,4.0
a explanation,2,2,1.0
explanation case,4,2,2.0
case xc,2,2,1.0
xc an,2,2,1.0
an explanation,2,2,1.0
case captures,2,2,1.0
captures a,2,2,1.0
counterfactual between,1,1,1.0
between existing,4,2,2.0
existing instances,2,2,1.0
dataset that,4,3,1.3333333333333333
are in,3,3,1.0
in opposing,2,2,1.0
opposing classes,2,2,1.0
classes either,2,2,1.0
either side,6,2,3.0
side of,8,3,2.6666666666666665
a decision,2,2,1.0
boundary with,4,2,2.0
the constraint,8,2,4.0
constraint that,6,2,3.0
pair of,8,4,2.0
instances differ,2,2,1.0
differ by,2,2,1.0
by at,2,2,1.0
at most,3,3,1.0
most two,2,2,1.0
two for,2,2,1.0
example the,8,5,1.6
loan dataset,2,2,1.0
dataset could,2,2,1.0
could contain,2,2,1.0
contain two,2,2,1.0
two existing,2,2,1.0
existing cases,2,2,1.0
cases that,2,2,1.0
are counterfactually,2,2,1.0
counterfactually related,7,2,3.5
related one,2,2,1.0
one about,2,2,1.0
about a,2,2,1.0
a old,6,2,3.0
old female,2,2,1.0
female accountant,2,2,1.0
accountant earning,4,2,2.0
earning who,6,2,3.0
who was,6,2,3.0
was refused,4,2,2.0
refused a,4,2,2.0
is counterfactually,3,2,1.5
another instance,2,2,1.0
instance with,3,3,1.0
with a,22,5,4.4
different outcome,2,2,1.0
outcome namely,2,2,1.0
namely a,2,2,1.0
old accountant,1,1,1.0
was granted,2,2,1.0
granted a,2,2,1.0
loan differences,2,2,1.0
differences shown,2,2,1.0
shown in,12,5,2.4
in italics,3,3,1.0
italics this,2,2,1.0
this explanatory,2,2,1.0
explanatory case,2,2,1.0
case implicitly,2,2,1.0
implicitly suggests,2,2,1.0
if one,4,4,1.0
one earns,2,2,1.0
earns rather,2,2,1.0
than then,4,2,2.0
loan is,1,1,1.0
be granted,4,2,2.0
granted rather,2,2,1.0
than refused,2,2,1.0
refused so,2,2,1.0
so if,6,2,3.0
if i,4,2,2.0
i am,4,2,2.0
am a,2,2,1.0
old male,3,2,1.5
male teacher,2,2,1.0
teacher earning,2,2,1.0
loan then,2,2,1.0
then this,2,2,1.0
this algorithm,6,6,1.0
algorithm could,3,3,1.0
could find,2,2,1.0
find this,2,2,1.0
this as,2,2,1.0
a nearest,6,2,3.0
neighbour and,2,2,1.0
and suggest,2,2,1.0
suggest that,5,3,1.6666666666666667
this earned,2,2,1.0
earned more,2,2,1.0
more rather,2,2,1.0
loan would,2,2,1.0
granted in,2,2,1.0
xai context,2,2,1.0
context this,2,2,1.0
method has,2,2,1.0
generate close,2,2,1.0
close plausible,2,2,1.0
plausible counterfactuals,2,2,1.0
counterfactuals and,4,2,2.0
and appears,2,2,1.0
appears to,2,2,1.0
to avoid,3,3,1.0
avoid the,2,2,1.0
the pitfalls,2,2,1.0
pitfalls that,2,2,1.0
that arise,2,2,1.0
arise in,3,3,1.0
in techniques,1,1,1.0
techniques see,2,2,1.0
see from,2,2,1.0
from a,14,5,2.8
a data,5,3,1.6666666666666667
augmentation perspective,4,2,2.0
perspective this,2,2,1.0
method can,9,4,2.25
be seen,24,3,8.0
seen as,3,3,1.0
as supporting,2,2,1.0
supporting the,2,2,1.0
the creation,2,2,1.0
creation of,2,2,1.0
of synthetic,21,5,4.2
using information,2,2,1.0
information from,3,3,1.0
from these,6,3,2.0
these known,2,2,1.0
known counterfactual,4,2,2.0
counterfactual pairs,8,2,4.0
pairs however,2,2,1.0
however few,2,2,1.0
few xai,2,2,1.0
xai techniques,2,2,1.0
techniques have,4,4,1.0
to data,9,3,3.0
next subsection,2,2,1.0
subsection we,4,3,1.3333333333333333
sketch this,2,2,1.0
this small,2,2,1.0
literature on,2,2,1.0
the topic,2,2,1.0
topic temraz,2,2,1.0
augmentation using,4,2,2.0
using counterfactuals,6,2,3.0
counterfactuals for,2,2,1.0
augmentation beyond,2,2,1.0
beyond xai,2,2,1.0
xai our,2,2,1.0
our hypothesis,2,2,1.0
hypothesis is,6,4,1.5
that counterfactual,2,2,1.0
also play,2,2,1.0
a role,3,3,1.0
data to,7,5,1.4
problems that,2,2,1.0
that generated,6,2,3.0
generated synthetic,9,3,3.0
counterfactual cases,1,1,1.0
cases could,2,2,1.0
could the,1,1,1.0
the predictive,2,2,1.0
predictive accuracy,2,2,1.0
accuracy of,11,5,2.2
ai models,2,2,1.0
models although,2,2,1.0
although there,2,2,1.0
are now,2,2,1.0
now of,2,2,1.0
of papers,5,3,1.6666666666666667
papers on,2,2,1.0
on in,1,1,1.0
xai only,2,2,1.0
only a,3,3,1.0
a handful,2,2,1.0
handful of,2,2,1.0
papers consider,2,2,1.0
consider their,2,2,1.0
augmentation in,5,2,2.5
in evaluating,3,3,1.0
evaluating xai,2,2,1.0
methods mothilal,2,2,1.0
al suggested,3,3,1.0
suggested that,6,4,1.5
that a,11,6,1.8333333333333333
a good,17,5,3.4
good method,2,2,1.0
should generate,2,2,1.0
of counterfactuals,2,2,1.0
counterfactuals that,2,2,1.0
that can,6,5,1.2
can substitute,2,2,1.0
substitute for,2,2,1.0
dataset calling,2,2,1.0
calling it,2,2,1.0
it that,1,1,1.0
is if,3,3,1.0
of generated,2,2,1.0
generated counterfactuals,6,2,3.0
counterfactuals were,2,2,1.0
were plausible,2,2,1.0
plausible and,4,2,2.0
and close,6,2,3.0
data then,4,2,2.0
then their,2,2,1.0
their predictive,2,2,1.0
predictive performance,2,2,1.0
performance should,2,2,1.0
should parallel,2,2,1.0
parallel that,2,2,1.0
dataset however,3,3,1.0
however mothilal,1,1,1.0
al did,2,2,1.0
did not,5,3,1.6666666666666667
not consider,2,2,1.0
consider using,2,2,1.0
using their,4,2,2.0
their counterfactual,2,2,1.0
a student,2,2,1.0
student project,2,2,1.0
project hasan,2,2,1.0
hasan did,2,2,1.0
did and,2,2,1.0
and tried,2,2,1.0
tried to,2,2,1.0
to determine,9,4,2.25
determine whether,2,2,1.0
whether an,2,2,1.0
an augmented,2,2,1.0
augmented dataset,4,2,2.0
dataset based,2,2,1.0
on generated,1,1,1.0
generated could,1,1,1.0
could act,2,2,1.0
act as,2,2,1.0
a proxy,2,2,1.0
proxy dataset,2,2,1.0
dataset but,2,2,1.0
but only,3,3,1.0
only found,2,2,1.0
found modest,2,2,1.0
modest success,2,2,1.0
success a,2,2,1.0
a selection,2,2,1.0
selection of,6,3,2.0
of other,4,4,1.0
other papers,3,3,1.0
papers in,4,3,1.3333333333333333
in diverse,2,2,1.0
diverse areas,2,2,1.0
areas have,2,2,1.0
also circled,2,2,1.0
circled the,2,2,1.0
the issue,3,3,1.0
issue of,4,4,1.0
of using,4,3,1.3333333333333333
using counterfactual,4,2,2.0
counterfactual techniques,2,2,1.0
augmentation subbaswamy,2,2,1.0
subbaswamy and,2,2,1.0
and saria,2,2,1.0
saria considered,2,2,1.0
considered the,3,3,1.0
of dataset,4,3,1.3333333333333333
dataset shift,4,2,2.0
shift where,2,2,1.0
where there,3,3,1.0
there is,16,6,2.6666666666666665
a divergence,2,2,1.0
divergence between,2,2,1.0
the context,7,5,1.4
context in,2,2,1.0
which a,5,3,1.6666666666666667
a model,9,4,2.25
model was,3,3,1.0
was trained,1,1,1.0
trained and,1,1,1.0
and tested,4,2,2.0
tested they,2,2,1.0
they use,5,3,1.6666666666666667
use the,12,5,2.4
the notion,10,4,2.5
notion of,9,4,2.25
counterfactual risk,2,2,1.0
risk to,2,2,1.0
to diagnose,2,2,1.0
diagnose this,2,2,1.0
using causal,4,2,2.0
causal models,2,2,1.0
models zeng,2,2,1.0
zeng et,2,2,1.0
counterfactual generator,4,2,2.0
generator which,2,2,1.0
which generates,2,2,1.0
generates counterfactual,2,2,1.0
counterfactual examples,6,2,3.0
examples for,8,4,2.0
data and,12,6,2.0
and found,3,3,1.0
found that,7,4,1.75
counterfactuals improved,2,2,1.0
improved the,4,2,2.0
the generalizability,2,2,1.0
generalizability of,2,2,1.0
of models,2,2,1.0
models under,2,2,1.0
under limited,2,2,1.0
limited observational,2,2,1.0
observational examples,2,2,1.0
examples pitis,2,2,1.0
pitis et,2,2,1.0
proposed counterfactual,1,1,1.0
augmentation coda,2,2,1.0
coda for,2,2,1.0
generating counterfactual,2,2,1.0
counterfactual experiences,2,2,1.0
experiences in,2,2,1.0
in reinforcement,2,2,1.0
reinforcement learning,2,2,1.0
learning rl,2,2,1.0
rl in,2,2,1.0
method increases,2,2,1.0
increases the,3,3,1.0
the size,6,6,1.0
size of,9,6,1.5
of available,3,3,1.0
available training,3,3,1.0
data with,4,4,1.0
with counterfactual,2,2,1.0
examples by,6,4,1.5
by stitching,2,2,1.0
stitching subsamples,1,1,1.0
subsamples from,2,2,1.0
the environment,2,2,1.0
environment they,2,2,1.0
they found,2,2,1.0
that coda,2,2,1.0
coda significantly,2,2,1.0
significantly improved,2,2,1.0
of rl,2,2,1.0
rl agents,2,2,1.0
agents in,2,2,1.0
in tasks,2,2,1.0
tasks for,2,2,1.0
for and,7,3,2.3333333333333335
and keane,2,2,1.0
augmentation conditioned,2,2,1.0
conditioned settings,2,2,1.0
settings the,2,2,1.0
these papers,2,2,1.0
papers is,2,2,1.0
use bespoke,2,2,1.0
bespoke counterfactual,2,2,1.0
counterfactual developed,1,1,1.0
developed for,6,4,1.5
for specific,3,3,1.0
specific task,2,2,1.0
task domains,2,2,1.0
domains rather,2,2,1.0
the tried,2,2,1.0
tried and,2,2,1.0
tested techniques,1,1,1.0
techniques from,1,1,1.0
literature therefore,2,2,1.0
therefore their,2,2,1.0
their performance,2,2,1.0
performance robustness,2,2,1.0
robustness and,3,3,1.0
and generalizability,2,2,1.0
generalizability is,2,2,1.0
is at,6,5,1.2
at best,3,3,1.0
best however,2,2,1.0
however one,3,3,1.0
one study,2,2,1.0
study has,3,3,1.0
has applied,2,2,1.0
a xai,2,2,1.0
augmentation temraz,2,2,1.0
temraz et,8,2,4.0
al used,2,2,1.0
used the,6,3,2.0
present counterfactual,2,2,1.0
data for,11,4,2.75
a prediction,8,2,4.0
prediction problem,4,2,2.0
problem their,2,2,1.0
their problem,2,2,1.0
problem domain,6,2,3.0
domain involved,1,1,1.0
involved a,1,1,1.0
model for,3,3,1.0
for grass,2,2,1.0
grass growth,10,2,5.0
growth prediction,2,2,1.0
prediction that,6,2,3.0
that relies,2,2,1.0
relies on,8,4,2.0
on an,3,3,1.0
an historical,2,2,1.0
historical dataset,2,2,1.0
dataset of,2,2,1.0
of specific,2,2,1.0
specific measurements,2,2,1.0
measurements of,2,2,1.0
of climate,2,2,1.0
climate and,2,2,1.0
and grass,3,3,1.0
growth on,2,2,1.0
on dairy,2,2,1.0
dairy farms,2,2,1.0
farms in,4,2,2.0
in ireland,4,2,2.0
ireland covering,2,2,1.0
covering the,3,3,1.0
the model,12,5,2.4
model does,4,2,2.0
does reasonably,2,2,1.0
reasonably well,2,2,1.0
well at,5,3,1.6666666666666667
at predicting,4,2,2.0
predicting grass,4,2,2.0
growth for,4,2,2.0
for individual,2,2,1.0
individual farms,2,2,1.0
the coming,2,2,1.0
coming week,2,2,1.0
week using,2,2,1.0
using this,8,5,1.6
this historical,2,2,1.0
historical data,4,2,2.0
data but,3,3,1.0
but with,6,2,3.0
with climate,2,2,1.0
climate change,6,2,3.0
change there,2,2,1.0
are an,2,2,1.0
an increasing,2,2,1.0
increasing number,2,2,1.0
of events,2,2,1.0
events events,2,2,1.0
events that,2,2,1.0
that diverge,2,2,1.0
diverge significantly,2,2,1.0
significantly from,2,2,1.0
the recorded,1,1,1.0
recorded in,2,2,1.0
the historical,4,2,2.0
data extreme,2,2,1.0
extreme values,3,3,1.0
values for,22,3,7.333333333333333
for key,2,2,1.0
key weather,2,2,1.0
weather variables,1,1,1.0
variables like,1,1,1.0
like solar,2,2,1.0
solar or,1,1,1.0
or soil,2,2,1.0
soil moisture,4,2,2.0
moisture for,2,2,1.0
in there,2,2,1.0
there was,2,2,1.0
was a,2,2,1.0
a significant,3,3,1.0
significant drought,2,2,1.0
drought across,2,2,1.0
across europe,2,2,1.0
europe that,2,2,1.0
that effectively,2,2,1.0
effectively halted,2,2,1.0
halted grass,2,2,1.0
growth in,5,3,1.6666666666666667
ireland during,2,2,1.0
during what,2,2,1.0
what is,2,2,1.0
is usually,4,4,1.0
usually the,4,3,1.3333333333333333
the of,20,5,4.0
of july,2,2,1.0
july if,2,2,1.0
if soil,2,2,1.0
moisture drops,2,2,1.0
drops then,2,2,1.0
then grass,2,2,1.0
grass stops,2,2,1.0
stops growing,2,2,1.0
growing indeed,2,2,1.0
indeed high,2,2,1.0
high solar,2,2,1.0
solar radiation,2,2,1.0
radiation will,2,2,1.0
will burn,2,2,1.0
burn grass,2,2,1.0
grass the,1,1,1.0
not do,2,2,1.0
do very,2,2,1.0
very well,2,2,1.0
for these,3,3,1.0
these months,1,1,1.0
months of,2,2,1.0
of because,2,2,1.0
are historically,2,2,1.0
historically unique,2,2,1.0
unique temraz,2,2,1.0
al defined,2,2,1.0
defined a,4,2,2.0
a class,8,5,1.6
boundary in,3,3,1.0
dataset creating,2,2,1.0
creating a,2,2,1.0
a division,2,2,1.0
division between,2,2,1.0
between normal,2,2,1.0
normal cases,4,2,2.0
cases with,4,2,2.0
with values,4,2,2.0
values within,3,3,1.0
within standard,2,2,1.0
standard deviations,4,2,2.0
deviations of,4,2,2.0
of historical,4,2,2.0
historical means,4,2,2.0
means and,2,2,1.0
and cases,2,2,1.0
values standard,2,2,1.0
means from,2,2,1.0
a perspective,1,1,1.0
perspective these,2,2,1.0
these normal,2,2,1.0
cases were,2,2,1.0
were the,3,3,1.0
the cases,5,3,1.6666666666666667
cases an,3,3,1.0
an unpublished,2,2,1.0
unpublished paper,2,2,1.0
paper reports,2,2,1.0
reports an,2,2,1.0
an identical,2,2,1.0
identical method,2,2,1.0
to wachter,2,2,1.0
s counterfactual,2,2,1.0
counterfactual optimization,2,2,1.0
optimization method,2,2,1.0
augmentation however,2,2,1.0
however this,12,3,4.0
paper does,2,2,1.0
not reference,2,2,1.0
reference wachter,2,2,1.0
al or,2,2,1.0
or any,2,2,1.0
any of,4,4,1.0
literature temraz,2,2,1.0
augmentation are,2,2,1.0
they then,2,2,1.0
then used,4,2,2.0
create new,2,2,1.0
synthetic cases,4,2,2.0
cases and,5,3,1.6666666666666667
and showed,3,3,1.0
showed that,11,3,3.6666666666666665
model s,4,2,2.0
performance specifically,4,2,2.0
specifically improved,2,2,1.0
improved on,2,2,1.0
on predicting,3,3,1.0
predicting events,2,2,1.0
events in,2,2,1.0
in using,3,3,1.0
using these,3,2,1.5
these minority,1,1,1.0
minority interestingly,1,1,1.0
interestingly temraz,2,2,1.0
al s,3,3,1.0
s experiments,2,2,1.0
experiments showed,3,3,1.0
that that,2,2,1.0
the did,1,1,1.0
did better,2,2,1.0
than in,4,4,1.0
domain specifically,2,2,1.0
the dice,1,1,1.0
method however,2,2,1.0
this work,3,3,1.0
work only,2,2,1.0
only considers,4,2,2.0
considers one,2,2,1.0
one specific,2,2,1.0
specific problem,2,2,1.0
domain classifier,2,2,1.0
classifier and,6,2,3.0
and dataset,2,2,1.0
dataset it,2,2,1.0
it remains,4,3,1.3333333333333333
remains to,2,2,1.0
seen whether,2,2,1.0
whether this,2,2,1.0
counterfactual approach,2,2,1.0
approach generalizes,2,2,1.0
generalizes to,2,2,1.0
to other,8,6,1.3333333333333333
other problem,1,1,1.0
problem domains,1,1,1.0
domains classifiers,2,2,1.0
and datasets,5,3,1.6666666666666667
datasets and,8,5,1.6
and specifically,2,2,1.0
specifically to,2,2,1.0
to datasets,2,2,1.0
datasets where,7,4,1.75
where problems,2,2,1.0
problems arise,2,2,1.0
arise hence,2,2,1.0
hence this,2,2,1.0
the aim,4,4,1.0
aim for,2,2,1.0
the remainder,3,3,1.0
remainder of,3,3,1.0
paper the,6,4,1.5
augmentation algorithm,2,2,1.0
algorithm cfa,2,2,1.0
cfa this,2,2,1.0
paper advances,2,2,1.0
advances the,2,2,1.0
augmentation as,2,2,1.0
a solution,3,3,1.0
solution to,5,4,1.25
with more,3,3,1.0
problems the,4,2,2.0
this xai,2,2,1.0
xai method,2,2,1.0
augmentation was,2,2,1.0
was motivated,2,2,1.0
motivated by,5,3,1.6666666666666667
the observation,1,1,1.0
observation that,1,1,1.0
it seemed,2,2,1.0
seemed to,2,2,1.0
generate plausible,4,2,2.0
plausible synthetic,4,2,2.0
datapoints for,4,2,2.0
for explanatory,2,2,1.0
explanatory purposes,2,2,1.0
purposes furthermore,2,2,1.0
furthermore the,2,2,1.0
the evaluation,4,3,1.3333333333333333
evaluation metrics,10,4,2.5
metrics in,8,4,2.0
xai showed,2,2,1.0
these datapoints,2,2,1.0
datapoints were,2,2,1.0
were generally,2,2,1.0
generally valid,4,2,2.0
valid and,6,2,3.0
existing datapoints,2,2,1.0
datapoints accordingly,2,2,1.0
accordingly the,2,2,1.0
the extension,2,2,1.0
extension of,4,2,2.0
techniques to,4,4,1.0
problems seemed,2,2,1.0
seemed like,2,2,1.0
a promising,2,2,1.0
promising avenue,2,2,1.0
avenue of,2,2,1.0
of research,5,4,1.25
research in,3,3,1.0
this section,16,4,4.0
section we,8,4,2.0
we describe,4,2,2.0
describe a,2,2,1.0
new oversampling,2,2,1.0
oversampling method,6,2,3.0
method using,3,3,1.0
a reasoning,1,1,1.0
reasoning to,1,1,1.0
to generating,4,2,2.0
synthetic counterfactuals,6,2,3.0
counterfactuals in,14,2,7.0
be applied,6,4,1.5
to binary,6,2,3.0
binary problems,1,1,1.0
problems consider,2,2,1.0
consider a,6,3,2.0
a simple,3,3,1.0
simple scenario,2,2,1.0
scenario to,2,2,1.0
to show,4,4,1.0
show how,2,2,1.0
how this,2,2,1.0
method operates,2,2,1.0
operates temraz,2,2,1.0
augmentation a,4,2,2.0
counterfactual example,2,2,1.0
example for,6,4,1.5
for class,4,3,1.3333333333333333
class imbalances,11,4,2.75
imbalances imagine,2,2,1.0
imagine an,2,2,1.0
an ml,2,2,1.0
ml classifier,2,2,1.0
classifier being,2,2,1.0
being used,5,3,1.6666666666666667
to predict,14,3,4.666666666666667
predict whether,4,2,2.0
whether farm,2,2,1.0
farm animals,2,2,1.0
animals are,2,2,1.0
are likely,2,2,1.0
be healthy,4,2,2.0
healthy or,2,2,1.0
or fall,2,2,1.0
fall ill,2,2,1.0
ill mastitis,2,2,1.0
mastitis in,4,2,2.0
in cows,2,2,1.0
cows see,2,2,1.0
dataset recording,2,2,1.0
recording a,2,2,1.0
a herd,2,2,1.0
herd of,2,2,1.0
of cows,2,2,1.0
cows on,2,2,1.0
on most,5,3,1.6666666666666667
most farms,2,2,1.0
farms will,2,2,1.0
be imbalanced,2,2,1.0
imbalanced in,5,3,1.6666666666666667
in that,6,4,1.5
that most,5,5,1.0
most cows,2,2,1.0
cows will,2,2,1.0
will tend,3,3,1.0
tend to,7,3,2.3333333333333335
healthy rather,2,2,1.0
than an,2,2,1.0
an analysis,4,4,1.0
this dataset,9,3,3.0
dataset shows,2,2,1.0
shows that,5,4,1.25
some pairs,2,2,1.0
instances instance,2,2,1.0
instance pairs,2,2,1.0
pairs be,2,2,1.0
be counterfactually,6,2,3.0
to one,4,4,1.0
another for,2,2,1.0
example a,3,3,1.0
instance of,4,3,1.3333333333333333
certain breed,2,2,1.0
breed age,6,2,3.0
age and,4,2,2.0
and that,8,4,2.0
is classed,4,2,2.0
classed as,4,2,2.0
as healthy,2,2,1.0
healthy can,2,2,1.0
counterfactually paired,2,2,1.0
paired with,2,2,1.0
instance that,4,2,2.0
is of,3,3,1.0
the same,37,6,6.166666666666667
same breed,2,2,1.0
age but,2,2,1.0
different they,1,1,1.0
have ill,2,2,1.0
ill several,2,2,1.0
several times,2,2,1.0
times that,2,2,1.0
as likely,2,2,1.0
be this,3,3,1.0
this which,2,2,1.0
which we,7,5,1.4
we call,2,2,1.0
call a,2,2,1.0
a native,11,2,5.5
native counterfactual,19,2,9.5
counterfactual tells,2,2,1.0
tells us,3,3,1.0
us that,2,2,1.0
a difference,2,2,1.0
difference in,5,3,1.6666666666666667
the feature,51,6,8.5
feature can,2,2,1.0
can change,2,2,1.0
class of,20,4,5.0
a cow,4,2,2.0
cow from,2,2,1.0
from healthy,2,2,1.0
healthy to,2,2,1.0
to so,2,2,1.0
if we,6,3,2.0
we want,2,2,1.0
want to,2,2,1.0
to fix,2,2,1.0
fix the,2,2,1.0
imbalance in,6,4,1.5
using our,1,1,1.0
our counterfactual,2,2,1.0
method then,2,2,1.0
then one,2,2,1.0
one can,14,4,3.5
can generate,4,2,2.0
new minority,7,3,2.3333333333333335
instance by,2,2,1.0
this known,1,1,1.0
counterfactual relationship,2,2,1.0
relationship imagine,2,2,1.0
imagine we,2,2,1.0
we pick,2,2,1.0
pick another,2,2,1.0
another majority,2,2,1.0
instance a,2,2,1.0
cow that,2,2,1.0
has no,3,3,1.0
no existing,2,2,1.0
existing counterfactual,4,2,2.0
counterfactual pair,6,2,3.0
pair and,2,2,1.0
and find,2,2,1.0
find a,2,2,1.0
neighbour to,2,2,1.0
to that,5,3,1.6666666666666667
is part,3,3,1.0
part of,7,4,1.75
of known,4,2,2.0
known the,2,2,1.0
pair using,2,2,1.0
this and,2,2,1.0
the native,10,2,5.0
native we,2,2,1.0
we can,16,5,3.2
instance using,1,1,1.0
the from,3,2,1.5
from so,2,2,1.0
so this,4,2,2.0
instance would,2,2,1.0
have the,6,4,1.5
of for,4,3,1.3333333333333333
for breed,2,2,1.0
and and,6,4,1.5
from for,2,2,1.0
for along,2,2,1.0
the prediction,9,5,1.8
it will,5,5,1.0
be so,3,3,1.0
so we,5,3,1.6666666666666667
have now,2,2,1.0
now created,2,2,1.0
created a,2,2,1.0
to where,3,3,1.0
instance needs,2,2,1.0
needs to,6,2,3.0
be verified,2,2,1.0
verified by,2,2,1.0
the underlying,10,5,2.0
underlying ml,2,2,1.0
ml model,2,2,1.0
model this,2,2,1.0
this example,3,3,1.0
example keane,2,2,1.0
keane smyth,6,2,3.0
smyth have,2,2,1.0
have shown,6,4,1.5
shown good,2,2,1.0
good native,10,2,5.0
native those,2,2,1.0
those with,2,2,1.0
with differences,2,2,1.0
differences between,2,2,1.0
between them,2,2,1.0
them are,3,3,1.0
are quite,4,2,2.0
quite rare,2,2,1.0
rare in,2,2,1.0
in most,9,3,3.0
most datasets,3,3,1.0
datasets of,2,2,1.0
of all,7,4,1.75
all instances,2,2,1.0
instances but,5,3,1.6666666666666667
with some,2,2,1.0
some tolerance,2,2,1.0
tolerance in,4,2,2.0
in they,2,2,1.0
they can,3,3,1.0
be increased,3,3,1.0
increased to,2,2,1.0
to temraz,2,2,1.0
augmentation describes,2,2,1.0
describes the,2,2,1.0
of one,5,5,1.0
one minority,2,2,1.0
in our,12,5,2.4
our experiments,3,2,1.5
we do,6,2,3.0
do this,4,2,2.0
this iteratively,1,1,1.0
iteratively for,1,1,1.0
for all,18,5,3.6
all those,2,2,1.0
those majority,2,2,1.0
are not,13,5,2.6
not paired,2,2,1.0
paired in,2,2,1.0
in native,4,2,2.0
native counterfactuals,19,2,9.5
counterfactuals a,2,2,1.0
step that,2,2,1.0
that results,2,2,1.0
results in,19,5,3.8
of many,4,3,1.3333333333333333
more minority,2,2,1.0
class in,16,5,3.2
next we,2,2,1.0
algorithm more,2,2,1.0
more formally,4,2,2.0
formally the,2,2,1.0
method counterfactual,4,2,2.0
augmentation cfa,8,2,4.0
cfa the,4,2,2.0
a technique,3,3,1.0
technique for,10,4,2.5
synthetic examples,2,2,1.0
examples in,20,4,5.0
methods see,2,2,1.0
xai the,4,2,2.0
the cfa,6,2,3.0
cfa method,6,2,3.0
method generates,2,2,1.0
in three,4,3,1.3333333333333333
main steps,3,3,1.0
steps see,2,2,1.0
see figure,2,2,1.0
figure figure,2,2,1.0
figure counterfactual,2,2,1.0
cfa an,1,1,1.0
an unpaired,5,2,2.5
unpaired instance,12,2,6.0
instance 𝒙,6,2,3.0
𝒙 grey,4,2,2.0
grey circle,4,2,2.0
circle finds,2,2,1.0
finds a,2,2,1.0
nearest neighbor,19,6,3.1666666666666665
neighbor 𝒙,2,2,1.0
𝒙 blue,2,2,1.0
blue circle,4,2,2.0
circle taking,2,2,1.0
taking part,4,2,2.0
native in,5,2,2.5
dataset 𝒄𝒇,4,2,2.0
𝒄𝒇 𝒙,6,2,3.0
𝒙 𝒑,6,2,3.0
𝒑 pairing,2,2,1.0
pairing of,2,2,1.0
of blue,2,2,1.0
circle and,2,2,1.0
and yellow,2,2,1.0
yellow box,2,2,1.0
box and,2,2,1.0
then uses,2,2,1.0
uses the,5,3,1.6666666666666667
the 𝒑,2,2,1.0
𝒑 box,1,1,1.0
box to,2,2,1.0
synthetic 𝒑,1,1,1.0
𝒑 green,4,2,2.0
green box,4,2,2.0
box combining,2,2,1.0
combining them,2,2,1.0
them with,2,2,1.0
original unpaired,4,2,2.0
circle the,2,2,1.0
the generated,6,4,1.5
synthetic instance,8,4,2.0
instance 𝒑,2,2,1.0
box is,2,2,1.0
dataset to,6,4,1.5
improve future,2,2,1.0
future prediction,2,2,1.0
prediction temraz,2,2,1.0
augmentation i,2,2,1.0
i good,2,2,1.0
counterfactuals 𝑐𝑓,4,2,2.0
𝑐𝑓 𝑥,17,2,8.5
𝑥 𝑝,22,2,11.0
are initially,2,2,1.0
initially computed,2,2,1.0
computed over,2,2,1.0
whole dataset,2,2,1.0
dataset 𝑇,2,2,1.0
𝑇 identifying,2,2,1.0
identifying combinations,2,2,1.0
combinations of,2,2,1.0
instances 𝑥,2,2,1.0
𝑥 from,2,2,1.0
and 𝑝,10,2,5.0
class ii,2,2,1.0
ii given,2,2,1.0
given an,2,2,1.0
instance 𝑥,8,2,4.0
𝑥 its,2,2,1.0
its paired,4,2,2.0
paired instance,11,2,5.5
𝑥 is,8,2,4.0
is found,2,2,1.0
and used,2,2,1.0
identify a,3,3,1.0
a close,2,2,1.0
close existing,2,2,1.0
pair 𝑐𝑓,4,2,2.0
𝑝 and,2,2,1.0
then iii,2,2,1.0
iii a,2,2,1.0
is produced,2,2,1.0
produced in,2,2,1.0
using features,2,2,1.0
features from,3,2,1.5
𝑥 and,6,2,3.0
and values,2,2,1.0
values from,6,2,3.0
from more,2,2,1.0
formally definitions,2,2,1.0
definitions 𝑋,2,2,1.0
𝑋 is,2,2,1.0
class 𝑐𝑙𝑎𝑠𝑠efg,2,2,1.0
𝑐𝑙𝑎𝑠𝑠efg 𝑃,2,2,1.0
𝑃 is,2,2,1.0
class 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑥,1,1,1.0
𝑥 𝑥,5,2,2.5
𝑥 where,4,2,2.0
where 𝑥,7,2,3.5
a paired,6,2,3.0
in 𝑐𝑙𝑎𝑠𝑠efg,4,2,2.0
𝑐𝑙𝑎𝑠𝑠efg 𝑥,2,2,1.0
𝑥 𝑥j,3,2,1.5
𝑥j 𝑥k,4,2,2.0
𝑥k 𝑥l,4,2,2.0
𝑥l 𝑥,4,2,2.0
𝑝 𝑥,2,2,1.0
is an,9,6,1.5
𝑐𝑙𝑎𝑠𝑠efg where,2,2,1.0
𝑝 where,4,2,2.0
where 𝑝,3,2,1.5
in 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑝j,1,1,1.0
𝑝j 𝑝k,4,2,2.0
𝑝k 𝑝l,4,2,2.0
𝑝l 𝑝,4,2,2.0
a synthetic,6,3,2.0
instance generated,2,2,1.0
be added,2,2,1.0
to 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
𝑐𝑙𝑎𝑠𝑠e is,2,2,1.0
is 𝑐𝑓,2,2,1.0
𝑝 𝑡𝑎𝑟𝑔𝑒𝑡,2,2,1.0
𝑡𝑎𝑟𝑔𝑒𝑡 𝑥,2,2,1.0
𝑝 neighbors,2,2,1.0
neighbors assume,2,2,1.0
that 𝑥,2,2,1.0
instance which,4,2,2.0
which belongs,4,2,2.0
belongs to,8,3,2.6666666666666665
class 𝑋,2,2,1.0
𝑃 𝑥j,1,1,1.0
𝑥 𝑝j,1,1,1.0
𝑝 the,2,2,1.0
the procedure,11,6,1.8333333333333333
procedure for,4,4,1.0
is as,2,2,1.0
follows temraz,2,2,1.0
augmentation step,4,2,2.0
step compute,2,2,1.0
compute the,6,3,2.0
the for,4,2,2.0
𝒑 cfa,2,2,1.0
cfa first,2,2,1.0
first finds,2,2,1.0
finds all,2,2,1.0
all possible,2,2,1.0
possible good,2,2,1.0
pairs 𝑐𝑓,2,2,1.0
𝑝 between,2,2,1.0
that already,2,2,1.0
already exist,4,2,2.0
a 𝑇,1,1,1.0
𝑇 these,2,2,1.0
these native,8,2,4.0
counterfactuals pair,2,2,1.0
pair an,2,2,1.0
an instance,6,3,2.0
majority space,2,2,1.0
space called,4,2,2.0
called the,4,2,2.0
the paired,2,2,1.0
instance and,6,3,2.0
and its,8,4,2.0
its instance,2,2,1.0
minority space,2,2,1.0
in other,8,5,1.6
other words,4,3,1.3333333333333333
words for,2,2,1.0
every 𝑥,2,2,1.0
𝑥 in,2,2,1.0
class 𝑥,2,2,1.0
𝑥 we,2,2,1.0
we find,2,2,1.0
find its,6,2,3.0
its counterfactual,2,2,1.0
counterfactual 𝑝,2,2,1.0
from 𝑝,4,2,2.0
class these,2,2,1.0
𝑝 pair,2,2,1.0
pair instances,2,2,1.0
instances either,4,2,2.0
of decision,2,2,1.0
boundary they,2,2,1.0
are called,4,2,2.0
called native,2,2,1.0
in one,5,3,1.6666666666666667
one sense,2,2,1.0
sense they,2,2,1.0
they already,2,2,1.0
dataset each,2,2,1.0
each of,29,4,7.25
native pairs,2,2,1.0
pairs has,2,2,1.0
has a,4,3,1.3333333333333333
of where,2,2,1.0
the differences,4,4,1.0
differences determine,2,2,1.0
determine the,7,3,2.3333333333333335
class change,4,2,2.0
change over,2,2,1.0
boundary step,2,2,1.0
step for,3,3,1.0
each unpaired,2,2,1.0
𝒙 from,2,2,1.0
class find,2,2,1.0
instance x,2,2,1.0
x taking,2,2,1.0
counterfactual 𝒄𝒇,2,2,1.0
𝒑 for,2,2,1.0
𝑥 cfa,4,2,2.0
cfa uses,4,2,2.0
a to,2,2,1.0
nearest neighboring,2,2,1.0
neighboring 𝑥,2,2,1.0
𝑥 a,2,2,1.0
paired involved,1,1,1.0
involved in,5,2,2.5
𝑝 by,2,2,1.0
definition 𝑥,2,2,1.0
𝑥 belongs,2,2,1.0
not occur,2,2,1.0
occur any,2,2,1.0
any native,2,2,1.0
pairs notably,2,2,1.0
notably this,1,1,1.0
this means,3,2,1.5
means that,6,4,1.5
that all,10,5,2.0
datapoints generated,2,2,1.0
by cfa,2,2,1.0
cfa come,2,2,1.0
come from,2,2,1.0
these instances,2,2,1.0
not already,4,2,2.0
already to,2,2,1.0
to instances,2,2,1.0
class euclidean,2,2,1.0
distance is,4,4,1.0
is used,18,6,3.0
in finding,6,2,3.0
finding these,2,2,1.0
these nearest,2,2,1.0
neighbors euclidean,2,2,1.0
distance ed,2,2,1.0
ed 𝑝,1,1,1.0
𝑝 ke,1,1,1.0
ke temraz,1,1,1.0
step transfer,2,2,1.0
transfer from,2,2,1.0
from 𝒑,2,2,1.0
𝒑 to,2,2,1.0
to 𝒑,2,2,1.0
𝒑 and,2,2,1.0
and from,3,3,1.0
from 𝒙,2,2,1.0
𝒙 𝐭o,2,2,1.0
𝐭o 𝒑,2,2,1.0
𝒑 having,2,2,1.0
having identified,2,2,1.0
identified a,2,2,1.0
counterfactual 𝑐𝑓,2,2,1.0
𝑝 for,4,2,2.0
for 𝑥,2,2,1.0
generates a,2,2,1.0
class 𝑝,2,2,1.0
using from,2,2,1.0
from and,3,3,1.0
𝑝 such,2,2,1.0
such that,8,5,1.6
that for,16,4,4.0
the between,6,2,3.0
between 𝑥,4,2,2.0
𝑝 take,4,2,2.0
the values,12,3,4.0
𝑝 into,2,2,1.0
into the,12,5,2.4
counterfactual case,4,2,2.0
case 𝑝,4,2,2.0
from 𝑥,2,2,1.0
𝑥 into,2,2,1.0
new counterfactual,2,2,1.0
that tolerance,2,2,1.0
tolerance is,2,2,1.0
one parameter,2,2,1.0
parameter in,5,3,1.6666666666666667
in cfa,4,2,2.0
cfa algorithm,2,2,1.0
algorithm which,2,2,1.0
which is,27,6,4.5
the availability,2,2,1.0
availability of,3,3,1.0
of good,4,2,2.0
dataset without,6,2,3.0
without tolerance,4,2,2.0
tolerance fewer,4,2,2.0
fewer counterfactuals,2,2,1.0
counterfactuals would,2,2,1.0
be found,11,6,1.8333333333333333
the generative,4,2,2.0
generative benefits,4,2,2.0
benefits of,4,2,2.0
of them,9,4,2.25
them would,4,2,2.0
would likely,4,2,2.0
likely diminish,4,2,2.0
diminish in,2,2,1.0
finding and,3,2,1.5
and between,6,4,1.5
between two,6,4,1.5
two instances,4,2,2.0
instances for,13,3,4.333333333333333
counterfactual cfa,2,2,1.0
cfa computes,1,1,1.0
computes a,1,1,1.0
a by,1,1,1.0
by finding,2,2,1.0
finding the,3,3,1.0
the mean,14,3,4.666666666666667
mean 𝜇,2,2,1.0
𝜇 and,2,2,1.0
and standard,4,3,1.3333333333333333
standard deviation,10,3,3.3333333333333335
deviation 𝜎,2,2,1.0
𝜎 for,2,2,1.0
each feature,4,2,2.0
feature then,2,2,1.0
then it,2,2,1.0
it allows,2,2,1.0
allows features,2,2,1.0
features to,4,2,2.0
to match,4,2,2.0
match if,4,2,2.0
if their,4,2,2.0
their values,4,2,2.0
values are,7,4,1.75
are within,6,3,2.0
within of,4,2,2.0
the standard,11,4,2.75
deviation from,4,2,2.0
mean all,3,2,1.5
for that,4,2,2.0
that feature,4,3,1.3333333333333333
feature two,2,2,1.0
two subtle,2,2,1.0
subtle differences,2,2,1.0
differences that,4,2,2.0
that distinguish,2,2,1.0
distinguish this,2,2,1.0
this data,4,3,1.3333333333333333
augmentation version,2,2,1.0
version of,16,5,3.2
algorithm from,1,1,1.0
from its,2,2,1.0
its xai,2,2,1.0
xai counterpart,2,2,1.0
counterpart first,2,2,1.0
first although,2,2,1.0
although both,3,3,1.0
both algorithms,2,2,1.0
algorithms adopt,2,2,1.0
adopt the,3,3,1.0
same definition,2,2,1.0
definition of,2,2,1.0
good pairing,1,1,1.0
pairing the,2,2,1.0
the do,2,2,1.0
do so,10,3,3.3333333333333335
so for,2,2,1.0
for different,10,5,2.0
different reasons,2,2,1.0
reasons on,2,2,1.0
on psychological,4,2,2.0
psychological grounds,4,2,2.0
grounds keane,2,2,1.0
keane and,2,2,1.0
and smyth,1,1,1.0
smyth defined,1,1,1.0
good counterfactual,8,2,4.0
counterfactual to,2,2,1.0
be one,2,2,1.0
one with,2,2,1.0
with no,5,3,1.6666666666666667
no more,4,2,2.0
more than,5,3,1.6666666666666667
than two,4,2,2.0
two from,2,2,1.0
xai perspective,2,2,1.0
perspective researchers,2,2,1.0
researchers argue,2,2,1.0
argue that,2,2,1.0
that sparse,2,2,1.0
sparse counterfactuals,4,2,2.0
counterfactuals with,6,2,3.0
with fewer,2,2,1.0
fewer feature,2,2,1.0
feature differences,8,2,4.0
differences are,4,4,1.0
are better,3,3,1.0
better because,2,2,1.0
because people,2,2,1.0
people find,2,2,1.0
find them,4,2,2.0
them more,2,2,1.0
more understandable,2,2,1.0
understandable confirmed,2,2,1.0
confirmed by,2,2,1.0
by user,2,2,1.0
user studies,2,2,1.0
studies from,2,2,1.0
perspective basing,2,2,1.0
basing synthetic,2,2,1.0
counterfactuals on,2,2,1.0
on sparse,2,2,1.0
sparse pairs,4,2,2.0
pairs also,2,2,1.0
also makes,2,2,1.0
makes sense,2,2,1.0
sense because,2,2,1.0
the implicit,2,2,1.0
implicit causal,2,2,1.0
causal dependencies,2,2,1.0
dependencies between,2,2,1.0
between matched,2,2,1.0
matched and,2,2,1.0
and difference,2,2,1.0
difference features,2,2,1.0
features are,5,3,1.6666666666666667
more decision,1,1,1.0
boundary temraz,1,1,1.0
augmentation likely,2,2,1.0
be preserved,2,2,1.0
preserved in,2,2,1.0
in hence,2,2,1.0
hence generated,2,2,1.0
synthetic using,1,1,1.0
these sparse,2,2,1.0
pairs should,2,2,1.0
more likely,9,5,1.8
be valid,2,2,1.0
and second,4,2,2.0
second there,2,2,1.0
a critical,4,3,1.3333333333333333
critical between,1,1,1.0
xai and,2,2,1.0
augmentation contexts,2,2,1.0
contexts with,2,2,1.0
the selection,4,3,1.3333333333333333
of test,3,3,1.0
test instances,8,2,4.0
the test,27,6,4.5
is typically,3,3,1.0
typically a,2,2,1.0
novel problem,2,2,1.0
problem for,6,3,2.0
for which,2,2,1.0
classifier has,2,2,1.0
has made,2,2,1.0
made a,2,2,1.0
prediction a,2,2,1.0
that needs,2,2,1.0
counterfactually explained,2,2,1.0
explained hence,2,2,1.0
hence typically,2,2,1.0
typically the,2,2,1.0
already in,2,2,1.0
data in,12,5,2.4
augmentation the,4,2,2.0
instances used,4,2,2.0
synthetic have,2,2,1.0
have to,4,3,1.3333333333333333
be in,4,3,1.3333333333333333
training dataset,13,3,4.333333333333333
dataset specifically,2,2,1.0
specifically they,2,2,1.0
are all,2,2,1.0
data that,8,5,1.6
not take,2,2,1.0
take part,2,2,1.0
counterfactuals this,1,1,1.0
is why,3,3,1.0
instances are,12,3,4.0
called unpaired,2,2,1.0
unpaired instances,2,2,1.0
augmentation purposes,2,2,1.0
purposes the,4,4,1.0
are a,2,2,1.0
a residual,2,2,1.0
residual set,2,2,1.0
instances left,2,2,1.0
left after,2,2,1.0
after the,2,2,1.0
native have,2,2,1.0
been identified,2,2,1.0
identified how,2,2,1.0
how counterfactuals,2,2,1.0
counterfactuals differ,2,2,1.0
differ from,2,2,1.0
from smote,5,3,1.6666666666666667
variants it,2,2,1.0
be apparent,2,2,1.0
apparent that,2,2,1.0
method is,13,5,2.6
smote and,32,4,8.0
its variants,6,2,3.0
variants though,2,2,1.0
though it,2,2,1.0
is consistent,2,2,1.0
consistent with,2,2,1.0
many insights,2,2,1.0
insights from,2,2,1.0
the literature,10,3,3.3333333333333335
literature first,2,2,1.0
first by,2,2,1.0
definition the,2,2,1.0
method addresses,2,2,1.0
addresses regions,2,2,1.0
boundary a,2,2,1.0
counterfactual records,2,2,1.0
records the,2,2,1.0
the minimal,3,3,1.0
minimal that,2,2,1.0
that result,2,2,1.0
change as,2,2,1.0
as in,12,4,3.0
in and,8,3,2.6666666666666665
second this,2,2,1.0
method relies,2,2,1.0
on native,2,2,1.0
dataset pairings,2,2,1.0
pairings between,4,2,2.0
existing majority,2,2,1.0
such is,2,2,1.0
is exploiting,2,2,1.0
exploiting relationships,2,2,1.0
relationships between,2,2,1.0
between both,2,2,1.0
both classes,4,4,1.0
classes as,4,2,2.0
in adasyn,3,3,1.0
adasyn third,2,2,1.0
third we,2,2,1.0
we are,7,5,1.4
are highly,5,3,1.6666666666666667
highly selective,2,2,1.0
selective in,2,2,1.0
instances as,2,2,1.0
the many,2,2,1.0
many smote,2,2,1.0
is we,3,3,1.0
we only,4,3,1.3333333333333333
only work,2,2,1.0
work of,4,3,1.3333333333333333
of those,2,2,1.0
those involved,1,1,1.0
in known,2,2,1.0
known counterfactuals,2,2,1.0
with two,2,2,1.0
two feature,2,2,1.0
differences however,2,2,1.0
quite keane,2,2,1.0
augmentation ferent,2,2,1.0
ferent in,2,2,1.0
other significant,2,2,1.0
significant respects,2,2,1.0
respects first,3,3,1.0
first it,4,3,1.3333333333333333
it does,7,5,1.4
not use,2,2,1.0
use interpolation,2,2,1.0
interpolation between,2,2,1.0
but rather,5,4,1.25
rather uses,2,2,1.0
the as,3,3,1.0
a template,2,2,1.0
template for,2,2,1.0
generating new,4,3,1.3333333333333333
instances ii,2,2,1.0
ii it,2,2,1.0
not rely,4,2,2.0
rely on,4,2,2.0
in swim,2,2,1.0
swim but,2,2,1.0
but acts,2,2,1.0
acts in,2,2,1.0
very local,2,2,1.0
local way,2,2,1.0
way using,2,2,1.0
counterfactual relation,2,2,1.0
relation between,5,4,1.25
between a,3,3,1.0
a single,16,6,2.6666666666666665
single majority,2,2,1.0
minority one,4,4,1.0
one iii,2,2,1.0
iii does,2,2,1.0
on any,2,2,1.0
any clustering,2,2,1.0
clustering analysis,2,2,1.0
such it,2,2,1.0
it represents,3,3,1.0
represents quite,2,2,1.0
quite a,2,2,1.0
novel departure,2,2,1.0
departure relative,2,2,1.0
relative to,7,3,2.3333333333333335
existing smote,2,2,1.0
variants competitive,2,2,1.0
competitive tests,2,2,1.0
tests of,2,2,1.0
augmentation methods,6,2,3.0
current study,4,3,1.3333333333333333
study we,3,3,1.0
we competitively,2,2,1.0
competitively test,2,2,1.0
test the,8,4,2.0
cfa against,2,2,1.0
against the,6,4,1.5
the benchmark,2,2,1.0
benchmark techniques,2,2,1.0
literature using,2,2,1.0
using six,2,2,1.0
six oversampling,1,1,1.0
methods smote,9,2,4.5
smote adasyn,85,3,28.333333333333332
adasyn these,1,1,1.0
these specific,2,2,1.0
specific methods,2,2,1.0
methods were,5,3,1.6666666666666667
were chosen,2,2,1.0
chosen based,2,2,1.0
based of,2,2,1.0
of their,6,3,2.0
their conceptual,2,2,1.0
conceptual closeness,2,2,1.0
closeness to,2,2,1.0
method their,2,2,1.0
their popularity,2,2,1.0
popularity amongst,2,2,1.0
amongst smote,2,2,1.0
variants and,4,2,2.0
and their,7,5,1.4
their as,2,2,1.0
the six,6,2,3.0
six techniques,2,2,1.0
techniques were,2,2,1.0
were tested,4,2,2.0
tested on,4,2,2.0
a representative,2,2,1.0
representative selection,1,1,1.0
of keel,2,2,1.0
keel datasets,2,2,1.0
datasets from,5,4,1.25
from which,4,3,1.3333333333333333
which were,4,2,2.0
were produced,2,2,1.0
produced with,2,2,1.0
with four,2,2,1.0
different ml,2,2,1.0
ml classifiers,2,2,1.0
classifiers including,2,2,1.0
including random,2,2,1.0
random forest,9,3,3.0
forest rf,4,2,2.0
rf neighbor,2,2,1.0
neighbor logistic,2,2,1.0
logistic regression,4,2,2.0
regression lr,4,2,2.0
lr and,4,2,2.0
and multilayer,2,2,1.0
multilayer perceptron,5,3,1.6666666666666667
perceptron mlp,5,3,1.6666666666666667
mlp models,2,2,1.0
models several,2,2,1.0
several alternative,2,2,1.0
alternative ml,2,2,1.0
models were,2,2,1.0
were used,10,4,2.5
used because,2,2,1.0
because models,1,1,1.0
models find,2,2,1.0
find different,2,2,1.0
different decision,2,2,1.0
decision boundaries,3,3,1.0
boundaries for,3,3,1.0
given dataset,8,5,1.6
dataset differences,3,2,1.5
that could,5,5,1.0
could impact,2,2,1.0
impact the,3,3,1.0
the success,6,2,3.0
success of,4,2,2.0
it relies,2,2,1.0
relies heavily,3,3,1.0
heavily on,3,3,1.0
s decision,2,2,1.0
boundary the,2,2,1.0
given classifier,4,2,2.0
classifier recorded,2,2,1.0
recorded the,2,2,1.0
model on,2,2,1.0
without any,2,2,1.0
any data,4,2,2.0
augmentation applied,2,2,1.0
applied several,2,2,1.0
several standard,2,2,1.0
standard measures,2,2,1.0
measures were,2,2,1.0
to assess,7,4,1.75
assess the,4,3,1.3333333333333333
the four,17,3,5.666666666666667
four methods,5,3,1.6666666666666667
methods namely,2,2,1.0
namely precision,2,2,1.0
precision recall,10,4,2.5
recall and,7,5,1.4
and plots,2,2,1.0
plots of,2,2,1.0
of roc,11,4,2.75
roc curves,27,4,6.75
curves temraz,2,2,1.0
method datasets,2,2,1.0
datasets setup,2,2,1.0
setup table,2,2,1.0
table shows,3,3,1.0
shows the,24,5,4.8
main characteristics,2,2,1.0
characteristics of,7,4,1.75
datasets drawn,2,2,1.0
drawn from,2,2,1.0
from both,4,3,1.3333333333333333
both uci,2,2,1.0
uci and,2,2,1.0
and keel,2,2,1.0
keel repositories,2,2,1.0
repositories as,2,2,1.0
the focus,2,2,1.0
focus is,2,2,1.0
is on,3,3,1.0
on binary,3,3,1.0
classification problems,9,4,2.25
problems and,7,5,1.4
and some,5,3,1.6666666666666667
these datasets,4,3,1.3333333333333333
are they,2,2,1.0
they were,5,3,1.6666666666666667
were converted,4,2,2.0
converted to,4,2,2.0
binary classes,4,3,1.3333333333333333
classes the,6,3,2.0
the ovo,4,2,2.0
ovo or,2,2,1.0
or and,2,2,1.0
and ovr,1,1,1.0
ovr or,2,2,1.0
or methods,2,2,1.0
to do,4,3,1.3333333333333333
this conversion,2,2,1.0
conversion the,2,2,1.0
ovo method,2,2,1.0
method splits,2,2,1.0
splits a,2,2,1.0
a classification,2,2,1.0
classification into,2,2,1.0
into one,2,2,1.0
one binary,2,2,1.0
classification dataset,2,2,1.0
each pair,2,2,1.0
of classes,11,4,2.75
classes whereas,2,2,1.0
whereas the,3,3,1.0
the ovr,2,2,1.0
ovr approach,2,2,1.0
approach selects,2,2,1.0
selects one,2,2,1.0
the multiple,3,3,1.0
multiple classes,2,2,1.0
and predicts,2,2,1.0
predicts it,2,2,1.0
it against,1,1,1.0
against all,2,2,1.0
all other,15,3,5.0
other classes,9,3,3.0
classes so,2,2,1.0
so that,17,5,3.4
that one,2,2,1.0
classes is,2,2,1.0
is treated,4,2,2.0
treated as,12,3,4.0
the positive,10,6,1.6666666666666667
positive minority,4,4,1.0
and all,9,4,2.25
classes are,14,4,3.5
are treated,6,2,3.0
the negative,10,5,2.0
negative class,12,6,2.0
class majority,3,3,1.0
majority in,2,2,1.0
datasets were,8,2,4.0
were modified,2,2,1.0
modified using,9,2,4.5
using both,1,1,1.0
both methods,3,3,1.0
methods one,2,2,1.0
one method,2,2,1.0
method per,2,2,1.0
per dataset,2,2,1.0
to vary,2,2,1.0
vary the,2,2,1.0
class ratio,7,4,1.75
ratio of,8,5,1.6
of class,13,5,2.6
imbalance among,2,2,1.0
datasets see,4,2,2.0
see table,10,2,5.0
table the,11,3,3.6666666666666665
the base,6,3,2.0
base datasets,2,2,1.0
were abalone,2,2,1.0
abalone dataset,11,4,2.75
dataset a,15,3,5.0
a dataset,19,3,6.333333333333333
dataset analyzed,2,2,1.0
analyzed to,2,2,1.0
the age,2,2,1.0
age of,2,2,1.0
of abalone,2,2,1.0
abalone from,2,2,1.0
from physical,2,2,1.0
physical measurements,2,2,1.0
measurements consisting,2,2,1.0
consisting of,6,2,3.0
that was,5,3,1.6666666666666667
was modified,4,2,2.0
using and,7,4,1.75
and glass,2,2,1.0
glass dataset,2,2,1.0
dataset used,10,2,5.0
to classify,13,3,4.333333333333333
classify based,2,2,1.0
the chemical,2,2,1.0
chemical consisting,1,1,1.0
that modified,4,2,2.0
using to,2,2,1.0
to treat,2,2,1.0
treat the,2,2,1.0
class yeast,2,2,1.0
yeast dataset,3,3,1.0
predict the,4,4,1.0
the cellular,2,2,1.0
cellular localization,2,2,1.0
localization sites,4,2,2.0
sites of,2,2,1.0
of proteins,2,2,1.0
proteins consisting,2,2,1.0
and pima,2,2,1.0
pima indians,2,2,1.0
indians diabetes,2,2,1.0
diabetes dataset,2,2,1.0
whether or,2,2,1.0
or not,4,3,1.3333333333333333
not a,5,3,1.6666666666666667
a patient,2,2,1.0
patient has,2,2,1.0
has diabetes,2,2,1.0
diabetes based,2,2,1.0
on certain,3,3,1.0
certain diagnostic,2,2,1.0
diagnostic measurements,2,2,1.0
measurements included,2,2,1.0
included in,3,3,1.0
dataset phoneme,2,2,1.0
phoneme dataset,3,3,1.0
to distinguish,2,2,1.0
distinguish between,3,3,1.0
between nasal,2,2,1.0
nasal and,2,2,1.0
and oral,2,2,1.0
oral sounds,3,3,1.0
sounds temraz,2,2,1.0
augmentation vehicle,2,2,1.0
vehicle dataset,2,2,1.0
dataset the,7,5,1.4
the vehicle,2,2,1.0
vehicle data,2,2,1.0
data set,17,5,3.4
dataset with,11,3,3.6666666666666665
with classes,5,2,2.5
problem is,14,4,3.5
is to,29,6,4.833333333333333
classify a,2,2,1.0
given silhouette,2,2,1.0
silhouette as,2,2,1.0
as one,4,3,1.3333333333333333
of four,5,5,1.0
four types,2,2,1.0
types of,3,3,1.0
of vehicle,2,2,1.0
vehicle using,2,2,1.0
of features,7,5,1.4
the silhouette,2,2,1.0
silhouette again,2,2,1.0
again this,2,2,1.0
dataset was,7,3,2.3333333333333335
using so,2,2,1.0
class van,2,2,1.0
van is,2,2,1.0
class ecoli,2,2,1.0
ecoli dataset,3,3,1.0
dataset this,4,3,1.3333333333333333
dataset is,12,5,2.4
classify ecoli,2,2,1.0
ecoli proteins,2,2,1.0
proteins using,2,2,1.0
their amino,2,2,1.0
amino acid,2,2,1.0
acid sequences,2,2,1.0
sequences in,2,2,1.0
in their,4,4,1.0
their cell,2,2,1.0
cell localization,2,2,1.0
sites dataset,2,2,1.0
classify all,3,3,1.0
the blocks,2,2,1.0
blocks of,2,2,1.0
the page,2,2,1.0
page layout,2,2,1.0
layout of,2,2,1.0
a document,2,2,1.0
document that,2,2,1.0
been detected,2,2,1.0
detected by,2,2,1.0
by a,10,4,2.5
a segmentation,2,2,1.0
segmentation process,2,2,1.0
process wine,2,2,1.0
wine quality,4,2,2.0
quality dataset,2,2,1.0
dataset red,2,2,1.0
red and,4,2,2.0
and white,4,2,2.0
white two,2,2,1.0
two datasets,3,3,1.0
datasets related,2,2,1.0
to red,2,2,1.0
white vinho,2,2,1.0
vinho verde,2,2,1.0
verde wine,2,2,1.0
wine samples,2,2,1.0
samples the,5,3,1.6666666666666667
classify wine,2,2,1.0
quality based,2,2,1.0
on physicochemical,2,2,1.0
physicochemical tests,2,2,1.0
tests poker,2,2,1.0
poker dataset,2,2,1.0
classes used,2,2,1.0
predict poker,2,2,1.0
poker hands,2,2,1.0
hands the,2,2,1.0
the overall,7,4,1.75
overall performance,10,4,2.5
of each,21,6,3.5
each classifier,6,2,3.0
classifier was,2,2,1.0
was tested,2,2,1.0
tested using,2,2,1.0
using cross,2,2,1.0
cross validation,2,2,1.0
validation with,2,2,1.0
with k,3,3,1.0
k so,3,3,1.0
so each,2,2,1.0
each dataset,10,4,2.5
is randomly,2,2,1.0
randomly partitioned,2,2,1.0
partitioned into,2,2,1.0
into disjoint,2,2,1.0
disjoint subsets,2,2,1.0
subsets where,2,2,1.0
where each,4,3,1.3333333333333333
each subset,1,1,1.0
subset included,1,1,1.0
included equal,1,1,1.0
equal size,2,2,1.0
then a,2,2,1.0
single subset,2,2,1.0
subset was,2,2,1.0
was retained,2,2,1.0
retained as,3,3,1.0
test set,2,2,1.0
set with,3,3,1.0
the remaining,11,5,2.2
remaining subsets,2,2,1.0
subsets being,2,2,1.0
used as,12,5,2.4
data different,2,2,1.0
different datasets,3,3,1.0
datasets have,5,4,1.25
have minority,2,2,1.0
classes see,2,2,1.0
table for,5,3,1.6666666666666667
for details,3,3,1.0
details for,3,3,1.0
smote methods,2,2,1.0
methods we,3,3,1.0
we split,2,2,1.0
split each,2,2,1.0
dataset into,1,1,1.0
into training,2,2,1.0
training and,5,4,1.25
and validation,2,2,1.0
validation folds,2,2,1.0
folds then,2,2,1.0
then on,2,2,1.0
on each,3,3,1.0
each fold,2,2,1.0
fold we,2,2,1.0
we oversample,4,2,2.0
class train,2,2,1.0
train the,3,3,1.0
classifier on,8,2,4.0
training folds,2,2,1.0
folds and,2,2,1.0
and finally,4,3,1.3333333333333333
finally validate,2,2,1.0
validate the,4,3,1.3333333333333333
remaining fold,2,2,1.0
fold for,2,2,1.0
data were,2,2,1.0
were computed,2,2,1.0
computed and,3,3,1.0
then all,2,2,1.0
remaining unpaired,2,2,1.0
unpaired were,2,2,1.0
were run,2,2,1.0
run through,3,3,1.0
through cfa,2,2,1.0
cfa to,6,2,3.0
this augmented,2,2,1.0
was then,2,2,1.0
for testing,2,2,1.0
testing these,2,2,1.0
these generated,2,2,1.0
generated datasets,1,1,1.0
from cfa,2,2,1.0
cfa were,2,2,1.0
were compared,2,2,1.0
compared with,4,4,1.0
without data,2,2,1.0
augmentation and,2,2,1.0
datasets generated,2,2,1.0
by temraz,2,2,1.0
adasyn and,14,3,4.666666666666667
for our,2,2,1.0
our we,1,1,1.0
using each,2,2,1.0
methods until,2,2,1.0
until we,2,2,1.0
same number,3,3,1.0
in each,10,4,2.5
each class,7,5,1.4
a result,5,4,1.25
result fully,2,2,1.0
fully balanced,2,2,1.0
balanced datasets,4,3,1.3333333333333333
were created,2,2,1.0
created for,5,3,1.6666666666666667
classifier based,2,2,1.0
original imbalanced,4,3,1.3333333333333333
imbalanced dataset,6,5,1.2
was also,2,2,1.0
also run,1,1,1.0
run as,2,2,1.0
a baseline,4,3,1.3333333333333333
baseline without,2,2,1.0
without using,2,2,1.0
using any,2,2,1.0
method finally,2,2,1.0
finally to,3,3,1.0
the optimal,12,4,3.0
optimal values,2,2,1.0
all classifiers,2,2,1.0
classifiers we,2,2,1.0
we applied,2,2,1.0
applied hyperparameters,2,2,1.0
hyperparameters tuning,2,2,1.0
tuning using,2,2,1.0
using gridsearchcv,2,2,1.0
gridsearchcv function,2,2,1.0
function from,2,2,1.0
from gridsearchcv,2,2,1.0
gridsearchcv performs,2,2,1.0
performs across,2,2,1.0
across all,7,3,2.3333333333333335
all hyperparameter,2,2,1.0
hyperparameter combinations,2,2,1.0
combinations and,2,2,1.0
and finds,2,2,1.0
best score,2,2,1.0
classifier to,3,3,1.0
to achieve,5,4,1.25
achieve this,4,4,1.0
this we,2,2,1.0
we defined,2,2,1.0
defined our,2,2,1.0
our grid,2,2,1.0
grid of,4,2,2.0
of parameters,5,3,1.6666666666666667
parameters for,4,2,2.0
each method,7,2,3.5
method rf,2,2,1.0
rf lr,10,2,5.0
lr mlp,10,2,5.0
mlp and,4,2,2.0
and oversampling,4,2,2.0
smote its,4,2,2.0
then ran,2,2,1.0
ran the,2,2,1.0
the grid,2,2,1.0
grid search,2,2,1.0
search see,2,2,1.0
a full,5,3,1.6666666666666667
full description,4,2,2.0
description table,2,2,1.0
table a,2,2,1.0
a grid,2,2,1.0
of hyperparameter,2,2,1.0
hyperparameter values,2,2,1.0
methods learner,2,2,1.0
learner parameter,2,2,1.0
parameter variants,2,2,1.0
variants random,2,2,1.0
rf 𝑚𝑎𝑥r,1,1,1.0
𝑚𝑎𝑥r neighbors,1,1,1.0
neighbors 𝑛,2,2,1.0
𝑛 xuyz,2,2,1.0
xuyz logistic,2,2,1.0
lr 𝑚𝑎𝑥,2,2,1.0
𝑚𝑎𝑥 t,2,2,1.0
t multilayer,1,1,1.0
mlp oversampling,1,1,1.0
variants 𝑘,2,2,1.0
𝑘 f,2,2,1.0
f xuyz,2,2,1.0
xuyz temraz,2,2,1.0
augmentation table,8,2,4.0
table datasets,2,2,1.0
datasets dataset,4,3,1.3333333333333333
dataset variants,2,2,1.0
variants used,2,2,1.0
the experiment,2,2,1.0
experiment imbalance,2,2,1.0
imbalance ratio,5,4,1.25
ratio id,2,2,1.0
id dataset,2,2,1.0
dataset features,2,2,1.0
features instances,2,2,1.0
instances minority,2,2,1.0
minority majority,4,3,1.3333333333333333
majority ir,2,2,1.0
ir pima,2,2,1.0
pima phoneme,2,2,1.0
phoneme vehicle,7,3,2.3333333333333335
vehicle metrics,2,2,1.0
metrics measures,2,2,1.0
measures in,2,2,1.0
the labels,3,3,1.0
labels can,2,2,1.0
be either,2,2,1.0
either positive,2,2,1.0
positive or,2,2,1.0
or negative,1,1,1.0
negative so,1,1,1.0
so the,7,3,2.3333333333333335
prediction made,2,2,1.0
made by,2,2,1.0
classifier is,3,3,1.0
is represented,2,2,1.0
represented as,2,2,1.0
a confusion,2,2,1.0
confusion matrix,21,5,4.2
matrix see,2,2,1.0
the confusion,7,5,1.4
confusion temraz,2,2,1.0
augmentation matrix,2,2,1.0
matrix summarizes,2,2,1.0
summarizes the,4,3,1.3333333333333333
of classifiers,5,3,1.6666666666666667
classifiers for,2,2,1.0
four possible,2,2,1.0
possible outcomes,2,2,1.0
outcomes of,2,2,1.0
a true,3,3,1.0
true positive,10,5,2.0
positive tp,8,4,2.0
tp true,4,4,1.0
true negative,7,5,1.4
negative tn,5,3,1.6666666666666667
tn false,2,2,1.0
false positive,8,4,2.0
positive fp,6,2,3.0
fp and,2,2,1.0
and false,4,2,2.0
false negative,6,4,1.5
negative fn,4,2,2.0
fn was,1,1,1.0
was not,2,2,1.0
a measure,2,2,1.0
measure because,2,2,1.0
because as,2,2,1.0
as discussed,2,2,1.0
discussed earlier,2,2,1.0
earlier it,2,2,1.0
be spuriously,2,2,1.0
high for,2,2,1.0
for datasets,3,2,1.5
datasets it,2,2,1.0
all datasets,8,4,2.0
datasets used,6,4,1.5
experiments were,1,1,1.0
binary datasets,3,3,1.0
datasets using,5,3,1.6666666666666667
using two,2,2,1.0
two of,3,3,1.0
most strategies,2,2,1.0
strategies and,2,2,1.0
and see,2,2,1.0
section for,2,2,1.0
description hence,2,2,1.0
metrics used,2,2,1.0
used were,2,2,1.0
were as,3,3,1.0
as precision,2,2,1.0
recall auc,4,3,1.3333333333333333
auc and,4,3,1.3333333333333333
and defined,2,2,1.0
defined as,2,2,1.0
follows 𝑇𝑃,1,1,1.0
𝑇𝑃 𝑇𝑃,3,2,1.5
𝑇𝑃 𝐴𝑈𝐶,1,1,1.0
𝐴𝑈𝐶 table,1,1,1.0
table confusion,4,4,1.0
matrix for,5,5,1.0
for classifications,2,2,1.0
classifications receiver,2,2,1.0
receiver operating,4,4,1.0
operating characteristic,4,4,1.0
characteristic curves,2,2,1.0
curves roc,2,2,1.0
roc curve,26,5,5.2
curve were,2,2,1.0
were reported,2,2,1.0
reported as,4,2,2.0
as they,8,2,4.0
are often,3,3,1.0
often used,2,2,1.0
to evaluate,10,4,2.5
evaluate classification,2,2,1.0
classification models,2,2,1.0
models for,2,2,1.0
for imbalanced,33,6,5.5
data sets,77,6,12.833333333333334
sets the,4,3,1.3333333333333333
the roc,21,5,4.2
curve is,4,4,1.0
a graph,3,3,1.0
graph in,3,3,1.0
which true,2,2,1.0
tp rate,8,3,2.6666666666666665
rate is,7,3,2.3333333333333335
is plotted,6,3,2.0
plotted on,6,3,2.0
fp rate,8,3,2.6666666666666665
the one,9,4,2.25
one advantage,2,2,1.0
advantage of,5,5,1.0
curves is,2,2,1.0
not affected,2,2,1.0
affected by,3,3,1.0
ratio between,2,2,1.0
between minority,4,3,1.3333333333333333
see figures,2,2,1.0
figures for,2,2,1.0
for results,2,2,1.0
results area,2,2,1.0
area under,8,5,1.6
under curve,3,3,1.0
curve auc,4,4,1.0
auc predicted,2,2,1.0
predicted class,3,3,1.0
class actual,2,2,1.0
actual class,3,3,1.0
class p,2,2,1.0
p n,2,2,1.0
n p,2,2,1.0
p true,6,3,2.0
tp false,2,2,1.0
fn n,2,2,1.0
n false,2,2,1.0
fp true,2,2,1.0
tn temraz,2,2,1.0
augmentation scores,2,2,1.0
scores were,2,2,1.0
were also,3,3,1.0
also reported,2,2,1.0
are used,5,4,1.25
to measure,3,3,1.0
measure the,3,3,1.0
the area,6,5,1.2
area that,2,2,1.0
that lies,2,2,1.0
lies under,2,2,1.0
under the,12,6,2.0
curve results,2,2,1.0
results discussion,2,2,1.0
discussion overall,2,2,1.0
overall the,2,2,1.0
cfa performs,2,2,1.0
performs better,3,3,1.0
than all,8,3,2.6666666666666665
methods on,2,2,1.0
main metrics,2,2,1.0
metrics reported,2,2,1.0
reported for,2,2,1.0
for most,5,4,1.25
the classifiers,3,3,1.0
classifiers tested,1,1,1.0
tested see,1,1,1.0
see recall,1,1,1.0
recall the,3,3,1.0
on four,2,2,1.0
four classifiers,12,2,6.0
classifiers rf,4,2,2.0
mlp with,3,3,1.0
baseline no,2,2,1.0
no data,5,3,1.6666666666666667
and cfa,6,2,3.0
cfa tables,2,2,1.0
tables report,2,2,1.0
report the,2,2,1.0
main metric,2,2,1.0
metric for,4,3,1.3333333333333333
each on,1,1,1.0
datasets for,9,3,3.0
the rf,6,2,3.0
rf classifier,12,2,6.0
classifier table,8,2,4.0
the results,50,6,8.333333333333334
results show,5,3,1.6666666666666667
cfa does,10,2,5.0
does better,6,2,3.0
datasets out,2,2,1.0
out of,17,3,5.666666666666667
of with,2,2,1.0
with being,2,1,2.0
being the,11,3,3.6666666666666665
next best,7,2,3.5
best in,5,2,2.5
in only,6,2,3.0
only datasets,4,2,2.0
datasets both,2,2,1.0
both adasyn,2,2,1.0
and had,4,2,2.0
had the,6,2,3.0
the highest,12,3,4.0
highest for,2,2,1.0
for one,2,2,1.0
one dataset,2,2,1.0
each for,2,2,1.0
table it,2,2,1.0
is observed,2,2,1.0
observed that,6,3,2.0
cfa also,2,2,1.0
also achieved,2,2,1.0
achieved a,2,2,1.0
a greater,2,2,1.0
greater in,1,1,1.0
results also,3,3,1.0
in out,12,2,6.0
of datasets,27,5,5.4
datasets with,18,4,4.5
with adasyn,2,2,1.0
adasyn being,2,2,1.0
best with,6,2,3.0
with datasets,6,2,3.0
datasets smote,2,2,1.0
highest in,2,1,2.0
in dataset,2,2,1.0
the lr,4,2,2.0
lr classifier,10,2,5.0
results are,9,4,2.25
different in,2,2,1.0
they show,2,2,1.0
for metric,2,2,1.0
metric cfa,2,2,1.0
cfa doing,2,2,1.0
doing better,6,2,3.0
better in,7,3,2.3333333333333335
only out,2,2,1.0
the being,1,1,1.0
datasets whereas,4,2,2.0
whereas baseline,2,2,1.0
baseline adasyn,2,2,1.0
and doing,3,3,1.0
datasets finally,4,2,2.0
the mlp,9,3,3.0
mlp classifier,8,2,4.0
table again,2,2,1.0
again the,2,2,1.0
results showed,2,2,1.0
whereas doing,2,2,1.0
datasets baseline,2,2,1.0
baseline and,2,2,1.0
adasyn had,2,2,1.0
in temraz,2,2,1.0
method notably,2,2,1.0
notably these,2,2,1.0
these results,6,4,1.5
show for,2,2,1.0
for certain,2,2,1.0
certain datasets,2,2,1.0
and classifiers,2,2,1.0
classifiers the,2,2,1.0
the does,2,2,1.0
does quite,2,2,1.0
quite well,2,2,1.0
well however,2,2,1.0
however when,5,4,1.25
when data,5,5,1.0
augmentation can,2,2,1.0
can make,2,2,1.0
make a,3,3,1.0
a contribution,2,2,1.0
contribution it,1,1,1.0
it seems,2,2,1.0
seems to,5,3,1.6666666666666667
be cfa,2,2,1.0
cfa that,4,2,2.0
that contributes,2,2,1.0
contributes the,2,2,1.0
most to,2,2,1.0
performance improvements,2,2,1.0
improvements table,2,2,1.0
table auc,8,2,4.0
auc values,11,3,3.6666666666666665
classifier for,8,2,4.0
each data,10,3,3.3333333333333335
method dataset,8,2,4.0
dataset baseline,8,2,4.0
baseline smote,12,2,6.0
adasyn smote,14,3,4.666666666666667
smote smote,26,3,8.666666666666666
smote cfa,12,2,6.0
cfa total,8,2,4.0
total temraz,10,2,5.0
augmentation notably,2,2,1.0
notably if,2,2,1.0
we assess,2,2,1.0
assess overall,3,3,1.0
performance by,3,3,1.0
by noting,2,2,1.0
noting occasions,2,2,1.0
occasions for,2,2,1.0
given method,4,2,2.0
method when,4,4,1.0
score is,2,2,1.0
is highest,2,2,1.0
highest we,2,2,1.0
we see,3,3,1.0
see that,16,4,4.0
cfa scores,4,2,2.0
scores best,4,2,2.0
best see,2,2,1.0
table in,1,1,1.0
in of,5,2,2.5
of cases,4,2,2.0
cases it,4,2,2.0
has the,6,5,1.2
highest precision,4,2,2.0
precision score,2,2,1.0
score as,4,2,2.0
as opposed,8,4,2.0
opposed to,8,4,2.0
to for,6,4,1.5
for for,9,2,4.5
for adasyn,4,2,2.0
adasyn for,6,2,3.0
for smote,4,3,1.3333333333333333
and it,6,4,1.5
is also,14,6,2.3333333333333335
also observed,2,2,1.0
highest recall,2,2,1.0
recall score,2,2,1.0
for both,5,4,1.25
both smote,3,3,1.0
for baseline,2,2,1.0
baseline table,2,2,1.0
method showing,2,2,1.0
showing the,2,2,1.0
precision and,9,3,3.0
and recall,9,3,3.0
recall scores,2,2,1.0
scores for,2,2,1.0
method smote,2,2,1.0
cfa on,4,2,2.0
a selected,2,2,1.0
selected classifier,2,2,1.0
classifier precision,2,2,1.0
precision classifier,2,2,1.0
classifier baseline,4,2,2.0
cfa rf,4,2,2.0
mlp total,4,2,2.0
total recall,2,2,1.0
recall classifier,2,2,1.0
a rf,6,2,3.0
classifier b,4,2,2.0
b classifier,2,2,1.0
classifier c,2,2,1.0
c lr,4,2,2.0
classifier d,2,2,1.0
d mlp,4,2,2.0
classifier figure,2,2,1.0
figure values,2,2,1.0
the different,5,4,1.25
different conditions,2,2,1.0
conditions across,2,2,1.0
across datasets,2,2,1.0
classifiers a,2,2,1.0
b c,5,3,1.6666666666666667
and d,3,3,1.0
mlp figure,2,2,1.0
figure shows,5,2,2.5
the comparisons,2,2,1.0
comparisons for,2,2,1.0
on datasets,5,3,1.6666666666666667
mlp in,2,2,1.0
in general,7,4,1.75
general higher,2,2,1.0
higher values,4,2,2.0
values indicate,2,2,1.0
indicate better,2,2,1.0
better classifier,2,2,1.0
classifier performance,3,3,1.0
performance overall,2,2,1.0
overall there,2,2,1.0
is no,5,5,1.0
no significant,2,2,1.0
significant difference,2,2,1.0
between smote,2,2,1.0
variants keane,2,2,1.0
and these,2,2,1.0
these achieved,1,1,1.0
achieved lower,2,2,1.0
lower values,2,2,1.0
values than,2,2,1.0
than cfa,6,2,3.0
cfa in,2,2,1.0
most cases,3,3,1.0
cases perhaps,2,2,1.0
perhaps the,4,4,1.0
most interesting,1,1,1.0
interesting result,1,1,1.0
result is,2,2,1.0
cfa achieved,4,2,2.0
achieved higher,2,2,1.0
values in,2,2,1.0
datasets when,6,3,2.0
when using,10,4,2.5
and in,8,5,1.6
using classifier,2,2,1.0
classifier it,2,2,1.0
it outperformed,2,2,1.0
outperformed the,2,2,1.0
baseline with,2,2,1.0
and smote,6,3,2.0
variants similarly,2,2,1.0
similarly when,2,2,1.0
when applying,6,4,1.5
applying lr,2,2,1.0
classifier cfa,4,2,2.0
achieved the,2,2,1.0
datasets although,2,2,1.0
although the,6,5,1.2
the improvement,3,3,1.0
improvement varied,2,2,1.0
varied with,2,2,1.0
with different,3,3,1.0
different data,6,2,3.0
sets finally,1,1,1.0
finally in,2,2,1.0
cfa still,2,2,1.0
still achieved,2,2,1.0
achieved highest,2,2,1.0
highest auc,2,2,1.0
auc in,3,3,1.0
finally the,3,3,1.0
curves which,2,2,1.0
show the,5,4,1.25
between sensitivity,2,2,1.0
sensitivity and,2,2,1.0
and specificity,1,1,1.0
specificity are,1,1,1.0
are presented,5,4,1.25
presented in,13,4,3.25
in figure,2,2,1.0
figure and,2,2,1.0
and figure,2,2,1.0
shows selected,2,2,1.0
selected examples,7,3,2.3333333333333335
curves where,8,2,4.0
where cfa,4,2,2.0
cfa methods,1,1,1.0
methods obtained,2,2,1.0
obtained for,5,3,1.6666666666666667
methods using,6,2,3.0
classifiers on,6,2,3.0
on data,8,4,2.0
sets according,2,2,1.0
to figure,2,2,1.0
figure cfa,2,2,1.0
cfa clearly,2,2,1.0
clearly outperformed,2,2,1.0
outperformed other,2,2,1.0
other data,2,2,1.0
example when,4,2,2.0
when running,4,2,2.0
running a,4,2,2.0
dataset we,6,4,1.5
can see,12,4,3.0
cfa had,2,2,1.0
had better,6,2,3.0
performance than,6,3,2.0
than methods,3,3,1.0
methods with,3,3,1.0
to roc,2,2,1.0
curve figure,2,2,1.0
figure these,2,2,1.0
results support,1,1,1.0
support our,3,3,1.0
our previous,3,3,1.0
previous results,2,2,1.0
results which,3,3,1.0
were showing,2,2,1.0
showing that,4,2,2.0
cfa can,2,2,1.0
can work,2,2,1.0
work as,2,2,1.0
a successful,2,2,1.0
successful technique,2,2,1.0
augmentation to,3,2,1.5
to handling,3,3,1.0
handling the,5,3,1.6666666666666667
imbalanced problem,5,3,1.6666666666666667
problem on,2,2,1.0
hand figure,1,1,1.0
shows several,2,2,1.0
several examples,2,2,1.0
where smote,5,3,1.6666666666666667
variants smote,2,2,1.0
adasyn do,2,2,1.0
do better,2,2,1.0
cfa for,2,2,1.0
a lr,2,2,1.0
the pima,2,2,1.0
pima dataset,2,2,1.0
that methods,2,2,1.0
methods had,2,2,1.0
cfa figure,2,2,1.0
figure temraz,2,2,1.0
augmentation figure,4,2,2.0
figure selected,4,2,2.0
for roc,4,2,2.0
cfa outperformed,2,2,1.0
outperformed methods,2,2,1.0
six methods,4,2,2.0
on different,6,3,2.0
sets temraz,4,2,2.0
variants outperformed,2,2,1.0
outperformed cfa,2,2,1.0
cfa obtained,2,2,1.0
augmentation why,2,2,1.0
why does,4,2,2.0
does cfa,2,2,1.0
cfa work,2,2,1.0
work in,3,3,1.0
been found,2,2,1.0
found to,4,4,1.0
create plausible,2,2,1.0
for purposes,2,2,1.0
purposes indeed,2,2,1.0
indeed the,4,3,1.3333333333333333
the evaluative,2,2,1.0
evaluative metrics,2,2,1.0
xai show,2,2,1.0
these explanatory,2,2,1.0
explanatory are,1,1,1.0
are generally,2,2,1.0
existing this,2,2,1.0
this experience,4,2,2.0
experience in,2,2,1.0
xai is,2,2,1.0
the backdrop,2,2,1.0
backdrop and,2,2,1.0
and motivation,2,2,1.0
motivation for,5,4,1.25
for applying,2,2,1.0
applying this,3,3,1.0
data as,5,3,1.6666666666666667
as we,4,2,2.0
saw earlier,2,2,1.0
earlier initial,2,2,1.0
initial tests,2,2,1.0
tests on,3,3,1.0
problem showed,1,1,1.0
class improved,1,1,1.0
improved performance,3,3,1.0
specifically dealing,2,2,1.0
dealing with,13,4,3.25
dataset drift,2,2,1.0
drift caused,2,2,1.0
caused by,5,3,1.6666666666666667
by climate,2,2,1.0
change see,2,2,1.0
see this,3,3,1.0
experience led,2,2,1.0
led to,2,2,1.0
present tests,2,2,1.0
tests to,3,3,1.0
the generality,2,2,1.0
generality of,2,2,1.0
these effects,2,2,1.0
effects as,2,2,1.0
method seems,2,2,1.0
to work,3,3,1.0
work well,2,2,1.0
well across,3,3,1.0
across wide,2,2,1.0
wide range,4,2,2.0
range of,8,4,2.0
datasets ml,2,2,1.0
and different,2,2,1.0
different imbalance,3,3,1.0
imbalance ratios,5,3,1.6666666666666667
ratios but,2,2,1.0
but why,2,2,1.0
does it,2,2,1.0
it work,2,2,1.0
work so,2,2,1.0
so well,2,2,1.0
well from,2,2,1.0
the climate,2,2,1.0
climate example,2,2,1.0
example our,2,2,1.0
our initial,3,3,1.0
initial explanation,2,2,1.0
explanation was,2,2,1.0
was that,5,3,1.6666666666666667
does well,2,2,1.0
well because,2,2,1.0
because it,5,3,1.6666666666666667
it minority,1,1,1.0
are counterfactual,2,2,1.0
counterfactual offsets,2,2,1.0
offsets from,2,2,1.0
from known,2,2,1.0
known but,1,1,1.0
but this,3,3,1.0
this account,2,2,1.0
account does,2,2,1.0
not answer,2,2,1.0
answer why,2,2,1.0
the offsets,2,2,1.0
offsets tend,2,2,1.0
be useful,2,2,1.0
useful our,2,2,1.0
our best,3,3,1.0
best account,2,2,1.0
account hinges,2,2,1.0
on ideas,2,2,1.0
ideas from,2,2,1.0
from how,2,2,1.0
how reasoning,2,2,1.0
reasoning cbr,2,2,1.0
cbr systems,2,2,1.0
systems operate,2,2,1.0
operate in,3,3,1.0
in cbr,6,2,3.0
cbr target,2,2,1.0
target problems,2,2,1.0
problems are,2,2,1.0
are solved,2,2,1.0
solved by,3,3,1.0
by similar,1,1,1.0
similar cases,2,2,1.0
sometimes adapting,2,2,1.0
adapting them,2,2,1.0
generate predictions,2,2,1.0
predictions so,2,2,1.0
am trying,2,2,1.0
predict in,2,2,1.0
a city,2,2,1.0
city and,2,2,1.0
and my,2,2,1.0
my cbr,2,2,1.0
cbr system,2,2,1.0
system is,2,2,1.0
is presented,3,3,1.0
presented with,2,2,1.0
a apartment,3,2,1.5
apartment with,3,2,1.5
with and,2,2,1.0
closest retrieved,2,2,1.0
retrieved case,4,2,2.0
case is,3,3,1.0
system could,2,2,1.0
could have,2,2,1.0
have an,2,2,1.0
an adaptation,4,2,2.0
adaptation rule,4,2,2.0
rule than,2,2,1.0
than can,2,2,1.0
can bridges,2,2,1.0
bridges the,2,2,1.0
the gap,2,2,1.0
gap between,2,2,1.0
historical case,1,1,1.0
case and,3,2,1.5
the target,10,6,1.6666666666666667
target case,4,2,2.0
case for,6,4,1.5
instance there,2,2,1.0
there may,3,3,1.0
be an,4,4,1.0
rule that,3,3,1.0
that says,2,2,1.0
says in,2,2,1.0
general an,2,2,1.0
an additional,3,3,1.0
additional bathroom,2,2,1.0
bathroom is,2,2,1.0
is worth,3,3,1.0
worth more,2,2,1.0
more so,2,2,1.0
so in,6,4,1.5
a typical,2,2,1.0
typical this,2,2,1.0
this rule,2,2,1.0
rule would,2,2,1.0
the retrieved,2,2,1.0
case to,2,2,1.0
to bring,2,2,1.0
bring it,2,2,1.0
it closer,2,2,1.0
closer to,4,4,1.0
and improve,2,2,1.0
prediction in,2,2,1.0
cbr adaptation,2,2,1.0
adaptation rules,8,2,4.0
rules were,2,2,1.0
were often,2,2,1.0
often but,2,2,1.0
but they,2,2,1.0
they may,4,2,2.0
may also,4,4,1.0
also be,5,4,1.25
be learned,6,4,1.5
learned from,4,2,2.0
from analyses,2,2,1.0
of patterns,16,3,5.333333333333333
patterns between,2,2,1.0
between temraz,2,2,1.0
the so,2,2,1.0
the rule,2,2,1.0
rule could,2,2,1.0
from differences,1,1,1.0
differences found,2,2,1.0
found between,2,2,1.0
between historical,2,2,1.0
historical instances,2,2,1.0
instances showing,2,2,1.0
they often,2,2,1.0
often lead,2,2,1.0
a uplift,2,2,1.0
uplift in,2,2,1.0
in price,2,2,1.0
price other,2,2,1.0
other features,4,2,2.0
features being,2,2,1.0
being equal,2,2,1.0
equal so,2,2,1.0
cbr these,2,2,1.0
these adaptation,2,2,1.0
rules help,2,2,1.0
help to,2,2,1.0
to bridge,2,2,1.0
bridge holes,2,2,1.0
holes in,2,2,1.0
the by,3,3,1.0
by providing,2,2,1.0
providing plausible,2,2,1.0
plausible transformations,2,2,1.0
transformations of,2,2,1.0
known datapoints,2,2,1.0
datapoints counterfactuals,2,2,1.0
are special,2,2,1.0
special case,2,2,1.0
case of,15,5,3.0
of an,6,5,1.2
an adaption,2,2,1.0
adaption rule,2,2,1.0
rule they,2,2,1.0
they capture,2,2,1.0
capture the,2,2,1.0
the key,6,4,1.5
key that,1,1,1.0
that lead,2,2,1.0
to class,7,5,1.4
class changes,2,2,1.0
changes across,2,2,1.0
across the,4,3,1.3333333333333333
boundary between,3,3,1.0
instances so,2,2,1.0
so when,2,2,1.0
when we,3,3,1.0
we apply,5,3,1.6666666666666667
apply them,2,2,1.0
create synthetic,2,2,1.0
instances they,1,1,1.0
they stand,2,2,1.0
stand a,2,2,1.0
good chance,2,2,1.0
chance of,3,3,1.0
of being,4,3,1.3333333333333333
being plausible,2,2,1.0
plausible as,2,2,1.0
are based,9,5,1.8
on prior,2,2,1.0
prior local,2,2,1.0
local transformations,2,2,1.0
transformations though,2,2,1.0
though they,1,1,1.0
they lack,1,1,1.0
lack generality,2,2,1.0
generality they,2,2,1.0
not created,2,2,1.0
created from,2,2,1.0
from multiple,2,2,1.0
multiple pairings,2,2,1.0
pairings of,2,2,1.0
same as,3,2,1.5
as is,3,3,1.0
the case,14,5,2.8
for learned,2,2,1.0
learned adaption,2,2,1.0
adaption rules,2,2,1.0
rules perhaps,2,2,1.0
perhaps they,2,2,1.0
may work,4,2,2.0
work because,4,2,2.0
are so,2,2,1.0
so constrained,2,2,1.0
constrained and,4,2,2.0
and local,2,2,1.0
local cfa,1,1,1.0
cfa only,2,2,1.0
considers native,2,2,1.0
counterfactual with,2,2,1.0
with feature,4,2,2.0
differences so,2,2,1.0
the is,4,3,1.3333333333333333
is highly,2,2,1.0
highly constrained,2,2,1.0
and specific,2,2,1.0
specific to,2,2,1.0
the instances,5,3,1.6666666666666667
are already,2,2,1.0
already very,1,1,1.0
similar all,2,2,1.0
are essentially,2,2,1.0
essentially identical,2,2,1.0
identical to,2,2,1.0
to put,4,2,2.0
put it,2,2,1.0
it simply,2,2,1.0
simply cfa,2,2,1.0
cfa delivers,2,2,1.0
delivers good,2,2,1.0
counterfactual rules,2,2,1.0
rules that,2,2,1.0
that work,2,2,1.0
work locally,2,2,1.0
locally to,2,2,1.0
plausible datapoints,2,2,1.0
datapoints that,2,2,1.0
are predictively,2,2,1.0
predictively useful,2,2,1.0
useful so,2,2,1.0
so it,6,3,2.0
it looks,2,2,1.0
looks like,2,2,1.0
like the,2,2,1.0
the nature,5,5,1.0
nature of,12,6,2.0
is important,4,4,1.0
method what,2,2,1.0
what are,2,2,1.0
are cfa,2,2,1.0
s limitations,2,2,1.0
limitations the,2,2,1.0
the to,2,2,1.0
success question,2,2,1.0
question is,2,2,1.0
the failure,2,2,1.0
failure one,2,2,1.0
one namely,2,2,1.0
namely when,2,2,1.0
when do,2,2,1.0
do we,2,2,1.0
we think,2,2,1.0
think cfa,2,2,1.0
cfa will,2,2,1.0
will fail,2,2,1.0
fail and,2,2,1.0
and what,2,2,1.0
what limitations,2,2,1.0
limitations might,2,2,1.0
might it,2,2,1.0
it encounter,2,2,1.0
encounter the,2,2,1.0
current experiments,2,2,1.0
experiments a,2,2,1.0
a version,2,2,1.0
of cfa,2,2,1.0
that performs,3,3,1.0
performs well,5,3,1.6666666666666667
well so,2,2,1.0
not immediately,2,2,1.0
immediately clear,2,2,1.0
clear what,2,2,1.0
what would,2,2,1.0
would lead,2,2,1.0
lead cfa,2,2,1.0
to fail,5,3,1.6666666666666667
fail however,1,1,1.0
however there,4,3,1.3333333333333333
are several,5,4,1.25
several conditions,2,2,1.0
which cfa,2,2,1.0
be less,5,4,1.25
less good,2,2,1.0
good with,2,2,1.0
to i,2,2,1.0
i the,2,2,1.0
the quality,5,2,2.5
quality of,8,3,2.6666666666666665
differences ii,2,2,1.0
ii the,2,2,1.0
constraint and,2,2,1.0
and iii,4,2,2.0
iii the,2,2,1.0
of different,7,5,1.4
different tolerances,2,2,1.0
tolerances temraz,2,2,1.0
augmentation quality,2,2,1.0
dataset fundamentally,2,2,1.0
fundamentally cfa,2,2,1.0
cfa depends,2,2,1.0
depends on,5,3,1.6666666666666667
of native,2,2,1.0
for its,3,3,1.0
success to,2,2,1.0
put this,2,2,1.0
this another,2,2,1.0
another way,3,3,1.0
way there,2,2,1.0
there needs,2,2,1.0
be a,7,5,1.4
a rich,2,2,1.0
rich and,2,2,1.0
and diverse,4,2,2.0
diverse set,2,2,1.0
counterfactual pairings,2,2,1.0
a clear,2,2,1.0
clear decision,2,2,1.0
boundary without,2,2,1.0
without these,2,2,1.0
these counterfactuals,2,2,1.0
counterfactuals the,2,2,1.0
the ability,3,3,1.0
ability to,3,3,1.0
datapoints will,1,1,1.0
be hampered,1,1,1.0
hampered current,2,2,1.0
current indications,2,2,1.0
indications are,2,2,1.0
are that,2,2,1.0
that at,2,2,1.0
at least,6,5,1.2
least of,2,2,1.0
class need,2,2,1.0
be involved,2,2,1.0
in these,2,2,1.0
in order,22,6,3.6666666666666665
order to,22,6,3.6666666666666665
provide a,10,5,2.0
a basis,2,2,1.0
however we,3,3,1.0
have not,3,3,1.0
not systematically,2,2,1.0
systematically tested,1,1,1.0
tested how,1,1,1.0
how changes,2,2,1.0
changes in,4,2,2.0
this percentage,2,2,1.0
percentage affect,2,2,1.0
affect performance,2,2,1.0
performance what,2,2,1.0
what we,2,2,1.0
do know,2,2,1.0
know is,2,2,1.0
many datasets,2,2,1.0
datasets the,7,4,1.75
current parameters,2,2,1.0
cfa deliver,2,2,1.0
deliver good,2,2,1.0
good performance,2,2,1.0
performance so,2,2,1.0
this factor,2,2,1.0
factor might,2,2,1.0
be quite,3,3,1.0
quite robust,2,2,1.0
robust to,2,2,1.0
to disruption,2,2,1.0
disruption the,2,2,1.0
constraint a,2,2,1.0
key hyperparameter,2,2,1.0
hyperparameter for,2,2,1.0
counterfactuals build,2,2,1.0
build from,2,2,1.0
dataset involve,2,2,1.0
involve no,2,2,1.0
two this,2,2,1.0
a strong,3,3,1.0
strong constraint,2,2,1.0
was made,4,4,1.0
made originally,2,2,1.0
originally on,2,2,1.0
grounds in,2,2,1.0
xai that,2,2,1.0
is it,2,2,1.0
shown that,7,4,1.75
that people,2,2,1.0
people prefer,2,2,1.0
prefer sparse,2,2,1.0
differences as,2,2,1.0
to comprehend,2,2,1.0
comprehend however,2,2,1.0
this rationale,2,2,1.0
rationale from,2,2,1.0
from xai,2,2,1.0
xai does,2,2,1.0
not apply,2,2,1.0
apply to,2,2,1.0
the may,2,2,1.0
it produces,2,2,1.0
produces very,2,2,1.0
very counterfactual,1,1,1.0
pairs so,2,2,1.0
so they,2,2,1.0
they produce,2,2,1.0
produce very,2,2,1.0
very simple,3,3,1.0
simple adaptation,2,2,1.0
rules in,2,2,1.0
which most,3,3,1.0
most features,2,2,1.0
features remain,2,2,1.0
remain the,2,2,1.0
same between,2,2,1.0
a small,5,5,1.0
small number,3,3,1.0
features differ,2,2,1.0
differ these,2,2,1.0
these difference,2,2,1.0
difference patterns,2,2,1.0
patterns may,2,2,1.0
more representative,2,2,1.0
representative of,2,2,1.0
of valid,2,2,1.0
valid in,2,2,1.0
dataset and,8,5,1.6
hence be,2,2,1.0
to produce,5,3,1.6666666666666667
produce good,2,2,1.0
good synthetic,2,2,1.0
datapoints these,2,2,1.0
these is,2,2,1.0
is some,3,3,1.0
some evidence,3,3,1.0
evidence to,2,2,1.0
to support,5,3,1.6666666666666667
support this,2,2,1.0
this proposition,2,2,1.0
proposition in,2,2,1.0
in prior,2,2,1.0
prior work,2,2,1.0
work temraz,2,2,1.0
al report,2,2,1.0
report that,2,2,1.0
that in,12,4,3.0
in pilot,2,2,1.0
pilot runs,2,2,1.0
runs of,3,3,1.0
their experiments,2,2,1.0
experiments they,2,2,1.0
they explored,2,2,1.0
explored using,1,1,1.0
and counterfactuals,2,2,1.0
counterfactuals but,2,2,1.0
but found,2,2,1.0
found they,2,2,1.0
they did,2,2,1.0
not significantly,2,2,1.0
significantly improve,2,2,1.0
improve importance,1,1,1.0
importance that,2,2,1.0
is they,2,2,1.0
were less,2,2,1.0
generate useful,2,2,1.0
useful minority,1,1,1.0
instances we,2,2,1.0
not know,2,2,1.0
know whether,2,2,1.0
whether similar,2,2,1.0
similar results,2,2,1.0
results would,2,2,1.0
for other,3,3,1.0
other datasets,2,2,1.0
datasets though,2,2,1.0
though the,4,4,1.0
the fact,6,5,1.2
fact that,6,5,1.2
the keane,2,2,1.0
augmentation difference,2,2,1.0
difference constraint,2,2,1.0
constraint works,4,2,2.0
works here,2,2,1.0
here for,2,2,1.0
with features,2,2,1.0
features suggests,2,2,1.0
this constraint,2,2,1.0
works quite,2,2,1.0
quite generally,2,2,1.0
generally so,2,2,1.0
again we,2,2,1.0
we would,5,4,1.25
would expect,2,2,1.0
expect cfa,2,2,1.0
fail if,2,2,1.0
if higher,2,2,1.0
higher numbers,2,2,1.0
numbers of,6,5,1.2
of were,1,1,1.0
computing the,4,4,1.0
counterfactuals when,2,2,1.0
applying the,6,3,2.0
method the,8,4,2.0
of tolerance,4,2,2.0
tolerance a,2,2,1.0
a final,3,3,1.0
final key,2,2,1.0
key parameter,2,2,1.0
counterfactual we,2,2,1.0
apply a,3,3,1.0
a tolerance,2,2,1.0
tolerance to,4,2,2.0
feature values,3,3,1.0
values specifically,2,2,1.0
specifically we,2,2,1.0
we allow,2,2,1.0
allow features,2,2,1.0
feature this,1,1,1.0
this was,1,1,1.0
was applied,2,2,1.0
applied uniformly,2,2,1.0
uniformly across,2,2,1.0
of our,4,4,1.0
our datasets,2,2,1.0
datasets keane,2,2,1.0
smyth used,2,2,1.0
used a,3,3,1.0
a more,15,3,5.0
more tolerance,1,1,1.0
tolerance scheme,2,2,1.0
scheme that,2,2,1.0
that tailored,2,2,1.0
tailored the,2,2,1.0
the tolerance,2,2,1.0
to each,4,4,1.0
they varied,2,2,1.0
varied the,2,2,1.0
the tolerances,2,2,1.0
tolerances for,2,2,1.0
feature until,2,2,1.0
until changes,2,2,1.0
classification of,6,4,1.5
dataset arose,2,2,1.0
arose and,2,2,1.0
then chose,2,2,1.0
chose a,2,2,1.0
a relative,3,3,1.0
relative tolerance,2,2,1.0
tolerance that,2,2,1.0
that produced,3,3,1.0
produced no,2,2,1.0
no classification,3,3,1.0
classification change,2,2,1.0
change obviously,2,2,1.0
obviously without,2,2,1.0
fewer would,1,1,1.0
diminish the,2,2,1.0
boundary fundamentally,2,2,1.0
fundamentally like,2,2,1.0
like cfa,2,2,1.0
cfa works,2,2,1.0
works with,2,2,1.0
with instances,2,2,1.0
so clearly,1,1,1.0
clearly the,1,1,1.0
the definition,2,2,1.0
definition and,2,2,1.0
and nature,2,2,1.0
of that,3,3,1.0
that decision,2,2,1.0
boundary is,6,4,1.5
is critical,2,2,1.0
critical to,2,2,1.0
to its,4,4,1.0
its successful,2,2,1.0
successful performance,2,2,1.0
performance if,2,2,1.0
instances around,2,2,1.0
around the,2,2,1.0
are noisy,2,2,1.0
noisy and,4,3,1.3333333333333333
is less,2,2,1.0
less then,2,2,1.0
then cfa,2,2,1.0
to disimprove,1,1,1.0
disimprove in,2,2,1.0
this respect,2,2,1.0
respect it,2,2,1.0
is interesting,2,2,1.0
interesting note,2,2,1.0
note that,26,5,5.2
does best,2,2,1.0
best using,2,2,1.0
and random,5,4,1.25
random forests,4,2,2.0
forests models,2,2,1.0
models relative,2,2,1.0
and linear,2,2,1.0
linear regression,2,2,1.0
regression models,2,2,1.0
models with,2,2,1.0
latter doing,2,2,1.0
doing the,2,2,1.0
the worst,5,3,1.6666666666666667
worst on,2,2,1.0
on auc,2,2,1.0
auc conclusion,2,2,1.0
conclusion in,2,2,1.0
paper a,2,2,1.0
novel oversampling,2,2,1.0
cfa was,2,2,1.0
was proposed,8,4,2.0
to handle,7,4,1.75
handle the,3,3,1.0
for binary,2,2,1.0
tasks cfa,2,2,1.0
a approach,5,3,1.6666666666666667
the essence,2,2,1.0
essence of,2,2,1.0
this temraz,2,2,1.0
it oversamples,2,2,1.0
oversamples by,2,2,1.0
by adaptively,3,3,1.0
adaptively combining,2,2,1.0
combining actual,2,2,1.0
actual from,2,2,1.0
from dataset,2,2,1.0
dataset instances,2,2,1.0
than values,2,2,1.0
instances the,5,3,1.6666666666666667
key discoveries,2,2,1.0
discoveries made,2,2,1.0
made are,2,2,1.0
are i,2,2,1.0
i counterfactual,2,2,1.0
methods developed,3,3,1.0
for xai,2,2,1.0
xai can,2,2,1.0
be usefully,2,2,1.0
usefully deployed,2,2,1.0
deployed to,2,2,1.0
to augment,2,2,1.0
augment datasets,2,2,1.0
with synthetic,2,2,1.0
cases in,7,3,2.3333333333333335
the ml,1,1,1.0
models ii,2,2,1.0
ii this,2,2,1.0
can successfully,2,2,1.0
successfully introduce,2,2,1.0
introduce new,2,2,1.0
minority examples,21,4,5.25
by leveraging,2,2,1.0
leveraging known,1,1,1.0
known in,1,1,1.0
iii this,2,2,1.0
can outperform,2,2,1.0
outperform many,2,2,1.0
many key,2,2,1.0
key benchmark,2,2,1.0
benchmark smote,2,2,1.0
a wide,3,3,1.0
with differing,2,2,1.0
differing imbalance,2,2,1.0
ratios using,2,2,1.0
using representative,2,2,1.0
representative ml,2,2,1.0
models acknowledgements,2,2,1.0
acknowledgements this,2,2,1.0
this publication,3,3,1.0
publication has,2,2,1.0
has emanated,2,2,1.0
emanated from,2,2,1.0
from research,2,2,1.0
research conducted,2,2,1.0
conducted with,2,2,1.0
the financial,2,2,1.0
financial support,2,2,1.0
support of,3,3,1.0
of i,2,2,1.0
i science,2,2,1.0
science foundation,2,2,1.0
foundation ireland,2,2,1.0
ireland sfi,2,2,1.0
sfi to,2,2,1.0
analytics under,2,2,1.0
under grant,4,2,2.0
grant number,4,2,2.0
number and,2,2,1.0
ii sfi,2,2,1.0
sfi and,2,2,1.0
the department,6,4,1.5
department of,9,5,1.8
of agriculture,2,2,1.0
agriculture food,2,2,1.0
food and,2,2,1.0
and marine,2,2,1.0
marine on,2,2,1.0
on behalf,2,2,1.0
behalf of,2,2,1.0
the government,2,2,1.0
government of,2,2,1.0
of ireland,2,2,1.0
ireland to,2,2,1.0
the vistamilk,2,2,1.0
centre under,2,2,1.0
number references,2,2,1.0
references aggarwal,2,2,1.0
aggarwal chen,2,2,1.0
chen han,2,2,1.0
han j,2,2,1.0
j the,2,2,1.0
the inverse,2,2,1.0
inverse classification,4,2,2.0
classification problem,4,3,1.3333333333333333
problem journal,2,2,1.0
journal of,39,6,6.5
science and,15,5,3.0
and technology,6,4,1.5
technology fernandez,2,2,1.0
fernandez luengo,2,2,1.0
luengo derrac,2,2,1.0
derrac garcía,2,2,1.0
garcía sánchez,2,2,1.0
sánchez herrera,2,2,1.0
herrera keel,2,2,1.0
keel software,2,2,1.0
software tool,2,2,1.0
tool data,2,2,1.0
set repository,2,2,1.0
repository integration,2,2,1.0
integration of,4,3,1.3333333333333333
and experimental,2,2,1.0
experimental analysis,2,2,1.0
analysis framework,2,2,1.0
framework journal,2,2,1.0
of logic,2,2,1.0
logic and,2,2,1.0
and soft,5,3,1.6666666666666667
soft computing,5,3,1.6666666666666667
computing asuncion,2,2,1.0
asuncion newman,2,2,1.0
newman uci,3,3,1.0
uci machine,7,4,1.75
learning repository,7,4,1.75
repository https,2,2,1.0
https bache,2,2,1.0
bache lichman,2,2,1.0
lichman uci,3,3,1.0
repository batista,2,2,1.0
batista prati,4,4,1.0
prati monard,3,3,1.0
monard a,4,4,1.0
a study,5,4,1.25
study of,6,4,1.5
the behavior,4,4,1.0
behavior of,4,4,1.0
of several,5,5,1.0
several methods,5,5,1.0
for balancing,4,4,1.0
balancing machine,4,4,1.0
learning training,5,5,1.0
data sigkdd,6,4,1.5
sigkdd explor,3,3,1.0
explor temraz,2,2,1.0
augmentation bellinger,2,2,1.0
bellinger sharma,2,2,1.0
sharma japkowicz,2,2,1.0
japkowicz zaïane,2,2,1.0
zaïane framework,2,2,1.0
framework for,7,4,1.75
for extreme,2,2,1.0
extreme imbalance,2,2,1.0
imbalance classification,4,2,2.0
classification with,3,3,1.0
class knowledge,2,2,1.0
knowledge and,9,3,3.0
and systems,1,1,1.0
systems bishop,2,2,1.0
bishop pattern,2,2,1.0
pattern recognition,13,4,3.25
recognition and,3,3,1.0
and machine,3,3,1.0
learning springer,2,2,1.0
springer blake,2,2,1.0
blake merz,2,2,1.0
merz uci,2,2,1.0
uci repository,3,3,1.0
repository of,2,2,1.0
of machine,11,5,2.2
learning databases,2,2,1.0
databases department,2,2,1.0
university of,9,5,1.8
of california,2,2,1.0
california bunkhumpornpat,2,2,1.0
bunkhumpornpat sinapiromsaran,5,3,1.6666666666666667
sinapiromsaran lursinsap,4,2,2.0
lursinsap minority,2,2,1.0
for handling,8,4,2.0
class problem,1,1,1.0
in proceedings,61,5,12.2
proceedings of,65,5,13.0
the conference,6,2,3.0
conference on,87,5,17.4
on advances,6,3,2.0
advances in,11,5,2.2
in knowledge,4,3,1.3333333333333333
knowledge discovery,12,5,2.4
discovery and,9,5,1.8
data mining,33,6,5.5
mining bunkhumpornpat,2,2,1.0
lursinsap dbsmote,2,2,1.0
dbsmote synthetic,1,1,1.0
technique applied,2,2,1.0
applied intelligence,2,2,1.0
intelligence chawla,2,2,1.0
chawla bowyer,4,4,1.0
bowyer hall,5,5,1.0
hall kegelmeyer,2,2,1.0
kegelmeyer smote,5,5,1.0
technique journal,4,4,1.0
of artificial,6,3,2.0
artificial intelligence,24,4,6.0
intelligence research,7,4,1.75
research chawla,2,2,1.0
chawla lazarevic,4,4,1.0
lazarevic hall,4,4,1.0
hall bowyer,3,3,1.0
bowyer smoteboost,3,3,1.0
smoteboost improving,3,3,1.0
improving prediction,4,4,1.0
prediction of,4,4,1.0
in boosting,15,4,3.75
boosting knowledge,2,2,1.0
discovery in,3,3,1.0
in databases,3,3,1.0
databases pkdd,2,2,1.0
pkdd cristianini,2,2,1.0
cristianini j,2,2,1.0
j an,2,2,1.0
an introduction,2,2,1.0
introduction to,2,2,1.0
support vector,14,5,2.8
vector machines,9,5,1.8
machines and,3,3,1.0
and other,6,4,1.5
other learning,2,2,1.0
learning methods,12,5,2.4
methods cambridge,2,2,1.0
cambridge uk,2,2,1.0
uk cambridge,2,2,1.0
cambridge university,2,2,1.0
university press,2,2,1.0
press dandl,2,2,1.0
dandl molnar,2,2,1.0
molnar binder,2,2,1.0
binder bischl,2,2,1.0
bischl b,2,2,1.0
b counterfactual,1,1,1.0
explanations in,6,2,3.0
in international,15,3,5.0
international conference,51,5,10.2
on parallel,2,2,1.0
parallel problem,2,2,1.0
problem solving,2,2,1.0
solving from,2,2,1.0
from nature,2,2,1.0
nature d,2,2,1.0
d aquin,2,2,1.0
aquin badra,2,2,1.0
badra lafrogne,2,2,1.0
lafrogne lieber,2,2,1.0
lieber napoli,2,2,1.0
napoli szathmary,2,2,1.0
szathmary case,2,2,1.0
case base,2,2,1.0
base mining,2,2,1.0
mining for,5,3,1.6666666666666667
for adaptation,2,2,1.0
adaptation knowledge,2,2,1.0
knowledge acquisition,2,2,1.0
acquisition in,2,2,1.0
the twentieth,2,2,1.0
twentieth joint,1,1,1.0
joint conference,15,4,3.75
on artificial,11,3,3.6666666666666665
intelligence dasarathy,2,2,1.0
dasarathy b,2,2,1.0
b minimal,2,2,1.0
minimal consistent,2,2,1.0
consistent set,2,2,1.0
set mcs,2,2,1.0
mcs identification,2,2,1.0
identification for,4,2,2.0
for optimal,3,3,1.0
optimal nearest,1,1,1.0
neighbor decision,2,2,1.0
decision systems,2,2,1.0
systems design,2,2,1.0
design ieee,2,2,1.0
ieee transactions,36,5,7.2
transactions on,37,5,7.4
on systems,8,4,2.0
systems man,8,4,2.0
man and,8,4,2.0
and cybernetics,7,4,1.75
cybernetics delaney,2,2,1.0
delaney greene,2,2,1.0
greene keane,2,2,1.0
counterfactual for,1,1,1.0
for time,2,2,1.0
time series,4,2,2.0
series classification,2,2,1.0
classification in,10,4,2.5
on reasoning,9,2,4.5
reasoning springer,8,2,4.0
springer germany,8,2,4.0
germany temraz,2,2,1.0
augmentation douzas,2,2,1.0
douzas bacao,4,2,2.0
bacao map,2,2,1.0
map oversampling,2,2,1.0
oversampling somo,2,2,1.0
somo for,2,2,1.0
set learning,5,4,1.25
learning expert,2,2,1.0
expert systems,2,2,1.0
systems with,2,2,1.0
with applications,2,2,1.0
applications douzas,2,2,1.0
bacao last,2,2,1.0
last improving,2,2,1.0
improving imbalanced,2,2,1.0
imbalanced learning,28,4,7.0
learning through,2,2,1.0
through a,4,3,1.3333333333333333
a oversampling,2,2,1.0
method based,2,2,1.0
on and,3,3,1.0
smote information,2,2,1.0
information sciences,2,2,1.0
sciences elkan,2,2,1.0
elkan the,2,2,1.0
the foundations,2,2,1.0
foundations of,2,2,1.0
of learning,16,5,3.2
learning in,23,5,4.6
in seventeenth,2,2,1.0
seventeenth international,2,2,1.0
international joint,12,4,3.0
intelligence fernandez,2,2,1.0
fernandez garcia,4,2,2.0
garcia herrera,4,2,2.0
herrera chawla,2,2,1.0
chawla smote,2,2,1.0
for learning,14,5,2.8
from imbalanced,35,6,5.833333333333333
data progress,2,2,1.0
progress and,2,2,1.0
and challenges,4,4,1.0
challenges marking,2,2,1.0
marking the,2,2,1.0
the anniversary,2,2,1.0
anniversary the,2,2,1.0
the journal,2,2,1.0
research jair,2,2,1.0
jair förster,2,2,1.0
förster klier,4,2,2.0
klier kluge,4,2,2.0
kluge sigler,4,2,2.0
sigler i,4,2,2.0
i evaluating,2,2,1.0
evaluating explainable,2,2,1.0
explainable artifical,2,2,1.0
artifical intelligence,2,2,1.0
intelligence what,2,2,1.0
what users,2,2,1.0
users really,2,2,1.0
really appreciate,2,2,1.0
appreciate in,2,2,1.0
in european,4,2,2.0
european conference,5,4,1.25
on information,4,4,1.0
information systems,6,2,3.0
systems förster,2,2,1.0
i fostering,2,2,1.0
fostering human,2,2,1.0
human agency,2,2,1.0
agency a,2,2,1.0
a process,3,3,1.0
process for,5,4,1.25
the design,6,3,2.0
design of,2,2,1.0
of xai,2,2,1.0
xai systems,2,2,1.0
systems in,2,2,1.0
in icis,2,2,1.0
icis proceedings,2,2,1.0
proceedings freund,2,2,1.0
freund schapire,2,2,1.0
schapire a,4,4,1.0
a generalization,4,4,1.0
generalization of,5,4,1.25
learning and,11,5,2.2
and an,6,5,1.2
an application,5,5,1.0
application to,4,4,1.0
to boosting,4,4,1.0
boosting journal,3,3,1.0
computer and,5,4,1.25
and system,3,3,1.0
system sciences,3,3,1.0
sciences galar,2,2,1.0
galar fernandez,2,2,1.0
fernandez barrenechea,2,2,1.0
barrenechea bustince,3,3,1.0
bustince herrera,2,2,1.0
herrera a,5,3,1.6666666666666667
a review,8,5,1.6
review on,3,3,1.0
on ensembles,3,3,1.0
ensembles for,4,3,1.3333333333333333
and approaches,4,3,1.3333333333333333
approaches ieee,3,3,1.0
cybernetics han,2,2,1.0
han kamber,2,2,1.0
kamber data,2,2,1.0
mining concepts,2,2,1.0
concepts and,2,2,1.0
and technique,2,2,1.0
technique han,2,2,1.0
han wang,5,4,1.25
wang mao,3,3,1.0
mao a,3,3,1.0
new method,4,4,1.0
in imbalanced,11,5,2.2
sets learning,6,5,1.2
on intelligent,3,3,1.0
intelligent computing,3,3,1.0
computing hanney,2,2,1.0
hanney keane,2,2,1.0
keane learning,2,2,1.0
learning adaptation,2,2,1.0
rules from,2,2,1.0
a in,2,2,1.0
third european,2,2,1.0
european workshop,4,2,2.0
workshop on,18,4,4.5
reasoning berlin,2,2,1.0
berlin springer,2,2,1.0
springer hasan,2,2,1.0
hasan use,2,2,1.0
use case,2,2,1.0
examples data,2,2,1.0
augmentation proceedings,2,2,1.0
of student,2,2,1.0
student research,2,2,1.0
research and,2,2,1.0
and creative,2,2,1.0
creative inquiry,2,2,1.0
inquiry day,2,2,1.0
day he,2,2,1.0
he bai,2,2,1.0
bai garcia,4,4,1.0
garcia li,2,2,1.0
li adasyn,4,4,1.0
adasyn adaptive,4,4,1.0
adaptive synthetic,7,4,1.75
synthetic sampling,3,3,1.0
sampling approach,3,3,1.0
approach for,8,5,1.6
in ieee,5,4,1.25
ieee international,4,3,1.3333333333333333
on neural,29,4,7.25
neural networks,31,5,6.2
networks temraz,2,2,1.0
augmentation he,2,2,1.0
he garcia,2,2,1.0
garcia learning,4,4,1.0
data ieee,8,6,1.3333333333333333
on knowledge,10,5,2.0
data engineering,5,3,1.6666666666666667
engineering holte,2,2,1.0
holte acker,2,2,1.0
acker porter,2,2,1.0
porter b,2,2,1.0
b concept,2,2,1.0
concept learning,2,2,1.0
and accuracy,2,2,1.0
of small,5,3,1.6666666666666667
small disjuncts,12,5,2.4
disjuncts in,2,2,1.0
the llth,2,2,1.0
llth international,2,2,1.0
intelligence hsu,2,2,1.0
hsu lin,2,2,1.0
lin a,2,2,1.0
a comparison,6,4,1.5
comparison of,6,5,1.2
of methods,2,2,1.0
for multiclass,4,2,2.0
multiclass support,2,2,1.0
vector ieee,1,1,1.0
networks hu,2,2,1.0
hu liang,2,2,1.0
liang ma,2,2,1.0
ma he,2,2,1.0
he y,4,4,1.0
y msmote,2,2,1.0
msmote improving,2,2,1.0
improving classification,2,2,1.0
classification when,4,3,1.3333333333333333
when training,4,4,1.0
data is,9,6,1.5
is imbalanced,2,2,1.0
the second,14,6,2.3333333333333335
second international,2,2,1.0
international workshop,3,3,1.0
on computer,4,3,1.3333333333333333
and engineering,6,4,1.5
engineering jeni,2,2,1.0
jeni cohn,2,2,1.0
cohn torre,2,2,1.0
torre facing,2,2,1.0
facing imbalanced,2,2,1.0
data recommendations,2,2,1.0
recommendations for,2,2,1.0
of performance,3,3,1.0
performance metrics,2,2,1.0
the humaine,2,2,1.0
humaine association,2,2,1.0
association conference,2,2,1.0
on affective,2,2,1.0
affective computing,2,2,1.0
computing and,3,3,1.0
and intelligent,2,2,1.0
intelligent interaction,2,2,1.0
interaction jiang,2,2,1.0
jiang lu,2,2,1.0
lu xia,2,2,1.0
xia a,2,2,1.0
novel algorithm,2,2,1.0
algorithm for,8,5,1.6
for imbalance,5,3,1.6666666666666667
imbalance data,4,4,1.0
data classification,9,3,3.0
classification based,1,1,1.0
on genetic,3,3,1.0
genetic algorithm,3,3,1.0
algorithm improved,2,2,1.0
improved smote,2,2,1.0
smote arabian,2,2,1.0
arabian journal,2,2,1.0
journal for,2,2,1.0
for science,2,2,1.0
engineering karimi,2,2,1.0
karimi barthe,2,2,1.0
barthe schölkopf,2,2,1.0
schölkopf valera,2,2,1.0
valera i,2,2,1.0
i a,4,4,1.0
a survey,6,2,3.0
survey of,2,2,1.0
of algorithmic,2,2,1.0
recourse definitions,2,2,1.0
definitions formulations,2,2,1.0
formulations solutions,2,2,1.0
solutions and,2,2,1.0
and prospects,2,2,1.0
prospects keane,2,2,1.0
keane kenny,2,2,1.0
kenny delaney,2,2,1.0
delaney smyth,2,2,1.0
smyth b,6,2,3.0
b if,2,2,1.0
if only,2,2,1.0
only we,2,2,1.0
we had,2,2,1.0
better counterfactual,2,2,1.0
the international,22,5,4.4
intelligence montreal,2,2,1.0
montreal canada,2,2,1.0
canada keane,2,2,1.0
b good,2,2,1.0
good counterfactuals,4,2,2.0
and how,3,3,1.0
them in,3,3,1.0
germany kotsiantis,2,2,1.0
kotsiantis supervised,2,2,1.0
supervised machine,2,2,1.0
learning a,5,3,1.6666666666666667
review of,8,5,1.6
of classification,4,3,1.3333333333333333
classification techniques,2,2,1.0
techniques informatica,2,2,1.0
informatica krawczyk,2,2,1.0
krawczyk koziarski,2,2,1.0
koziarski woźniak,2,2,1.0
woźniak oversampling,2,2,1.0
oversampling for,4,2,2.0
multiclass imbalanced,3,3,1.0
classification ieee,3,3,1.0
networks and,4,4,1.0
and learning,8,4,2.0
learning systems,5,4,1.25
systems kubat,2,2,1.0
kubat holte,4,4,1.0
holte matwin,2,2,1.0
matwin machine,3,3,1.0
learning for,9,5,1.8
the detection,3,3,1.0
images machine,2,2,1.0
learning lash,2,2,1.0
lash lin,2,2,1.0
lin street,2,2,1.0
street robinson,2,2,1.0
robinson ohlmann,2,2,1.0
ohlmann j,2,2,1.0
j generalized,2,2,1.0
generalized inverse,2,2,1.0
the siam,2,2,1.0
siam international,2,2,1.0
mining temraz,2,2,1.0
augmentation laugel,2,2,1.0
laugel lesot,2,2,1.0
lesot marsala,2,2,1.0
marsala renard,2,2,1.0
renard detyniecki,2,2,1.0
detyniecki the,2,2,1.0
the dangers,2,2,1.0
dangers of,2,2,1.0
of interpretability,2,2,1.0
interpretability unjustified,2,2,1.0
unjustified counterfactual,2,2,1.0
explanations proceedings,2,2,1.0
intelligence lewis,2,2,1.0
lewis counterfactuals,2,2,1.0
counterfactuals john,2,2,1.0
john wiley,3,3,1.0
wiley sons,3,3,1.0
sons ling,2,2,1.0
ling li,2,2,1.0
li data,2,2,1.0
for direct,2,2,1.0
direct marketing,2,2,1.0
marketing problems,2,2,1.0
and solutions,2,2,1.0
solutions in,3,3,1.0
the fourth,4,4,1.0
fourth international,6,3,2.0
mining kdd,2,2,1.0
kdd luengo,2,2,1.0
luengo fernandez,2,2,1.0
herrera addressing,2,2,1.0
addressing data,2,2,1.0
data complexity,2,2,1.0
complexity for,5,3,1.6666666666666667
sets analysis,2,2,1.0
of oversampling,3,3,1.0
oversampling and,5,3,1.6666666666666667
and evolutionally,1,1,1.0
evolutionally underdamping,1,1,1.0
underdamping soft,2,2,1.0
computing luo,2,2,1.0
luo liu,2,2,1.0
liu minority,2,2,1.0
imbalanced classification,7,3,2.3333333333333335
classification ma,2,2,1.0
ma fan,2,2,1.0
fan algorithm,2,2,1.0
algorithm and,4,2,2.0
and hybrid,2,2,1.0
hybrid algorithm,2,2,1.0
for feature,3,3,1.0
feature and,1,1,1.0
and parameter,2,2,1.0
parameter optimization,2,2,1.0
optimization based,2,2,1.0
on random,7,3,2.3333333333333335
forests bmc,2,2,1.0
bmc bioinformatics,2,2,1.0
bioinformatics maciejewski,2,2,1.0
maciejewski stefanowski,2,2,1.0
stefanowski j,2,2,1.0
j local,2,2,1.0
local neighbourhood,3,3,1.0
neighbourhood extension,2,2,1.0
for mining,2,2,1.0
mining imbalanced,4,3,1.3333333333333333
ieee symposium,2,2,1.0
symposium on,3,3,1.0
on computational,4,4,1.0
computational intelligence,12,5,2.4
intelligence and,5,4,1.25
mining cidm,2,2,1.0
cidm mckenna,2,2,1.0
mckenna smyth,2,2,1.0
b editing,2,2,1.0
editing techniques,2,2,1.0
in reasoning,2,2,1.0
reasoning mothilal,2,2,1.0
mothilal sharma,2,2,1.0
sharma tan,2,2,1.0
tan explaining,2,2,1.0
explaining machine,2,2,1.0
learning classifiers,3,3,1.0
classifiers through,2,2,1.0
through diverse,2,2,1.0
explanations conference,2,2,1.0
on fairness,2,2,1.0
fairness accountability,2,2,1.0
accountability and,2,2,1.0
and transparency,2,2,1.0
transparency fat,2,2,1.0
fat ng,2,2,1.0
ng hu,2,2,1.0
hu yeung,2,2,1.0
yeung yin,2,2,1.0
yin roli,2,2,1.0
roli diversified,2,2,1.0
diversified undersampling,2,2,1.0
undersampling for,5,3,1.6666666666666667
problems ieee,2,2,1.0
on cybernetics,2,2,1.0
cybernetics nguyen,2,2,1.0
nguyen cooper,2,2,1.0
cooper kamei,2,2,1.0
kamei borderline,3,3,1.0
borderline for,3,3,1.0
classification international,2,2,1.0
international journal,11,4,2.75
of knowledge,4,4,1.0
knowledge engineering,3,3,1.0
engineering and,3,3,1.0
soft data,3,3,1.0
data paradigms,2,2,1.0
paradigms nugent,2,2,1.0
nugent doyle,2,2,1.0
doyle cunningham,2,2,1.0
cunningham gaining,2,2,1.0
gaining insight,2,2,1.0
insight through,2,2,1.0
through explanation,2,2,1.0
explanation journal,2,2,1.0
of intelligent,2,2,1.0
intelligent information,2,2,1.0
systems pitis,2,2,1.0
pitis creager,2,2,1.0
creager garg,2,2,1.0
garg a,2,2,1.0
using locally,2,2,1.0
locally factored,2,2,1.0
factored dynamics,2,2,1.0
dynamics advances,2,2,1.0
in neural,3,3,1.0
neural information,8,4,2.0
information processing,8,4,2.0
processing systems,4,3,1.3333333333333333
systems temraz,2,2,1.0
augmentation provost,2,2,1.0
provost machine,2,2,1.0
sets aaai,2,2,1.0
aaai workshop,4,4,1.0
on imbalanced,7,3,2.3333333333333335
sets prusty,2,2,1.0
prusty jayanthi,2,2,1.0
jayanthi velusamy,2,2,1.0
velusamy a,2,2,1.0
a modification,3,3,1.0
modification to,2,2,1.0
for event,2,2,1.0
event classification,2,2,1.0
in sodium,2,2,1.0
sodium cooled,2,2,1.0
cooled fast,2,2,1.0
fast reactors,2,2,1.0
reactors progress,2,2,1.0
progress in,3,3,1.0
in nuclear,2,2,1.0
nuclear energy,2,2,1.0
energy ramentol,2,2,1.0
ramentol caballero,2,2,1.0
caballero bello,3,3,1.0
bello herrera,2,2,1.0
a hybrid,4,3,1.3333333333333333
hybrid preprocessing,2,2,1.0
preprocessing approach,2,2,1.0
and undersampling,3,3,1.0
for high,2,2,1.0
high imbalanced,2,2,1.0
imbalanced using,2,2,1.0
using smote,4,3,1.3333333333333333
and rough,2,2,1.0
sets theory,2,2,1.0
theory knowledge,2,2,1.0
and information,3,3,1.0
systems ryan,2,2,1.0
ryan gúeret,2,2,1.0
gúeret berry,2,2,1.0
berry corcoran,2,2,1.0
corcoran keane,2,2,1.0
keane mac,2,2,1.0
mac namee,2,2,1.0
namee b,2,2,1.0
b predicting,2,2,1.0
predicting illness,2,2,1.0
illness for,2,2,1.0
a sustainable,2,2,1.0
sustainable dairy,2,2,1.0
dairy agriculture,2,2,1.0
agriculture predicting,2,2,1.0
predicting and,2,2,1.0
and explaining,2,2,1.0
explaining the,2,2,1.0
the onset,2,2,1.0
onset of,2,2,1.0
of mastitis,2,2,1.0
in dairy,2,2,1.0
dairy cows,2,2,1.0
cows in,2,2,1.0
in workshop,2,2,1.0
on explainable,2,2,1.0
explainable agency,2,2,1.0
agency in,2,2,1.0
xai sandhan,2,2,1.0
sandhan choi,2,2,1.0
choi y,2,2,1.0
y handling,2,2,1.0
handling imbalanced,5,4,1.25
datasets by,2,2,1.0
by partially,2,2,1.0
partially guided,2,2,1.0
guided sampling,1,1,1.0
sampling for,3,3,1.0
for pattern,2,2,1.0
recognition international,2,2,1.0
on pattern,2,2,1.0
recognition schleich,2,2,1.0
schleich geng,2,2,1.0
geng zhang,2,2,1.0
zhang suciu,2,2,1.0
suciu geco,2,2,1.0
geco quality,2,2,1.0
quality counterfactual,2,2,1.0
in real,3,3,1.0
real time,2,2,1.0
time arxiv,2,2,1.0
arxiv preprint,6,2,3.0
preprint shorten,2,2,1.0
shorten khoshgoftaar,2,2,1.0
khoshgoftaar a,2,2,1.0
survey on,2,2,1.0
on image,2,2,1.0
image data,2,2,1.0
learning journal,3,3,1.0
of big,2,2,1.0
big data,11,3,3.6666666666666665
data smyth,2,2,1.0
smyth keane,4,2,2.0
keane a,2,2,1.0
a few,3,3,1.0
few good,2,2,1.0
counterfactuals generating,2,2,1.0
generating interpretable,2,2,1.0
interpretable plausible,2,2,1.0
explanations arxiv,2,2,1.0
preprint subbaswamy,2,2,1.0
subbaswamy saria,2,2,1.0
saria counterfactual,2,2,1.0
counterfactual normalization,2,2,1.0
normalization proactively,2,2,1.0
proactively addressing,2,2,1.0
addressing dataset,2,2,1.0
shift using,2,2,1.0
causal mechanisms,2,2,1.0
mechanisms in,2,2,1.0
on uncertainty,3,3,1.0
uncertainty in,3,3,1.0
in artificial,3,3,1.0
intelligence uai,2,2,1.0
uai tek,2,2,1.0
tek dempster,2,2,1.0
dempster kale,2,2,1.0
kale parasite,2,2,1.0
parasite detection,2,2,1.0
detection and,4,4,1.0
and identification,2,2,1.0
for automated,2,2,1.0
automated thin,2,2,1.0
thin blood,2,2,1.0
blood film,2,2,1.0
film malaria,2,2,1.0
malaria diagnosis,2,2,1.0
diagnosis computer,2,2,1.0
computer vision,5,4,1.25
vision and,3,3,1.0
and image,2,2,1.0
image understanding,2,2,1.0
understanding temraz,2,2,1.0
temraz kenny,2,2,1.0
kenny ruelle,2,2,1.0
ruelle shalloo,2,2,1.0
shalloo smyth,2,2,1.0
keane handling,2,2,1.0
handling climate,2,2,1.0
change using,2,2,1.0
counterfactuals using,2,2,1.0
predict crop,2,2,1.0
crop growth,2,2,1.0
in an,10,6,1.6666666666666667
an uncertain,2,2,1.0
uncertain climate,2,2,1.0
climate future,2,2,1.0
future in,2,2,1.0
germany torres,2,2,1.0
torres a,2,2,1.0
a deterministic,2,2,1.0
deterministic version,2,2,1.0
smote pattern,2,2,1.0
recognition temraz,2,2,1.0
augmentation vapnik,2,2,1.0
vapnik statistical,2,2,1.0
statistical learning,2,2,1.0
learning theory,4,3,1.3333333333333333
theory wiley,2,2,1.0
wiley wachter,2,2,1.0
wachter mittelstadt,2,2,1.0
mittelstadt russell,2,2,1.0
russell counterfactual,2,2,1.0
explanations without,2,2,1.0
without opening,2,2,1.0
opening the,2,2,1.0
the black,2,2,1.0
black box,2,2,1.0
box automated,2,2,1.0
decisions and,2,2,1.0
the gdpr,2,2,1.0
gdpr harvard,2,2,1.0
harvard journal,2,2,1.0
of law,2,2,1.0
law technology,2,2,1.0
technology wang,2,2,1.0
wang xu,2,2,1.0
xu wang,2,2,1.0
wang zhang,2,2,1.0
zhang j,2,2,1.0
j classification,2,2,1.0
of imbalanced,11,6,1.8333333333333333
by using,8,6,1.3333333333333333
smote algorithm,12,6,2.0
and locally,2,2,1.0
locally linear,2,2,1.0
linear embedding,2,2,1.0
embedding international,2,2,1.0
on signal,2,2,1.0
signal processing,3,3,1.0
processing weiss,2,2,1.0
weiss mccarthy,2,2,1.0
mccarthy zabar,2,2,1.0
zabar b,2,2,1.0
b learning,2,2,1.0
learning sampling,3,3,1.0
sampling which,2,2,1.0
is best,2,2,1.0
best for,2,2,1.0
handling unbalanced,2,2,1.0
unbalanced classes,2,2,1.0
classes with,3,3,1.0
with unequal,2,2,1.0
unequal error,2,2,1.0
error costs,3,3,1.0
costs in,3,3,1.0
mining dmin,2,2,1.0
dmin wen,2,2,1.0
wen sun,2,2,1.0
sun yang,2,2,1.0
yang song,2,2,1.0
song gao,2,2,1.0
gao xue,2,2,1.0
xue huan,2,2,1.0
huan x,2,2,1.0
x time,2,2,1.0
series data,2,2,1.0
survey arxiv,2,2,1.0
preprint wong,2,2,1.0
wong gatt,2,2,1.0
gatt stamatescu,2,2,1.0
stamatescu mcdonnell,2,2,1.0
mcdonnell understanding,2,2,1.0
understanding data,2,2,1.0
for classification,4,2,2.0
when to,2,2,1.0
to warp,2,2,1.0
warp in,2,2,1.0
on digital,2,2,1.0
digital image,2,2,1.0
image computing,2,2,1.0
computing techniques,2,2,1.0
techniques and,2,2,1.0
and applications,3,3,1.0
applications dicta,2,2,1.0
dicta ye,2,2,1.0
ye leake,2,2,1.0
leake jalali,2,2,1.0
jalali crandall,2,2,1.0
crandall learning,2,2,1.0
learning adaptations,2,2,1.0
adaptations for,2,2,1.0
classification a,3,3,1.0
a neural,3,3,1.0
neural network,6,3,2.0
network approach,2,2,1.0
approach in,3,3,1.0
germany yun,2,2,1.0
yun ha,2,2,1.0
ha lee,2,2,1.0
lee j,2,2,1.0
j automatic,2,2,1.0
automatic determination,2,2,1.0
determination of,2,2,1.0
of neighborhood,2,2,1.0
neighborhood size,2,2,1.0
size in,3,3,1.0
on ubiquitous,2,2,1.0
ubiquitous information,2,2,1.0
information management,2,2,1.0
management and,2,2,1.0
and communication,2,2,1.0
communication zeng,2,2,1.0
zeng li,2,2,1.0
li zhai,2,2,1.0
zhai zhang,2,2,1.0
zhang y,2,2,1.0
y counterfactual,2,2,1.0
generator in,2,2,1.0
on empirical,2,2,1.0
empirical methods,3,3,1.0
in natural,2,2,1.0
natural language,2,2,1.0
language processing,2,2,1.0
processing zheng,2,2,1.0
zheng wu,3,3,1.0
wu srihari,2,2,1.0
srihari feature,3,3,1.0
feature selection,20,4,5.0
selection for,5,3,1.6666666666666667
for text,4,3,1.3333333333333333
text categorization,4,3,1.3333333333333333
categorization on,3,3,1.0
data acm,3,3,1.0
acm sigkdd,8,3,2.6666666666666665
sigkdd explorations,15,4,3.75
of computational,8,1,8.0
intelligence systems,8,1,8.0
systems vol,2,2,1.0
vol pp,20,4,5.0
pp doi,1,1,1.0
doi https,1,1,1.0
https issn,1,1,1.0
issn eissn,1,1,1.0
eissn https,1,1,1.0
https distributed,1,1,1.0
distributed synthetic,1,1,1.0
technique sakshi,1,1,1.0
sakshi suman,1,1,1.0
suman research,1,1,1.0
research scholar,1,1,1.0
scholar ipu,1,1,1.0
ipu new,1,1,1.0
new delhi,2,1,2.0
delhi india,2,1,2.0
india associate,1,1,1.0
associate professor,1,1,1.0
professor msit,1,1,1.0
msit new,1,1,1.0
india a,1,1,1.0
a rt,1,1,1.0
rt i,1,1,1.0
i c,3,3,1.0
c l,2,2,1.0
l e,2,2,1.0
e i,7,2,3.5
i n,10,4,2.5
n f,1,1,1.0
f o,4,3,1.3333333333333333
o article,1,1,1.0
article history,1,1,1.0
history received,1,1,1.0
received may,1,1,1.0
may accepted,1,1,1.0
accepted jul,1,1,1.0
jul keywords,1,1,1.0
keywords smote,1,1,1.0
smote apache,1,1,1.0
apache spark,4,1,4.0
spark prediction,1,1,1.0
prediction machine,1,1,1.0
learning imbalanced,2,2,1.0
a b,4,2,2.0
b s,1,1,1.0
s t,5,4,1.25
t r,2,2,1.0
r ac,1,1,1.0
ac t,1,1,1.0
t real,1,1,1.0
real world,5,2,2.5
world problems,1,1,1.0
for prediction,1,1,1.0
prediction usually,1,1,1.0
usually try,1,1,1.0
try to,2,1,2.0
predict rare,1,1,1.0
rare occurrences,2,1,2.0
occurrences application,1,1,1.0
application of,7,2,3.5
of standard,1,1,1.0
standard classification,1,1,1.0
classification algorithm,1,1,1.0
algorithm is,8,4,2.0
is biased,1,1,1.0
biased toward,2,2,1.0
toward against,1,1,1.0
against these,1,1,1.0
these rare,2,1,2.0
rare events,4,1,4.0
events due,1,1,1.0
due to,17,4,4.25
to this,13,3,4.333333333333333
data imbalance,4,2,2.0
imbalance typicalapproaches,1,1,1.0
typicalapproaches to,1,1,1.0
solve this,2,1,2.0
imbalance involve,1,1,1.0
involve oversampling,1,1,1.0
oversampling these,1,1,1.0
events or,1,1,1.0
or under,1,1,1.0
under sampling,1,1,1.0
sampling the,3,2,1.5
majority occurring,1,1,1.0
occurring events,1,1,1.0
events synthetic,1,1,1.0
one technique,1,1,1.0
technique that,2,2,1.0
that addresses,1,1,1.0
addresses this,1,1,1.0
this class,3,3,1.0
imbalance effectively,1,1,1.0
effectively however,1,1,1.0
however the,2,2,1.0
the existing,6,2,3.0
existing implementations,1,1,1.0
implementations of,1,1,1.0
smote fail,1,1,1.0
fail when,1,1,1.0
data grows,1,1,1.0
grows and,1,1,1.0
and can,6,4,1.5
can t,2,1,2.0
t be,2,1,2.0
be stored,2,1,2.0
stored on,1,1,1.0
single machine,6,1,6.0
machine in,2,2,1.0
paper present,1,1,1.0
present our,1,1,1.0
our solution,1,1,1.0
to address,8,4,2.0
address the,7,4,1.75
the big,1,1,1.0
data challenge,1,1,1.0
challenge we,1,1,1.0
we provide,4,2,2.0
a distributed,5,1,5.0
distributed version,2,1,2.0
smote by,1,1,1.0
using scalable,2,1,2.0
scalable and,3,1,3.0
and with,1,1,1.0
this implementation,3,2,1.5
implementation of,10,3,3.3333333333333335
smote we,3,1,3.0
we were,1,1,1.0
were able,1,1,1.0
able to,4,3,1.3333333333333333
to oversample,1,1,1.0
the rare,1,1,1.0
events and,1,1,1.0
and achieve,1,1,1.0
achieve results,1,1,1.0
existing python,1,1,1.0
python version,5,1,5.0
the authors,4,4,1.0
authors published,1,1,1.0
published by,1,1,1.0
by atlantis,1,1,1.0
atlantis press,1,1,1.0
press sarl,1,1,1.0
sarl this,1,1,1.0
an open,1,1,1.0
open access,1,1,1.0
access article,1,1,1.0
article distributed,1,1,1.0
distributed under,1,1,1.0
the cc,1,1,1.0
cc license,1,1,1.0
license http,1,1,1.0
http introduction,1,1,1.0
introduction in,1,1,1.0
the aspect,1,1,1.0
aspect of,3,3,1.0
learning one,1,1,1.0
popular issues,1,1,1.0
issues involves,1,1,1.0
involves classification,1,1,1.0
in particular,5,4,1.25
particular it,1,1,1.0
is expected,1,1,1.0
expected that,1,1,1.0
sets containing,2,2,1.0
containing information,1,1,1.0
information for,1,1,1.0
for extraction,1,1,1.0
extraction constitute,1,1,1.0
constitute all,1,1,1.0
the required,1,1,1.0
required content,1,1,1.0
content through,1,1,1.0
through which,5,1,5.0
which relevant,1,1,1.0
relevant concepts,1,1,1.0
concepts can,1,1,1.0
learned especially,1,1,1.0
especially in,1,1,1.0
in relation,5,2,2.5
relation to,5,2,2.5
to certain,1,1,1.0
certain underlying,1,1,1.0
underlying generating,1,1,1.0
generating functions,1,1,1.0
functions given,1,1,1.0
a sification,1,1,1.0
sification problem,1,1,1.0
problem what,1,1,1.0
what remains,1,1,1.0
remains notable,3,1,3.0
notable is,1,1,1.0
is challenging,1,1,1.0
challenging to,1,1,1.0
the outcomes,3,1,3.0
outcomes some,1,1,1.0
these classification,1,1,1.0
problems include,1,1,1.0
include oil,1,1,1.0
spills fraud,1,1,1.0
and intrusion,2,2,1.0
intrusion detection,4,3,1.3333333333333333
detection a,1,1,1.0
a dilemma,1,1,1.0
dilemma that,1,1,1.0
that translates,1,1,1.0
translates into,1,1,1.0
a state,1,1,1.0
state of,2,1,2.0
in classes,1,1,1.0
classes it,4,3,1.3333333333333333
also worth,1,1,1.0
worth indicating,1,1,1.0
indicating that,3,2,1.5
the resultant,4,1,4.0
resultant state,1,1,1.0
of asymmetry,1,1,1.0
asymmetry could,1,1,1.0
be puzzled,1,1,1.0
puzzled due,1,1,1.0
the infrequent,1,1,1.0
infrequent nature,1,1,1.0
of such,4,2,2.0
such rare,1,1,1.0
events instead,1,1,1.0
instead the,1,1,1.0
the events,1,1,1.0
events emerge,1,1,1.0
emerge as,1,1,1.0
as exceptional,1,1,1.0
exceptional situations,1,1,1.0
situations that,3,1,3.0
are mostly,1,1,1.0
mostly neglected,1,1,1.0
neglected or,1,1,1.0
or unexplored,1,1,1.0
unexplored in,1,1,1.0
in some,10,3,3.3333333333333335
some cases,9,2,4.5
cases these,1,1,1.0
these events,1,1,1.0
events have,1,1,1.0
been treated,2,1,2.0
as extreme,1,1,1.0
values including,1,1,1.0
including outliers,1,1,1.0
outliers or,1,1,1.0
or noisy,1,1,1.0
noisy disturbance,1,1,1.0
disturbance it,1,1,1.0
also ironical,1,1,1.0
ironical that,1,1,1.0
classes attract,1,1,1.0
attract greater,1,1,1.0
greater concern,1,1,1.0
concern and,1,1,1.0
and much,1,1,1.0
much significance,1,1,1.0
significance and,2,1,2.0
and acknowledgment,1,1,1.0
acknowledgment are,1,1,1.0
are extended,1,1,1.0
extended to,3,1,3.0
them including,1,1,1.0
including cases,1,1,1.0
cases such,1,1,1.0
as those,1,1,1.0
those involving,1,1,1.0
the identification,1,1,1.0
identification of,2,2,1.0
of credit,1,1,1.0
credit card,3,2,1.5
card data,1,1,1.0
data breaches,1,1,1.0
breaches in,1,1,1.0
in online,1,1,1.0
online transactions,1,1,1.0
transactions as,1,1,1.0
such most,1,1,1.0
data experiences,1,1,1.0
experiences irregular,1,1,1.0
irregular or,1,1,1.0
or disproportionate,1,1,1.0
disproportionate observation,1,1,1.0
observation ratios,1,1,1.0
ratios in,1,1,1.0
in ferent,1,1,1.0
ferent classes,1,1,1.0
and end,1,1,1.0
end up,1,1,1.0
up yielding,1,1,1.0
yielding unacceptable,1,1,1.0
unacceptable classification,1,1,1.0
classification to,1,1,1.0
to fraud,1,1,1.0
fraud case,1,1,1.0
case identification,1,1,1.0
identification this,1,1,1.0
this trend,2,1,2.0
trend points,1,1,1.0
points to,2,1,2.0
the criticality,1,1,1.0
criticality of,1,1,1.0
a relevant,1,1,1.0
relevant model,1,1,1.0
model capable,1,1,1.0
capable of,3,2,1.5
of identifying,1,1,1.0
identifying the,2,1,2.0
classes or,2,2,1.0
or rare,1,1,1.0
occurrences in,1,1,1.0
in various,2,2,1.0
various sets,1,1,1.0
data associated,1,1,1.0
associated with,10,4,2.5
with higher,1,1,1.0
higher accuracy,1,1,1.0
accuracy as,1,1,1.0
as mentioned,2,2,1.0
mentioned earlier,1,1,1.0
earlier most,1,1,1.0
minority ples,3,2,1.5
ples have,1,1,1.0
have continually,1,1,1.0
continually been,1,1,1.0
and ended,1,1,1.0
ended up,1,1,1.0
up being,1,1,1.0
being ignored,1,1,1.0
ignored the,1,1,1.0
the eventuality,2,1,2.0
eventuality is,2,1,2.0
that valuable,1,1,1.0
valuable data,1,1,1.0
data has,2,2,1.0
been lost,1,1,1.0
lost relative,1,1,1.0
relative corresponding,1,1,1.0
corresponding author,2,2,1.0
author email,1,1,1.0
email sakshihoodars,1,1,1.0
sakshihoodars to,1,1,1.0
the affected,1,1,1.0
affected samples,1,1,1.0
samples an,1,1,1.0
an example,8,3,2.6666666666666665
example is,4,3,1.3333333333333333
a case,5,2,2.5
case in,4,3,1.3333333333333333
given set,2,1,2.0
data exhibits,1,1,1.0
exhibits an,2,2,1.0
an imbalanced,3,3,1.0
imbalanced ratio,8,3,2.6666666666666665
ratio in,2,2,1.0
the form,5,3,1.6666666666666667
form this,1,1,1.0
this ple,1,1,1.0
ple ratio,1,1,1.0
ratio implies,1,1,1.0
implies that,1,1,1.0
that training,1,1,1.0
training examples,6,3,2.0
a negative,1,1,1.0
class cide,1,1,1.0
cide with,1,1,1.0
with one,3,2,1.5
one positive,1,1,1.0
positive class,10,4,2.5
class training,1,1,1.0
training sample,2,2,1.0
sample when,1,1,1.0
is employed,6,3,2.0
employed it,1,1,1.0
it could,4,2,2.0
could generate,2,2,1.0
generate the,4,2,2.0
into negative,1,1,1.0
negative forms,1,1,1.0
forms to,1,1,1.0
to ensure,9,2,4.5
ensure that,7,1,7.0
the classification,7,2,3.5
classification rule,1,1,1.0
rule accuracy,1,1,1.0
accuracy is,3,2,1.5
is maximized,1,1,1.0
maximized yielding,1,1,1.0
yielding accuracy,1,1,1.0
accuracy notably,1,1,1.0
notably there,1,1,1.0
there exist,3,3,1.0
exist unique,1,1,1.0
unique categorizations,1,1,1.0
categorizations of,1,1,1.0
of solutions,2,2,1.0
solutions through,1,1,1.0
which problems,1,1,1.0
problems linked,1,1,1.0
linked to,2,1,2.0
to imbalanced,5,3,1.6666666666666667
classification could,1,1,1.0
be solved,2,2,1.0
solved one,1,1,1.0
such approaches,1,1,1.0
approaches entails,1,1,1.0
entails the,1,1,1.0
the learning,19,4,4.75
learning process,6,3,2.0
process ification,1,1,1.0
ification with,1,1,1.0
with class,3,2,1.5
class dispersion,1,1,1.0
dispersion on,1,1,1.0
on focus,2,1,2.0
focus this,1,1,1.0
process is,4,3,1.3333333333333333
is achieved,3,2,1.5
achieved by,2,2,1.0
by focusing,1,1,1.0
the undersampling,2,1,2.0
undersampling majority,1,1,1.0
class oversampling,1,1,1.0
oversampling minority,1,1,1.0
or both,2,2,1.0
both in,2,2,1.0
other situations,1,1,1.0
situations approaches,1,1,1.0
approaches can,1,1,1.0
be employed,1,1,1.0
employed to,3,2,1.5
the respective,8,2,4.0
respective classes,3,2,1.5
classes states,1,1,1.0
states of,1,1,1.0
of misclassification,3,2,1.5
misclassification with,1,1,1.0
with growth,1,1,1.0
the volume,2,2,1.0
volume of,3,1,3.0
data it,1,1,1.0
also important,2,1,2.0
the traditional,2,1,2.0
traditional methods,1,1,1.0
methods of,2,2,1.0
data preprocessing,2,1,2.0
preprocessing indeed,1,1,1.0
indeed this,2,1,2.0
approach also,1,1,1.0
also applies,2,2,1.0
applies to,1,1,1.0
to situations,2,1,2.0
situations where,4,1,4.0
where a,2,2,1.0
machine might,1,1,1.0
might not,2,1,2.0
not be,11,4,2.75
better placed,1,1,1.0
placed to,1,1,1.0
to process,1,1,1.0
process the,5,3,1.6666666666666667
for big,3,1,3.0
data case,1,1,1.0
case studies,1,1,1.0
studies the,1,1,1.0
the scalability,1,1,1.0
scalability issues,1,1,1.0
issues related,4,2,2.0
related with,1,1,1.0
with data,1,1,1.0
preprocessing need,1,1,1.0
be tackled,1,1,1.0
tackled appropriately,1,1,1.0
appropriately in,1,1,1.0
to formulate,1,1,1.0
formulate novel,1,1,1.0
novel solutions,1,1,1.0
solutions or,1,1,1.0
or accommodate,1,1,1.0
accommodate existing,1,1,1.0
existing ones,1,1,1.0
ones data,1,1,1.0
for smaller,1,1,1.0
smaller class,1,1,1.0
class minority,3,3,1.0
minority that,1,1,1.0
be produced,1,1,1.0
produced artificially,1,1,1.0
artificially with,1,1,1.0
the assistance,1,1,1.0
assistance of,1,1,1.0
the thetic,1,1,1.0
thetic minority,1,1,1.0
smote indeed,1,1,1.0
approach refers,1,1,1.0
refers plays,1,1,1.0
plays a,1,1,1.0
role of,4,1,4.0
of minimizing,1,1,1.0
minimizing class,1,1,1.0
or data,1,1,1.0
data sample,2,2,1.0
sample at,1,1,1.0
at present,1,1,1.0
present nondistributed,1,1,1.0
nondistributed ronment,1,1,1.0
ronment is,1,1,1.0
is making,1,1,1.0
making use,1,1,1.0
of availability,1,1,1.0
smote although,1,1,1.0
main confront,1,1,1.0
confront arises,1,1,1.0
arises when,1,1,1.0
have necessity,1,1,1.0
necessity to,1,1,1.0
to yield,4,3,1.3333333333333333
yield the,2,2,1.0
for large,2,2,1.0
large minority,1,1,1.0
class artificially,1,1,1.0
artificially using,1,1,1.0
smote data,1,1,1.0
for minority,8,3,2.6666666666666665
is generated,4,3,1.3333333333333333
generated through,1,1,1.0
through smote,2,1,2.0
smote with,3,2,1.5
the help,4,1,4.0
help of,4,1,4.0
of interpolation,1,1,1.0
interpolation of,2,2,1.0
data hooda,1,1,1.0
hooda and,7,1,7.0
and mann,7,1,7.0
mann international,7,1,7.0
systems instances,1,1,1.0
are related,2,2,1.0
minority as,1,1,1.0
as well,13,4,3.25
well as,7,2,3.5
as which,1,1,1.0
are closure,1,1,1.0
closure with,1,1,1.0
with each,3,3,1.0
each other,3,2,1.5
other there,1,1,1.0
are issues,1,1,1.0
to distributed,1,1,1.0
distributed smote,8,1,8.0
smote as,2,2,1.0
well one,1,1,1.0
these issues,2,2,1.0
issues is,1,1,1.0
the appropriate,1,1,1.0
appropriate management,1,1,1.0
management of,2,1,2.0
be distributed,1,1,1.0
distributed among,1,1,1.0
among a,1,1,1.0
a cluster,7,1,7.0
cluster of,4,1,4.0
of machines,2,1,2.0
machines for,1,1,1.0
for ing,1,1,1.0
ing to,5,2,2.5
ensure effective,1,1,1.0
effective data,1,1,1.0
data generation,24,3,8.0
generation via,1,1,1.0
via the,3,2,1.5
smote technique,2,2,1.0
technique it,2,1,2.0
it becomes,1,1,1.0
becomes imperative,1,1,1.0
imperative to,1,1,1.0
to preserve,2,2,1.0
preserve sample,1,1,1.0
sample spatial,1,1,1.0
spatial arrangement,1,1,1.0
arrangement propriate,1,1,1.0
propriate management,1,1,1.0
management may,1,1,1.0
may result,1,1,1.0
in inconsistent,1,1,1.0
inconsistent dissemination,1,1,1.0
dissemination of,1,1,1.0
data among,1,1,1.0
the nodes,1,1,1.0
nodes cluster,1,1,1.0
cluster therefore,1,1,1.0
therefore may,1,1,1.0
may affect,1,1,1.0
affect the,1,1,1.0
sampling and,4,3,1.3333333333333333
and consequently,1,1,1.0
consequently affecting,1,1,1.0
affecting the,1,1,1.0
the efficiency,1,1,1.0
efficiency of,1,1,1.0
to tackle,1,1,1.0
tackle the,1,1,1.0
problem the,11,4,2.75
the solution,5,2,2.5
solution should,1,1,1.0
be efficient,1,1,1.0
efficient scalable,1,1,1.0
and parallel,2,1,2.0
parallel we,1,1,1.0
we introduce,1,1,1.0
introduce an,1,1,1.0
an algorithm,4,2,2.0
algorithm that,4,2,2.0
that utilizes,2,2,1.0
utilizes scalable,1,1,1.0
to synthetically,1,1,1.0
synthetically generate,1,1,1.0
minority data,11,3,3.6666666666666665
we use,7,3,2.3333333333333335
use apache,1,1,1.0
spark for,1,1,1.0
for implementing,1,1,1.0
implementing the,1,1,1.0
the distributed,2,1,2.0
we believe,2,2,1.0
believe that,2,2,1.0
that distributed,1,1,1.0
an active,2,2,1.0
active area,1,1,1.0
area of,1,1,1.0
research our,1,1,1.0
our research,1,1,1.0
research can,1,1,1.0
be extended,2,1,2.0
extended further,1,1,1.0
further toward,1,1,1.0
toward developing,1,1,1.0
developing an,2,2,1.0
an effective,1,1,1.0
effective and,1,1,1.0
and efficient,2,1,2.0
efficient algorithm,1,1,1.0
algorithm background,1,1,1.0
background the,1,1,1.0
classification is,3,2,1.5
is termed,1,1,1.0
termed imbalanced,1,1,1.0
class samples,1,1,1.0
samples are,7,2,3.5
are far,2,2,1.0
far too,1,1,1.0
too few,1,1,1.0
few compared,1,1,1.0
negative samples,1,1,1.0
samples given,1,1,1.0
given negative,1,1,1.0
negative and,3,2,1.5
and positive,3,2,1.5
positive samples,1,1,1.0
the mance,2,2,1.0
mance on,1,1,1.0
latter overwhelms,1,1,1.0
overwhelms that,1,1,1.0
that which,2,2,1.0
is felt,1,1,1.0
felt on,1,1,1.0
former type,1,1,1.0
type of,3,3,1.0
of sample,3,1,3.0
sample in,3,1,3.0
in imbalance,1,1,1.0
classification issues,1,1,1.0
issues we,1,1,1.0
we may,4,2,2.0
may neglect,1,1,1.0
neglect the,1,1,1.0
the asymmetry,1,1,1.0
asymmetry of,1,1,1.0
data across,1,1,1.0
across a,3,2,1.5
single class,1,1,1.0
class occasionally,1,1,1.0
occasionally and,1,1,1.0
this asymmetry,1,1,1.0
asymmetry within,1,1,1.0
within a,4,4,1.0
is cited,1,1,1.0
cited for,1,1,1.0
for small,1,1,1.0
small disjunct,1,1,1.0
disjunct these,1,1,1.0
these small,1,1,1.0
disjuncts can,1,1,1.0
be influenced,1,1,1.0
influenced through,1,1,1.0
through the,5,2,2.5
undersampling of,2,1,2.0
may have,3,2,1.5
to confront,1,1,1.0
confront the,1,1,1.0
the possible,1,1,1.0
possible downfall,1,1,1.0
downfall of,1,1,1.0
of essential,1,1,1.0
essential information,1,1,1.0
information through,1,1,1.0
through this,1,1,1.0
this undersampling,1,1,1.0
class analogously,1,1,1.0
analogously we,1,1,1.0
to face,1,1,1.0
face the,1,1,1.0
the issues,3,1,3.0
issues of,3,2,1.5
data tion,4,2,2.0
tion in,3,2,1.5
in oversampling,1,1,1.0
oversampling which,2,1,2.0
which will,7,3,2.3333333333333335
will enhance,1,1,1.0
enhance the,1,1,1.0
the count,1,1,1.0
count of,1,1,1.0
of illustrations,1,1,1.0
illustrations excluding,1,1,1.0
excluding the,1,1,1.0
the addition,1,1,1.0
addition of,1,1,1.0
of any,3,2,1.5
any value,1,1,1.0
value to,2,2,1.0
as any,1,1,1.0
any tion,1,1,1.0
tion related,1,1,1.0
the chances,1,1,1.0
chances of,1,1,1.0
of over,2,2,1.0
over fitting,1,1,1.0
fitting can,1,1,1.0
can additionally,1,1,1.0
additionally be,1,1,1.0
be enhanced,1,1,1.0
enhanced by,1,1,1.0
which can,6,3,2.0
can reinforce,1,1,1.0
reinforce all,1,1,1.0
all minority,2,2,1.0
minority ters,1,1,1.0
ters without,1,1,1.0
without taking,1,1,1.0
taking into,1,1,1.0
account their,1,1,1.0
their tangible,1,1,1.0
tangible participation,1,1,1.0
participation to,1,1,1.0
problem lot,1,1,1.0
lot of,2,2,1.0
of exploration,1,1,1.0
exploration has,1,1,1.0
been going,1,1,1.0
going to,1,1,1.0
solution for,2,1,2.0
a disseminated,1,1,1.0
disseminated distributed,1,1,1.0
distributed environment,1,1,1.0
environment the,1,1,1.0
the difficulties,1,1,1.0
difficulties faced,1,1,1.0
faced in,1,1,1.0
in classifying,1,1,1.0
classifying imbalanced,1,1,1.0
the concerns,1,1,1.0
concerns related,1,1,1.0
of imitated,1,1,1.0
imitated or,1,1,1.0
or we,2,1,2.0
can say,2,1,2.0
say synthetic,1,1,1.0
problems which,2,1,2.0
are extremely,1,1,1.0
extremely imbalanced,1,1,1.0
imbalanced is,1,1,1.0
is examined,1,1,1.0
examined by,1,1,1.0
by chawla,2,1,2.0
chawla applying,1,1,1.0
smote requires,1,1,1.0
requires us,1,1,1.0
us to,3,3,1.0
find neighbors,2,1,2.0
neighbors when,1,1,1.0
is big,1,1,1.0
big the,1,1,1.0
the cost,9,4,2.25
cost of,9,4,2.25
of computing,2,2,1.0
neighbors increases,1,1,1.0
increases for,1,1,1.0
for problems,2,1,2.0
problems where,3,2,1.5
is huge,2,1,2.0
huge and,1,1,1.0
and distributed,1,1,1.0
distributed one,1,1,1.0
one approach,2,2,1.0
approach would,1,1,1.0
be to,1,1,1.0
to group,1,1,1.0
group the,1,1,1.0
the samples,12,1,12.0
samples into,1,1,1.0
into groups,1,1,1.0
groups and,1,1,1.0
and move,1,1,1.0
move these,1,1,1.0
groups on,1,1,1.0
different machines,2,1,2.0
machines where,2,2,1.0
where can,1,1,1.0
be achieved,1,1,1.0
achieved lsh,1,1,1.0
lsh is,1,1,1.0
another technique,1,1,1.0
for distributed,1,1,1.0
distributed knn,2,1,2.0
knn search,3,1,3.0
search in,4,1,4.0
in high,3,2,1.5
high dimensions,2,1,2.0
dimensions clustering,1,1,1.0
clustering problems,1,1,1.0
problems have,1,1,1.0
been one,1,1,1.0
most commonly,1,1,1.0
commonly researched,1,1,1.0
researched problems,1,1,1.0
problems more,1,1,1.0
more details,1,1,1.0
details on,2,2,1.0
on various,2,2,1.0
various clustering,1,1,1.0
clustering algorithms,2,1,2.0
algorithms can,1,1,1.0
found in,4,3,1.3333333333333333
the common,2,2,1.0
common clustering,1,1,1.0
algorithms involves,1,1,1.0
involves the,4,1,4.0
the simple,1,1,1.0
simple nature,1,1,1.0
algorithm accounts,1,1,1.0
accounts for,1,1,1.0
its popularity,1,1,1.0
popularity in,1,1,1.0
in recent,7,3,2.3333333333333335
recent scholarly,1,1,1.0
scholarly investigations,1,1,1.0
investigations much,1,1,1.0
much tion,1,1,1.0
tion has,1,1,1.0
been extended,1,1,1.0
to initialization,1,1,1.0
initialization improvement,1,1,1.0
improvement the,1,1,1.0
the need,1,1,1.0
need for,2,1,2.0
better convergence,1,1,1.0
convergence has,1,1,1.0
been attributed,1,1,1.0
attributed to,3,2,1.5
trend given,1,1,1.0
given better,1,1,1.0
better initialization,1,1,1.0
initialization a,1,1,1.0
a resultant,1,1,1.0
resultant merit,1,1,1.0
merit is,1,1,1.0
it yields,1,1,1.0
yields cant,1,1,1.0
cant improvements,1,1,1.0
improvements in,4,2,2.0
algorithm s,1,1,1.0
s running,1,1,1.0
running time,1,1,1.0
distributed setup,1,1,1.0
setup it,1,1,1.0
notable that,2,1,2.0
that there,12,4,3.0
also increasing,1,1,1.0
increasing attention,1,1,1.0
attention and,1,1,1.0
and effort,1,1,1.0
effort toward,1,1,1.0
toward understanding,1,1,1.0
understanding the,1,1,1.0
the running,1,1,1.0
running of,1,1,1.0
algorithm the,6,3,2.0
the role,2,1,2.0
the scalable,3,1,3.0
scalable lies,1,1,1.0
lies in,2,1,2.0
in center,1,1,1.0
center ization,1,1,1.0
ization having,1,1,1.0
having evolved,1,1,1.0
evolved in,1,1,1.0
a form,1,1,1.0
form that,1,1,1.0
that supports,1,1,1.0
supports parallel,1,1,1.0
parallel mentation,1,1,1.0
mentation given,1,1,1.0
given which,1,1,1.0
is inherently,1,1,1.0
inherently sequential,1,1,1.0
sequential its,1,1,1.0
its efficient,1,1,1.0
efficient and,1,1,1.0
parallel version,1,1,1.0
version involves,1,1,1.0
scalable able,1,1,1.0
able sampleso,1,1,1.0
sampleso m,1,1,1.0
m points,1,1,1.0
points in,8,3,2.6666666666666665
each round,2,2,1.0
round and,1,1,1.0
and repeats,1,1,1.0
repeats the,1,1,1.0
the process,9,3,3.0
for approximatelyo,1,1,1.0
approximatelyo logn,1,1,1.0
logn rounds,1,1,1.0
rounds instead,1,1,1.0
of sampling,4,4,1.0
sampling a,2,2,1.0
single point,1,1,1.0
point in,3,2,1.5
each pass,1,1,1.0
pass as,1,1,1.0
as used,1,1,1.0
in algorithm,2,2,1.0
algorithm so,2,2,1.0
so o,1,1,1.0
o mlogn,1,1,1.0
mlogn are,1,1,1.0
are sampled,1,1,1.0
sampled which,1,1,1.0
typically more,1,1,1.0
the m,3,3,1.0
m clusters,3,1,3.0
clusters which,1,1,1.0
are then,2,2,1.0
then clustered,1,1,1.0
clustered into,1,1,1.0
into m,2,1,2.0
m initial,1,1,1.0
initial centers,1,1,1.0
centers for,1,1,1.0
for llyod,1,1,1.0
llyod s,1,1,1.0
s iteration,1,1,1.0
iteration since,1,1,1.0
since the,11,4,2.75
sample is,2,1,2.0
is smaller,1,1,1.0
smaller than,2,2,1.0
the input,23,2,11.5
input size,1,1,1.0
size the,1,1,1.0
the can,1,1,1.0
be done,1,1,1.0
done quickly,1,1,1.0
quickly once,1,1,1.0
once the,2,2,1.0
are tered,1,1,1.0
tered into,1,1,1.0
clusters each,1,1,1.0
these clusters,1,1,1.0
clusters can,1,1,1.0
can use,3,3,1.0
use any,1,1,1.0
the widely,1,1,1.0
widely available,1,1,1.0
available algorithms,1,1,1.0
algorithms to,6,3,2.0
neighbors we,1,1,1.0
used space,1,1,1.0
space indexing,1,1,1.0
indexing algorithms,1,1,1.0
to index,2,1,2.0
index the,2,1,2.0
samples and,1,1,1.0
then search,1,1,1.0
search for,4,2,2.0
for neighbors,1,1,1.0
neighbors whenever,1,1,1.0
whenever the,1,1,1.0
the dimensionality,9,2,4.5
dimensionality is,1,1,1.0
is small,3,2,1.5
small we,1,1,1.0
we take,1,1,1.0
take advantage,3,3,1.0
of space,1,1,1.0
space partitioning,1,1,1.0
partitioning approaches,1,1,1.0
approaches that,2,2,1.0
that includes,1,1,1.0
includes methods,1,1,1.0
methods alike,1,1,1.0
alike trees,1,1,1.0
trees which,1,1,1.0
is capable,1,1,1.0
of performing,1,1,1.0
performing extremely,1,1,1.0
extremely better,1,1,1.0
that case,1,1,1.0
case the,6,2,3.0
the apportioning,1,1,1.0
apportioning of,1,1,1.0
the points,5,3,1.6666666666666667
points results,1,1,1.0
in disjoint,1,1,1.0
disjoint sets,1,1,1.0
sets a,3,3,1.0
process achieved,1,1,1.0
achieved through,1,1,1.0
through titioning,1,1,1.0
titioning in,1,1,1.0
target cell,1,1,1.0
cell all,1,1,1.0
all points,1,1,1.0
points are,4,2,2.0
are considered,2,2,1.0
considered nearest,1,1,1.0
nearest to,1,1,1.0
its to,1,1,1.0
other pivots,1,1,1.0
pivots regarding,1,1,1.0
regarding the,2,2,1.0
the indexing,1,1,1.0
indexing and,2,1,2.0
and searching,1,1,1.0
searching of,1,1,1.0
of metric,1,1,1.0
metric spaces,3,1,3.0
spaces to,1,1,1.0
to discern,1,1,1.0
discern similarity,1,1,1.0
similarity another,1,1,1.0
another efficient,1,1,1.0
efficient method,1,1,1.0
method entails,1,1,1.0
entails this,1,1,1.0
this technique,5,3,1.6666666666666667
technique has,1,1,1.0
been mented,1,1,1.0
mented to,1,1,1.0
be the,13,3,4.333333333333333
most efficient,2,1,2.0
efficient if,1,1,1.0
the situation,1,1,1.0
situation on,1,1,1.0
focus entails,1,1,1.0
entails a,1,1,1.0
a large,6,2,3.0
large number,4,2,2.0
of records,1,1,1.0
records or,1,1,1.0
or numbers,1,1,1.0
of dimensions,3,2,1.5
dimensions indeed,1,1,1.0
indeed as,1,1,1.0
technique of,1,1,1.0
of object,1,1,1.0
object indexing,1,1,1.0
indexing seeks,1,1,1.0
seeks to,3,1,3.0
support similarity,1,1,1.0
similarity queries,1,1,1.0
queries by,1,1,1.0
by comparing,1,1,1.0
comparing features,1,1,1.0
features based,1,1,1.0
the parameter,4,3,1.3333333333333333
parameter of,3,2,1.5
of distance,1,1,1.0
distance given,1,1,1.0
given m,1,1,1.0
m the,3,2,1.5
the metric,4,2,2.0
metric space,4,1,4.0
space is,11,3,3.6666666666666665
is established,2,2,1.0
established in,1,1,1.0
form m,1,1,1.0
m o,1,1,1.0
o d,1,1,1.0
d in,2,2,1.0
this case,23,4,5.75
case d,1,1,1.0
d represents,1,1,1.0
represents the,13,4,3.25
the distance,12,4,3.0
distance function,2,2,1.0
function while,1,1,1.0
while o,1,1,1.0
o refers,1,1,1.0
refers to,2,1,2.0
values domain,1,1,1.0
domain d,1,1,1.0
d ox,5,1,5.0
ox oy,4,1,4.0
oy d,3,1,3.0
d oy,1,1,1.0
oy ox,2,1,2.0
ox d,2,1,2.0
ox ox,1,1,1.0
ox oz,1,1,1.0
oz d,1,1,1.0
d oz,1,1,1.0
oz oy,2,1,2.0
oy in,1,1,1.0
in o,1,1,1.0
o the,2,2,1.0
the objects,3,2,1.5
objects are,2,1,2.0
are represented,1,1,1.0
represented by,4,3,1.3333333333333333
by oz,1,1,1.0
oy and,1,1,1.0
and ox,1,1,1.0
ox to,1,1,1.0
determine any,1,1,1.0
any similarity,1,1,1.0
similarity or,1,1,1.0
or dissimilarity,1,1,1.0
dissimilarity between,1,1,1.0
between objects,1,1,1.0
objects d,1,1,1.0
d is,2,2,1.0
the informative,2,2,1.0
informative parameter,1,1,1.0
parameter for,2,2,1.0
the entries,3,2,1.5
entries that,1,1,1.0
stored can,1,1,1.0
can go,1,1,1.0
go up,1,1,1.0
up to,3,3,1.0
to l,1,1,1.0
l hence,1,1,1.0
hence l,1,1,1.0
l represents,1,1,1.0
the node,1,1,1.0
node capacity,1,1,1.0
capacity it,1,1,1.0
to highlight,1,1,1.0
highlight that,1,1,1.0
that exhibits,2,2,1.0
exhibits two,1,1,1.0
two internal,1,1,1.0
internal node,1,1,1.0
node types,1,1,1.0
types these,1,1,1.0
these nodes,1,1,1.0
nodes include,1,1,1.0
include the,5,2,2.5
the leaf,3,1,3.0
leaf nodes,5,1,5.0
nodes and,1,1,1.0
the routing,3,1,3.0
routing nodes,2,1,2.0
nodes poised,1,1,1.0
poised to,1,1,1.0
to represent,1,1,1.0
represent data,1,1,1.0
or samples,1,1,1.0
samples indeed,2,1,2.0
indeed tions,1,1,1.0
tions through,1,1,1.0
through sample,1,1,1.0
sample organization,1,1,1.0
organization to,1,1,1.0
that nodes,1,1,1.0
nodes are,1,1,1.0
are achieved,1,1,1.0
achieved notably,1,1,1.0
notably the,2,1,2.0
resultant nodes,1,1,1.0
nodes correspond,1,1,1.0
correspond to,4,2,2.0
space region,1,1,1.0
region grows,1,1,1.0
grows in,1,1,1.0
a bottom,1,1,1.0
bottom up,1,1,1.0
up fashion,1,1,1.0
fashion when,1,1,1.0
of leaf,2,1,2.0
nodes grows,1,1,1.0
grows beyond,1,1,1.0
beyond the,2,2,1.0
the capacity,5,3,1.6666666666666667
capacity of,4,1,4.0
routing node,4,1,4.0
node a,2,1,2.0
new node,1,1,1.0
node from,1,1,1.0
the pool,1,1,1.0
pool is,1,1,1.0
is promoted,1,1,1.0
promoted as,1,1,1.0
a routing,1,1,1.0
node and,1,1,1.0
the tree,2,1,2.0
tree is,1,1,1.0
is split,1,1,1.0
split and,1,1,1.0
and nodes,1,1,1.0
nodes partitioned,1,1,1.0
partitioned between,1,1,1.0
the two,10,4,2.5
two routing,1,1,1.0
nodes based,1,1,1.0
the partitioning,1,1,1.0
partitioning policy,1,1,1.0
policy hooda,1,1,1.0
systems provides,1,1,1.0
provides a,5,3,1.6666666666666667
very fast,1,1,1.0
fast and,2,2,1.0
efficient nearest,1,1,1.0
neighbor search,4,2,2.0
search search,1,1,1.0
search is,1,1,1.0
the concept,1,1,1.0
concept that,1,1,1.0
if a,2,2,1.0
a sample,6,3,2.0
sample q,1,1,1.0
q is,2,2,1.0
at a,4,1,4.0
a distance,3,2,1.5
distance of,7,3,2.3333333333333335
of d,2,1,2.0
d or,6,1,6.0
or q,3,1,3.0
q and,5,2,2.5
and another,1,1,1.0
another sample,2,1,2.0
sample p,5,1,5.0
p is,3,2,1.5
or p,3,1,3.0
p then,2,2,1.0
distance between,8,4,2.0
between d,1,1,1.0
d q,2,1,2.0
q p,4,2,2.0
p will,1,1,1.0
will always,1,1,1.0
always be,1,1,1.0
be greater,1,1,1.0
greater than,4,2,2.0
than d,1,1,1.0
q d,2,1,2.0
p this,2,1,2.0
this follows,1,1,1.0
follows from,1,1,1.0
the triangle,3,2,1.5
triangle inequality,1,1,1.0
inequality which,1,1,1.0
which states,1,1,1.0
states that,1,1,1.0
that d,2,2,1.0
p d,1,1,1.0
this simple,1,1,1.0
simple inequality,1,1,1.0
inequality optimizes,1,1,1.0
optimizes the,1,1,1.0
the search,1,1,1.0
nearest bors,1,1,1.0
bors of,1,1,1.0
each sample,6,1,6.0
sample as,2,1,2.0
as all,1,1,1.0
all these,5,3,1.6666666666666667
these points,1,1,1.0
are organized,1,1,1.0
organized in,1,1,1.0
a metric,3,3,1.0
space imperatively,1,1,1.0
imperatively these,1,1,1.0
methods are,4,3,1.3333333333333333
are meant,1,1,1.0
meant to,1,1,1.0
to operate,1,1,1.0
operate on,2,2,1.0
on single,1,1,1.0
single machines,1,1,1.0
machines as,1,1,1.0
such the,1,1,1.0
techniques become,1,1,1.0
become impractical,1,1,1.0
impractical and,1,1,1.0
and cient,1,1,1.0
cient when,1,1,1.0
given sample,1,1,1.0
sample involves,1,1,1.0
involves big,1,1,1.0
the ous,1,1,1.0
ous investigations,1,1,1.0
investigations it,1,1,1.0
it was,7,3,2.3333333333333335
was documented,1,1,1.0
documented that,1,1,1.0
a combination,3,2,1.5
of spill,1,1,1.0
spill trees,2,1,2.0
trees and,4,2,2.0
and scales,2,2,1.0
scales in,1,1,1.0
case involving,1,1,1.0
involving distributed,1,1,1.0
distributed sample,1,1,1.0
sample space,1,1,1.0
space ensuring,1,1,1.0
ensuring further,1,1,1.0
further that,2,1,2.0
that similar,1,1,1.0
similar samples,2,1,2.0
are searched,2,1,2.0
searched efficiently,1,1,1.0
efficiently in,1,1,1.0
current investigation,1,1,1.0
investigation the,2,1,2.0
the central,1,1,1.0
central purpose,1,1,1.0
purpose is,1,1,1.0
to present,1,1,1.0
present a,3,3,1.0
hybrid technique,1,1,1.0
technique through,1,1,1.0
which artificial,1,1,1.0
artificial minority,1,1,1.0
data could,2,1,2.0
generated using,3,3,1.0
scalable similar,1,1,1.0
are clustered,2,1,2.0
clustered together,2,2,1.0
together also,1,1,1.0
also are,1,1,1.0
samples before,1,1,1.0
before ing,1,1,1.0
ing for,1,1,1.0
the perceived,2,1,2.0
perceived samples,1,1,1.0
samples performance,1,1,1.0
performance measures,1,1,1.0
measures as,2,2,1.0
minority smaller,1,1,1.0
smaller data,1,1,1.0
is devastated,1,1,1.0
devastated by,1,1,1.0
majority data,3,2,1.5
data present,1,1,1.0
present for,1,1,1.0
for this,14,4,3.5
this reason,2,2,1.0
reason the,1,1,1.0
classification which,1,1,1.0
which was,3,2,1.5
was described,1,1,1.0
described previously,1,1,1.0
previously can,1,1,1.0
be contrasted,1,1,1.0
contrasted through,1,1,1.0
of accuracy,3,2,1.5
accuracy metrics,1,1,1.0
metrics for,5,2,2.5
for illustration,1,1,1.0
illustration the,1,1,1.0
the accuracy,7,4,1.75
prediction can,1,1,1.0
can simply,1,1,1.0
simply be,1,1,1.0
be taken,1,1,1.0
taken as,3,2,1.5
as accurate,1,1,1.0
accurate in,1,1,1.0
in case,1,1,1.0
case a,1,1,1.0
a provided,1,1,1.0
provided dataset,1,1,1.0
dataset has,2,2,1.0
a distribution,4,3,1.3333333333333333
distribution by,1,1,1.0
by means,3,3,1.0
means of,3,3,1.0
of merely,1,1,1.0
merely predicting,1,1,1.0
predicting entire,1,1,1.0
entire data,1,1,1.0
class table,2,2,1.0
table is,1,1,1.0
is being,2,1,2.0
being implemented,1,1,1.0
implemented based,1,1,1.0
based upon,1,1,1.0
upon plete,1,1,1.0
plete confusion,1,1,1.0
matrix and,5,3,1.6666666666666667
that confusion,1,1,1.0
matrix is,2,2,1.0
is essential,1,1,1.0
essential for,1,1,1.0
classification relative,1,1,1.0
matrix it,2,2,1.0
is evident,1,1,1.0
evident that,1,1,1.0
performance categorization,1,1,1.0
categorization for,1,1,1.0
for negative,2,2,1.0
positive classes,2,2,1.0
classes can,2,2,1.0
be discerned,1,1,1.0
discerned the,1,1,1.0
the tpr,2,1,2.0
tpr recall,1,1,1.0
recall or,1,1,1.0
or true,1,1,1.0
positive becomes,1,1,1.0
becomes tpr,1,1,1.0
tpr tp,1,1,1.0
tp fn,3,1,3.0
fn the,1,1,1.0
resultant value,1,1,1.0
value obtained,2,2,1.0
obtained from,7,3,2.3333333333333335
the equation,1,1,1.0
equation above,1,1,1.0
above represents,1,1,1.0
the percentage,5,2,2.5
percentage of,5,2,2.5
of positive,4,2,2.0
positive situations,2,1,2.0
that have,2,1,2.0
been categorized,2,1,2.0
categorized accurately,2,1,2.0
accurately regarding,1,1,1.0
regarding fpr,1,1,1.0
fpr the,2,1,2.0
the rate,3,2,1.5
rate it,2,1,2.0
of negative,1,1,1.0
negative situations,1,1,1.0
not been,1,1,1.0
accurately it,1,1,1.0
is expressed,3,1,3.0
expressed as,3,1,3.0
as fnr,2,1,2.0
fnr tp,2,1,2.0
fn table,1,1,1.0
sample problem,1,1,1.0
problem actual,1,1,1.0
class predicted,1,1,1.0
class positive,1,1,1.0
positive negative,2,2,1.0
negative negative,2,2,1.0
class fp,2,2,1.0
fp false,2,2,1.0
positive tn,2,2,1.0
tn true,2,2,1.0
negative positive,1,1,1.0
class tp,1,1,1.0
positive fn,2,2,1.0
fn false,2,2,1.0
negative lastly,1,1,1.0
lastly the,1,1,1.0
the fnr,1,1,1.0
fnr which,1,1,1.0
which indicates,1,1,1.0
indicates the,1,1,1.0
it refers,1,1,1.0
there has,1,1,1.0
been priate,1,1,1.0
priate categorization,1,1,1.0
categorization the,1,1,1.0
parameter is,1,1,1.0
fn combining,1,1,1.0
combining the,3,3,1.0
the fpr,1,1,1.0
fpr and,1,1,1.0
tpr in,1,1,1.0
one metric,1,1,1.0
metric calls,1,1,1.0
calls for,1,1,1.0
the ting,1,1,1.0
ting of,1,1,1.0
of tpr,1,1,1.0
tpr against,1,1,1.0
against fpr,1,1,1.0
resultant curve,1,1,1.0
curve becomes,2,1,2.0
becomes the,2,1,2.0
roc receiver,1,1,1.0
characteristic the,1,1,1.0
the curve,2,2,1.0
curve the,1,1,1.0
the auc,9,2,4.5
auc other,1,1,1.0
other common,1,1,1.0
common metrics,1,1,1.0
metrics through,1,1,1.0
classes joint,1,1,1.0
joint mance,1,1,1.0
mance has,1,1,1.0
been maximized,1,1,1.0
maximized include,1,1,1.0
the true,3,3,1.0
true rates,1,1,1.0
rates geometric,1,1,1.0
geometric mean,6,2,3.0
mean gm,1,1,1.0
gm and,2,2,1.0
auc whereas,1,1,1.0
aim of,2,2,1.0
the gm,1,1,1.0
gm lies,1,1,1.0
in racy,1,1,1.0
racy maximization,1,1,1.0
maximization relative,1,1,1.0
auc ric,1,1,1.0
ric seeks,1,1,1.0
to manifest,1,1,1.0
manifest between,1,1,1.0
between costs,1,1,1.0
costs and,3,2,1.5
and benefits,1,1,1.0
benefits gm,1,1,1.0
gm is,1,1,1.0
as gm,1,1,1.0
gm tprate,1,1,1.0
tprate tnrate,1,1,1.0
tnrate algorithm,1,1,1.0
algorithm in,6,3,2.0
in situations,6,2,3.0
where distributed,1,1,1.0
distributed environments,1,1,1.0
environments have,1,1,1.0
have seen,1,1,1.0
seen based,1,1,1.0
based oversampling,1,1,1.0
oversampling procedures,1,1,1.0
procedures applied,1,1,1.0
applied most,1,1,1.0
results have,2,2,1.0
have proved,1,1,1.0
proved less,1,1,1.0
less successful,1,1,1.0
successful this,1,1,1.0
this failure,1,1,1.0
failure continues,1,1,1.0
continues to,1,1,1.0
be attributed,1,1,1.0
data s,3,2,1.5
s random,1,1,1.0
random partitioning,1,1,1.0
partitioning as,1,1,1.0
as artificial,1,1,1.0
artificial sample,1,1,1.0
sample eration,1,1,1.0
eration with,1,1,1.0
these parameters,1,1,1.0
parameters lacking,1,1,1.0
lacking spatial,1,1,1.0
spatial relationships,1,1,1.0
relationships in,1,1,1.0
this study,5,1,5.0
we try,1,1,1.0
to resolve,2,2,1.0
resolve that,1,1,1.0
that issue,1,1,1.0
issue with,1,1,1.0
of effectually,1,1,1.0
effectually segmenting,1,1,1.0
segmenting and,1,1,1.0
and dispensing,1,1,1.0
dispensing the,1,1,1.0
in space,2,2,1.0
space apache,1,1,1.0
apache based,1,1,1.0
based smote,1,1,1.0
smote implementation,3,1,3.0
implementation is,2,1,2.0
being represented,1,1,1.0
represented in,1,1,1.0
our work,1,1,1.0
work for,1,1,1.0
data smaller,1,1,1.0
smaller minority,1,1,1.0
minority dataset,4,2,2.0
we need,3,2,1.5
determine est,1,1,1.0
est neighbors,2,2,1.0
neighbors for,3,2,1.5
a point,4,3,1.3333333333333333
in d,2,2,1.0
d dimensional,1,1,1.0
dimensional space,2,2,1.0
space in,7,3,2.3333333333333335
to inate,1,1,1.0
inate samples,1,1,1.0
samples for,2,1,2.0
class synthetically,1,1,1.0
synthetically for,1,1,1.0
context of,5,3,1.6666666666666667
of issue,1,1,1.0
issue or,1,1,1.0
or problem,1,1,1.0
problem which,1,1,1.0
are tackling,1,1,1.0
tackling the,2,2,1.0
data distribution,15,3,5.0
distribution to,3,2,1.5
to several,1,1,1.0
several different,1,1,1.0
different nodes,2,1,2.0
nodes which,1,1,1.0
done randomly,1,1,1.0
randomly in,1,1,1.0
distributed cluster,1,1,1.0
cluster might,1,1,1.0
might result,1,1,1.0
in distribution,1,1,1.0
of points,7,3,2.3333333333333335
to different,3,3,1.0
nodes rather,1,1,1.0
than their,1,1,1.0
their nearest,2,2,1.0
nearest nodes,1,1,1.0
nodes hence,1,1,1.0
will become,1,1,1.0
become unfeasible,1,1,1.0
unfeasible for,1,1,1.0
different individual,1,1,1.0
individual nodes,1,1,1.0
nodes to,1,1,1.0
identify their,1,1,1.0
their closest,1,1,1.0
closest neighbors,1,1,1.0
neighbors therefore,1,1,1.0
therefore for,2,2,1.0
is very,8,4,2.0
very big,1,1,1.0
big and,1,1,1.0
and most,2,2,1.0
most probably,1,1,1.0
probably distributed,1,1,1.0
distributed across,1,1,1.0
machines it,1,1,1.0
is significant,1,1,1.0
significant that,1,1,1.0
the identical,1,1,1.0
identical samples,1,1,1.0
clustered and,1,1,1.0
and samples,2,2,1.0
samples belonging,2,1,2.0
belonging to,17,4,4.25
cluster brought,1,1,1.0
brought to,1,1,1.0
a machine,2,1,2.0
machine so,1,1,1.0
samples ing,1,1,1.0
cluster now,1,1,1.0
now are,1,1,1.0
are present,2,1,2.0
present on,2,1,2.0
machine that,1,1,1.0
we the,2,2,1.0
in such,6,3,2.0
such a,3,1,3.0
a way,3,1,3.0
way that,4,2,2.0
samples which,3,1,3.0
which belong,1,1,1.0
cluster are,1,1,1.0
machine these,1,1,1.0
these different,1,1,1.0
different ters,1,1,1.0
ters on,1,1,1.0
machines can,1,1,1.0
can then,1,1,1.0
then be,1,1,1.0
be processed,2,1,2.0
processed in,1,1,1.0
in parallel,2,1,2.0
parallel to,1,1,1.0
to erate,1,1,1.0
erate synthetic,2,2,1.0
data using,2,1,2.0
smote though,1,1,1.0
huge the,1,1,1.0
the individual,4,3,1.3333333333333333
individual groups,1,1,1.0
groups that,1,1,1.0
that we,3,3,1.0
we create,2,2,1.0
create can,1,1,1.0
can fit,1,1,1.0
fit on,1,1,1.0
machine and,1,1,1.0
hence can,1,1,1.0
processed independently,1,1,1.0
independently and,1,1,1.0
parallel thus,1,1,1.0
thus ing,1,1,1.0
ing motivation,1,1,1.0
motivation to,1,1,1.0
this complex,1,1,1.0
complex problem,1,1,1.0
problem during,1,1,1.0
during the,1,1,1.0
the application,5,2,2.5
main objective,1,1,1.0
objective is,3,2,1.5
to mine,1,1,1.0
mine the,1,1,1.0
efficient way,1,1,1.0
way through,1,1,1.0
be tioned,1,1,1.0
tioned one,1,1,1.0
perceived convenient,1,1,1.0
convenient ways,1,1,1.0
ways involves,1,1,1.0
the random,6,3,2.0
random grouping,1,1,1.0
grouping of,1,1,1.0
to obtain,8,4,2.0
obtain m,1,1,1.0
clusters despite,1,1,1.0
despite the,1,1,1.0
the contributory,1,1,1.0
contributory role,1,1,1.0
it requires,1,1,1.0
requires that,1,1,1.0
that each,1,1,1.0
each query,1,1,1.0
query is,1,1,1.0
is run,1,1,1.0
clusters in,1,1,1.0
the entirety,1,1,1.0
entirety algorithms,1,1,1.0
algorithms detail,1,1,1.0
detail the,2,2,1.0
the searching,1,1,1.0
searching indexing,1,1,1.0
and partitioning,1,1,1.0
partitioning hooda,1,1,1.0
systems given,1,1,1.0
given larger,1,1,1.0
larger sets,1,1,1.0
data they,2,2,1.0
they could,1,1,1.0
be clustered,1,1,1.0
clustered or,1,1,1.0
or segmented,1,1,1.0
segmented using,1,1,1.0
using different,6,3,2.0
different techniques,1,1,1.0
techniques an,1,1,1.0
example of,3,2,1.5
a fast,1,1,1.0
fast algorithm,1,1,1.0
could achieve,2,2,1.0
this role,1,1,1.0
role is,1,1,1.0
is scalable,1,1,1.0
scalable in,1,1,1.0
study the,3,2,1.5
the spark,3,1,3.0
spark implementation,2,1,2.0
of scalable,1,1,1.0
scalable k,1,1,1.0
k is,5,3,1.6666666666666667
employed the,2,1,2.0
approach is,4,3,1.3333333333333333
that large,1,1,1.0
large sets,2,1,2.0
data are,7,4,1.75
are segmented,1,1,1.0
segmented to,1,1,1.0
yield m,1,1,1.0
m spaces,1,1,1.0
spaces upon,1,1,1.0
upon data,1,1,1.0
data clustering,1,1,1.0
clustering are,1,1,1.0
are established,1,1,1.0
established for,1,1,1.0
for sample,1,1,1.0
sample indexing,1,1,1.0
indexing before,1,1,1.0
before establishing,1,1,1.0
establishing the,1,1,1.0
nearest for,1,1,1.0
respective samples,3,1,3.0
indeed serves,1,1,1.0
serves the,1,1,1.0
of partitioning,1,1,1.0
partitioning objects,1,1,1.0
objects via,1,1,1.0
via relative,1,1,1.0
relative distances,1,1,1.0
distances in,2,2,1.0
this investigation,3,1,3.0
the euclidean,10,3,3.3333333333333335
euclidean approach,1,1,1.0
employed and,1,1,1.0
and seeks,1,1,1.0
are formed,1,1,1.0
formed into,1,1,1.0
into nodes,1,1,1.0
nodes aligning,1,1,1.0
aligning them,1,1,1.0
space constrained,1,1,1.0
constrained regions,1,1,1.0
regions upon,1,1,1.0
upon point,1,1,1.0
point clustering,1,1,1.0
clustering each,1,1,1.0
each cluster,3,1,3.0
points is,1,1,1.0
is pushed,1,1,1.0
pushed to,1,1,1.0
the tive,1,1,1.0
tive nodes,1,1,1.0
nodes before,1,1,1.0
before being,1,1,1.0
being arranged,1,1,1.0
arranged relative,1,1,1.0
to distances,2,2,1.0
distances given,1,1,1.0
given the,6,2,3.0
neighbors are,2,2,1.0
searched before,1,1,1.0
before generating,1,1,1.0
synthetic samples,6,3,2.0
samples dataset,1,1,1.0
the represents,1,1,1.0
set employed,1,1,1.0
employed in,3,1,3.0
this tion,1,1,1.0
of vancouver,1,1,1.0
vancouver canada,1,1,1.0
canada this,1,1,1.0
this set,1,1,1.0
set was,2,2,1.0
was used,3,2,1.5
used at,1,1,1.0
at one,2,2,1.0
mining competitions,1,1,1.0
competitions the,1,1,1.0
set involves,1,1,1.0
involves a,1,1,1.0
a big,1,1,1.0
data problem,1,1,1.0
with imbalance,2,2,1.0
imbalance bioinformatics,1,1,1.0
bioinformatics whereas,1,1,1.0
whereas there,1,1,1.0
there were,4,2,2.0
were attributes,1,1,1.0
attributes and,1,1,1.0
and million,1,1,1.0
million instances,1,1,1.0
instances positive,1,1,1.0
positive instances,1,1,1.0
instances were,3,2,1.5
were found,2,2,1.0
study relies,1,1,1.0
s two,1,1,1.0
two imbalanced,1,1,1.0
imbalanced sets,1,1,1.0
data the,8,3,2.6666666666666665
the sets,3,2,1.5
sets are,7,3,2.3333333333333335
are generated,5,3,1.6666666666666667
generated in,2,2,1.0
same algorithm,1,1,1.0
algorithm cluster,1,1,1.0
cluster the,2,1,2.0
dataset cluster,1,1,1.0
samples using,1,1,1.0
scalable api,1,1,1.0
api provided,1,1,1.0
provided by,1,1,1.0
by spark,1,1,1.0
spark ml,1,1,1.0
ml library,1,1,1.0
library number,1,1,1.0
of groups,1,1,1.0
groups given,1,1,1.0
given as,1,1,1.0
as input,2,1,2.0
input for,1,1,1.0
for clustering,2,1,2.0
clustering should,1,1,1.0
be equal,2,2,1.0
equal to,8,3,2.6666666666666665
sample do,2,1,2.0
do id,1,1,1.0
id closest,1,1,1.0
closest pivot,1,1,1.0
pivot for,1,1,1.0
each point,2,2,1.0
point end,1,1,1.0
end for,2,1,2.0
do calculate,2,1,2.0
calculate tance,1,1,1.0
tance with,1,1,1.0
with pivots,1,1,1.0
pivots parent,1,1,1.0
parent pivot,1,1,1.0
pivot pivot,1,1,1.0
pivot with,1,1,1.0
with shortest,1,1,1.0
shortest distance,1,1,1.0
distance done,1,1,1.0
done for,1,1,1.0
cluster do,1,1,1.0
do push,1,1,1.0
push each,1,1,1.0
cluster to,1,1,1.0
different done,1,1,1.0
done up,1,1,1.0
up sampling,1,1,1.0
sampling ratio,1,1,1.0
ratio and,3,2,1.5
and nearest,1,1,1.0
neighbors k,1,1,1.0
k provided,1,1,1.0
provided as,1,1,1.0
input number,1,1,1.0
generated per,1,1,1.0
per sample,1,1,1.0
sample m,1,1,1.0
m is,3,3,1.0
given by,3,3,1.0
this part,1,1,1.0
part runs,1,1,1.0
runs on,1,1,1.0
on individual,1,1,1.0
individual given,1,1,1.0
respective partition,1,1,1.0
partition perform,1,1,1.0
perform the,2,2,1.0
for ual,1,1,1.0
ual samples,1,1,1.0
samples also,1,1,1.0
also establish,1,1,1.0
establish an,1,1,1.0
an the,1,1,1.0
second algorithm,1,1,1.0
algorithm end,1,1,1.0
samples in,3,2,1.5
the selected,2,2,1.0
selected or,1,1,1.0
or target,1,1,1.0
target partitions,1,1,1.0
partitions before,1,1,1.0
before searching,1,1,1.0
searching k,1,1,1.0
k nearest,11,3,3.6666666666666665
the establish,1,1,1.0
establish the,1,1,1.0
third algorithm,1,1,1.0
algorithm given,1,1,1.0
individual k,1,1,1.0
k neighbors,2,1,2.0
neighbors establish,1,1,1.0
establish m,1,1,1.0
m thetic,1,1,1.0
thetic samples,1,1,1.0
samples to,3,2,1.5
achieve the,1,1,1.0
fourth algorithm,1,1,1.0
algorithm algorithm,1,1,1.0
algorithm build,1,1,1.0
build m,1,1,1.0
m tree,1,1,1.0
tree this,1,1,1.0
algorithm takes,1,1,1.0
takes the,2,2,1.0
the c,3,3,1.0
c as,1,1,1.0
input init,1,1,1.0
init add,1,1,1.0
add the,2,1,2.0
the first,7,2,3.5
first point,1,1,1.0
point as,1,1,1.0
a router,1,1,1.0
router for,1,1,1.0
for p,1,1,1.0
p each,1,1,1.0
the partition,3,1,3.0
partition do,2,1,2.0
calculate the,4,2,2.0
the distances,3,2,1.5
distances from,1,1,1.0
from each,2,2,1.0
each router,1,1,1.0
router choose,1,1,1.0
choose the,1,1,1.0
the router,3,1,3.0
router with,1,1,1.0
with minimum,1,1,1.0
minimum distance,1,1,1.0
distance if,1,1,1.0
if distance,2,1,2.0
distance router,1,1,1.0
router covering,1,1,1.0
covering radius,2,1,2.0
radius update,1,1,1.0
update covering,1,1,1.0
radius add,1,1,1.0
the sample,4,2,2.0
as leaf,1,1,1.0
leaf node,2,1,2.0
node to,2,1,2.0
router increment,1,1,1.0
increment leaf,1,1,1.0
leaf count,2,1,2.0
count if,1,1,1.0
if leaf,1,1,1.0
count maxleafsize,1,1,1.0
maxleafsize promote,1,1,1.0
promote another,1,1,1.0
another leaf,1,1,1.0
to router,1,1,1.0
router split,1,1,1.0
split the,2,1,2.0
nodes between,1,1,1.0
two routers,1,1,1.0
routers done,1,1,1.0
done algorithm,1,1,1.0
algorithm finding,1,1,1.0
finding nearest,1,1,1.0
p in,2,1,2.0
do store,1,1,1.0
store the,3,1,3.0
first k,1,1,1.0
k points,1,1,1.0
a priority,1,1,1.0
priority queue,1,1,1.0
queue of,1,1,1.0
of size,2,2,1.0
size k,1,1,1.0
k priority,1,1,1.0
priority distance,1,1,1.0
distance from,4,3,1.3333333333333333
from point,1,1,1.0
point p,1,1,1.0
p smaller,1,1,1.0
smaller distance,1,1,1.0
distance means,1,1,1.0
means higher,1,1,1.0
higher priority,1,1,1.0
priority if,1,1,1.0
p from,1,1,1.0
from router,1,1,1.0
router distance,1,1,1.0
of another,1,1,1.0
sample belonging,1,1,1.0
router the,1,1,1.0
the max,1,1,1.0
max distance,1,1,1.0
samples stored,1,1,1.0
stored in,1,1,1.0
the priority,2,1,2.0
priority q,2,1,2.0
q ignore,1,1,1.0
ignore the,3,2,1.5
sample else,1,1,1.0
else calculate,1,1,1.0
from sample,1,1,1.0
p and,4,2,2.0
and store,1,1,1.0
q end,1,1,1.0
end if,1,1,1.0
if done,1,1,1.0
done class,1,1,1.0
considered for,2,2,1.0
for training,2,2,1.0
training purposes,1,1,1.0
the tigation,1,1,1.0
tigation also,1,1,1.0
also considers,1,1,1.0
considers a,1,1,1.0
a larger,1,1,1.0
larger dataset,1,1,1.0
when it,2,2,1.0
it comes,1,1,1.0
comes to,1,1,1.0
to testing,1,1,1.0
testing procedure,1,1,1.0
procedure a,2,2,1.0
a smaller,2,1,2.0
smaller set,2,1,2.0
used with,3,2,1.5
with senting,1,1,1.0
senting a,1,1,1.0
features the,2,2,1.0
feature reduction,3,1,3.0
reduction procedure,1,1,1.0
procedure hooda,1,1,1.0
systems algorithm,1,1,1.0
algorithm smote,1,1,1.0
of samples,2,1,2.0
in partition,1,1,1.0
partition for,1,1,1.0
the k,9,3,3.0
neighbors gap,1,1,1.0
gap random,1,1,1.0
synthetic attribute,1,1,1.0
attribute p,1,1,1.0
p attribute,1,1,1.0
attribute gap,1,1,1.0
gap difference,1,1,1.0
in values,1,1,1.0
values of,7,3,2.3333333333333335
this attribute,1,1,1.0
attribute between,1,1,1.0
between p,1,1,1.0
and neighbor,1,1,1.0
neighbor done,1,1,1.0
done the,1,1,1.0
the choice,3,3,1.0
choice for,1,1,1.0
for n,2,1,2.0
n took,1,1,1.0
took some,1,1,1.0
some experimentation,1,1,1.0
experimentation we,1,1,1.0
we tried,1,1,1.0
tried for,1,1,1.0
for various,1,1,1.0
various distinct,1,1,1.0
distinct values,1,1,1.0
of n,2,2,1.0
n and,1,1,1.0
the optimum,1,1,1.0
optimum results,1,1,1.0
results were,1,1,1.0
were obtained,1,1,1.0
n has,1,1,1.0
been employed,1,1,1.0
that relevant,1,1,1.0
relevant and,1,1,1.0
and utmost,1,1,1.0
utmost ate,1,1,1.0
ate features,1,1,1.0
are acquired,1,1,1.0
acquired in,1,1,1.0
particular the,2,2,1.0
reduction lates,1,1,1.0
lates into,1,1,1.0
into concentration,1,1,1.0
concentration on,1,1,1.0
on out,1,1,1.0
original features,1,1,1.0
features using,1,1,1.0
the hadoop,1,1,1.0
hadoop map,1,1,1.0
map reduce,1,1,1.0
reduce approach,1,1,1.0
approach the,4,3,1.3333333333333333
implementation which,1,1,1.0
is distributed,1,1,1.0
distributed is,1,1,1.0
is benchmarked,1,1,1.0
benchmarked using,1,1,1.0
to compare,4,3,1.3333333333333333
compare the,2,2,1.0
algorithm with,2,2,1.0
results obtained,10,2,5.0
from previous,1,1,1.0
previous approaches,3,2,1.5
approaches such,2,2,1.0
as python,1,1,1.0
python implementation,4,1,4.0
smote two,1,1,1.0
two additional,2,2,1.0
additional sets,1,1,1.0
used they,1,1,1.0
they include,1,1,1.0
include sat,1,1,1.0
sat image,2,1,2.0
image and,1,1,1.0
and yeast,1,1,1.0
yeast which,1,1,1.0
which originate,1,1,1.0
originate form,1,1,1.0
form uci,1,1,1.0
uci viz,1,1,1.0
viz abalone,1,1,1.0
abalone the,1,1,1.0
the framework,2,2,1.0
framework or,1,1,1.0
say infrastructure,1,1,1.0
infrastructure which,1,1,1.0
was utilized,1,1,1.0
utilized for,1,1,1.0
these experiments,2,2,1.0
experiments consist,1,1,1.0
consist of,2,2,1.0
four centos,1,1,1.0
centos linux,1,1,1.0
linux machines,1,1,1.0
machines each,1,1,1.0
each having,1,1,1.0
having eight,1,1,1.0
eight cores,1,1,1.0
cores and,1,1,1.0
and gb,1,1,1.0
gb ram,2,2,1.0
ram spark,1,1,1.0
spark was,1,1,1.0
to configure,1,1,1.0
configure the,1,1,1.0
the cluster,3,1,3.0
cluster and,2,2,1.0
and input,1,1,1.0
input as,1,1,1.0
as output,1,1,1.0
output data,1,1,1.0
data was,2,1,2.0
was stored,1,1,1.0
stored with,1,1,1.0
of hadoop,1,1,1.0
hadoop hdfs,1,1,1.0
hdfs analysis,1,1,1.0
analysis the,1,1,1.0
input parameters,3,1,3.0
parameters to,2,2,1.0
algorithm were,1,1,1.0
follows the,3,2,1.5
of clusters,6,1,6.0
clusters to,1,1,1.0
be created,2,2,1.0
by scalable,1,1,1.0
scalable was,1,1,1.0
was taken,1,1,1.0
as eight,1,1,1.0
eight the,1,1,1.0
clusters must,1,1,1.0
must be,3,2,1.5
be tuned,2,1,2.0
tuned so,1,1,1.0
is neither,1,1,1.0
neither too,1,1,1.0
too high,2,1,2.0
high nor,1,1,1.0
nor too,1,1,1.0
too low,1,1,1.0
low providing,1,1,1.0
providing a,2,2,1.0
clusters may,1,1,1.0
may again,1,1,1.0
again lead,1,1,1.0
a problem,4,3,1.3333333333333333
problem where,1,1,1.0
where all,1,1,1.0
cluster may,1,1,1.0
may not,9,3,3.0
not fit,2,1,2.0
fit into,1,1,1.0
machine disk,1,1,1.0
disk memory,1,1,1.0
memory setting,1,1,1.0
setting the,1,1,1.0
clusters too,1,1,1.0
high though,1,1,1.0
though may,1,1,1.0
may increase,2,1,2.0
increase the,6,3,2.0
the parallelism,1,1,1.0
parallelism and,1,1,1.0
and throughput,1,1,1.0
throughput but,1,1,1.0
but may,2,1,2.0
may cause,1,1,1.0
cause the,1,1,1.0
are similar,2,2,1.0
similar and,1,1,1.0
and near,1,1,1.0
near to,2,2,1.0
another to,1,1,1.0
to fall,2,2,1.0
fall into,3,2,1.5
into different,1,1,1.0
different clusters,1,1,1.0
clusters thereby,1,1,1.0
thereby decreasing,1,1,1.0
decreasing the,1,1,1.0
data produced,1,1,1.0
produced as,1,1,1.0
algorithm relies,2,2,1.0
on finding,1,1,1.0
neighbors this,1,1,1.0
this number,1,1,1.0
number must,1,1,1.0
tuned based,1,1,1.0
the dimension,1,1,1.0
dimension of,2,2,1.0
the segment,1,1,1.0
segment cluster,1,1,1.0
cluster specifications,1,1,1.0
specifications with,1,1,1.0
the emphasis,2,2,1.0
emphasis on,2,2,1.0
on using,2,1,2.0
the least,1,1,1.0
least ble,1,1,1.0
ble for,1,1,1.0
cluster configuration,1,1,1.0
configuration another,1,1,1.0
another crucial,1,1,1.0
crucial input,1,1,1.0
input parameter,1,1,1.0
parameter that,1,1,1.0
worth considering,1,1,1.0
considering involves,1,1,1.0
of nearest,8,3,2.6666666666666665
nearest numbers,1,1,1.0
numbers that,1,1,1.0
that need,3,2,1.5
be used,19,4,4.75
generated particularly,1,1,1.0
particularly the,1,1,1.0
the stage,1,1,1.0
stage focuses,1,1,1.0
focuses on,2,2,1.0
neighbors to,2,2,1.0
be sought,1,1,1.0
sought from,1,1,1.0
respective ples,1,1,1.0
ples upon,1,1,1.0
upon which,1,1,1.0
to establish,2,2,1.0
establish synthetic,1,1,1.0
investigation five,1,1,1.0
five is,1,1,1.0
number at,1,1,1.0
at which,1,1,1.0
neighbors used,2,2,1.0
is kept,1,1,1.0
kept in,1,1,1.0
the study,3,2,1.5
study by,1,1,1.0
chawla this,1,1,1.0
approach was,2,2,1.0
was employed,1,1,1.0
employed ratio,1,1,1.0
ratio determines,1,1,1.0
determines the,2,2,1.0
the fraction,1,1,1.0
fraction by,1,1,1.0
by which,2,2,1.0
a ratio,1,1,1.0
one would,2,2,1.0
would generate,1,1,1.0
minority samples,3,2,1.5
samples so,1,1,1.0
equal in,1,1,1.0
in count,1,1,1.0
count with,1,1,1.0
majority samples,2,1,2.0
samples providing,1,1,1.0
providing an,1,1,1.0
an ratio,1,1,1.0
of will,1,1,1.0
will create,1,1,1.0
create a,3,2,1.5
final distribution,1,1,1.0
samples vs,1,1,1.0
vs minority,1,1,1.0
the ratio,4,2,2.0
of are,1,1,1.0
are balanced,1,1,1.0
balanced trees,1,1,1.0
and each,14,2,7.0
each routing,1,1,1.0
node of,1,1,1.0
tree has,1,1,1.0
a maximum,1,1,1.0
maximum size,1,1,1.0
size capacity,1,1,1.0
nodes that,1,1,1.0
can store,1,1,1.0
the knn,1,1,1.0
is performed,2,2,1.0
performed in,2,2,1.0
these leaf,1,1,1.0
nodes ing,1,1,1.0
large capacity,1,1,1.0
capacity may,2,1,2.0
may improve,1,1,1.0
search but,1,1,1.0
may impact,1,1,1.0
the throughput,1,1,1.0
throughput as,1,1,1.0
as larger,1,1,1.0
larger number,1,1,1.0
samples need,1,1,1.0
be searched,1,1,1.0
searched for,1,1,1.0
for identifying,1,1,1.0
neighbors while,1,1,1.0
while a,1,1,1.0
smaller leaf,1,1,1.0
leaf capacity,1,1,1.0
may split,1,1,1.0
the similar,2,2,1.0
similar ples,1,1,1.0
ples into,1,1,1.0
into two,2,2,1.0
different groups,1,1,1.0
groups thereby,1,1,1.0
thereby reducing,1,1,1.0
reducing the,3,3,1.0
samples generated,2,1,2.0
generated tuning,1,1,1.0
tuning the,16,2,8.0
important and,4,3,1.3333333333333333
this experiment,5,3,1.6666666666666667
experiment we,2,2,1.0
we arrived,1,1,1.0
arrived at,1,1,1.0
a size,1,1,1.0
of after,1,1,1.0
after few,1,1,1.0
few iments,1,1,1.0
iments minority,1,1,1.0
was originated,1,1,1.0
originated synthetically,1,1,1.0
synthetically with,1,1,1.0
our algorithms,1,1,1.0
algorithms then,1,1,1.0
then we,2,2,1.0
we merged,1,1,1.0
merged that,1,1,1.0
that data,2,2,1.0
set which,2,2,1.0
have set,1,1,1.0
set alongside,1,1,1.0
alongside for,1,1,1.0
the purpose,7,2,3.5
purpose of,7,2,3.5
of training,6,4,1.5
training given,1,1,1.0
the ecbdl,2,1,2.0
ecbdl set,2,1,2.0
was assessed,1,1,1.0
assessed using,1,1,1.0
using classification,1,1,1.0
classification algorithms,1,1,1.0
algorithms linked,1,1,1.0
distributed random,2,1,2.0
forest drf,1,1,1.0
drf this,1,1,1.0
process targeted,1,1,1.0
targeted the,1,1,1.0
test data,3,2,1.5
was achieved,1,1,1.0
achieved after,1,1,1.0
after feature,1,1,1.0
reduction the,1,1,1.0
model parameters,1,1,1.0
parameters employed,1,1,1.0
study included,1,1,1.0
included sat,1,1,1.0
image datasets,1,1,1.0
the scikit,1,1,1.0
scikit python,1,1,1.0
of random,2,2,1.0
forest for,1,1,1.0
the abalone,5,2,2.5
abalone tables,1,1,1.0
tables and,2,1,2.0
implementation the,1,1,1.0
s accuracy,1,1,1.0
accuracy was,1,1,1.0
was benchmarked,1,1,1.0
benchmarked via,1,1,1.0
via minority,1,1,1.0
generation facilitated,1,1,1.0
facilitated through,1,1,1.0
smote s,2,2,1.0
s python,1,1,1.0
implementation also,1,1,1.0
also various,1,1,1.0
various input,1,1,1.0
parameters aided,1,1,1.0
aided in,1,1,1.0
the tuning,1,1,1.0
tuning of,1,1,1.0
algorithm indeed,1,1,1.0
indeed optimal,1,1,1.0
optimal outcomes,2,1,2.0
outcomes were,2,1,2.0
were realized,1,1,1.0
realized at,1,1,1.0
point when,1,1,1.0
when four,1,1,1.0
four was,1,1,1.0
was set,3,2,1.5
set as,3,3,1.0
set s,2,2,1.0
s leaf,1,1,1.0
leaf size,2,1,2.0
size set,1,1,1.0
set at,2,1,2.0
at for,1,1,1.0
the smaller,1,1,1.0
of was,1,1,1.0
was less,1,1,1.0
less than,5,3,1.6666666666666667
than for,4,4,1.0
the larger,1,1,1.0
larger data,1,1,1.0
set optimal,1,1,1.0
also table,2,2,1.0
table distribution,1,1,1.0
distribution for,4,4,1.0
dataset being,1,1,1.0
used dataset,1,1,1.0
dataset attr,1,1,1.0
attr class,1,1,1.0
class max,2,1,2.0
max min,2,1,2.0
min class,2,1,2.0
class ecbdl,1,1,1.0
ecbdl million,1,1,1.0
million samples,1,1,1.0
samples keel,2,1,2.0
keel samples,2,1,2.0
samples uci,1,1,1.0
uci satimage,1,1,1.0
satimage samples,1,1,1.0
samples table,1,1,1.0
table input,1,1,1.0
parameters parameter,1,1,1.0
parameter value,1,1,1.0
value of,15,4,3.75
clusters nearest,1,1,1.0
neighbor upsampling,1,1,1.0
upsampling ratio,1,1,1.0
ratio capacity,1,1,1.0
capacity hooda,1,1,1.0
systems achieved,1,1,1.0
achieved when,1,1,1.0
study s,1,1,1.0
s conditions,1,1,1.0
conditions were,1,1,1.0
were set,1,1,1.0
set in,2,2,1.0
size was,1,1,1.0
at and,1,1,1.0
cluster number,1,1,1.0
number established,1,1,1.0
established at,1,1,1.0
at or,1,1,1.0
or tables,1,1,1.0
and represent,1,1,1.0
represent the,7,3,2.3333333333333335
matrix gm,1,1,1.0
gm recall,1,1,1.0
and auc,6,3,2.0
auc for,2,2,1.0
the implemented,1,1,1.0
implemented drf,1,1,1.0
drf and,1,1,1.0
forest indeed,1,1,1.0
outcomes demonstrate,1,1,1.0
demonstrate that,2,1,2.0
is data,2,2,1.0
sampling in,8,3,2.6666666666666665
data findings,1,1,1.0
findings suggest,2,1,2.0
suggest further,1,1,1.0
spark tation,1,1,1.0
tation tends,1,1,1.0
tends to,2,1,2.0
to exceed,1,1,1.0
exceed or,1,1,1.0
or match,1,1,1.0
match compared,1,1,1.0
the python,1,1,1.0
python smote,1,1,1.0
is applied,6,3,2.0
applied these,1,1,1.0
these observations,1,1,1.0
observations are,2,2,1.0
are evidenced,1,1,1.0
evidenced by,1,1,1.0
the information,3,3,1.0
information in,1,1,1.0
in tables,1,1,1.0
tables therefore,1,1,1.0
therefore the,8,3,2.6666666666666665
the tributed,1,1,1.0
tributed spark,1,1,1.0
spark smote,1,1,1.0
smote exhibits,2,1,2.0
exhibits superior,3,1,3.0
superior performance,4,1,4.0
performance with,2,1,2.0
a lar,1,1,1.0
lar trend,1,1,1.0
trend observed,1,1,1.0
observed even,1,1,1.0
even in,1,1,1.0
situations involving,3,1,3.0
involving an,1,1,1.0
an increased,1,1,1.0
increased amount,1,1,1.0
amount of,2,2,1.0
dataset given,1,1,1.0
large dataset,1,1,1.0
dataset an,1,1,1.0
is seen,3,2,1.5
seen to,3,2,1.5
fail to,1,1,1.0
to steer,1,1,1.0
steer improvements,1,1,1.0
in minority,2,1,2.0
not employed,1,1,1.0
the implication,1,1,1.0
implication is,1,1,1.0
that smote,4,2,2.0
is unlikely,1,1,1.0
unlikely to,1,1,1.0
to prove,2,2,1.0
prove beneficial,1,1,1.0
beneficial to,1,1,1.0
the given,2,2,1.0
dataset because,1,1,1.0
because does,1,1,1.0
not yield,1,1,1.0
yield a,2,2,1.0
significant improvement,1,1,1.0
improvement in,1,1,1.0
class prediction,1,1,1.0
prediction on,1,1,1.0
hand distributed,1,1,1.0
performance when,3,3,1.0
when employed,1,1,1.0
involving large,3,2,1.5
data outperforming,1,1,1.0
outperforming sklearn,1,1,1.0
sklearn python,1,1,1.0
implementation hence,1,1,1.0
results validate,1,1,1.0
validate our,1,1,1.0
our implementation,3,2,1.5
implementation indeed,1,1,1.0
the findings,2,1,2.0
findings demonstrate,1,1,1.0
for lishing,1,1,1.0
lishing the,1,1,1.0
neighbor and,1,1,1.0
and also,2,2,1.0
also the,3,2,1.5
the implementation,2,1,2.0
the distributing,1,1,1.0
distributing data,1,1,1.0
data aid,1,1,1.0
aid in,1,1,1.0
in preserving,1,1,1.0
preserving spatial,1,1,1.0
spatial sample,1,1,1.0
sample arrangements,1,1,1.0
arrangements in,1,1,1.0
in turn,4,3,1.3333333333333333
turn artificial,1,1,1.0
artificial samples,1,1,1.0
generated ensuring,1,1,1.0
ensuring that,1,1,1.0
the desired,2,2,1.0
desired scale,1,1,1.0
scale and,1,1,1.0
and degree,1,1,1.0
degree of,4,3,1.3333333333333333
accuracy are,1,1,1.0
are realized,1,1,1.0
realized with,1,1,1.0
with increasing,2,1,2.0
increasing volume,2,1,2.0
datasets table,2,2,1.0
table random,1,1,1.0
forest parameters,1,1,1.0
parameters algorithm,1,1,1.0
algorithm parameters,1,1,1.0
parameters distributed,1,1,1.0
forest number,1,1,1.0
of tress,1,1,1.0
tress sample,1,1,1.0
sample rat,1,1,1.0
rat maximum,1,1,1.0
maximum tree,1,1,1.0
tree depth,1,1,1.0
depth nbins,1,1,1.0
nbins table,1,1,1.0
table performance,4,1,4.0
performance comparison,6,2,3.0
comparison abalone,1,1,1.0
dataset technique,4,1,4.0
technique auc,4,1,4.0
auc recall,4,1,4.0
recall gm,4,1,4.0
gm confusion,4,1,4.0
matrix no,4,1,4.0
no sampling,4,1,4.0
sampling python,4,1,4.0
smote spark,4,1,4.0
spark distributed,4,1,4.0
smote gm,4,1,4.0
gm geometric,4,1,4.0
mean smote,4,1,4.0
technique table,2,1,2.0
comparison y,1,1,1.0
y dataset,1,1,1.0
technique observations,1,1,1.0
observations given,1,1,1.0
the proposed,10,3,3.3333333333333335
proposed algorithm,1,1,1.0
suggest the,1,1,1.0
the capability,1,1,1.0
capability of,1,1,1.0
of realizing,1,1,1.0
realizing quality,1,1,1.0
quality samples,1,1,1.0
ples prediction,1,1,1.0
prediction is,1,1,1.0
be improved,1,1,1.0
improved with,1,1,1.0
results indicate,2,2,1.0
indicate that,3,3,1.0
traditional smote,1,1,1.0
smote tends,1,1,1.0
less efficient,1,1,1.0
efficient making,1,1,1.0
making it,2,2,1.0
it less,1,1,1.0
less applicable,1,1,1.0
applicable in,1,1,1.0
large datasets,2,2,1.0
datasets previous,1,1,1.0
previous research,1,1,1.0
research work,1,1,1.0
work guided,1,1,1.0
guided this,1,1,1.0
investigation paving,1,1,1.0
paving the,1,1,1.0
the way,2,2,1.0
way for,1,1,1.0
the expansion,1,1,1.0
expansion of,1,1,1.0
distributed implementation,1,1,1.0
implementation from,1,1,1.0
outcomes especially,1,1,1.0
especially when,2,2,1.0
when various,1,1,1.0
various parameters,1,1,1.0
parameters are,1,1,1.0
are benchmarked,1,1,1.0
benchmarked the,1,1,1.0
the tation,1,1,1.0
tation exhibits,1,1,1.0
with our,1,1,1.0
are able,1,1,1.0
distributed ronment,1,1,1.0
ronment when,1,1,1.0
dataset may,1,1,1.0
fit in,1,1,1.0
machine an,1,1,1.0
existing implementation,1,1,1.0
implementation can,1,1,1.0
can include,1,1,1.0
include an,1,1,1.0
an lap,1,1,1.0
lap when,1,1,1.0
when indexing,1,1,1.0
indexing samples,1,1,1.0
in so,1,1,1.0
that samples,1,1,1.0
are at,1,1,1.0
partition boundary,1,1,1.0
boundary may,1,1,1.0
be included,1,1,1.0
in both,5,3,1.6666666666666667
the partitions,1,1,1.0
partitions this,1,1,1.0
this may,3,2,1.5
generated however,1,1,1.0
however it,2,2,1.0
be possible,1,1,1.0
possible that,1,1,1.0
smote may,1,1,1.0
the answer,1,1,1.0
answer of,1,1,1.0
imbalance issues,1,1,1.0
issues within,1,1,1.0
within class,12,3,4.0
imbalance is,6,3,2.0
not handled,1,1,1.0
handled properly,1,1,1.0
properly with,1,1,1.0
of distributed,1,1,1.0
smote there,1,1,1.0
a need,2,1,2.0
to merge,1,1,1.0
merge the,1,1,1.0
current solution,1,1,1.0
solution with,1,1,1.0
to tify,1,1,1.0
tify within,1,1,1.0
and handle,1,1,1.0
the within,5,3,1.6666666666666667
imbalance accordingly,1,1,1.0
accordingly also,1,1,1.0
also standard,1,1,1.0
standard might,1,1,1.0
be optimum,1,1,1.0
optimum class,1,1,1.0
class semination,1,1,1.0
semination in,1,1,1.0
data problems,1,1,1.0
are imbalanced,3,3,1.0
imbalanced so,1,1,1.0
ratio may,1,1,1.0
be varied,1,1,1.0
varied to,1,1,1.0
achieve a,1,1,1.0
a better,6,4,1.5
better accuracy,1,1,1.0
different scenarios,1,1,1.0
scenarios in,1,1,1.0
future there,1,1,1.0
the further,1,1,1.0
further investigation,1,1,1.0
investigation of,1,1,1.0
in its,2,2,1.0
its entirety,1,1,1.0
entirety summary,1,1,1.0
summary this,1,1,1.0
study involved,1,1,1.0
involved the,1,1,1.0
smote via,1,1,1.0
the apache,1,1,1.0
spark framework,1,1,1.0
framework the,2,2,1.0
the investigation,1,1,1.0
investigation was,1,1,1.0
was conducted,1,1,1.0
conducted based,1,1,1.0
and algorithms,2,2,1.0
algorithms in,8,2,4.0
future the,1,1,1.0
the scope,1,1,1.0
scope of,1,1,1.0
such table,1,1,1.0
comparison ucisat,1,1,1.0
ucisat dataset,1,1,1.0
comparison ecbdl,1,1,1.0
ecbdl dataset,1,1,1.0
technique hooda,1,1,1.0
systems a,1,1,1.0
study could,1,1,1.0
be expanded,1,1,1.0
expanded by,1,1,1.0
considering alternative,1,1,1.0
alternative based,1,1,1.0
based algorithms,1,1,1.0
algorithms including,2,2,1.0
including and,1,1,1.0
and borderline,1,1,1.0
borderline smote,2,1,2.0
in big,2,1,2.0
the dependency,1,1,1.0
dependency reflects,1,1,1.0
reflects the,1,1,1.0
the clustering,3,1,3.0
in use,1,1,1.0
use proving,1,1,1.0
proving more,1,1,1.0
more effective,2,2,1.0
effective the,1,1,1.0
to exhibit,1,1,1.0
exhibit superior,1,1,1.0
performance due,2,2,1.0
the provision,1,1,1.0
provision of,1,1,1.0
of more,3,2,1.5
more accurate,1,1,1.0
accurate results,1,1,1.0
results notably,1,1,1.0
smote mentation,1,1,1.0
mentation could,1,1,1.0
it incorporates,1,1,1.0
incorporates class,1,1,1.0
class juncts,1,1,1.0
juncts other,1,1,1.0
other areas,1,1,1.0
areas that,1,1,1.0
are worth,1,1,1.0
worth utilizing,1,1,1.0
utilizing include,1,1,1.0
include hybrid,1,1,1.0
hybrid trees,1,1,1.0
and spill,1,1,1.0
trees through,1,1,1.0
through their,1,1,1.0
their implementation,1,1,1.0
implementation it,1,1,1.0
is predicted,1,1,1.0
predicted that,1,1,1.0
there might,2,2,1.0
be improvements,1,1,1.0
neighbor algorithm,1,1,1.0
algorithm as,3,3,1.0
clustering process,2,1,2.0
process references,2,2,1.0
references yu,1,1,1.0
yu hong,1,1,1.0
hong yang,1,1,1.0
yang ni,1,1,1.0
ni y,1,1,1.0
y dan,1,1,1.0
dan qin,1,1,1.0
qin recognition,1,1,1.0
recognition of,1,1,1.0
of multiple,2,2,1.0
multiple imbalanced,1,1,1.0
imbalanced cancer,1,1,1.0
cancer types,1,1,1.0
types based,1,1,1.0
on dna,1,1,1.0
dna microarray,1,1,1.0
microarray data,1,1,1.0
using ensemble,1,1,1.0
ensemble classifiers,1,1,1.0
classifiers biomed,1,1,1.0
biomed res,1,1,1.0
res int,1,1,1.0
int y,1,1,1.0
y chen,2,2,1.0
chen an,1,1,1.0
an empirical,2,2,1.0
empirical study,3,3,1.0
hybrid rst,1,1,1.0
rst classification,1,1,1.0
classification procedure,1,1,1.0
procedure to,4,3,1.3333333333333333
to elucidate,1,1,1.0
elucidate therapeutic,1,1,1.0
therapeutic effects,1,1,1.0
effects in,1,1,1.0
in uremia,1,1,1.0
uremia patients,1,1,1.0
patients med,1,1,1.0
med biol,1,1,1.0
biol eng,1,1,1.0
eng comput,1,1,1.0
comput haixiang,1,1,1.0
haixiang yijing,1,1,1.0
yijing yanan,1,1,1.0
yanan xiao,1,1,1.0
xiao jinling,1,1,1.0
jinling knn,1,1,1.0
knn ensemble,1,1,1.0
ensemble learning,11,3,3.6666666666666665
learning algorithm,12,4,3.0
algorithm imbalanced,1,1,1.0
classification appl,1,1,1.0
appl artif,1,1,1.0
artif intell,5,1,5.0
intell he,1,1,1.0
he garcía,1,1,1.0
garcía learning,1,1,1.0
ieee trans,17,3,5.666666666666667
trans knowl,6,2,3.0
knowl data,6,2,3.0
data eng,5,2,2.5
eng y,1,1,1.0
y sun,1,1,1.0
sun wong,1,1,1.0
wong mohamed,1,1,1.0
mohamed classification,1,1,1.0
data a,1,1,1.0
review int,1,1,1.0
int pattern,1,1,1.0
pattern recognit,2,1,2.0
recognit artif,1,1,1.0
intell fernández,2,1,2.0
fernández chawla,1,1,1.0
chawla garcía,1,1,1.0
garcía v,1,1,1.0
v palade,1,1,1.0
palade herrera,1,1,1.0
herrera an,1,1,1.0
an insight,2,1,2.0
insight into,2,1,2.0
into classification,1,1,1.0
with imbalanced,9,4,2.25
data empirical,1,1,1.0
empirical results,2,2,1.0
results and,3,2,1.5
and current,1,1,1.0
current trends,1,1,1.0
trends on,1,1,1.0
using data,1,1,1.0
data intrinsic,1,1,1.0
intrinsic characteristics,1,1,1.0
characteristics plex,1,1,1.0
plex intell,1,1,1.0
intell syst,2,1,2.0
syst krawczyk,1,1,1.0
krawczyk learning,1,1,1.0
data open,1,1,1.0
open challenges,1,1,1.0
challenges and,1,1,1.0
and future,2,2,1.0
future directions,3,2,1.5
directions prog,1,1,1.0
prog artif,1,1,1.0
intell batista,1,1,1.0
the ior,1,1,1.0
ior of,1,1,1.0
explor ramentol,1,1,1.0
ramentol vluymans,1,1,1.0
vluymans verbiest,1,1,1.0
verbiest y,1,1,1.0
y caballero,1,1,1.0
bello cornelis,1,1,1.0
cornelis herrera,1,1,1.0
herrera ifrow,1,1,1.0
ifrow ann,1,1,1.0
ann imbalanced,1,1,1.0
imbalanced ordered,1,1,1.0
ordered weighted,1,1,1.0
weighted average,1,1,1.0
average nearest,1,1,1.0
neighbor classification,1,1,1.0
trans fuzzy,1,1,1.0
fuzzy syst,1,1,1.0
syst p,1,1,1.0
p domingos,1,1,1.0
domingos metacost,3,3,1.0
metacost a,3,3,1.0
a general,5,3,1.6666666666666667
general method,2,2,1.0
for making,3,3,1.0
making classifiers,2,2,1.0
classifiers in,4,2,2.0
mining vol,1,1,1.0
pp v,5,3,1.6666666666666667
v lópez,1,1,1.0
lópez fernández,1,1,1.0
fernández herrera,1,1,1.0
herrera analysis,1,1,1.0
of preprocessing,1,1,1.0
preprocessing learning,1,1,1.0
imbalanced fication,1,1,1.0
fication open,1,1,1.0
open problems,1,1,1.0
problems on,1,1,1.0
on intrinsic,1,1,1.0
intrinsic data,1,1,1.0
data characteristics,1,1,1.0
characteristics expert,1,1,1.0
expert syst,1,1,1.0
syst appl,1,1,1.0
appl laney,1,1,1.0
laney data,1,1,1.0
data management,1,1,1.0
management controlling,1,1,1.0
controlling data,1,1,1.0
data volume,1,1,1.0
volume velocity,1,1,1.0
velocity and,1,1,1.0
and variety,1,1,1.0
variety meta,1,1,1.0
meta group,1,1,1.0
group kambatla,1,1,1.0
kambatla kollias,1,1,1.0
kollias v,1,1,1.0
v kumar,1,1,1.0
kumar grama,1,1,1.0
grama trends,1,1,1.0
trends in,1,1,1.0
analytics parallel,1,1,1.0
parallel distrib,1,1,1.0
distrib comput,1,1,1.0
comput p,2,1,2.0
p zikopoulos,1,1,1.0
zikopoulos eaton,1,1,1.0
eaton deroos,1,1,1.0
deroos deutsch,1,1,1.0
deutsch lapis,1,1,1.0
lapis understanding,1,1,1.0
understanding big,1,1,1.0
big for,1,1,1.0
for enterprise,1,1,1.0
enterprise class,1,1,1.0
class hadoop,1,1,1.0
hadoop and,1,1,1.0
and streaming,1,1,1.0
streaming data,1,1,1.0
data osborne,1,1,1.0
osborne media,1,1,1.0
media new,1,1,1.0
new y,5,2,2.5
y ork,5,2,2.5
ork wu,1,1,1.0
wu zhu,1,1,1.0
zhu wu,2,2,1.0
wu w,1,1,1.0
w ding,1,1,1.0
ding data,1,1,1.0
mining with,3,3,1.0
with bigdata,1,1,1.0
bigdata ieee,1,1,1.0
eng chawla,1,1,1.0
hall w,1,1,1.0
w kegelmeyer,1,1,1.0
kegelmeyer synthetic,1,1,1.0
technique artif,1,1,1.0
intell res,2,2,1.0
res apache,1,1,1.0
spark https,1,1,1.0
https japkowicz,1,1,1.0
japkowicz the,1,1,1.0
problem significance,1,1,1.0
and strategies,1,1,1.0
strategies in,2,2,1.0
intelligence pp,5,2,2.5
pp prati,3,3,1.0
prati batista,3,3,1.0
batista monard,1,1,1.0
monard learning,1,1,1.0
learning with,9,4,2.25
class skews,1,1,1.0
skews and,1,1,1.0
and small,1,1,1.0
disjuncts adv,1,1,1.0
adv artif,1,1,1.0
fernández sara,1,1,1.0
sara del,1,1,1.0
del chawla,1,1,1.0
chawla herreral,1,1,1.0
herreral an,1,1,1.0
into imbalanced,1,1,1.0
imbalanced big,1,1,1.0
classification outcomes,1,1,1.0
outcomes and,1,1,1.0
challenges complex,1,1,1.0
complex intell,1,1,1.0
syst garcía,1,1,1.0
garcía herrera,1,1,1.0
herrera evolutionary,1,1,1.0
evolutionary undersampling,2,2,1.0
for cation,1,1,1.0
cation with,1,1,1.0
datasets proposals,1,1,1.0
proposals and,1,1,1.0
and taxonomy,1,1,1.0
taxonomy evol,1,1,1.0
evol comput,1,1,1.0
p haghani,1,1,1.0
haghani michel,1,1,1.0
michel p,1,1,1.0
p aberer,1,1,1.0
aberer lsh,1,1,1.0
lsh at,1,1,1.0
at large,1,1,1.0
large distributed,1,1,1.0
dimensions in,2,2,1.0
the web,1,1,1.0
web and,1,1,1.0
and databases,1,1,1.0
databases webdb,1,1,1.0
webdb vancouver,1,1,1.0
vancouver kanungo,1,1,1.0
kanungo mount,1,1,1.0
mount netanyahu,1,1,1.0
netanyahu piatko,1,1,1.0
piatko man,1,1,1.0
man wu,1,1,1.0
wu an,1,1,1.0
an efficient,2,1,2.0
efficient clustering,1,1,1.0
algorithm analysis,1,1,1.0
analysis and,5,3,1.6666666666666667
and implementation,1,1,1.0
implementation ieee,1,1,1.0
trans pattern,1,1,1.0
pattern anal,1,1,1.0
anal mach,1,1,1.0
mach intell,1,1,1.0
intell kaufman,1,1,1.0
kaufman p,1,1,1.0
p rousseeuw,1,1,1.0
rousseeuw finding,1,1,1.0
finding groups,1,1,1.0
groups in,1,1,1.0
data an,1,1,1.0
an duction,1,1,1.0
duction to,1,1,1.0
to cluster,1,1,1.0
cluster analysis,1,1,1.0
analysis john,1,1,1.0
sons new,1,1,1.0
ork v,1,1,1.0
v capoyleas,1,1,1.0
capoyleas woeginger,1,1,1.0
woeginger ªgeometric,1,1,1.0
ªgeometric clusterings,1,1,1.0
clusterings algorithms,1,1,1.0
algorithms jain,1,1,1.0
jain dubes,1,1,1.0
dubes algorithms,1,1,1.0
algorithms for,5,3,1.6666666666666667
clustering data,1,1,1.0
data prentice,1,1,1.0
prentice hall,1,1,1.0
hall englewood,1,1,1.0
englewood cliffs,1,1,1.0
cliffs jain,1,1,1.0
jain murty,1,1,1.0
murty p,1,1,1.0
p ªdata,1,1,1.0
ªdata clustering,1,1,1.0
clustering a,1,1,1.0
review acm,1,1,1.0
acm comput,1,1,1.0
comput surv,1,1,1.0
surv bahmani,1,1,1.0
bahmani moseley,1,1,1.0
moseley vattani,1,1,1.0
vattani kumar,1,1,1.0
kumar scalable,1,1,1.0
scalable proc,1,1,1.0
proc vldb,1,1,1.0
vldb endowment,1,1,1.0
endowment choi,1,1,1.0
choi lee,1,1,1.0
lee distributed,1,1,1.0
distributed high,1,1,1.0
high dimensional,3,2,1.5
dimensional indexing,1,1,1.0
indexing for,1,1,1.0
for search,1,1,1.0
search supercomput,1,1,1.0
supercomput jiang,1,1,1.0
jiang tseng,1,1,1.0
tseng su,1,1,1.0
su clustering,1,1,1.0
for outliers,1,1,1.0
outliers detection,1,1,1.0
detection pattern,1,1,1.0
recognit lett,1,1,1.0
lett guttman,1,1,1.0
guttman a,1,1,1.0
a dynamic,2,1,2.0
dynamic index,2,1,2.0
index structure,1,1,1.0
structure for,1,1,1.0
for spatial,1,1,1.0
spatial ing,1,1,1.0
ing in,1,1,1.0
the acm,1,1,1.0
acm sigmod,1,1,1.0
sigmod international,1,1,1.0
on management,1,1,1.0
data sigmod,1,1,1.0
sigmod acm,1,1,1.0
acm new,2,1,2.0
ork pp,2,1,2.0
pp bentley,1,1,1.0
bentley trees,1,1,1.0
trees for,1,1,1.0
for semidynamic,1,1,1.0
semidynamic point,1,1,1.0
point sets,1,1,1.0
sets in,4,3,1.3333333333333333
in ings,3,3,1.0
ings of,3,3,1.0
the sixth,2,2,1.0
sixth annual,1,1,1.0
annual symposium,1,1,1.0
computational try,1,1,1.0
try scg,1,1,1.0
scg acm,1,1,1.0
pp p,4,2,2.0
p ciaccia,3,1,3.0
ciaccia patella,2,1,2.0
patella p,1,1,1.0
p zezula,3,1,3.0
zezula an,1,1,1.0
efficient access,1,1,1.0
access method,1,1,1.0
for similarity,1,1,1.0
similarity search,1,1,1.0
in metric,1,1,1.0
spaces in,1,1,1.0
in vldb,1,1,1.0
vldb in,1,1,1.0
on very,1,1,1.0
very large,3,2,1.5
large data,1,1,1.0
data bases,1,1,1.0
bases pp,1,1,1.0
patella rabitti,1,1,1.0
rabitti p,1,1,1.0
zezula indexing,1,1,1.0
indexing metric,1,1,1.0
spaces with,1,1,1.0
with in,3,2,1.5
proceedings qunito,1,1,1.0
qunito convegno,1,1,1.0
convegno nazionale,1,1,1.0
nazionale sebd,1,1,1.0
sebd pp,1,1,1.0
pp moore,1,1,1.0
moore the,1,1,1.0
the anchors,1,1,1.0
anchors hierarchy,1,1,1.0
hierarchy using,1,1,1.0
triangle ity,1,1,1.0
ity to,1,1,1.0
to survive,1,1,1.0
survive high,1,1,1.0
dimensional data,1,1,1.0
the teenth,1,1,1.0
teenth conference,1,1,1.0
zezula p,1,1,1.0
ciaccia rabitti,1,1,1.0
rabitti a,1,1,1.0
index for,1,1,1.0
for ilarity,1,1,1.0
ilarity queries,1,1,1.0
queries in,1,1,1.0
in multimedia,1,1,1.0
multimedia databases,1,1,1.0
databases in,1,1,1.0
in technical,1,1,1.0
technical report,2,2,1.0
report hermes,1,1,1.0
hermes esprit,1,1,1.0
esprit ltr,1,1,1.0
ltr projects,1,1,1.0
projects liu,1,1,1.0
liu rosenberg,1,1,1.0
rosenberg rowley,1,1,1.0
rowley clustering,1,1,1.0
clustering billions,1,1,1.0
billions of,1,1,1.0
of images,1,1,1.0
images with,1,1,1.0
with large,2,2,1.0
large scale,5,3,1.6666666666666667
scale nearest,1,1,1.0
ieee workshop,2,1,2.0
on applications,2,1,2.0
applications of,2,1,2.0
vision published,1,1,1.0
published in,1,1,1.0
vision hooda,1,1,1.0
systems scikitlearn,1,1,1.0
scikitlearn http,1,1,1.0
http uci,1,1,1.0
uci satellite,1,1,1.0
satellite image,1,1,1.0
image dataset,1,1,1.0
dataset https,3,1,3.0
https y,1,1,1.0
y east,1,1,1.0
east dataset,1,1,1.0
https abalone,1,1,1.0
https https,1,1,1.0
https han,1,1,1.0
han w,1,1,1.0
w wang,1,1,1.0
mao borderline,1,1,1.0
smote a,3,3,1.0
new over,1,1,1.0
sampling method,4,3,1.3333333333333333
learning int,1,1,1.0
int conf,16,2,8.0
conf intell,2,2,1.0
intell comput,2,2,1.0
see discussions,1,1,1.0
discussions stats,1,1,1.0
stats and,1,1,1.0
and author,1,1,1.0
author profiles,1,1,1.0
profiles for,1,1,1.0
publication at,1,1,1.0
at https,1,1,1.0
https on,1,1,1.0
problem article,1,1,1.0
article october,1,1,1.0
october doi,1,1,1.0
doi citations,1,1,1.0
citations reads,1,1,1.0
reads authors,1,1,1.0
authors including,1,1,1.0
including gongping,1,1,1.0
gongping yang,3,1,3.0
yang shandong,1,1,1.0
shandong university,3,1,3.0
university publications,1,1,1.0
publications citations,1,1,1.0
citations see,1,1,1.0
see profile,1,1,1.0
profile all,1,1,1.0
all content,1,1,1.0
content following,1,1,1.0
following this,1,1,1.0
this page,1,1,1.0
page was,1,1,1.0
was uploaded,1,1,1.0
uploaded by,1,1,1.0
by gongping,1,1,1.0
yang on,1,1,1.0
on october,1,1,1.0
october the,1,1,1.0
the user,1,1,1.0
user has,1,1,1.0
has requested,1,1,1.0
requested enhancement,1,1,1.0
enhancement of,1,1,1.0
the downloaded,1,1,1.0
downloaded the,1,1,1.0
problem xinjian,1,1,1.0
xinjian guo,1,1,1.0
guo yilong,1,1,1.0
yilong cailing,1,1,1.0
cailing dong,1,1,1.0
dong gongping,1,1,1.0
yang guangtong,1,1,1.0
guangtong zhou,1,1,1.0
zhou school,1,1,1.0
technology shandong,2,1,2.0
university jinan,1,1,1.0
jinan china,1,1,1.0
china xinjianguo,1,1,1.0
xinjianguo ylyin,1,1,1.0
ylyin this,1,1,1.0
paper concentrates,1,1,1.0
concentrates on,1,1,1.0
and by,1,1,1.0
by convention,1,1,1.0
convention the,1,1,1.0
class label,18,2,9.0
label of,4,2,2.0
is positive,3,2,1.5
positive and,8,2,4.0
is negative,3,2,1.5
negative corresponding,1,1,1.0
author yilong,1,1,1.0
yilong yin,1,1,1.0
yin is,1,1,1.0
is with,3,3,1.0
with school,1,1,1.0
university ylyin,1,1,1.0
ylyin abstract,1,1,1.0
abstract the,1,1,1.0
many practical,1,1,1.0
practical domains,1,1,1.0
domains and,1,1,1.0
topic of,1,1,1.0
recent years,6,2,3.0
years in,1,1,1.0
in su,1,1,1.0
su ch,1,1,1.0
ch a,1,1,1.0
problem almost,1,1,1.0
almost all,4,2,2.0
the examples,12,2,6.0
examples are,7,2,3.5
are labeled,4,1,4.0
labeled as,4,1,4.0
class while,3,1,3.0
while far,2,1,2.0
far fewer,3,1,3.0
fewer examples,2,1,2.0
class usually,2,1,2.0
the more,3,2,1.5
more important,6,3,2.0
important class,2,1,2.0
case standard,2,1,2.0
standard machine,3,1,3.0
algorithms tend,1,1,1.0
be overwhelmed,2,1,2.0
overwhelmed by,2,1,2.0
and ignore,2,1,2.0
class since,2,2,1.0
since traditional,1,1,1.0
traditional classifiers,1,1,1.0
classifiers seeking,1,1,1.0
seeking an,1,1,1.0
an accurate,1,1,1.0
accurate performance,1,1,1.0
full range,1,1,1.0
paper reviewed,2,1,2.0
reviewed academic,2,1,2.0
academic activities,2,1,2.0
activities special,1,1,1.0
special for,2,1,2.0
problem firstly,1,1,1.0
firstly then,1,1,1.0
then investigated,1,1,1.0
investigated various,1,1,1.0
various remedies,1,1,1.0
remedies in,1,1,1.0
in four,1,1,1.0
different levels,4,2,2.0
levels acco,1,1,1.0
acco rding,1,1,1.0
rding to,1,1,1.0
to learning,3,2,1.5
learning phases,1,1,1.0
phases following,1,1,1.0
following surveying,1,1,1.0
surveying evaluation,1,1,1.0
metrics and,3,2,1.5
some other,4,1,4.0
other related,2,1,2.0
related factors,1,1,1.0
factors this,1,1,1.0
paper showed,1,1,1.0
showed some,2,1,2.0
some future,2,1,2.0
directions at,1,1,1.0
at last,1,1,1.0
last introduction,1,1,1.0
introduction many,1,1,1.0
many traditional,1,1,1.0
traditional algorithms,1,1,1.0
to machine,1,1,1.0
mining problems,1,1,1.0
problems assume,1,1,1.0
target classes,1,1,1.0
classes share,1,1,1.0
share similar,1,1,1.0
similar prior,1,1,1.0
prior probabilities,1,1,1.0
probabilities however,1,1,1.0
however in,3,3,1.0
many real,1,1,1.0
world applications,3,1,3.0
applications such,3,2,1.5
as detection,1,1,1.0
detection network,1,1,1.0
network intrusion,2,2,1.0
detection fraud,2,1,2.0
detection this,1,1,1.0
this assumption,2,1,2.0
assumption is,2,1,2.0
is grossly,1,1,1.0
grossly violated,1,1,1.0
violated in,1,1,1.0
such problems,1,1,1.0
problems almost,1,1,1.0
this situation,1,1,1.0
situation is,1,1,1.0
is known,4,2,2.0
known as,5,3,1.6666666666666667
standard classifiers,1,1,1.0
classifiers tend,1,1,1.0
class its,3,2,1.5
its importance,1,1,1.0
importance grew,1,1,1.0
grew as,1,1,1.0
as more,1,1,1.0
more and,1,1,1.0
and more,3,1,3.0
more researchers,1,1,1.0
researchers realized,1,1,1.0
realized that,1,1,1.0
this imbalance,1,1,1.0
imbalance causes,1,1,1.0
causes suboptimal,1,1,1.0
suboptimal classification,1,1,1.0
classification performance,4,1,4.0
most algorithms,2,2,1.0
algorithms behave,1,1,1.0
behave badly,1,1,1.0
badly when,1,1,1.0
highly imbalanced,8,3,2.6666666666666665
imbalanced the,2,2,1.0
been a,1,1,1.0
in machine,4,2,2.0
years class,1,1,1.0
recognized to,1,1,1.0
be existing,1,1,1.0
existing in,1,1,1.0
in lots,1,1,1.0
lots of,2,1,2.0
of application,1,1,1.0
as spotting,1,1,1.0
spotting unreliable,1,1,1.0
unreliable telecommunication,1,1,1.0
telecommunication customers,1,1,1.0
customers detection,1,1,1.0
images learning,1,1,1.0
learning word,1,1,1.0
word pronunciations,1,1,1.0
pronunciations text,1,1,1.0
classification risk,1,1,1.0
risk management,1,1,1.0
management information,1,1,1.0
information retrieval,5,2,2.5
retrieval and,1,1,1.0
and filtering,1,1,1.0
filtering tasks,1,1,1.0
tasks medical,1,1,1.0
diagnosis rare,1,1,1.0
rare disease,2,1,2.0
disease and,1,1,1.0
and rare,3,1,3.0
rare genes,1,1,1.0
genes mutations,1,1,1.0
mutations network,1,1,1.0
network monitoring,1,1,1.0
monitoring and,1,1,1.0
detection shuttle,1,1,1.0
shuttle system,1,1,1.0
system failure,1,1,1.0
failure earthquakes,1,1,1.0
earthquakes and,1,1,1.0
and nuclear,1,1,1.0
nuclear explosions,1,1,1.0
explosions and,1,1,1.0
and helicopter,1,1,1.0
helicopter fault,1,1,1.0
fault monitoring,1,1,1.0
monitoring from,1,1,1.0
the view,1,1,1.0
view of,1,1,1.0
of applications,2,2,1.0
applications the,1,1,1.0
the imbalance,11,3,3.6666666666666665
imbalance falls,1,1,1.0
falls in,1,1,1.0
in two,4,3,1.3333333333333333
two cases,1,1,1.0
cases the,4,2,2.0
are naturally,1,1,1.0
naturally imbalanced,2,1,2.0
imbalanced credit,1,1,1.0
card frauds,1,1,1.0
frauds and,1,1,1.0
disease or,1,1,1.0
or the,3,2,1.5
not naturally,1,1,1.0
imbalanced but,1,1,1.0
but it,4,3,1.3333333333333333
is too,1,1,1.0
too expensive,2,1,2.0
expensive to,2,1,2.0
obtain data,1,1,1.0
data of,2,2,1.0
class shuttle,1,1,1.0
shuttle failure,1,1,1.0
failure for,1,1,1.0
learning there,1,1,1.0
there have,2,1,2.0
been lots,1,1,1.0
of researches,1,1,1.0
researches on,3,1,3.0
on class,2,2,1.0
problem paper,1,1,1.0
reviewed various,1,1,1.0
various techniques,3,2,1.5
handling imbalance,1,1,1.0
imbalance dataset,1,1,1.0
dataset problems,1,1,1.0
problems paper,1,1,1.0
paper traced,1,1,1.0
traced some,1,1,1.0
recent progress,1,1,1.0
the field,2,1,2.0
field of,2,1,2.0
which sofia,1,1,1.0
sofia visa,1,1,1.0
visa et,1,1,1.0
al argued,1,1,1.0
argued that,6,2,3.0
the poor,1,1,1.0
poor performance,1,1,1.0
classifiers produced,1,1,1.0
produced by,1,1,1.0
algorithms on,2,2,1.0
sets is,2,1,2.0
is mainly,1,1,1.0
mainly due,1,1,1.0
following three,1,1,1.0
three factors,1,1,1.0
factors accuracy,1,1,1.0
accuracy class,1,1,1.0
distribution and,5,2,2.5
and error,2,1,2.0
costs since,1,1,1.0
since they,4,3,1.3333333333333333
are rarely,1,1,1.0
rarely well,1,1,1.0
well satisfied,1,1,1.0
satisfied in,1,1,1.0
applications paper,1,1,1.0
paper discussed,1,1,1.0
discussed several,1,1,1.0
several issues,2,2,1.0
with skewed,3,1,3.0
skewed class,5,2,2.5
class distributions,28,2,14.0
distributions such,1,1,1.0
between learning,1,1,1.0
and class,5,2,2.5
distributions and,3,1,3.0
the limitations,1,1,1.0
limitations of,1,1,1.0
accuracy and,2,1,2.0
error rate,2,1,2.0
rate to,1,1,1.0
classifiers weiss,1,1,1.0
weiss presented,2,1,2.0
presented an,1,1,1.0
an overview,1,1,1.0
overview of,1,1,1.0
data he,1,1,1.0
he pays,1,1,1.0
pays particular,1,1,1.0
particular attention,1,1,1.0
to differences,2,1,2.0
differences and,1,1,1.0
and similarities,1,1,1.0
similarities between,1,1,1.0
the problems,3,1,3.0
problems of,3,1,3.0
of rare,1,1,1.0
rare classes,2,1,2.0
rare cases,2,1,2.0
cases he,1,1,1.0
he then,1,1,1.0
then discussed,1,1,1.0
discussed some,1,1,1.0
common issues,1,1,1.0
issues and,1,1,1.0
their range,1,1,1.0
in mining,2,1,2.0
the reminder,1,1,1.0
reminder of,1,1,1.0
the paper,6,2,3.0
paper is,3,2,1.5
is organized,2,2,1.0
organized as,2,2,1.0
follows section,2,2,1.0
section reviewed,1,1,1.0
activities including,1,1,1.0
including two,1,1,1.0
two fourth,1,1,1.0
on natural,3,1,3.0
natural computation,3,1,3.0
computation ieee,3,1,3.0
ieee doi,3,1,3.0
doi fourth,2,1,2.0
doi workshops,1,1,1.0
workshops and,1,1,1.0
and one,5,3,1.6666666666666667
one special,2,1,2.0
special issue,3,2,1.5
issue on,3,2,1.5
imbalance sections,1,1,1.0
sections surveyed,1,1,1.0
surveyed the,1,1,1.0
the remedies,2,1,2.0
remedies to,2,1,2.0
problem from,1,1,1.0
from four,1,1,1.0
levels popular,1,1,1.0
popular evaluation,2,1,2.0
sets were,1,1,1.0
were summarized,1,1,1.0
summarized in,1,1,1.0
in section,13,3,4.333333333333333
section section,1,1,1.0
section briefly,1,1,1.0
briefly analyzed,1,1,1.0
analyzed some,1,1,1.0
other factors,1,1,1.0
factors related,1,1,1.0
and section,1,1,1.0
section concluded,1,1,1.0
concluded the,1,1,1.0
paper and,4,3,1.3333333333333333
directions academic,1,1,1.0
academic activiti,1,1,1.0
activiti es,1,1,1.0
es on,1,1,1.0
as described,2,2,1.0
described above,1,1,1.0
above recognizing,1,1,1.0
recognizing class,1,1,1.0
problem exists,2,1,2.0
exists in,1,1,1.0
in extensive,1,1,1.0
extensive application,1,1,1.0
domains gave,1,1,1.0
gave rise,1,1,1.0
rise to,3,2,1.5
to two,3,3,1.0
two workshops,1,1,1.0
workshops held,1,1,1.0
held at,1,1,1.0
the top,1,1,1.0
top conferences,1,1,1.0
conferences in,1,1,1.0
ai and,1,1,1.0
on dealing,1,1,1.0
first workshop,2,1,2.0
workshop dedicated,1,1,1.0
dedicated to,1,1,1.0
problem was,3,1,3.0
was held,1,1,1.0
held in,1,1,1.0
in conjunction,1,1,1.0
conjunction with,1,1,1.0
the american,1,1,1.0
american association,1,1,1.0
association for,1,1,1.0
for artificial,1,1,1.0
intelligence conference,1,1,1.0
conference aaai,1,1,1.0
aaai its,1,1,1.0
its main,2,1,2.0
main contribution,1,1,1.0
contribution includes,1,1,1.0
includes observation,1,1,1.0
observation of,1,1,1.0
domains dealing,1,1,1.0
sets and,2,1,2.0
and several,1,1,1.0
important issues,1,1,1.0
issues such,1,1,1.0
as how,1,1,1.0
evaluate learning,1,1,1.0
algorithms what,1,1,1.0
what evaluation,1,1,1.0
evaluation measures,1,1,1.0
measures should,1,1,1.0
used one,1,1,1.0
class learning,2,1,2.0
learning versus,1,1,1.0
versus discriminating,1,1,1.0
discriminating methods,1,1,1.0
methods discussions,1,1,1.0
discussions over,1,1,1.0
over various,2,2,1.0
various methods,1,1,1.0
methods discussion,1,1,1.0
discussion of,2,2,1.0
the relation,3,2,1.5
between class,3,1,3.0
learning the,8,3,2.6666666666666665
the goal,3,2,1.5
goal of,1,1,1.0
of creating,1,1,1.0
creating classifiers,1,1,1.0
classifiers that,1,1,1.0
a range,1,1,1.0
of costs,1,1,1.0
and so,4,3,1.3333333333333333
so on,2,2,1.0
second workshop,1,1,1.0
workshop special,1,1,1.0
on machine,9,2,4.5
learning icml,1,1,1.0
icml in,1,1,1.0
most research,1,1,1.0
research on,2,1,2.0
was guided,1,1,1.0
guided by,1,1,1.0
workshop for,1,1,1.0
example roc,1,1,1.0
roc or,1,1,1.0
or cost,1,1,1.0
cost curves,1,1,1.0
curves were,1,1,1.0
as evaluation,1,1,1.0
metrics rather,1,1,1.0
than accuracy,1,1,1.0
accuracy the,1,1,1.0
the workshop,3,1,3.0
workshop was,1,1,1.0
was followed,1,1,1.0
followed by,1,1,1.0
by an,1,1,1.0
an interesting,2,1,2.0
interesting and,1,1,1.0
and vivid,1,1,1.0
vivid panel,1,1,1.0
panel discussion,1,1,1.0
discussion two,1,1,1.0
two major,1,1,1.0
major directions,1,1,1.0
directions presented,1,1,1.0
the research,2,2,1.0
research papers,1,1,1.0
papers of,1,1,1.0
workshop many,1,1,1.0
many papers,1,1,1.0
papers still,1,1,1.0
still reported,1,1,1.0
reported various,1,1,1.0
various tuning,1,1,1.0
tuning methods,1,1,1.0
methods applied,1,1,1.0
to decision,1,1,1.0
trees in,2,2,1.0
better on,1,1,1.0
sets even,1,1,1.0
even though,2,1,2.0
though presentations,1,1,1.0
presentations in,1,1,1.0
the previous,3,2,1.5
previous workshop,1,1,1.0
workshop showed,1,1,1.0
showed their,1,1,1.0
their shortcomings,1,1,1.0
shortcomings and,1,1,1.0
was commonly,1,1,1.0
commonly agreed,1,1,1.0
agreed that,1,1,1.0
that new,1,1,1.0
new classifiers,1,1,1.0
classifiers are,1,1,1.0
are needed,2,1,2.0
needed for,2,2,1.0
sets besides,1,1,1.0
besides under,1,1,1.0
under various,1,1,1.0
various aspects,1,1,1.0
aspects was,1,1,1.0
was present,1,1,1.0
present in,2,2,1.0
in half,1,1,1.0
half of,5,2,2.5
the papers,1,1,1.0
papers and,1,1,1.0
and was,2,2,1.0
was the,1,1,1.0
most debated,1,1,1.0
debated issue,1,1,1.0
issue even,1,1,1.0
though shows,1,1,1.0
that sampling,2,2,1.0
sampling has,1,1,1.0
same result,1,1,1.0
result as,1,1,1.0
as moving,1,1,1.0
moving the,1,1,1.0
decision threshold,2,2,1.0
threshold or,1,1,1.0
or adjusting,1,1,1.0
adjusting the,5,2,2.5
cost matrix,3,1,3.0
matrix a,1,1,1.0
result known,1,1,1.0
known since,1,1,1.0
since in,1,1,1.0
in addition,2,2,1.0
addition japkowicz,1,1,1.0
japkowicz questioned,1,1,1.0
questioned the,1,1,1.0
is responsible,1,1,1.0
responsible for,2,1,2.0
idea is,7,2,3.5
that within,1,1,1.0
imbalance leads,1,1,1.0
leads to,5,2,2.5
a severe,1,1,1.0
severe lack,1,1,1.0
lack of,2,2,1.0
of representation,1,1,1.0
representation of,8,3,2.6666666666666665
some important,1,1,1.0
important aspects,1,1,1.0
aspects of,1,1,1.0
sixth issue,1,1,1.0
of sigkdd,1,1,1.0
sigkdd exploration,1,1,1.0
exploration was,1,1,1.0
was dedicated,1,1,1.0
dedicated entirely,1,1,1.0
entirely to,1,1,1.0
which weiss,1,1,1.0
presented a,2,1,2.0
very good,3,2,1.5
good review,1,1,1.0
current research,4,2,2.0
on learning,8,3,2.6666666666666665
volume address,1,1,1.0
address mainly,1,1,1.0
mainly issues,1,1,1.0
sampling feature,1,1,1.0
selection and,1,1,1.0
example investigated,1,1,1.0
investigated a,1,1,1.0
a boosting,2,2,1.0
boosting method,1,1,1.0
method combined,1,1,1.0
combined with,2,2,1.0
with various,2,1,2.0
techniques of,1,1,1.0
the hard,2,1,2.0
hard to,1,1,1.0
classify examples,1,1,1.0
examples the,3,2,1.5
method improves,1,1,1.0
improves the,2,2,1.0
prediction accuracy,3,3,1.0
not sacrifice,1,1,1.0
sacrifice one,1,1,1.0
other with,1,1,1.0
with experiments,1,1,1.0
experiments on,2,2,1.0
sets remedies,1,1,1.0
remedies for,1,1,1.0
imbalance are,2,1,2.0
are of,6,2,3.0
levels according,1,1,1.0
the phases,1,1,1.0
phases in,1,1,1.0
in learning,6,2,3.0
learning changing,1,1,1.0
changing class,4,1,4.0
distributions mainly,1,1,1.0
mainly by,1,1,1.0
by techniques,1,1,1.0
techniques features,1,1,1.0
features selection,1,1,1.0
selection in,6,3,2.0
feature level,1,1,1.0
level classifiers,1,1,1.0
classifiers level,2,1,2.0
level by,1,1,1.0
by manipulating,3,2,1.5
manipulating classifiers,2,1,2.0
classifiers internally,2,1,2.0
internally and,1,1,1.0
for final,1,1,1.0
final classification,1,1,1.0
classification changing,1,1,1.0
distributions since,1,1,1.0
since examples,1,1,1.0
examples belong,1,1,1.0
fewer than,1,1,1.0
than those,3,1,3.0
those belong,1,1,1.0
situations of,2,1,2.0
problem one,1,1,1.0
one direct,1,1,1.0
direct way,1,1,1.0
to counter,1,1,1.0
counter the,1,1,1.0
change class,1,1,1.0
distributions balanced,1,1,1.0
balanced distributions,1,1,1.0
distributions can,1,1,1.0
be obtained,1,1,1.0
obtained by,3,2,1.5
class combining,1,1,1.0
the both,1,1,1.0
both and,4,2,2.0
other advanced,1,1,1.0
advanced sampling,6,2,3.0
sampling ways,1,1,1.0
ways there,1,1,1.0
are numerous,1,1,1.0
numerous researches,1,1,1.0
on changing,1,1,1.0
distributions weiss,1,1,1.0
weiss investigated,2,1,2.0
investigated the,3,2,1.5
the effect,8,3,2.6666666666666665
effect of,8,3,2.6666666666666665
distributions on,1,1,1.0
on decision,2,1,2.0
decision tree,3,2,1.5
tree by,1,1,1.0
by altering,1,1,1.0
altering class,1,1,1.0
distributions in,3,2,1.5
several ratios,1,1,1.0
ratios with,1,1,1.0
with accuracy,1,1,1.0
auc as,2,2,1.0
as metrics,1,1,1.0
in his,1,1,1.0
his doctoral,1,1,1.0
doctoral dissertation,1,1,1.0
dissertation all,1,1,1.0
the methods,10,2,5.0
methods falls,1,1,1.0
falls into,1,1,1.0
three basic,1,1,1.0
basic techniques,2,2,1.0
techniques heuristic,1,1,1.0
heuristic and,2,1,2.0
and heuristic,1,1,1.0
and sampling,3,2,1.5
and advanced,1,1,1.0
sampling compared,1,1,1.0
compared the,2,2,1.0
most naivest,1,1,1.0
naivest sampling,1,1,1.0
is random,1,1,1.0
random a,2,2,1.0
a method,3,2,1.5
method trying,1,1,1.0
balance class,2,1,2.0
distributions through,2,1,2.0
random elimination,1,1,1.0
elimination of,1,1,1.0
class examples,13,2,6.5
examples this,3,2,1.5
this leads,1,1,1.0
to discarding,1,1,1.0
discarding useful,1,1,1.0
useful data,1,1,1.0
for classifiers,2,1,2.0
classifiers there,1,1,1.0
been several,1,1,1.0
several heuristic,2,1,2.0
heuristic methods,2,1,2.0
methods proposed,1,1,1.0
proposed or,2,1,2.0
or introduced,2,1,2.0
introduced from,3,2,1.5
from data,1,1,1.0
data cleaning,1,1,1.0
cleaning in,1,1,1.0
years they,1,1,1.0
on either,1,1,1.0
either of,1,1,1.0
of two,4,2,2.0
different noise,1,1,1.0
noise model,1,1,1.0
model hypotheses,1,1,1.0
hypotheses one,1,1,1.0
one thinks,1,1,1.0
thinks examples,1,1,1.0
examples that,7,2,3.5
are near,1,1,1.0
classification boundary,1,1,1.0
two classes,4,3,1.3333333333333333
are noise,2,1,2.0
noise while,1,1,1.0
other considers,1,1,1.0
considers examples,1,1,1.0
examples with,6,2,3.0
more neighbors,1,1,1.0
different labels,1,1,1.0
labels are,1,1,1.0
noise condensed,1,1,1.0
condensed nearest,2,1,2.0
neighbor rule,3,1,3.0
rule cnn,1,1,1.0
cnn bases,1,1,1.0
bases on,1,1,1.0
a consistent,2,1,2.0
consistent subset,2,1,2.0
subset of,4,2,2.0
sample set,1,1,1.0
a subset,2,1,2.0
subset who,1,1,1.0
who can,1,1,1.0
can correctly,1,1,1.0
correctly classifies,1,1,1.0
classifies all,1,1,1.0
remaining examples,1,1,1.0
set when,1,1,1.0
when used,2,1,2.0
a stored,1,1,1.0
stored reference,1,1,1.0
reference set,1,1,1.0
set for,1,1,1.0
the nn,1,1,1.0
nn rule,1,1,1.0
rule if,1,1,1.0
the bayesian,2,1,2.0
bayesian risk,2,1,2.0
risk is,3,2,1.5
small if,1,1,1.0
underlying densities,1,1,1.0
densities of,1,1,1.0
the various,1,1,1.0
various classes,1,1,1.0
classes have,2,2,1.0
have small,1,1,1.0
small overlapping,2,1,2.0
overlapping then,1,1,1.0
algorithm will,1,1,1.0
to pick,1,1,1.0
pick out,1,1,1.0
out examples,1,1,1.0
examples near,2,1,2.0
near the,5,2,2.5
the perhaps,1,1,1.0
perhaps fuzzy,1,1,1.0
fuzzy boundary,1,1,1.0
classes typically,1,1,1.0
typically points,1,1,1.0
points deeply,1,1,1.0
deeply imbedded,1,1,1.0
imbedded within,1,1,1.0
class will,1,1,1.0
will not,2,2,1.0
be transferred,1,1,1.0
transferred to,1,1,1.0
to store,1,1,1.0
store since,1,1,1.0
they will,2,2,1.0
be correctly,1,1,1.0
correctly classified,4,1,4.0
classified if,1,1,1.0
is high,2,1,2.0
high then,1,1,1.0
then store,1,1,1.0
store will,1,1,1.0
will contain,1,1,1.0
contain essentially,1,1,1.0
essentially all,1,1,1.0
original training,2,2,1.0
set and,2,1,2.0
and no,1,1,1.0
no important,1,1,1.0
important reduction,1,1,1.0
reduction in,1,1,1.0
in training,1,1,1.0
training size,1,1,1.0
size will,1,1,1.0
will have,2,2,1.0
been achieved,1,1,1.0
achieved so,1,1,1.0
so cnn,1,1,1.0
cnn is,1,1,1.0
is effective,1,1,1.0
effective only,1,1,1.0
only binary,1,1,1.0
overlapping oss,1,1,1.0
oss randomly,1,1,1.0
randomly draws,1,1,1.0
draws one,1,1,1.0
one majority,2,2,1.0
class example,2,2,1.0
example and,7,2,3.5
all examples,2,1,2.0
then puts,1,1,1.0
puts these,1,1,1.0
these examples,3,1,3.0
in afterwards,1,1,1.0
afterwards use,1,1,1.0
use a,5,2,2.5
a over,1,1,1.0
in e,3,2,1.5
e to,1,1,1.0
classify the,1,1,1.0
in every,1,1,1.0
every misclassified,1,1,1.0
misclassified example,1,1,1.0
example from,2,2,1.0
from e,1,1,1.0
e is,5,2,2.5
is moved,1,1,1.0
moved to,1,1,1.0
to e,2,2,1.0
e the,1,1,1.0
idea behind,3,2,1.5
behind this,3,2,1.5
subset is,1,1,1.0
to eliminate,1,1,1.0
eliminate the,1,1,1.0
are distant,1,1,1.0
distant from,1,1,1.0
decision border,1,1,1.0
border since,1,1,1.0
since these,1,1,1.0
examples might,1,1,1.0
be considered,5,3,1.6666666666666667
considered less,1,1,1.0
less relevant,1,1,1.0
relevant for,1,1,1.0
learning wilson,1,1,1.0
wilson s,1,1,1.0
s edited,1,1,1.0
rule enn,1,1,1.0
enn removes,1,1,1.0
removes any,1,1,1.0
any example,2,1,2.0
example whose,1,1,1.0
whose class,1,1,1.0
label differs,1,1,1.0
differs from,1,1,1.0
of at,1,1,1.0
least two,1,1,1.0
its three,4,1,4.0
three nearest,4,1,4.0
neighbors different,1,1,1.0
from enn,1,1,1.0
enn neighborhood,1,1,1.0
neighborhood cleaning,1,1,1.0
cleaning rule,1,1,1.0
rule ncl,1,1,1.0
ncl deals,1,1,1.0
deals with,3,2,1.5
with majority,2,2,1.0
samples separately,1,1,1.0
separately when,1,1,1.0
when cleaning,1,1,1.0
cleaning the,1,1,1.0
sets ncl,1,1,1.0
ncl uses,1,1,1.0
uses enn,1,1,1.0
enn to,1,1,1.0
to remove,1,1,1.0
remove majority,1,1,1.0
majority examples,9,2,4.5
each example,7,2,3.5
example e,1,1,1.0
i in,3,2,1.5
set its,1,1,1.0
found if,1,1,1.0
if ei,1,1,1.0
ei belongs,1,1,1.0
classification given,1,1,1.0
by its,1,1,1.0
neighbors contradicts,1,1,1.0
contradicts the,1,1,1.0
original class,1,1,1.0
of e,3,3,1.0
i then,2,1,2.0
then ei,1,1,1.0
ei is,1,1,1.0
is removed,1,1,1.0
removed if,1,1,1.0
if e,1,1,1.0
i belongs,1,1,1.0
neighbors misclassify,1,1,1.0
misclassify e,1,1,1.0
are removed,2,1,2.0
removed compared,1,1,1.0
with above,1,1,1.0
above four,1,1,1.0
methods tomek,1,1,1.0
tomek links,3,2,1.5
links consider,1,1,1.0
consider samples,1,1,1.0
samples near,1,1,1.0
borderline should,1,1,1.0
be paid,1,1,1.0
paid more,1,1,1.0
attention given,1,1,1.0
given two,1,1,1.0
two examples,3,2,1.5
examples e,1,1,1.0
i and,1,1,1.0
and e,2,2,1.0
e j,2,1,2.0
j belonging,1,1,1.0
different classes,4,2,2.0
d e,2,2,1.0
i e,2,2,1.0
j is,1,1,1.0
between ei,1,1,1.0
ei and,1,1,1.0
and ej,1,1,1.0
ej a,1,1,1.0
a ei,1,1,1.0
ei ej,3,1,3.0
ej pair,1,1,1.0
pair is,1,1,1.0
called a,1,1,1.0
link if,1,1,1.0
if there,2,2,1.0
not an,1,1,1.0
example such,1,1,1.0
d ei,3,1,3.0
ei d,1,1,1.0
ej or,1,1,1.0
or d,1,1,1.0
d ej,1,1,1.0
ej d,1,1,1.0
ej if,1,1,1.0
if two,1,1,1.0
examples form,2,1,2.0
form a,3,2,1.5
link then,1,1,1.0
then either,1,1,1.0
either one,1,1,1.0
examples is,1,1,1.0
is noise,1,1,1.0
noise or,1,1,1.0
both examples,1,1,1.0
form borderline,1,1,1.0
borderline so,1,1,1.0
so tomek,1,1,1.0
link can,1,1,1.0
be viewed,2,1,2.0
viewed as,2,1,2.0
an method,3,2,1.5
when examples,1,1,1.0
removed it,1,1,1.0
that tomek,1,1,1.0
link enn,1,1,1.0
enn and,1,1,1.0
and ncl,1,1,1.0
ncl are,1,1,1.0
highly since,1,1,1.0
since for,1,1,1.0
for any,6,2,3.0
sets nearest,1,1,1.0
sample must,1,1,1.0
found so,1,1,1.0
is impossible,1,1,1.0
impossible for,1,1,1.0
datasets random,1,1,1.0
random is,1,1,1.0
that aims,1,1,1.0
aims to,2,2,1.0
random replication,2,2,1.0
replication of,2,2,1.0
examples random,1,1,1.0
random has,1,1,1.0
has two,1,1,1.0
two shortcomings,1,1,1.0
shortcomings first,1,1,1.0
will increase,3,2,1.5
the likelihood,1,1,1.0
likelihood of,1,1,1.0
of occurring,1,1,1.0
occurring since,1,1,1.0
since it,5,2,2.5
it makes,1,1,1.0
makes exact,1,1,1.0
exact copies,1,1,1.0
examples second,1,1,1.0
second sampling,1,1,1.0
sampling makes,1,1,1.0
makes learning,1,1,1.0
process more,1,1,1.0
more consuming,1,1,1.0
consuming if,1,1,1.0
is already,1,1,1.0
already fairly,1,1,1.0
fairly large,1,1,1.0
large but,1,1,1.0
but imbalanced,1,1,1.0
imbalanced there,1,1,1.0
methods mainly,1,1,1.0
mainly based,1,1,1.0
smote generates,2,2,1.0
main idea,2,2,1.0
to form,4,2,2.0
form new,1,1,1.0
interpolating between,1,1,1.0
that lie,1,1,1.0
lie together,1,1,1.0
together by,1,1,1.0
of replication,1,1,1.0
replication smote,1,1,1.0
and causes,1,1,1.0
causes the,1,1,1.0
to spread,1,1,1.0
spread further,1,1,1.0
further into,1,1,1.0
class space,1,1,1.0
space recognizing,1,1,1.0
recognizing examples,1,1,1.0
borderline of,1,1,1.0
more easily,2,2,1.0
easily misclassified,1,1,1.0
misclassified than,1,1,1.0
those far,1,1,1.0
far from,2,2,1.0
borderline was,1,1,1.0
proposed it,1,1,1.0
it only,1,1,1.0
only sample,1,1,1.0
sample the,3,2,1.5
borderline examples,1,1,1.0
while smote,2,2,1.0
random augment,1,1,1.0
augment the,1,1,1.0
class through,2,2,1.0
through all,1,1,1.0
or a,2,2,1.0
random subset,1,1,1.0
class experiments,1,1,1.0
experiments show,1,1,1.0
that their,2,1,2.0
their approaches,1,1,1.0
approaches achieve,1,1,1.0
achieve better,2,2,1.0
better tp,1,1,1.0
rate and,3,1,3.0
and than,1,1,1.0
than smote,1,1,1.0
random methods,1,1,1.0
methods advanced,1,1,1.0
sampling different,1,1,1.0
from various,1,1,1.0
various and,1,1,1.0
and methods,4,2,2.0
methods above,1,1,1.0
above the,1,1,1.0
following advanced,1,1,1.0
methods do,1,1,1.0
do based,1,1,1.0
results of,15,3,5.0
of preliminary,2,1,2.0
preliminary classifications,1,1,1.0
classifications boosting,1,1,1.0
boosting is,1,1,1.0
an iterative,2,2,1.0
iterative algorithm,1,1,1.0
that place,1,1,1.0
place different,1,1,1.0
different weights,1,1,1.0
weights on,1,1,1.0
training distributions,1,1,1.0
distributions each,2,1,2.0
each iteration,7,2,3.5
iteration after,1,1,1.0
after each,1,1,1.0
iteration boosting,1,1,1.0
boosting increases,1,1,1.0
the weights,4,1,4.0
weights associated,3,1,3.0
the incorrectly,2,1,2.0
incorrectly classified,3,1,3.0
classified examples,4,1,4.0
examples and,9,2,4.5
and decreases,1,1,1.0
decreases the,1,1,1.0
the correctly,2,1,2.0
examples separately,1,1,1.0
separately this,1,1,1.0
this forces,1,1,1.0
forces the,2,2,1.0
the learner,2,1,2.0
learner to,3,2,1.5
to focus,4,2,2.0
focus more,2,2,1.0
more on,2,2,1.0
next iteration,2,2,1.0
iteration note,1,1,1.0
that boosting,2,1,2.0
boosting effectively,1,1,1.0
effectively alters,1,1,1.0
alters the,2,1,2.0
the distributions,5,2,2.5
distributions of,2,1,2.0
considered to,4,3,1.3333333333333333
a type,1,1,1.0
of advanced,1,1,1.0
sampling technique,2,2,1.0
technique weiss,1,1,1.0
weiss proposed,1,1,1.0
proposed a,3,2,1.5
a heuristic,1,1,1.0
heuristic algorithm,1,1,1.0
for selecting,2,2,1.0
selecting training,1,1,1.0
that approximates,1,1,1.0
approximates optimum,1,1,1.0
optimum the,1,1,1.0
the sensitive,1,1,1.0
sensitive sampling,1,1,1.0
sampling strategy,1,1,1.0
strategy makes,1,1,1.0
makes two,1,1,1.0
additional assumptions,1,1,1.0
assumptions first,1,1,1.0
it assumes,3,2,1.5
assumes that,2,2,1.0
of potentially,2,2,1.0
potentially available,1,1,1.0
is sufficiently,1,1,1.0
sufficiently large,1,1,1.0
large so,1,1,1.0
a training,2,2,1.0
with n,1,1,1.0
n examples,1,1,1.0
examples can,1,1,1.0
be formed,1,1,1.0
formed with,1,1,1.0
with any,3,2,1.5
any desired,1,1,1.0
desired marginal,1,1,1.0
marginal class,2,1,2.0
distributions the,2,1,2.0
second assumption,1,1,1.0
of executing,1,1,1.0
executing the,1,1,1.0
is negligible,1,1,1.0
negligible compared,1,1,1.0
of procuring,1,1,1.0
procuring examples,1,1,1.0
assumption permits,1,1,1.0
permits the,1,1,1.0
be run,1,1,1.0
run multiple,1,1,1.0
multiple times,1,1,1.0
times in,1,1,1.0
provide guidance,1,1,1.0
guidance about,1,1,1.0
about which,2,2,1.0
which examples,1,1,1.0
to select,5,2,2.5
select he,1,1,1.0
he argued,1,1,1.0
that though,1,1,1.0
the heuristically,1,1,1.0
heuristically determined,1,1,1.0
determined class,1,1,1.0
distributions associated,1,1,1.0
the final,1,1,1.0
final training,1,1,1.0
not guaranteed,1,1,1.0
guaranteed to,2,1,2.0
classifier the,2,1,2.0
classifier induced,1,1,1.0
induced using,1,1,1.0
distributions performs,1,1,1.0
well in,1,1,1.0
in practice,2,2,1.0
practice han,1,1,1.0
proposed an,1,1,1.0
algorithm based,2,1,2.0
on preliminary,2,1,2.0
preliminary classification,3,1,3.0
classification ospc,1,1,1.0
ospc firstly,1,1,1.0
firstly preliminary,1,1,1.0
classification was,1,1,1.0
made on,1,1,1.0
to save,1,1,1.0
save the,1,1,1.0
the useful,2,2,1.0
useful information,1,1,1.0
information of,4,2,2.0
as much,2,1,2.0
much as,1,1,1.0
as possible,2,2,1.0
possible then,1,1,1.0
were predicted,1,1,1.0
predicted to,1,1,1.0
to belong,1,1,1.0
class were,1,1,1.0
were reclassified,1,1,1.0
reclassified to,1,1,1.0
class ospc,1,1,1.0
ospc was,1,1,1.0
was argued,3,1,3.0
argued to,1,1,1.0
in terms,8,3,2.6666666666666665
terms of,8,3,2.6666666666666665
of changing,2,1,2.0
distributions above,1,1,1.0
above are,1,1,1.0
are trying,2,2,1.0
of imbalance,1,1,1.0
imbalance a,1,1,1.0
a proposed,1,1,1.0
improve accuracy,1,1,1.0
by dealing,1,1,1.0
of between,1,1,1.0
and within,2,2,1.0
imbalance simultaneously,1,1,1.0
simultaneously when,1,1,1.0
are severely,1,1,1.0
severely skewed,1,1,1.0
skewed sampling,1,1,1.0
often combined,1,1,1.0
combined to,1,1,1.0
improve generalization,1,1,1.0
learner batista,1,1,1.0
batista et,1,1,1.0
al presented,2,1,2.0
comparison and,2,1,2.0
and combination,1,1,1.0
of various,3,2,1.5
various sampling,1,1,1.0
sampling strategies,1,1,1.0
strategies they,1,1,1.0
they noted,3,1,3.0
that combining,1,1,1.0
combining focused,1,1,1.0
focused and,1,1,1.0
and such,1,1,1.0
smote combining,2,1,2.0
combining with,2,1,2.0
with tomek,2,2,1.0
link or,1,1,1.0
or smote,1,1,1.0
with enn,1,1,1.0
enn is,1,1,1.0
is applicable,1,1,1.0
applicable when,1,1,1.0
imbalanced or,1,1,1.0
or there,1,1,1.0
very few,1,1,1.0
few examples,1,1,1.0
class feature,1,1,1.0
selection the,1,1,1.0
majority of,1,1,1.0
of work,1,1,1.0
in feature,1,1,1.0
sets has,1,1,1.0
has focused,1,1,1.0
focused on,7,3,2.3333333333333335
on text,2,1,2.0
classification or,2,1,2.0
or web,2,1,2.0
web categorization,2,1,2.0
categorization domain,1,1,1.0
domain a,1,1,1.0
a couple,1,1,1.0
couple of,1,1,1.0
this issue,6,3,2.0
issue look,1,1,1.0
look at,1,1,1.0
at feature,1,1,1.0
sets albeit,1,1,1.0
albeit in,1,1,1.0
in text,1,1,1.0
categorization zheng,1,1,1.0
zheng et,1,1,1.0
that existing,1,1,1.0
existing measures,2,1,2.0
measures used,1,1,1.0
selection are,1,1,1.0
not very,1,1,1.0
very appropriate,1,1,1.0
appropriate for,1,1,1.0
sets they,3,1,3.0
they proposed,1,1,1.0
a feature,1,1,1.0
selection framework,1,1,1.0
framework which,1,1,1.0
which selects,1,1,1.0
selects features,1,1,1.0
features for,3,2,1.5
for positive,1,1,1.0
and negative,6,2,3.0
negative classes,1,1,1.0
classes separately,1,1,1.0
separately and,1,1,1.0
then explicitly,1,1,1.0
explicitly combines,1,1,1.0
combines them,1,1,1.0
them the,1,1,1.0
authors showed,1,1,1.0
showed simple,1,1,1.0
simple ways,1,1,1.0
ways of,1,1,1.0
of converting,1,1,1.0
converting existing,1,1,1.0
measures so,1,1,1.0
they separately,1,1,1.0
separately consider,1,1,1.0
consider features,1,1,1.0
classes castillo,1,1,1.0
castillo and,3,1,3.0
and serrano,3,1,3.0
serrano did,1,1,1.0
not particularly,2,1,2.0
particularly focus,2,1,2.0
focus on,8,3,2.6666666666666665
on feature,2,1,2.0
selection but,2,1,2.0
but made,1,1,1.0
made it,1,1,1.0
it a,3,2,1.5
a part,2,1,2.0
their complete,2,1,2.0
complete framework,2,1,2.0
framework putten,1,1,1.0
putten and,2,1,2.0
and someren,1,1,1.0
someren analyzed,1,1,1.0
analyzed the,1,1,1.0
the coil,2,1,2.0
coil data,1,1,1.0
sets using,3,3,1.0
the decomposition,1,1,1.0
decomposition and,1,1,1.0
and they,2,1,2.0
they reported,1,1,1.0
reported that,2,1,2.0
key issue,1,1,1.0
issue for,2,2,1.0
this particular,1,1,1.0
particular data,1,1,1.0
was avoiding,1,1,1.0
avoiding they,1,1,1.0
they concluded,1,1,1.0
concluded that,4,2,2.0
such domains,1,1,1.0
domains is,1,1,1.0
is even,1,1,1.0
even more,1,1,1.0
important than,2,2,1.0
choice of,1,1,1.0
learning method,3,2,1.5
method classifiers,1,1,1.0
level manipulating,1,1,1.0
internally drummond,1,1,1.0
drummond and,2,1,2.0
and holte,2,1,2.0
holte reported,1,1,1.0
that when,5,2,2.5
using s,1,1,1.0
s default,1,1,1.0
default settings,1,1,1.0
settings is,1,1,1.0
is surprisingly,1,1,1.0
surprisingly ineffective,1,1,1.0
ineffective often,1,1,1.0
often producing,1,1,1.0
producing little,1,1,1.0
little or,1,1,1.0
or no,1,1,1.0
no change,1,1,1.0
change in,1,1,1.0
in performance,2,1,2.0
in response,1,1,1.0
response to,1,1,1.0
to modifications,1,1,1.0
modifications of,2,1,2.0
misclassification costs,4,1,4.0
distributions moreover,2,2,1.0
moreover they,1,1,1.0
that prunes,1,1,1.0
prunes less,1,1,1.0
less and,1,1,1.0
and therefore,4,3,1.3333333333333333
therefore generalizes,1,1,1.0
generalizes less,1,1,1.0
than and,1,1,1.0
modification of,1,1,1.0
s parameter,1,1,1.0
parameter settings,1,1,1.0
settings to,1,1,1.0
to increase,2,1,2.0
the influence,1,1,1.0
influence of,1,1,1.0
of pruning,1,1,1.0
pruning and,1,1,1.0
other avoidance,1,1,1.0
avoidance factors,1,1,1.0
factors can,1,1,1.0
can reestablish,1,1,1.0
reestablish the,1,1,1.0
some classifiers,1,1,1.0
classifiers such,2,1,2.0
the naive,1,1,1.0
naive bayes,4,1,4.0
bayes classifier,1,1,1.0
classifier or,1,1,1.0
or some,1,1,1.0
some neural,1,1,1.0
networks yield,1,1,1.0
score that,1,1,1.0
that represents,1,1,1.0
the degree,8,2,4.0
degree to,1,1,1.0
to which,2,2,1.0
which an,1,1,1.0
a member,1,1,1.0
member of,2,2,1.0
class such,1,1,1.0
such ranking,1,1,1.0
ranking can,1,1,1.0
produce several,1,1,1.0
several classifiers,1,1,1.0
classifiers by,1,1,1.0
by varying,1,1,1.0
varying the,1,1,1.0
the threshold,1,1,1.0
threshold of,1,1,1.0
example pertaining,1,1,1.0
pertaining to,1,1,1.0
for internally,1,1,1.0
internally biasing,1,1,1.0
biasing the,1,1,1.0
the discrimination,1,1,1.0
discrimination procedure,1,1,1.0
a weighted,3,3,1.0
weighted distance,2,1,2.0
function was,1,1,1.0
proposed in,6,3,2.0
in to,3,2,1.5
classification phase,1,1,1.0
phase of,2,1,2.0
of knn,1,1,1.0
knn the,1,1,1.0
the basic,2,2,1.0
basic idea,2,2,1.0
this weighted,1,1,1.0
to compensate,3,2,1.5
compensate for,3,2,1.5
sample without,1,1,1.0
without actually,1,1,1.0
actually altering,1,1,1.0
altering the,1,1,1.0
distributions thus,1,1,1.0
thus weights,1,1,1.0
weights are,2,1,2.0
are assigned,2,1,2.0
assigned unlike,1,1,1.0
unlike in,1,1,1.0
the usual,1,1,1.0
usual weighted,1,1,1.0
weighted rule,1,1,1.0
rule to,1,1,1.0
and not,1,1,1.0
not to,1,1,1.0
individual prototypes,1,1,1.0
prototypes in,1,1,1.0
way since,1,1,1.0
the weighting,1,1,1.0
weighting factor,1,1,1.0
factor is,1,1,1.0
is greater,1,1,1.0
greater for,1,1,1.0
class than,2,1,2.0
one the,3,2,1.5
distance to,3,2,1.5
to positive,1,1,1.0
class prototypes,1,1,1.0
prototypes becomes,1,1,1.0
becomes much,1,1,1.0
much lower,3,2,1.5
lower than,1,1,1.0
to prototypes,1,1,1.0
prototypes of,2,1,2.0
this produces,1,1,1.0
produces a,1,1,1.0
a tendency,1,1,1.0
tendency for,1,1,1.0
new patterns,3,2,1.5
patterns to,8,2,4.0
find their,1,1,1.0
neighbor among,1,1,1.0
the prototypes,1,1,1.0
class approach,1,1,1.0
to dealing,1,1,1.0
using svm,1,1,1.0
svm biases,1,1,1.0
biases the,1,1,1.0
the academic,2,1,2.0
academic is,1,1,1.0
is further,1,1,1.0
further away,1,1,1.0
away from,2,2,1.0
done in,6,2,3.0
the skew,1,1,1.0
skew associated,1,1,1.0
datasets which,1,1,1.0
which pushes,1,1,1.0
pushes the,1,1,1.0
the closer,1,1,1.0
this biasing,1,1,1.0
biasing can,1,1,1.0
be accomplished,2,2,1.0
accomplished in,1,1,1.0
various ways,1,1,1.0
ways in,1,1,1.0
is proposed,2,2,1.0
by changing,1,1,1.0
changing the,4,1,4.0
the kernel,42,3,14.0
kernel function,22,2,11.0
function to,4,3,1.3333333333333333
to develop,1,1,1.0
develop this,1,1,1.0
this bias,1,1,1.0
bias veropoulos,1,1,1.0
veropoulos et,1,1,1.0
s uggested,1,1,1.0
uggested using,1,1,1.0
different penalty,2,2,1.0
penalty constants,1,1,1.0
constants for,1,1,1.0
classes of,12,2,6.0
data making,1,1,1.0
making errors,1,1,1.0
errors on,2,1,2.0
on positive,1,1,1.0
positive examples,5,1,5.0
examples costlier,1,1,1.0
costlier than,1,1,1.0
than errors,1,1,1.0
on negative,1,1,1.0
negative examples,8,1,8.0
examples kaizhu,1,1,1.0
kaizhu huang,1,1,1.0
huang et,1,1,1.0
presented biased,1,1,1.0
biased minimax,2,1,2.0
minimax probability,2,1,2.0
probability machine,2,1,2.0
machine bmpm,1,1,1.0
bmpm to,1,1,1.0
resolve the,1,1,1.0
problem given,1,1,1.0
the reliable,1,1,1.0
reliable mean,1,1,1.0
mean and,3,2,1.5
and covariance,1,1,1.0
covariance matrices,2,2,1.0
matrices of,1,1,1.0
classes bmpm,1,1,1.0
bmpm can,1,1,1.0
can derive,1,1,1.0
derive the,1,1,1.0
decision by,1,1,1.0
by adjusting,2,2,1.0
the lower,1,1,1.0
lower bound,1,1,1.0
bound of,1,1,1.0
the real,2,2,1.0
real accuracy,1,1,1.0
the testing,3,2,1.5
testing set,1,1,1.0
learning besides,1,1,1.0
besides changing,1,1,1.0
distributions incorporating,1,1,1.0
incorporating costs,1,1,1.0
in making,1,1,1.0
making is,1,1,1.0
improve classifier,1,1,1.0
when learning,1,1,1.0
datasets cost,1,1,1.0
cost model,1,1,1.0
model takes,1,1,1.0
form of,2,2,1.0
a cost,2,2,1.0
matrix as,1,1,1.0
as shown,6,2,3.0
in where,2,1,2.0
of classifying,2,2,1.0
classifying a,1,1,1.0
sample from,1,1,1.0
true class,6,2,3.0
class j,3,1,3.0
j to,1,1,1.0
class i,3,1,3.0
i corresponds,1,1,1.0
corresponds to,10,2,5.0
the matrix,3,3,1.0
matrix entry,1,1,1.0
entry λij,1,1,1.0
λij this,1,1,1.0
this matrix,1,1,1.0
usually expressed,1,1,1.0
expressed in,1,1,1.0
of average,1,1,1.0
average misclassification,1,1,1.0
costs for,1,1,1.0
the diagonal,1,1,1.0
diagonal elements,1,1,1.0
elements are,1,1,1.0
are usually,1,1,1.0
usually set,1,1,1.0
set to,11,3,3.6666666666666665
to zero,2,2,1.0
zero meaning,1,1,1.0
meaning correct,1,1,1.0
correct classification,1,1,1.0
classification has,1,1,1.0
no cost,1,1,1.0
cost the,1,1,1.0
goal in,2,2,1.0
in sensitive,1,1,1.0
sensitive classification,1,1,1.0
to minimize,1,1,1.0
minimize the,1,1,1.0
misclassification which,1,1,1.0
be realized,1,1,1.0
realized by,1,1,1.0
by choosing,1,1,1.0
choosing the,3,2,1.5
the minimum,1,1,1.0
minimum conditional,1,1,1.0
conditional risk,1,1,1.0
risk prediction,1,1,1.0
prediction class,1,1,1.0
i class,1,1,1.0
j true,1,1,1.0
i ijλ,1,1,1.0
ijλ class,1,1,1.0
j jiλ,1,1,1.0
jiλ fig,1,1,1.0
fig cost,1,1,1.0
matrix metacost,1,1,1.0
metacost is,2,1,2.0
to make,6,2,3.0
procedure begins,1,1,1.0
begins to,2,1,2.0
learn an,2,1,2.0
an internal,2,1,2.0
internal model,2,1,2.0
model by,1,1,1.0
by applying,3,3,1.0
applying a,2,2,1.0
a sensitive,1,1,1.0
sensitive procedure,1,1,1.0
procedure which,1,1,1.0
which employs,1,1,1.0
employs a,2,2,1.0
a base,2,2,1.0
base learning,1,1,1.0
algorithm then,1,1,1.0
then metacost,1,1,1.0
metacost procedure,1,1,1.0
procedure estimates,1,1,1.0
estimates class,2,1,2.0
class probabilities,2,1,2.0
probabilities using,2,1,2.0
using bagging,2,1,2.0
bagging and,3,1,3.0
with their,2,1,2.0
their minimum,2,1,2.0
minimum expected,2,1,2.0
expected cost,2,1,2.0
cost classes,2,1,2.0
finally relearns,2,1,2.0
relearns a,2,1,2.0
model using,2,1,2.0
the modified,2,1,2.0
modified training,2,1,2.0
set adaboost,1,1,1.0
adaboost s,2,1,2.0
s rule,2,1,2.0
rule has,1,1,1.0
made sensitive,1,1,1.0
sensitive so,1,1,1.0
that examples,2,2,1.0
examples belonging,2,1,2.0
to rare,1,1,1.0
rare class,2,1,2.0
are misclassified,1,1,1.0
misclassified are,1,1,1.0
assigned higher,1,1,1.0
higher weights,1,1,1.0
weights than,1,1,1.0
those belonging,1,1,1.0
to common,1,1,1.0
common class,2,1,2.0
the resulting,5,2,2.5
resulting system,2,1,2.0
system adacost,2,1,2.0
adacost has,2,1,2.0
been empirically,3,1,3.0
empirically shown,2,1,2.0
produce lower,2,1,2.0
lower cumulative,2,1,2.0
cumulative misclassification,2,1,2.0
costs than,2,1,2.0
than adaboost,2,1,2.0
adaboost learning,1,1,1.0
learning learning,1,1,1.0
learning is,5,2,2.5
approach which,1,1,1.0
which provides,1,1,1.0
provides an,1,1,1.0
an alternative,1,1,1.0
alternative to,2,2,1.0
to discrimination,1,1,1.0
discrimination where,1,1,1.0
model can,1,1,1.0
created based,1,1,1.0
target class,2,1,2.0
class alone,1,1,1.0
alone here,1,1,1.0
here classification,1,1,1.0
is accomplished,3,2,1.5
accomplished by,3,2,1.5
by imposing,1,1,1.0
imposing a,1,1,1.0
a threshold,1,1,1.0
threshold on,1,1,1.0
the similarity,1,1,1.0
similarity value,1,1,1.0
value between,1,1,1.0
query object,1,1,1.0
object and,1,1,1.0
class mainly,1,1,1.0
mainly two,1,1,1.0
of learners,1,1,1.0
learners were,1,1,1.0
were previously,1,1,1.0
previously studied,1,1,1.0
studied in,1,1,1.0
the approach,6,3,2.0
approach svms,1,1,1.0
svms and,2,2,1.0
be competitive,1,1,1.0
competitive besides,1,1,1.0
besides systems,1,1,1.0
systems that,1,1,1.0
that learn,1,1,1.0
learn only,1,1,1.0
only the,6,3,2.0
class may,2,2,1.0
may still,1,1,1.0
still train,1,1,1.0
train using,1,1,1.0
using examples,1,1,1.0
to all,2,2,1.0
all classes,1,1,1.0
classes brute,1,1,1.0
brute shrink,1,1,1.0
shrink and,1,1,1.0
and ripper,1,1,1.0
ripper are,1,1,1.0
are three,1,1,1.0
three such,1,1,1.0
such data,1,1,1.0
mining systems,1,1,1.0
systems brute,1,1,1.0
brute has,1,1,1.0
been used,3,2,1.5
to look,1,1,1.0
look for,1,1,1.0
for flaws,1,1,1.0
flaws in,1,1,1.0
the boeing,1,1,1.0
boeing manufacturing,2,1,2.0
manufacturing process,1,1,1.0
process shrink,1,1,1.0
shrink uses,1,1,1.0
similar approach,1,1,1.0
to detect,1,1,1.0
detect rare,1,1,1.0
rare oil,1,1,1.0
spills from,1,1,1.0
from satellite,1,1,1.0
images based,1,1,1.0
assumption that,1,1,1.0
there will,1,1,1.0
be many,1,1,1.0
more negative,1,1,1.0
examples than,1,1,1.0
than positive,1,1,1.0
examples shrink,1,1,1.0
shrink labels,1,1,1.0
labels mixed,1,1,1.0
mixed regions,1,1,1.0
regions regions,1,1,1.0
regions with,1,1,1.0
with positive,1,1,1.0
class ripper,1,1,1.0
ripper is,1,1,1.0
a rule,1,1,1.0
induction system,1,1,1.0
system that,1,1,1.0
utilizes a,1,1,1.0
to iteratively,1,1,1.0
iteratively build,1,1,1.0
build rules,1,1,1.0
rules to,1,1,1.0
to cover,1,1,1.0
cover previously,1,1,1.0
previously uncovered,1,1,1.0
uncovered training,1,1,1.0
examples each,1,1,1.0
each rule,1,1,1.0
rule is,1,1,1.0
is grown,1,1,1.0
grown by,1,1,1.0
by adding,3,2,1.5
adding conditions,1,1,1.0
conditions until,1,1,1.0
until no,1,1,1.0
no negative,1,1,1.0
are covered,1,1,1.0
covered it,1,1,1.0
it normally,1,1,1.0
normally generates,1,1,1.0
generates rules,1,1,1.0
rules for,1,1,1.0
class from,2,2,1.0
most rare,1,1,1.0
most common,4,2,2.0
class so,2,2,1.0
this view,1,1,1.0
view ripper,1,1,1.0
ripper can,1,1,1.0
be view,1,1,1.0
view as,1,1,1.0
a learner,1,1,1.0
learner an,1,1,1.0
interesting aspect,1,1,1.0
of based,1,1,1.0
based learning,1,1,1.0
that under,1,1,1.0
under certain,1,1,1.0
certain conditions,2,1,2.0
conditions such,1,1,1.0
as of,1,1,1.0
domain space,1,1,1.0
space one,2,2,1.0
class approaches,1,1,1.0
approaches to,4,3,1.3333333333333333
to solving,1,1,1.0
problem may,1,1,1.0
may in,1,1,1.0
in fact,1,1,1.0
fact be,1,1,1.0
be superior,1,1,1.0
superior to,1,1,1.0
to discriminative,1,1,1.0
discriminative approaches,1,1,1.0
as decision,1,1,1.0
trees or,1,1,1.0
or neural,1,1,1.0
networks raskutti,1,1,1.0
raskutti and,2,1,2.0
and kowalczyk,2,1,2.0
kowalczyk demonstrated,1,1,1.0
demonstrated the,1,1,1.0
the optimality,1,1,1.0
optimality of,1,1,1.0
of svms,3,2,1.5
svms over,1,1,1.0
over ones,1,1,1.0
ones in,1,1,1.0
in certain,2,1,2.0
certain important,1,1,1.0
important domains,1,1,1.0
domains including,1,1,1.0
including genomic,1,1,1.0
genomic data,1,1,1.0
particular they,1,1,1.0
they showed,1,1,1.0
that class,4,1,4.0
is particularly,1,1,1.0
particularly useful,1,1,1.0
useful when,1,1,1.0
used on,1,1,1.0
on extremely,1,1,1.0
extremely unbalanced,1,1,1.0
unbalanced data,2,1,2.0
sets composed,1,1,1.0
composed of,2,2,1.0
a high,1,1,1.0
dimensional noisy,1,1,1.0
noisy feature,1,1,1.0
feature space,71,3,23.666666666666668
space they,1,1,1.0
they argued,1,1,1.0
to aggressive,1,1,1.0
aggressive feature,1,1,1.0
selection methods,1,1,1.0
methods but,1,1,1.0
but is,2,1,2.0
is more,4,3,1.3333333333333333
more practical,1,1,1.0
practical since,1,1,1.0
since feature,1,1,1.0
selection can,1,1,1.0
can often,1,1,1.0
often be,1,1,1.0
be too,1,1,1.0
apply ensemble,1,1,1.0
methods ensemble,1,1,1.0
learning has,2,2,1.0
has established,1,1,1.0
established its,1,1,1.0
its superiority,1,1,1.0
superiority in,1,1,1.0
years of,1,1,1.0
which boosting,1,1,1.0
boosting bagging,1,1,1.0
bagging are,1,1,1.0
most successful,1,1,1.0
successful approaches,1,1,1.0
approaches ensemble,1,1,1.0
been extensively,1,1,1.0
extensively used,1,1,1.0
handle class,1,1,1.0
imbalance problems,4,2,2.0
problems these,1,1,1.0
methods combine,1,1,1.0
combine the,1,1,1.0
many classifiers,1,1,1.0
classifiers their,1,1,1.0
their successes,1,1,1.0
successes attribute,1,1,1.0
attribute to,1,1,1.0
their base,1,1,1.0
base learners,1,1,1.0
learners usually,1,1,1.0
usually are,1,1,1.0
of diversity,1,1,1.0
diversity in,2,1,2.0
in principle,1,1,1.0
principle or,1,1,1.0
or induced,1,1,1.0
induced with,1,1,1.0
various class,1,1,1.0
distributions adaboost,1,1,1.0
adaboost introduced,1,1,1.0
introduced by,2,2,1.0
by freund,1,1,1.0
freund and,4,2,2.0
and schapire,4,2,2.0
schapire solved,1,1,1.0
solved many,1,1,1.0
the practical,1,1,1.0
practical difficulties,1,1,1.0
difficulties of,1,1,1.0
the earlier,1,1,1.0
earlier boosting,1,1,1.0
boosting algorithms,3,2,1.5
algorithms initially,1,1,1.0
initially all,1,1,1.0
all weights,1,1,1.0
are set,2,2,1.0
set equally,1,1,1.0
equally but,1,1,1.0
but on,2,2,1.0
round the,1,1,1.0
weights of,2,2,1.0
of incorrectly,1,1,1.0
are increased,1,1,1.0
increased so,1,1,1.0
the weak,4,2,2.0
weak learner,4,2,2.0
learner is,1,1,1.0
is forced,1,1,1.0
forced to,1,1,1.0
hard examples,1,1,1.0
as stated,6,2,3.0
stated in,2,1,2.0
learning by,4,2,2.0
by making,1,1,1.0
making adaboost,1,1,1.0
rule the,1,1,1.0
adaboost thus,1,1,1.0
thus it,1,1,1.0
address class,1,1,1.0
problem scales,1,1,1.0
scales examples,2,1,2.0
in proportion,2,1,2.0
proportion to,2,1,2.0
to how,2,1,2.0
how well,2,1,2.0
well they,2,1,2.0
are distinguished,2,1,2.0
distinguished from,2,1,2.0
from examples,2,1,2.0
examples another,1,1,1.0
another algorithm,1,1,1.0
that uses,1,1,1.0
uses boosting,1,1,1.0
boosting to,1,1,1.0
is smoteboost,1,1,1.0
smoteboost this,1,1,1.0
algorithm recognizes,1,1,1.0
recognizes that,1,1,1.0
boosting may,1,1,1.0
may suffer,2,2,1.0
suffer from,2,2,1.0
same problems,1,1,1.0
problems as,1,1,1.0
as overfitting,1,1,1.0
overfitting instead,1,1,1.0
by updating,1,1,1.0
updating the,1,1,1.0
example smoteboost,1,1,1.0
smoteboost alters,1,1,1.0
distributions by,1,1,1.0
adding new,1,1,1.0
new examples,1,1,1.0
algorithm experiment,1,1,1.0
experiment results,1,1,1.0
results indicated,1,1,1.0
indicated that,2,1,2.0
the experts,1,1,1.0
experts approach,1,1,1.0
approach performs,1,1,1.0
well generally,1,1,1.0
generally outperforming,1,1,1.0
outperforming adaboost,1,1,1.0
adaboost with,1,1,1.0
to precision,1,1,1.0
recall on,1,1,1.0
doing especially,1,1,1.0
especially well,1,1,1.0
at covering,1,1,1.0
examples more,1,1,1.0
more detailed,1,1,1.0
detailed experiments,1,1,1.0
experiments are,1,1,1.0
in metacost,1,1,1.0
another ensemble,1,1,1.0
ensemble method,1,1,1.0
method it,3,2,1.5
it begins,1,1,1.0
model then,1,1,1.0
then estimates,1,1,1.0
then labels,1,1,1.0
labels the,1,1,1.0
set chan,1,1,1.0
chan and,2,1,2.0
and stolfo,2,1,2.0
stolfo run,1,1,1.0
run a,1,1,1.0
preliminary experiments,1,1,1.0
experiments to,1,1,1.0
good class,2,1,2.0
then do,1,1,1.0
do resampling,1,1,1.0
resampling to,1,1,1.0
generate multiple,1,1,1.0
multiple training,1,1,1.0
training sets,6,3,2.0
sets with,3,2,1.5
desired class,1,1,1.0
each training,2,1,2.0
set typically,1,1,1.0
typically includes,1,1,1.0
includes all,1,1,1.0
examples however,1,1,1.0
however each,1,1,1.0
is guaranteed,1,1,1.0
to occur,1,1,1.0
occur in,2,2,1.0
in at,1,1,1.0
least one,2,2,1.0
one training,1,1,1.0
set so,1,1,1.0
so no,1,1,1.0
is wasted,1,1,1.0
wasted the,1,1,1.0
a composite,1,1,1.0
composite learner,1,1,1.0
learner from,1,1,1.0
resulting classifiers,1,1,1.0
classifiers since,1,1,1.0
a wrapper,1,1,1.0
wrapper method,1,1,1.0
any learning,2,2,1.0
method internally,1,1,1.0
internally the,1,1,1.0
same basic,1,1,1.0
basic approach,1,1,1.0
for partitioning,1,1,1.0
partitioning the,1,1,1.0
learning multiple,1,1,1.0
multiple classifiers,1,1,1.0
classifiers has,1,1,1.0
with support,1,1,1.0
machines the,1,1,1.0
resulting svm,1,1,1.0
svm ensembles,3,2,1.5
ensembles was,1,1,1.0
was shown,1,1,1.0
to outperform,1,1,1.0
outperform both,1,1,1.0
and while,1,1,1.0
while these,1,1,1.0
these ensemble,1,1,1.0
ensemble approaches,1,1,1.0
approaches are,1,1,1.0
are effective,2,1,2.0
effective for,1,1,1.0
for dealing,2,1,2.0
problem they,1,1,1.0
they assume,1,1,1.0
distributions is,1,1,1.0
known this,1,1,1.0
this can,3,2,1.5
be estimated,2,2,1.0
estimated using,1,1,1.0
some preliminary,1,1,1.0
preliminary runs,1,1,1.0
runs but,1,1,1.0
is time,1,1,1.0
time consuming,1,1,1.0
consuming from,1,1,1.0
the style,1,1,1.0
style constructing,1,1,1.0
constructing the,1,1,1.0
a variant,1,1,1.0
variant of,1,1,1.0
of bagging,1,1,1.0
bagging phua,1,1,1.0
phua et,1,1,1.0
al combined,1,1,1.0
combined bagging,1,1,1.0
and stacking,1,1,1.0
stacking to,1,1,1.0
identify the,1,1,1.0
best mix,1,1,1.0
mix of,1,1,1.0
their insurance,1,1,1.0
insurance fraud,1,1,1.0
detection domain,1,1,1.0
domain they,1,1,1.0
that bagging,1,1,1.0
bagging achieves,1,1,1.0
achieves the,2,2,1.0
best besides,1,1,1.0
besides ensemble,1,1,1.0
algorithms of,1,1,1.0
of boosting,3,2,1.5
boosting and,3,2,1.5
and bagging,1,1,1.0
bagging style,1,1,1.0
style kotsiantis,1,1,1.0
kotsiantis and,3,2,1.5
and pintelas,4,2,2.0
pintelas used,1,1,1.0
used three,1,1,1.0
three agents,1,1,1.0
agents the,1,1,1.0
first learns,1,1,1.0
learns using,1,1,1.0
using naive,1,1,1.0
bayes the,1,1,1.0
second using,1,1,1.0
third using,1,1,1.0
using on,1,1,1.0
a filtered,1,1,1.0
filtered version,1,1,1.0
and combined,1,1,1.0
combined their,1,1,1.0
their predictions,1,1,1.0
predictions according,1,1,1.0
a voting,1,1,1.0
voting scheme,1,1,1.0
scheme this,1,1,1.0
technique attempts,1,1,1.0
achieve diversity,1,1,1.0
the errors,1,1,1.0
errors of,1,1,1.0
academic models,1,1,1.0
models by,1,1,1.0
different learning,2,1,2.0
algorithms the,2,2,1.0
the models,1,1,1.0
models generated,1,1,1.0
learning biases,1,1,1.0
biases are,1,1,1.0
make errors,1,1,1.0
errors in,1,1,1.0
ways they,1,1,1.0
they also,1,1,1.0
also used,2,1,2.0
used feature,1,1,1.0
data because,1,1,1.0
because in,2,2,1.0
in small,1,1,1.0
small data,1,1,1.0
the amount,2,2,1.0
imbalance affects,1,1,1.0
affects more,1,1,1.0
more the,1,1,1.0
the induction,2,2,1.0
induction and,1,1,1.0
and thus,2,2,1.0
thus feature,1,1,1.0
selection makes,1,1,1.0
makes the,1,1,1.0
problem less,1,1,1.0
less difficult,1,1,1.0
difficult motivated,1,1,1.0
motivated zheng,1,1,1.0
zheng and,1,1,1.0
and srihari,2,1,2.0
srihari s,1,1,1.0
s work,1,1,1.0
work castillo,1,1,1.0
serrano do,1,1,1.0
but make,1,1,1.0
make it,1,1,1.0
framework they,1,1,1.0
classifier system,1,1,1.0
system to,1,1,1.0
to construct,1,1,1.0
construct multiple,1,1,1.0
multiple learners,1,1,1.0
learners each,1,1,1.0
each doing,1,1,1.0
doing its,1,1,1.0
its own,1,1,1.0
own feature,1,1,1.0
selection based,1,1,1.0
algorithm their,1,1,1.0
their proposed,1,1,1.0
proposed system,1,1,1.0
system also,1,1,1.0
also combines,1,1,1.0
combines the,1,1,1.0
each learner,1,1,1.0
learner using,1,1,1.0
using genetic,1,1,1.0
algorithms evaluation,1,1,1.0
metrics accuracy,1,1,1.0
common evaluation,1,1,1.0
evaluation metric,2,1,2.0
most traditional,2,2,1.0
traditional application,1,1,1.0
application but,1,1,1.0
but accuracy,1,1,1.0
not suitable,1,1,1.0
suitable to,1,1,1.0
evaluate imbalanced,1,1,1.0
sets since,1,1,1.0
since many,2,2,1.0
many practitioners,1,1,1.0
practitioners have,1,1,1.0
have observed,1,1,1.0
for extremely,1,1,1.0
extremely skewed,1,1,1.0
the recall,3,2,1.5
recall of,2,1,2.0
is often,1,1,1.0
often which,1,1,1.0
which means,3,2,1.5
are no,2,2,1.0
classification rules,1,1,1.0
rules generated,1,1,1.0
generated for,8,2,4.0
using terminology,1,1,1.0
terminology from,1,1,1.0
from information,2,1,2.0
retrieval the,1,1,1.0
class has,1,1,1.0
has much,1,1,1.0
lower precision,1,1,1.0
recall than,1,1,1.0
class places,1,1,1.0
places more,1,1,1.0
more weight,1,1,1.0
weight on,1,1,1.0
than on,2,2,1.0
on minority,1,1,1.0
class which,2,2,1.0
which makes,1,1,1.0
makes it,1,1,1.0
it difficult,1,1,1.0
difficult for,1,1,1.0
perform well,2,2,1.0
well on,1,1,1.0
reason additional,1,1,1.0
additional metrics,1,1,1.0
metrics are,2,2,1.0
are coming,1,1,1.0
coming into,1,1,1.0
into widespread,1,1,1.0
widespread use,1,1,1.0
years several,1,1,1.0
several new,1,1,1.0
new metrics,1,1,1.0
metrics have,1,1,1.0
from other,1,1,1.0
other domains,1,1,1.0
domains for,2,1,2.0
are precision,2,1,2.0
recall from,1,1,1.0
retrieval domain,1,1,1.0
domain roc,1,1,1.0
roc and,2,1,2.0
auc area,1,1,1.0
curve from,1,1,1.0
from medical,1,1,1.0
medical domain,1,1,1.0
domain value,1,1,1.0
value maximum,1,1,1.0
maximum geometry,1,1,1.0
geometry mean,1,1,1.0
mean mgm,1,1,1.0
mgm of,2,1,2.0
accuracy on,3,2,1.5
class maximum,1,1,1.0
maximum sum,1,1,1.0
sum ms,1,1,1.0
ms of,1,1,1.0
accuracy all,1,1,1.0
the metrics,1,1,1.0
metrics can,1,1,1.0
be classified,1,1,1.0
classified into,1,1,1.0
two categories,1,1,1.0
categories metrics,1,1,1.0
metrics based,2,1,2.0
on confusion,2,1,2.0
matrix directly,1,1,1.0
directly and,1,1,1.0
that based,1,1,1.0
on accuracy,1,1,1.0
of binary,1,1,1.0
or precision,1,1,1.0
recall directly,1,1,1.0
directly accuracy,1,1,1.0
accuracy precision,3,2,1.5
recall fp,1,1,1.0
rate tp,3,2,1.5
rate roc,1,1,1.0
auc fall,1,1,1.0
first while,1,1,1.0
while and,1,1,1.0
other more,1,1,1.0
more complex,3,2,1.5
complex metrics,1,1,1.0
metrics such,2,2,1.0
as mgm,1,1,1.0
class ms,1,1,1.0
ms fall,1,1,1.0
other table,1,1,1.0
good understanding,1,1,1.0
understanding to,1,1,1.0
to confusion,1,1,1.0
matrix will,4,2,2.0
be helpful,2,1,2.0
helpful table,1,1,1.0
matrix prediction,1,1,1.0
prediction positive,1,1,1.0
negative real,1,1,1.0
real positive,1,1,1.0
negative fp,1,1,1.0
negative as,1,1,1.0
as promised,1,1,1.0
promised at,1,1,1.0
the beginning,2,2,1.0
beginning of,2,2,1.0
negative fig,1,1,1.0
fig presents,1,1,1.0
presents the,2,2,1.0
most well,1,1,1.0
well known,1,1,1.0
known evaluation,1,1,1.0
metrics as,1,1,1.0
in table,12,2,6.0
table tp,1,1,1.0
tp and,1,1,1.0
and tn,1,1,1.0
tn denote,1,1,1.0
denote the,5,3,1.6666666666666667
are classified,1,1,1.0
classified correctly,1,1,1.0
correctly while,1,1,1.0
while fn,1,1,1.0
fn and,1,1,1.0
and fp,2,1,2.0
fp denote,1,1,1.0
of misclassified,1,1,1.0
misclassified positive,1,1,1.0
examples respectively,1,1,1.0
respectively by,1,1,1.0
definition accuracy,1,1,1.0
precision fp,1,1,1.0
and value,1,1,1.0
value can,1,1,1.0
be represented,1,1,1.0
by equations,1,1,1.0
equations from,1,1,1.0
from to,5,2,2.5
to as,6,2,3.0
where precision,1,1,1.0
recall are,1,1,1.0
rate denotes,1,1,1.0
denotes the,4,3,1.3333333333333333
the misclassified,1,1,1.0
misclassified negative,1,1,1.0
and tp,1,1,1.0
classified positive,1,1,1.0
the point,2,2,1.0
point is,1,1,1.0
the ideal,6,2,3.0
ideal point,1,1,1.0
point of,3,2,1.5
the learners,1,1,1.0
learners that,1,1,1.0
is there,1,1,1.0
no positive,1,1,1.0
examples were,2,2,1.0
were misclassified,1,1,1.0
misclassified to,1,1,1.0
to negative,1,1,1.0
and vice,1,1,1.0
vice versa,1,1,1.0
versa or,1,1,1.0
or is,1,1,1.0
a popular,1,1,1.0
problem it,2,1,2.0
a kind,1,1,1.0
kind of,1,1,1.0
of combination,2,2,1.0
of recall,2,2,1.0
and precision,2,1,2.0
precision which,1,1,1.0
effective metrics,1,1,1.0
for information,1,1,1.0
retrieval community,1,1,1.0
community where,1,1,1.0
exists is,1,1,1.0
high when,1,1,1.0
when both,3,2,1.5
both recall,1,1,1.0
precision are,1,1,1.0
are high,1,1,1.0
high and,1,1,1.0
be adjusted,1,1,1.0
adjusted through,1,1,1.0
through changing,1,1,1.0
the value,5,3,1.6666666666666667
of β,4,2,2.0
β where,1,1,1.0
where β,2,2,1.0
β corresponds,1,1,1.0
to relative,1,1,1.0
relative importance,1,1,1.0
of precision,1,1,1.0
recall for,1,1,1.0
example counts,1,1,1.0
counts both,1,1,1.0
both equally,1,1,1.0
equally while,1,1,1.0
while counts,1,1,1.0
counts recall,1,1,1.0
recall twice,1,1,1.0
twice as,1,1,1.0
much perhaps,1,1,1.0
common metric,1,1,1.0
metric to,2,2,1.0
overall classification,1,1,1.0
is roc,1,1,1.0
roc analysis,3,2,1.5
the associated,1,1,1.0
associated use,1,1,1.0
detail roc,1,1,1.0
which tp,1,1,1.0
curves like,1,1,1.0
like curves,1,1,1.0
curves can,1,1,1.0
assess different,2,2,1.0
different roc,1,1,1.0
curve depicts,1,1,1.0
depicts relative,1,1,1.0
relative between,1,1,1.0
between benefits,1,1,1.0
benefits tp,1,1,1.0
and costs,1,1,1.0
costs fp,1,1,1.0
rate that,1,1,1.0
examples correctly,1,1,1.0
classified can,1,1,1.0
increased at,1,1,1.0
the expense,1,1,1.0
expense of,1,1,1.0
of introducing,2,2,1.0
introducing additional,1,1,1.0
additional false,1,1,1.0
false positives,2,2,1.0
positives a,1,1,1.0
a major,1,1,1.0
major disadvantage,1,1,1.0
disadvantage of,1,1,1.0
analysis is,1,1,1.0
not deliver,1,1,1.0
deliver a,1,1,1.0
single easy,1,1,1.0
easy to,2,2,1.0
use performance,1,1,1.0
performance measure,1,1,1.0
measure like,1,1,1.0
like accuracy,1,1,1.0
accuracy directly,1,1,1.0
directly auc,1,1,1.0
auc does,1,1,1.0
not place,1,1,1.0
place more,1,1,1.0
more emphasis,1,1,1.0
class over,1,1,1.0
other so,1,1,1.0
not biased,1,1,1.0
biased against,1,1,1.0
class tnfpfntptntpaccuracy,1,1,1.0
tnfpfntptntpaccuracy pr,1,1,1.0
pr fptptpecison,1,1,1.0
fptptpecison re,1,1,1.0
re fntptpcall,1,1,1.0
fntptpcall tnfpfpratefp,1,1,1.0
tnfpfpratefp fntptpratetp,1,1,1.0
fntptpratetp ecisioncall,1,1,1.0
ecisioncall ecisioncallf,1,1,1.0
ecisioncallf value,1,1,1.0
value prre,1,1,1.0
prre pr,1,1,1.0
pr re,1,1,1.0
re β,1,1,1.0
β β,1,1,1.0
β accuracyaccuracymgm,1,1,1.0
accuracyaccuracymgm accuracyaccuracyms,1,1,1.0
accuracyaccuracyms evaluation,1,1,1.0
matrix besides,1,1,1.0
besides minimum,1,1,1.0
minimum cost,1,1,1.0
cost criterion,1,1,1.0
criterion is,1,1,1.0
evaluate the,3,2,1.5
sets when,1,1,1.0
when performing,2,2,1.0
performing learning,1,1,1.0
learning when,6,2,3.0
applying machine,1,1,1.0
to real,1,1,1.0
applications rarely,1,1,1.0
rarely would,1,1,1.0
would one,1,1,1.0
one or,3,3,1.0
or more,2,2,1.0
more of,2,2,1.0
these assumptions,1,1,1.0
assumptions hold,1,1,1.0
hold but,1,1,1.0
but to,1,1,1.0
select a,2,1,2.0
classifier certain,1,1,1.0
conditions must,1,1,1.0
must exist,1,1,1.0
exist and,1,1,1.0
and we,10,3,3.3333333333333335
may need,1,1,1.0
need more,1,1,1.0
more information,1,1,1.0
information if,1,1,1.0
one roc,1,1,1.0
curve dominates,1,1,1.0
dominates all,1,1,1.0
all others,1,1,1.0
others then,1,1,1.0
best method,3,2,1.5
produced the,1,1,1.0
the dominant,4,2,2.0
dominant curve,2,1,2.0
curve which,1,1,1.0
curve with,1,1,1.0
the largest,2,2,1.0
largest area,1,1,1.0
area with,1,1,1.0
with maximum,1,1,1.0
maximum auc,1,1,1.0
auc to,1,1,1.0
classifier from,1,1,1.0
curve we,1,1,1.0
need additional,1,1,1.0
additional information,1,1,1.0
information such,1,1,1.0
a target,1,1,1.0
target fp,1,1,1.0
rate on,2,2,1.0
hand if,1,1,1.0
if multiple,1,1,1.0
multiple curves,1,1,1.0
curves dominate,1,1,1.0
dominate in,1,1,1.0
different parts,1,1,1.0
parts of,1,1,1.0
roc space,5,2,2.5
space then,2,2,1.0
roc convex,1,1,1.0
convex hull,1,1,1.0
hull method,1,1,1.0
select the,3,2,1.5
optimal classifier,1,1,1.0
classifier relations,1,1,1.0
relations to,1,1,1.0
other problems,1,1,1.0
problems it,2,2,1.0
been observed,1,1,1.0
some domains,1,1,1.0
the sick,1,1,1.0
sick data,1,1,1.0
set standard,1,1,1.0
algorithms are,1,1,1.0
are capable,1,1,1.0
of inducing,1,1,1.0
inducing good,1,1,1.0
good classifiers,1,1,1.0
classifiers even,1,1,1.0
even using,1,1,1.0
using highly,1,1,1.0
imbalanced training,4,3,1.3333333333333333
sets this,1,1,1.0
this shows,1,1,1.0
not the,5,3,1.6666666666666667
the only,4,2,2.0
only problem,2,1,2.0
problem responsible,1,1,1.0
the decrease,1,1,1.0
decrease in,1,1,1.0
algorithms class,1,1,1.0
problem to,2,2,1.0
to contend,1,1,1.0
contend with,1,1,1.0
with besides,1,1,1.0
besides the,2,2,1.0
distributions within,1,1,1.0
within each,1,1,1.0
data within,1,1,1.0
are also,5,3,1.6666666666666667
also relevant,1,1,1.0
relevant it,1,1,1.0
was found,1,1,1.0
certain cases,1,1,1.0
cases addressing,1,1,1.0
addressing the,5,3,1.6666666666666667
the small,1,1,1.0
disjuncts problem,1,1,1.0
with regardless,1,1,1.0
regardless of,2,2,1.0
was sufficient,1,1,1.0
sufficient to,1,1,1.0
increase performance,1,1,1.0
performance experiments,1,1,1.0
experiments by,1,1,1.0
by jo,1,1,1.0
jo and,4,2,2.0
and japkowicz,6,3,2.0
japkowicz suggested,1,1,1.0
not directly,4,2,2.0
directly caused,2,1,2.0
by class,3,1,3.0
imbalances but,2,2,1.0
rather that,2,1,2.0
imbalances may,1,1,1.0
may yield,3,2,1.5
yield small,2,1,2.0
disjuncts which,2,1,2.0
which in,4,3,1.3333333333333333
turn will,2,1,2.0
will cause,2,1,2.0
cause degradation,2,1,2.0
degradation a,1,1,1.0
proposed whose,1,1,1.0
whose idea,1,1,1.0
to consider,2,2,1.0
consider not,1,1,1.0
not only,2,2,1.0
imbalance but,3,1,3.0
but also,1,1,1.0
dataset by,1,1,1.0
by rectifying,1,1,1.0
rectifying these,1,1,1.0
these two,2,2,1.0
two types,1,1,1.0
of imbalances,1,1,1.0
imbalances simultaneously,1,1,1.0
simultaneously the,1,1,1.0
experiments results,1,1,1.0
of prati,1,1,1.0
prati et,1,1,1.0
al using,1,1,1.0
a inductive,1,1,1.0
inductive scheme,1,1,1.0
scheme suggested,1,1,1.0
not solely,1,1,1.0
solely caused,1,1,1.0
also related,1,1,1.0
data overlapping,1,1,1.0
overlapping among,1,1,1.0
also found,1,1,1.0
data duplication,2,1,2.0
duplication is,1,1,1.0
is generally,3,2,1.5
generally harmful,1,1,1.0
harmful although,1,1,1.0
although for,1,1,1.0
as naive,1,1,1.0
bayes and,1,1,1.0
and perceptrons,1,1,1.0
perceptrons with,1,1,1.0
with margins,1,1,1.0
margins high,1,1,1.0
high degrees,1,1,1.0
degrees of,1,1,1.0
of duplication,1,1,1.0
duplication are,1,1,1.0
are necessary,1,1,1.0
necessary to,1,1,1.0
to harm,1,1,1.0
harm classification,1,1,1.0
classification it,1,1,1.0
the reason,1,1,1.0
reason why,1,1,1.0
why class,1,1,1.0
imbalances and,2,2,1.0
and overlapping,1,1,1.0
overlapping classes,2,2,1.0
related is,1,1,1.0
that misclassification,1,1,1.0
misclassification often,1,1,1.0
often occurs,1,1,1.0
occurs near,1,1,1.0
near class,1,1,1.0
class boundaries,3,2,1.5
boundaries where,1,1,1.0
where overlap,1,1,1.0
overlap usually,1,1,1.0
usually occurs,1,1,1.0
occurs as,1,1,1.0
well weiss,1,1,1.0
and training,1,1,1.0
set size,1,1,1.0
size experiments,1,1,1.0
the position,1,1,1.0
position of,1,1,1.0
best class,1,1,1.0
distributions varies,1,1,1.0
varies somewhat,1,1,1.0
somewhat with,1,1,1.0
with size,1,1,1.0
many cases,1,1,1.0
cases especially,1,1,1.0
especially with,1,1,1.0
with error,1,1,1.0
rate the,1,1,1.0
the variation,1,1,1.0
variation is,1,1,1.0
small which,1,1,1.0
which gives,1,1,1.0
gives support,1,1,1.0
support to,1,1,1.0
notion that,1,1,1.0
a best,1,1,1.0
best marginal,1,1,1.0
a learning,5,3,1.6666666666666667
learning task,2,2,1.0
task the,1,1,1.0
also indicated,1,1,1.0
any fixed,1,1,1.0
fixed class,1,1,1.0
distribution increasing,1,1,1.0
increasing the,2,2,1.0
set always,1,1,1.0
always leads,1,1,1.0
to improved,1,1,1.0
improved classifier,1,1,1.0
performance conclusion,1,1,1.0
conclusion learning,1,1,1.0
an important,1,1,1.0
important issue,2,2,1.0
issue in,5,2,2.5
a direct,1,1,1.0
direct method,1,1,1.0
is artificially,1,1,1.0
artificially balancing,1,1,1.0
balancing the,1,1,1.0
its effectiveness,1,1,1.0
effectiveness has,1,1,1.0
empirically analyzed,1,1,1.0
analyzed in,1,1,1.0
in however,1,1,1.0
evidence that,2,2,1.0
distributions artificially,1,1,1.0
artificially does,1,1,1.0
not have,3,3,1.0
have much,1,1,1.0
much effect,1,1,1.0
effect on,2,2,1.0
the induced,1,1,1.0
induced classifier,1,1,1.0
classifier since,1,1,1.0
since some,1,1,1.0
some learning,1,1,1.0
systems are,1,1,1.0
not sensitive,1,1,1.0
sensitive to,2,2,1.0
differences in,1,1,1.0
in class,3,2,1.5
distributions it,1,1,1.0
seems that,1,1,1.0
we still,1,1,1.0
still need,1,1,1.0
need a,1,1,1.0
a clearer,2,2,1.0
clearer and,1,1,1.0
and deeper,1,1,1.0
deeper understanding,2,1,2.0
understanding of,3,2,1.5
of how,1,1,1.0
how class,1,1,1.0
distribution affects,1,1,1.0
affects each,1,1,1.0
each phase,1,1,1.0
for more,1,1,1.0
more learners,1,1,1.0
learners except,1,1,1.0
except decision,1,1,1.0
trees a,1,1,1.0
a deeper,1,1,1.0
the basics,1,1,1.0
basics will,1,1,1.0
will help,1,1,1.0
help us,1,1,1.0
to design,1,1,1.0
design better,1,1,1.0
better methods,1,1,1.0
distributions as,1,1,1.0
is stated,1,1,1.0
section some,1,1,1.0
some data,3,3,1.0
are immune,1,1,1.0
immune to,1,1,1.0
imbalance may,1,1,1.0
degradation though,1,1,1.0
though maximum,1,1,1.0
maximum specification,1,1,1.0
specification bias,1,1,1.0
bias in,1,1,1.0
in induction,1,1,1.0
induction processes,1,1,1.0
processes and,1,1,1.0
and dealing,1,1,1.0
of within,1,1,1.0
imbalance have,1,1,1.0
have present,1,1,1.0
present their,1,1,1.0
their effectiveness,1,1,1.0
effectiveness according,1,1,1.0
class more,1,1,1.0
effective methods,1,1,1.0
needed current,1,1,1.0
current researches,1,1,1.0
on small,1,1,1.0
disjuncts are,2,1,2.0
are ad,1,1,1.0
ad hoc,1,1,1.0
hoc so,1,1,1.0
so standard,1,1,1.0
standard metrics,1,1,1.0
are deadly,1,1,1.0
deadly in,1,1,1.0
in need,1,1,1.0
need since,1,1,1.0
since machine,1,1,1.0
an science,1,1,1.0
related ones,1,1,1.0
ones are,1,1,1.0
of nature,1,1,1.0
nature realizing,1,1,1.0
realizing to,1,1,1.0
to explore,1,1,1.0
explore idiographic,1,1,1.0
idiographic solutions,1,1,1.0
solutions for,1,1,1.0
specific applications,1,1,1.0
applications is,1,1,1.0
very important,1,1,1.0
and valuable,1,1,1.0
valuable for,1,1,1.0
for practitioners,1,1,1.0
practitioners and,1,1,1.0
better data,1,1,1.0
data understanding,1,1,1.0
understanding and,1,1,1.0
more knowledge,1,1,1.0
knowledge on,1,1,1.0
domain will,1,1,1.0
helpful in,1,1,1.0
references kotsiantis,1,1,1.0
kotsiantis kanellopoulos,1,1,1.0
kanellopoulos and,1,1,1.0
pintelas handling,1,1,1.0
datasets a,1,1,1.0
review gests,1,1,1.0
gests international,1,1,1.0
international transactions,1,1,1.0
engineering pp,1,1,1.0
pp visa,1,1,1.0
visa and,1,1,1.0
and ralescu,1,1,1.0
ralescu issues,1,1,1.0
data review,1,1,1.0
review paper,1,1,1.0
the sixteen,1,1,1.0
sixteen midwest,1,1,1.0
midwest artificial,1,1,1.0
and cognitive,1,1,1.0
cognitive science,1,1,1.0
science conference,1,1,1.0
conference dayton,1,1,1.0
dayton pp,1,1,1.0
pp monard,1,1,1.0
monard and,1,1,1.0
and batista,1,1,1.0
batista learning,1,1,1.0
distribution in,2,2,1.0
in advances,2,2,1.0
logic artificial,1,1,1.0
and robotics,1,1,1.0
robotics sao,1,1,1.0
sao paulo,1,1,1.0
paulo sp,1,1,1.0
sp ios,1,1,1.0
ios press,1,1,1.0
press pp,4,2,2.0
pp weiss,4,2,2.0
weiss mining,2,2,1.0
with rarity,2,2,1.0
rarity a,2,2,1.0
a unifying,2,2,1.0
unifying framework,2,2,1.0
framework sigkdd,1,1,1.0
explorations pp,7,1,7.0
pp maloof,2,2,1.0
maloof learning,2,2,1.0
imbalanced and,3,3,1.0
and when,4,3,1.3333333333333333
when costs,2,2,1.0
costs are,2,2,1.0
are unequal,2,2,1.0
unequal and,2,2,1.0
and unknown,2,2,1.0
unknown in,2,2,1.0
the icml,2,1,2.0
icml workshop,4,1,4.0
sets ii,9,2,4.5
ii pp,4,2,2.0
pp stone,1,1,1.0
stone and,1,1,1.0
and olshen,1,1,1.0
olshen classification,1,1,1.0
and regression,2,2,1.0
regression trees,1,1,1.0
trees chapman,1,1,1.0
chapman and,1,1,1.0
and press,1,1,1.0
press japkowicz,1,1,1.0
japkowicz class,4,2,2.0
imbalances are,3,2,1.5
are we,2,2,1.0
we focusing,2,2,1.0
the right,3,3,1.0
right issue,2,2,1.0
issue proceedings,1,1,1.0
workshop learning,1,1,1.0
pp laurikkala,1,1,1.0
laurikkala improving,1,1,1.0
improving identification,1,1,1.0
of difficult,1,1,1.0
difficult small,1,1,1.0
small classes,1,1,1.0
classes by,1,1,1.0
by balancing,1,1,1.0
balancing class,1,1,1.0
distribution technical,1,1,1.0
report university,1,1,1.0
of tampere,1,1,1.0
tampere guo,1,1,1.0
guo and,2,2,1.0
and herna,1,1,1.0
herna learning,1,1,1.0
with boosting,2,2,1.0
generation the,3,2,1.5
data boosting,1,1,1.0
boosting approach,1,1,1.0
approach sigkdd,1,1,1.0
weiss the,1,1,1.0
disjuncts and,1,1,1.0
distribution on,3,2,1.5
tree learning,1,1,1.0
learning dissertation,1,1,1.0
dissertation department,1,1,1.0
science rutgers,1,1,1.0
rutgers university,1,1,1.0
university new,2,2,1.0
new brunswick,1,1,1.0
brunswick new,1,1,1.0
new jersey,1,1,1.0
jersey may,1,1,1.0
may japkowicz,1,1,1.0
japkowicz learning,2,2,1.0
various strategies,2,2,1.0
strategies aaai,1,1,1.0
sets menlo,1,1,1.0
menlo park,3,2,1.5
park ca,3,2,1.5
ca aaai,2,2,1.0
aaai press,2,2,1.0
press kotsiantis,1,1,1.0
pintelas mixture,1,1,1.0
mixture of,1,1,1.0
of expert,1,1,1.0
expert agents,1,1,1.0
agents for,1,1,1.0
sets annals,1,1,1.0
annals of,1,1,1.0
of mathematics,1,1,1.0
mathematics computing,1,1,1.0
computing teleinformatics,1,1,1.0
teleinformatics vol,1,1,1.0
pp hart,1,1,1.0
hart the,1,1,1.0
the condensed,1,1,1.0
rule ieee,1,1,1.0
information theory,2,2,1.0
theory pp,1,1,1.0
pp kubat,3,2,1.5
kubat and,3,3,1.0
and matwin,5,3,1.6666666666666667
matwin addressing,2,2,1.0
the curse,3,3,1.0
curse of,3,3,1.0
sets one,1,1,1.0
one sided,1,1,1.0
sided selection,1,1,1.0
the fourteenth,2,1,2.0
fourteenth international,1,1,1.0
learning nashville,2,2,1.0
nashville tennesse,1,1,1.0
tennesse morgan,1,1,1.0
morgan kaufmann,1,1,1.0
kaufmann pp,1,1,1.0
pp chawla,2,1,2.0
chawla hall,1,1,1.0
bowyer and,1,1,1.0
and kegelmeyer,2,2,1.0
research pp,4,1,4.0
pp han,3,2,1.5
wang and,2,2,1.0
and mao,2,2,1.0
mao smote,1,1,1.0
computing part,1,1,1.0
part i,1,1,1.0
i lncs,1,1,1.0
lncs pp,1,1,1.0
weiss and,2,2,1.0
and provost,2,2,1.0
provost learning,3,2,1.5
are costly,2,2,1.0
costly the,2,2,1.0
on tree,2,2,1.0
tree induction,2,2,1.0
induction journal,1,1,1.0
wang wen,1,1,1.0
wen and,1,1,1.0
and wang,1,1,1.0
wang sampling,1,1,1.0
sampling algorithm,1,1,1.0
computer allocations,1,1,1.0
allocations in,1,1,1.0
in chinese,1,1,1.0
chinese taeho,1,1,1.0
taeho jo,2,1,2.0
imbalances versus,2,1,2.0
versus small,1,1,1.0
disjuncts sigkdd,1,1,1.0
explorations volume,1,1,1.0
volume issue,1,1,1.0
issue pp,1,1,1.0
pp batista,1,1,1.0
prati and,1,1,1.0
and monard,3,2,1.5
pp forman,1,1,1.0
forman an,1,1,1.0
an extensive,2,2,1.0
extensive empirical,1,1,1.0
of feature,1,1,1.0
selection metrics,1,1,1.0
classification journal,2,1,2.0
learning research,6,2,3.0
pp mladenic,1,1,1.0
mladenic and,1,1,1.0
and grobelnik,1,1,1.0
grobelnik feature,1,1,1.0
for unbalanced,1,1,1.0
unbalanced class,1,1,1.0
and naive,1,1,1.0
bayes in,1,1,1.0
the sixteenth,2,1,2.0
sixteenth international,2,1,2.0
learning pp,6,1,6.0
pp zheng,1,1,1.0
wu and,6,3,2.0
pp castillo,1,1,1.0
serrano a,1,1,1.0
a multistrategy,1,1,1.0
multistrategy approach,1,1,1.0
for digital,1,1,1.0
digital text,1,1,1.0
categorization from,1,1,1.0
imbalanced documents,1,1,1.0
documents sigkdd,1,1,1.0
pp van,1,1,1.0
van der,1,1,1.0
der putten,1,1,1.0
and van,1,1,1.0
van someren,1,1,1.0
someren a,1,1,1.0
a variance,1,1,1.0
variance analysis,1,1,1.0
a real,2,2,1.0
world learning,1,1,1.0
learning problem,5,3,1.6666666666666667
coil challenge,1,1,1.0
challenge machine,1,1,1.0
pp wilson,1,1,1.0
wilson asymptotic,1,1,1.0
asymptotic properties,1,1,1.0
properties of,2,2,1.0
neighbor rules,1,1,1.0
rules using,1,1,1.0
using edited,1,1,1.0
edited data,1,1,1.0
trans on,1,1,1.0
cybernetics vol,2,2,1.0
pp drummond,1,1,1.0
holte class,1,1,1.0
and cost,3,2,1.5
cost sensitivity,1,1,1.0
sensitivity why,1,1,1.0
why beats,1,1,1.0
beats in,1,1,1.0
in icml,2,1,2.0
ii washington,4,2,2.0
washington dc,2,1,2.0
dc barandela,1,1,1.0
barandela sánchez,1,1,1.0
sánchez garcía,1,1,1.0
garcía and,1,1,1.0
and rangel,2,2,1.0
rangel strategies,2,2,1.0
strategies for,2,2,1.0
problems pattern,1,1,1.0
recognition pp,2,1,2.0
pp wu,3,3,1.0
and chang,1,1,1.0
chang alignment,1,1,1.0
alignment for,2,2,1.0
dataset learning,1,1,1.0
dc veropoulos,1,1,1.0
veropoulos campbell,1,1,1.0
campbell and,1,1,1.0
and cristianini,1,1,1.0
cristianini controlling,1,1,1.0
controlling the,4,2,2.0
the sensitivity,3,2,1.5
sensitivity of,1,1,1.0
of support,2,2,1.0
machines in,3,3,1.0
on ai,1,1,1.0
ai pp,1,1,1.0
pp huang,1,1,1.0
huang yang,1,1,1.0
yang king,1,1,1.0
king and,1,1,1.0
and lyu,1,1,1.0
lyu learning,1,1,1.0
classifiers from,1,1,1.0
data based,1,1,1.0
on biased,1,1,1.0
the ieee,3,2,1.5
ieee computer,1,1,1.0
computer society,1,1,1.0
society conference,1,1,1.0
and pattern,1,1,1.0
recognition domingos,1,1,1.0
the fifth,2,2,1.0
fifth international,1,1,1.0
mining acm,1,1,1.0
acm press,1,1,1.0
pp w,2,2,1.0
w f,1,1,1.0
f a,1,1,1.0
a n,11,3,3.6666666666666665
n s,2,2,1.0
s j,1,1,1.0
j s,1,1,1.0
t o,18,3,6.0
o l,2,2,1.0
l f,1,1,1.0
o j,1,1,1.0
j z,1,1,1.0
z h,1,1,1.0
h a,3,2,1.5
n g,6,2,3.0
g a,3,2,1.5
n d,9,3,3.0
d p,1,1,1.0
p k,2,2,1.0
k c,1,1,1.0
c h,2,2,1.0
n adacost,1,1,1.0
adacost misclassification,1,1,1.0
misclassification boosting,1,1,1.0
boosting in,5,2,2.5
pp japkowicz,4,3,1.3333333333333333
japkowicz supervised,1,1,1.0
supervised versus,1,1,1.0
versus unsupervised,1,1,1.0
unsupervised binary,1,1,1.0
binary learning,1,1,1.0
by feed,1,1,1.0
feed forward,1,1,1.0
forward neural,1,1,1.0
networks machine,2,2,1.0
pp scholkopf,2,2,1.0
scholkopf platt,1,1,1.0
platt smola,1,1,1.0
smola and,1,1,1.0
and williamson,1,1,1.0
williamson estimating,1,1,1.0
estimating the,1,1,1.0
the support,2,2,1.0
a dimensional,1,1,1.0
dimensional distribution,1,1,1.0
distribution neural,1,1,1.0
neural computation,1,1,1.0
computation pp,1,1,1.0
pp tax,1,1,1.0
tax classification,1,1,1.0
classification dissertation,1,1,1.0
dissertation delft,1,1,1.0
delft university,1,1,1.0
of technology,6,2,3.0
technology manevitz,1,1,1.0
manevitz and,1,1,1.0
and yousef,1,1,1.0
yousef svms,1,1,1.0
svms for,2,2,1.0
for document,1,1,1.0
document classification,1,1,1.0
pp riddle,1,1,1.0
riddle segal,1,1,1.0
segal and,1,1,1.0
and etzioni,1,1,1.0
etzioni representation,1,1,1.0
representation design,1,1,1.0
design and,1,1,1.0
and induction,1,1,1.0
induction in,2,1,2.0
a boeing,1,1,1.0
manufacturing design,1,1,1.0
design applied,1,1,1.0
applied artificial,1,1,1.0
holte and,2,2,1.0
matwin learning,1,1,1.0
when negative,1,1,1.0
examples abound,1,1,1.0
abound in,1,1,1.0
the ninth,1,1,1.0
ninth european,1,1,1.0
learning lnai,1,1,1.0
lnai springer,1,1,1.0
springer pp,1,1,1.0
pp cohen,1,1,1.0
cohen fast,1,1,1.0
fast effective,1,1,1.0
effective rule,1,1,1.0
the twelfth,1,1,1.0
twelfth international,1,1,1.0
pp tomek,1,1,1.0
tomek two,1,1,1.0
two modifications,1,1,1.0
of cnn,1,1,1.0
cnn ieee,1,1,1.0
and communications,1,1,1.0
communications pp,1,1,1.0
pp raskutti,1,1,1.0
kowalczyk extreme,1,1,1.0
extreme rebalancing,1,1,1.0
rebalancing svms,1,1,1.0
svms a,1,1,1.0
case study,2,1,2.0
study sigkdd,1,1,1.0
pp freund,1,1,1.0
sciences pp,1,1,1.0
pp m,1,1,1.0
m v,1,1,1.0
v j,1,1,1.0
j o,1,1,1.0
o s,4,2,2.0
s h,1,1,1.0
h i,2,2,1.0
i v,1,1,1.0
v k,1,1,1.0
k u,1,1,1.0
u m,1,1,1.0
m a,2,2,1.0
a r,5,3,1.6666666666666667
r a,7,3,2.3333333333333335
d r,1,1,1.0
r c,1,1,1.0
c a,1,1,1.0
a g,1,1,1.0
r w,1,1,1.0
w a,1,1,1.0
a l,2,2,1.0
l evaluating,1,1,1.0
evaluating boosting,1,1,1.0
classify rare,1,1,1.0
cases comparison,1,1,1.0
and improvements,1,1,1.0
first ieee,1,1,1.0
mining pp,3,2,1.5
hall and,5,3,1.6666666666666667
and bowyer,2,2,1.0
the seventh,1,1,1.0
seventh european,1,1,1.0
on principles,1,1,1.0
principles and,1,1,1.0
and practice,1,1,1.0
practice of,1,1,1.0
databases dubrovnik,1,1,1.0
dubrovnik croatia,1,1,1.0
croatia pp,2,2,1.0
pp estabrooks,2,1,2.0
estabrooks taeho,1,1,1.0
japkowicz a,2,1,2.0
a multiple,1,1,1.0
multiple resampling,1,1,1.0
resampling method,1,1,1.0
sets computational,1,1,1.0
pp chan,1,1,1.0
stolfo toward,1,1,1.0
toward scalable,1,1,1.0
scalable learning,1,1,1.0
cost distributions,2,2,1.0
distributions a,1,1,1.0
study in,1,1,1.0
in credit,1,1,1.0
card fraud,1,1,1.0
detection in,1,1,1.0
pp yan,1,1,1.0
yan liu,1,1,1.0
liu jin,1,1,1.0
jin and,1,1,1.0
and hauptmann,1,1,1.0
hauptmann on,1,1,1.0
predicting rare,1,1,1.0
with svm,2,2,1.0
ensembles in,2,2,1.0
in scene,1,1,1.0
scene classification,1,1,1.0
on acoustics,1,1,1.0
acoustics speech,1,1,1.0
speech and,1,1,1.0
and signal,1,1,1.0
processing phua,1,1,1.0
phua and,1,1,1.0
and alahakoon,1,1,1.0
alahakoon minority,1,1,1.0
minority report,1,1,1.0
report in,1,1,1.0
in fraud,2,2,1.0
detection classification,1,1,1.0
of skewed,1,1,1.0
skewed data,1,1,1.0
estabrooks and,1,1,1.0
a framework,1,1,1.0
from unbalanced,1,1,1.0
the intelligent,1,1,1.0
intelligent data,2,2,1.0
data analysis,4,3,1.3333333333333333
analysis conference,1,1,1.0
conference pp,1,1,1.0
pp bradley,1,1,1.0
bradley the,1,1,1.0
curve in,1,1,1.0
evaluation of,2,2,1.0
algorithms pattern,1,1,1.0
pp provost,4,2,2.0
provost and,3,2,1.5
and fawcett,3,2,1.5
fawcett robust,2,2,1.0
robust classification,1,1,1.0
classification for,1,1,1.0
for imprecise,2,2,1.0
imprecise environments,1,1,1.0
environments machine,1,1,1.0
japkowicz in,1,1,1.0
the presence,1,1,1.0
presence of,1,1,1.0
and imbalances,1,1,1.0
imbalances in,2,2,1.0
fourteenth conference,1,1,1.0
conference of,1,1,1.0
the canadian,1,1,1.0
canadian society,1,1,1.0
society for,1,1,1.0
for computational,1,1,1.0
computational studies,1,1,1.0
studies of,1,1,1.0
of intelligence,1,1,1.0
batista and,2,2,1.0
monard class,2,2,1.0
versus class,1,1,1.0
class overlapping,3,3,1.0
overlapping an,2,2,1.0
learning system,3,2,1.5
system behavior,2,2,1.0
behavior in,2,2,1.0
in micai,1,1,1.0
micai lnai,1,1,1.0
lnai pp,1,1,1.0
pp kolez,1,1,1.0
kolez chowdhury,1,1,1.0
chowdhury and,1,1,1.0
and alspector,1,1,1.0
alspector data,1,1,1.0
duplication an,1,1,1.0
an imbalance,1,1,1.0
ii view,1,1,1.0
view publication,1,1,1.0
publication stats,1,1,1.0
space p,1,1,1.0
p p,3,1,3.0
p guti,3,1,3.0
guti senior,1,1,1.0
senior member,2,1,2.0
member ieee,4,2,2.0
ieee p,1,1,1.0
p ti,2,1,2.0
ti ˇno,2,1,2.0
ˇno and,1,1,1.0
and c,1,1,1.0
c herv,1,1,1.0
herv senior,1,1,1.0
ieee imbalanced,1,1,1.0
imbalanced nature,3,1,3.0
current challenges,1,1,1.0
learning researchers,2,1,2.0
researchers one,1,1,1.0
one common,1,1,1.0
common approach,2,1,2.0
through convex,1,1,1.0
convex combination,16,1,16.0
its patterns,1,1,1.0
patterns w,2,1,2.0
w e,8,1,8.0
e explore,2,1,2.0
the general,1,1,1.0
general idea,1,1,1.0
idea of,2,2,1.0
space induced,3,1,3.0
induced by,6,1,6.0
a kernel,11,2,5.5
function as,1,1,1.0
to input,1,1,1.0
input space,22,1,22.0
space if,1,1,1.0
function matches,2,1,2.0
matches the,3,1,3.0
underlying problem,3,1,3.0
classes will,1,1,1.0
be linearly,1,1,1.0
linearly separable,11,2,5.5
separable and,2,1,2.0
and synthetically,1,1,1.0
synthetically generated,3,1,3.0
generated patterns,4,1,4.0
patterns will,2,1,2.0
will lie,2,1,2.0
lie on,3,1,3.0
class region,5,1,5.0
region since,1,1,1.0
directly accessible,2,1,2.0
accessible we,1,1,1.0
the empirical,24,1,24.0
empirical feature,24,1,24.0
space a,2,1,2.0
a euclidean,2,1,2.0
euclidean space,3,1,3.0
space isomorphic,1,1,1.0
isomorphic to,2,1,2.0
space for,6,2,3.0
proposed method,3,2,1.5
is framed,1,1,1.0
framed in,1,1,1.0
where imbalanced,1,1,1.0
datasets can,2,1,2.0
can pose,1,1,1.0
pose a,3,1,3.0
a serious,4,1,4.0
serious hindrance,2,1,2.0
hindrance the,1,1,1.0
is investigated,1,1,1.0
investigated in,1,1,1.0
three scenarios,1,1,1.0
scenarios sampling,1,1,1.0
the full,4,1,4.0
full and,3,1,3.0
and empirical,3,1,3.0
feature spaces,5,2,2.5
spaces a,1,1,1.0
kernel learning,18,1,18.0
learning technique,1,1,1.0
technique maximising,1,1,1.0
maximising the,1,1,1.0
data class,1,1,1.0
class separation,6,1,6.0
separation to,1,1,1.0
to study,2,1,2.0
the inﬂuence,3,2,1.5
inﬂuence of,4,2,2.0
space structure,3,1,3.0
structure implicitly,1,1,1.0
implicitly deﬁned,1,1,1.0
deﬁned by,2,1,2.0
function a,1,1,1.0
a uniﬁed,3,1,3.0
uniﬁed framework,2,1,2.0
for preferential,2,1,2.0
preferential that,1,1,1.0
that spans,1,1,1.0
spans some,1,1,1.0
approaches in,5,1,5.0
literature w,1,1,1.0
e support,1,1,1.0
our investigation,1,1,1.0
investigation with,1,1,1.0
with extensive,1,1,1.0
extensive experiments,1,1,1.0
experiments over,2,1,2.0
over imbalanced,3,1,3.0
datasets index,1,1,1.0
index t,2,2,1.0
t imbalanced,1,1,1.0
imbalanced classiﬁcation,8,1,8.0
classiﬁcation kernel,1,1,1.0
kernel methods,8,1,8.0
methods empirical,1,1,1.0
space support,1,1,1.0
machines i,1,1,1.0
n t,5,2,2.5
t ro,1,1,1.0
ro d,1,1,1.0
d u,1,1,1.0
u c,1,1,1.0
c t,1,1,1.0
t i,6,2,3.0
i o,4,2,2.0
o n,9,2,4.5
n classiﬁcation,1,1,1.0
classiﬁcation methods,1,1,1.0
methods often,1,1,1.0
often conveniently,1,1,1.0
conveniently assume,1,1,1.0
the prior,1,1,1.0
prior class,1,1,1.0
class probability,1,1,1.0
probability distribution,4,2,2.0
of high,1,1,1.0
high entropy,1,1,1.0
entropy however,1,1,1.0
many applications,2,2,1.0
applications from,1,1,1.0
from areas,1,1,1.0
areas such,1,1,1.0
diagnosis information,1,1,1.0
retrieval fraud,1,1,1.0
detection etc,1,1,1.0
etc the,1,1,1.0
the classiﬁcation,11,2,5.5
classiﬁcation paradigm,1,1,1.0
paradigm when,1,1,1.0
when one,1,1,1.0
or several,1,1,1.0
several classes,1,1,1.0
have a,1,1,1.0
a much,2,1,2.0
lower prior,1,1,1.0
prior probability,1,1,1.0
probability in,1,1,1.0
as imbalanced,1,1,1.0
classiﬁcation and,2,2,1.0
it poses,1,1,1.0
poses a,3,1,3.0
a difﬁcult,1,1,1.0
difﬁcult challenge,1,1,1.0
challenge for,1,1,1.0
researchers because,1,1,1.0
because of,6,2,3.0
that imbalanced,1,1,1.0
classiﬁcation is,2,1,2.0
is currently,5,2,2.5
currently receiving,1,1,1.0
receiving a,1,1,1.0
of attention,1,1,1.0
attention from,2,2,1.0
the pattern,1,1,1.0
learning communities,1,1,1.0
communities often,1,1,1.0
often the,1,1,1.0
class happens,1,1,1.0
happens to,1,1,1.0
majority one,2,1,2.0
one but,1,1,1.0
be much,1,1,1.0
more difﬁcult,2,2,1.0
difﬁcult to,4,2,2.0
to model,1,1,1.0
model due,1,1,1.0
the low,1,1,1.0
low number,1,1,1.0
available samples,1,1,1.0
samples since,1,1,1.0
since most,2,1,2.0
traditional learning,1,1,1.0
systems have,1,1,1.0
been designed,1,1,1.0
designed to,2,1,2.0
work on,1,1,1.0
on balanced,2,1,2.0
balanced data,2,2,1.0
will usually,1,1,1.0
usually be,1,1,1.0
be focused,1,1,1.0
on improving,2,1,2.0
improving overall,2,1,2.0
and be,1,1,1.0
be biased,1,1,1.0
biased towards,1,1,1.0
towards the,1,1,1.0
class consequently,1,1,1.0
consequently harming,1,1,1.0
harming the,1,1,1.0
one although,2,1,2.0
although from,1,1,1.0
a formal,1,1,1.0
formal deﬁnition,1,1,1.0
deﬁnition an,1,1,1.0
the work,3,2,1.5
of p,4,1,4.0
guti and,2,1,2.0
and herv,2,1,2.0
herv has,1,1,1.0
been subsidized,1,1,1.0
subsidized by,1,1,1.0
the project,2,1,2.0
project of,2,1,2.0
the spanish,1,1,1.0
spanish ministerial,1,1,1.0
ministerial commission,1,1,1.0
commission of,1,1,1.0
of science,3,2,1.5
and t,2,1,2.0
t echnology,1,1,1.0
echnology micyt,1,1,1.0
micyt feder,1,1,1.0
feder funds,1,1,1.0
funds and,1,1,1.0
the junta,1,1,1.0
junta de,1,1,1.0
de andaluc,1,1,1.0
andaluc spain,1,1,1.0
spain the,1,1,1.0
p tino,1,1,1.0
tino has,1,1,1.0
been supported,1,1,1.0
supported by,1,1,1.0
by epsrc,1,1,1.0
epsrc grant,1,1,1.0
grant p,1,1,1.0
p ortiz,1,1,1.0
ortiz p,1,1,1.0
herv are,1,1,1.0
are with,2,2,1.0
and numerical,1,1,1.0
numerical analysis,2,1,2.0
the university,2,1,2.0
of c,2,2,1.0
c spain,1,1,1.0
spain email,1,1,1.0
email pagutierrez,1,1,1.0
pagutierrez chervas,1,1,1.0
chervas p,1,1,1.0
ˇno is,1,1,1.0
the school,1,1,1.0
science of,1,1,1.0
of birmingham,1,1,1.0
birmingham birmingham,1,1,1.0
birmingham united,1,1,1.0
united kingdom,1,1,1.0
kingdom email,1,1,1.0
email dataset,1,1,1.0
is any,1,1,1.0
any set,1,1,1.0
of labelled,1,1,1.0
labelled data,1,1,1.0
data exhibiting,1,1,1.0
exhibiting an,1,1,1.0
an unequal,2,2,1.0
unequal distribution,2,2,1.0
distribution between,2,2,1.0
only factor,1,1,1.0
factor involved,1,1,1.0
involved hindering,1,1,1.0
hindering the,1,1,1.0
this context,1,1,1.0
context the,1,1,1.0
the complexity,2,1,2.0
complexity of,6,2,3.0
data existence,1,1,1.0
existence of,1,1,1.0
of noisy,1,1,1.0
samples or,1,1,1.0
or class,1,1,1.0
overlapping or,1,1,1.0
set data,1,1,1.0
or small,1,1,1.0
small sample,1,1,1.0
sample size,1,1,1.0
size can,1,1,1.0
be part,1,1,1.0
the approaches,2,1,2.0
approaches developed,1,1,1.0
developed over,1,1,1.0
the years,1,1,1.0
years for,1,1,1.0
for tackling,2,1,2.0
problem can,1,1,1.0
be categorised,1,1,1.0
categorised in,1,1,1.0
two groups,1,1,1.0
groups data,1,1,1.0
data approach,2,2,1.0
on sampling,1,1,1.0
methods including,1,1,1.0
including minority,1,1,1.0
minority groups,1,1,1.0
groups groups,2,1,2.0
groups of,1,1,1.0
of interesting,1,1,1.0
interesting rare,1,1,1.0
rare examples,1,1,1.0
examples or,1,1,1.0
or majority,1,1,1.0
groups with,1,1,1.0
large example,1,1,1.0
example sizes,1,1,1.0
sizes the,1,1,1.0
the combination,2,1,2.0
both being,1,1,1.0
being also,1,1,1.0
also very,1,1,1.0
popular algorithm,1,1,1.0
algorithm approach,1,1,1.0
approach forces,1,1,1.0
the classiﬁer,3,2,1.5
classiﬁer to,1,1,1.0
to pay,1,1,1.0
pay more,1,1,1.0
by learning,1,1,1.0
the analysis,1,1,1.0
analysis made,1,1,1.0
made in,2,1,2.0
is contextualised,1,1,1.0
contextualised on,1,1,1.0
data approaches,1,1,1.0
approaches thus,1,1,1.0
thus a,1,1,1.0
a brief,3,2,1.5
brief discussion,2,2,1.0
discussion on,2,2,1.0
on these,2,2,1.0
techniques is,1,1,1.0
is now,1,1,1.0
now given,1,1,1.0
given for,1,1,1.0
a detailed,2,2,1.0
detailed review,1,1,1.0
of see,1,1,1.0
see roughly,1,1,1.0
roughly speaking,2,1,2.0
speaking it,1,1,1.0
be said,5,1,5.0
said that,3,1,3.0
that and,1,1,1.0
sampling are,1,1,1.0
are opposite,1,1,1.0
opposite and,1,1,1.0
and equivalent,1,1,1.0
equivalent since,1,1,1.0
are aimed,1,1,1.0
aimed at,1,1,1.0
same purpose,1,1,1.0
purpose balance,1,1,1.0
distribution but,1,1,1.0
but using,1,1,1.0
different approaches,4,1,4.0
approaches formally,1,1,1.0
formally concerns,1,1,1.0
concerns to,1,1,1.0
process of,3,2,1.5
distribution with,1,1,1.0
a signiﬁcantly,1,1,1.0
signiﬁcantly higher,1,1,1.0
higher frequency,1,1,1.0
frequency than,1,1,1.0
given one,1,1,1.0
one and,1,1,1.0
of reducing,1,1,1.0
both cases,1,1,1.0
the methodologies,1,1,1.0
methodologies impose,1,1,1.0
impose a,1,1,1.0
a balance,1,1,1.0
balance in,1,1,1.0
avoid aliasing,1,1,1.0
aliasing and,1,1,1.0
and focus,1,1,1.0
classiﬁcation of,1,1,1.0
classes although,1,1,1.0
both sampling,1,1,1.0
improve classiﬁer,1,1,1.0
classiﬁer performance,3,2,1.5
datasets different,1,1,1.0
different studies,1,1,1.0
studies suggest,3,1,3.0
more useful,2,1,2.0
useful than,1,1,1.0
than specially,1,1,1.0
specially for,1,1,1.0
for highly,3,1,3.0
and complex,1,1,1.0
complex datasets,1,1,1.0
datasets recall,1,1,1.0
recall that,8,1,8.0
could entail,1,1,1.0
entail a,1,1,1.0
loss of,2,1,2.0
potentially meaningful,1,1,1.0
meaningful information,1,1,1.0
dataset concerning,1,1,1.0
concerning the,5,1,5.0
the ﬁrst,12,2,6.0
ﬁrst idea,2,1,2.0
perform a,2,1,2.0
this often,1,1,1.0
often leads,1,1,1.0
another common,1,1,1.0
generate new,2,1,2.0
synthetic patterns,12,1,12.0
patterns according,4,1,4.0
distribution one,1,1,1.0
most methods,1,1,1.0
so is,1,1,1.0
smote based,1,1,1.0
on erating,1,1,1.0
erating new,1,1,1.0
new instances,2,1,2.0
by convex,10,1,10.0
one point,1,1,1.0
point and,1,1,1.0
neighbours both,3,1,3.0
both belonging,3,1,3.0
classes in,3,2,1.5
general can,1,1,1.0
can not,5,2,2.5
be assumed,1,1,1.0
assumed to,2,1,2.0
be convex,1,1,1.0
convex and,1,1,1.0
hence smote,1,1,1.0
smote does,2,2,1.0
not avoid,1,1,1.0
avoid synthetic,1,1,1.0
fall inside,1,1,1.0
inside majority,1,1,1.0
majority regions,1,1,1.0
regions therefore,1,1,1.0
therefore more,2,2,1.0
more careful,1,1,1.0
careful techniques,1,1,1.0
to prevent,1,1,1.0
prevent this,1,1,1.0
issue prevent,1,1,1.0
prevent but,1,1,1.0
but solve,1,1,1.0
solve adaptive,1,1,1.0
synthetic and,1,1,1.0
are examples,1,1,1.0
more powerful,1,1,1.0
powerful techniques,1,1,1.0
techniques based,1,1,1.0
on extracting,1,1,1.0
extracting knowledge,1,1,1.0
knowledge from,2,2,1.0
to analyse,4,1,4.0
analyse which,2,1,2.0
which patterns,5,1,5.0
patterns and,7,1,7.0
and regions,1,1,1.0
regions of,4,2,2.0
the space,2,1,2.0
space are,5,1,5.0
more suitable,11,1,11.0
suitable for,5,1,5.0
for sampling,8,1,8.0
sampling this,1,1,1.0
this will,3,1,3.0
be referred,1,1,1.0
referred in,2,1,2.0
paper to,2,2,1.0
as preferential,1,1,1.0
preferential at,1,1,1.0
same time,2,1,2.0
time kernel,1,1,1.0
been spreading,1,1,1.0
spreading rapidly,1,1,1.0
rapidly and,1,1,1.0
and gaining,1,1,1.0
gaining acceptance,1,1,1.0
acceptance in,1,1,1.0
learning due,1,1,1.0
their good,1,1,1.0
good generalisation,2,1,2.0
generalisation ability,2,1,2.0
ability and,2,1,2.0
and determinism,1,1,1.0
determinism being,1,1,1.0
being one,1,1,1.0
support v,4,1,4.0
v ector,4,1,4.0
ector machine,1,1,1.0
machine svm,1,1,1.0
svm however,1,1,1.0
however for,4,1,4.0
for svm,1,1,1.0
svm imbalanced,1,1,1.0
data pose,1,1,1.0
serious challenge,1,1,1.0
challenge due,1,1,1.0
the formulation,1,1,1.0
formulation of,1,1,1.0
the maximisation,1,1,1.0
maximisation which,1,1,1.0
which focus,1,1,1.0
performance thus,1,1,1.0
thus the,3,2,1.5
of kernel,9,1,9.0
with techniques,1,1,1.0
tackling class,1,1,1.0
is widely,1,1,1.0
widely spread,1,1,1.0
spread it,1,1,1.0
is clear,5,2,2.5
clear that,5,2,2.5
that by,1,1,1.0
by linear,1,1,1.0
linear interpolation,4,2,2.0
interpolation is,1,1,1.0
not as,3,1,3.0
as suitable,1,1,1.0
suitable when,1,1,1.0
when dealing,4,1,4.0
with nonlinear,1,1,1.0
nonlinear classiﬁers,2,1,2.0
classiﬁers as,1,1,1.0
be than,1,1,1.0
than when,1,1,1.0
applying linear,1,1,1.0
linear classiﬁers,1,1,1.0
classiﬁers however,1,1,1.0
however linearly,1,1,1.0
separable datasets,1,1,1.0
not common,1,1,1.0
common in,1,1,1.0
in applications,4,2,2.0
applications thus,1,1,1.0
thus making,1,1,1.0
making advisable,2,1,2.0
advisable the,2,1,2.0
of classiﬁers,5,2,2.5
classiﬁers able,1,1,1.0
to capture,1,1,1.0
capture this,1,1,1.0
this nonlinearity,1,1,1.0
nonlinearity besides,1,1,1.0
the development,3,2,1.5
development of,4,2,2.0
a able,1,1,1.0
able nonlinear,1,1,1.0
nonlinear strategy,1,1,1.0
strategy could,1,1,1.0
be tricky,1,1,1.0
tricky thus,1,1,1.0
thus in,1,1,1.0
in contrast,2,2,1.0
contrast to,1,1,1.0
to previous,1,1,1.0
approaches we,1,1,1.0
we propose,3,1,3.0
propose to,1,1,1.0
space where,1,1,1.0
are ideally,1,1,1.0
ideally linearly,2,1,2.0
linearly separated,1,1,1.0
separated making,1,1,1.0
making generation,1,1,1.0
of new,2,2,1.0
synthetic points,2,1,2.0
points by,2,1,2.0
original points,1,1,1.0
points belonging,1,1,1.0
same class,1,1,1.0
class safe,2,1,2.0
safe this,1,1,1.0
done using,1,1,1.0
function for,4,2,2.0
the patterns,15,1,15.0
patterns rather,1,1,1.0
than using,1,1,1.0
space however,2,1,2.0
not so,1,1,1.0
so straightforward,1,1,1.0
straightforward because,1,1,1.0
because when,1,1,1.0
with kernel,6,1,6.0
methods the,1,1,1.0
only information,1,1,1.0
information available,1,1,1.0
available is,1,1,1.0
the dot,11,1,11.0
dot products,6,1,6.0
products of,2,1,2.0
the images,2,1,2.0
images of,5,1,5.0
patterns t,2,1,2.0
o cope,2,1,2.0
cope with,2,1,2.0
issue this,1,1,1.0
paper makes,1,1,1.0
makes use,1,1,1.0
space efs,4,1,4.0
efs which,2,1,2.0
is euclidean,1,1,1.0
euclidean and,1,1,1.0
and preserves,1,1,1.0
preserves the,3,1,3.0
the geometrical,1,1,1.0
geometrical structure,1,1,1.0
structure of,2,1,2.0
original feature,2,1,2.0
space given,1,1,1.0
given that,3,2,1.5
that distances,1,1,1.0
distances and,2,1,2.0
and angles,1,1,1.0
angles in,1,1,1.0
are uniquely,2,1,2.0
uniquely determined,2,1,2.0
determined by,2,1,2.0
by dot,1,1,1.0
products and,1,1,1.0
the corresponding,10,2,5.0
corresponding images,1,1,1.0
images are,1,1,1.0
original kernel,1,1,1.0
kernel values,1,1,1.0
values the,4,2,2.0
main motivation,1,1,1.0
for performing,3,1,3.0
performing in,2,1,2.0
the efs,46,1,46.0
efs instead,2,1,2.0
of in,5,1,5.0
the hypothesis,6,2,3.0
hypothesis that,2,1,2.0
space provide,1,1,1.0
suitable space,4,1,4.0
for via,1,1,1.0
via convex,1,1,1.0
combination because,2,1,2.0
separation will,1,1,1.0
be simpler,1,1,1.0
simpler and,1,1,1.0
and larger,1,1,1.0
larger ideally,1,1,1.0
ideally due,1,1,1.0
kernel trick,3,1,3.0
trick linearly,1,1,1.0
separable at,1,1,1.0
time this,1,1,1.0
technique can,1,1,1.0
general nonlinear,1,1,1.0
nonlinear in,1,1,1.0
space due,1,1,1.0
the nonlinear,3,2,1.5
nonlinear map,2,1,2.0
map φ,6,1,6.0
φ related,1,1,1.0
trick and,1,1,1.0
and could,1,1,1.0
in combination,1,1,1.0
combination with,1,1,1.0
any classiﬁer,1,1,1.0
classiﬁer t,1,1,1.0
best of,1,1,1.0
our knowledge,1,1,1.0
knowledge performing,1,1,1.0
space has,1,1,1.0
has only,1,1,1.0
only been,1,1,1.0
researched in,1,1,1.0
in recall,1,1,1.0
our case,1,1,1.0
case it,2,1,2.0
efs in,2,1,2.0
this previous,1,1,1.0
previous work,2,2,1.0
were generated,1,1,1.0
the geometric,2,1,2.0
geometric interpretation,1,1,1.0
interpretation of,1,1,1.0
products in,1,1,1.0
kernel matrix,25,2,12.5
were approximated,1,1,1.0
approximated based,1,1,1.0
distance relation,1,1,1.0
input one,1,1,1.0
one since,1,1,1.0
since inverse,1,1,1.0
inverse mapping,2,1,2.0
mapping φ,2,1,2.0
φ from,1,1,1.0
space to,3,1,3.0
not available,1,1,1.0
available our,1,1,1.0
our proposal,1,1,1.0
proposal is,1,1,1.0
is free,1,1,1.0
free of,1,1,1.0
the assumptions,1,1,1.0
assumptions of,1,1,1.0
this inverse,1,1,1.0
mapping approximation,1,1,1.0
approximation the,1,1,1.0
study made,1,1,1.0
paper intends,1,1,1.0
intends to,1,1,1.0
provide an,3,2,1.5
extensive analysis,1,1,1.0
efs and,3,1,3.0
be subdivided,1,1,1.0
subdivided in,1,1,1.0
three sections,1,1,1.0
sections the,1,1,1.0
ﬁrst one,2,1,2.0
one deals,1,1,1.0
of extending,1,1,1.0
extending the,1,1,1.0
and efs,1,1,1.0
efs the,5,1,5.0
the objective,4,2,2.0
to test,6,1,6.0
test whether,5,1,5.0
whether the,4,1,4.0
efs provides,2,1,2.0
suitable framework,1,1,1.0
for by,2,1,2.0
dimensionality of,10,1,10.0
second part,1,1,1.0
part deals,1,1,1.0
function choice,1,1,1.0
choice since,1,1,1.0
since our,1,1,1.0
our methodology,1,1,1.0
methodology depends,1,1,1.0
on how,1,1,1.0
how the,1,1,1.0
kernel matches,1,1,1.0
underlying classiﬁcation,1,1,1.0
classiﬁcation problem,7,2,3.5
we develop,1,1,1.0
develop a,1,1,1.0
a strategy,2,1,2.0
strategy for,1,1,1.0
for optimising,4,1,4.0
optimising the,6,1,6.0
space based,1,1,1.0
on analytical,1,1,1.0
analytical knowledge,1,1,1.0
knowledge using,1,1,1.0
of alignment,1,1,1.0
alignment ideally,1,1,1.0
ideally a,1,1,1.0
better ﬁtted,1,1,1.0
ﬁtted kernel,1,1,1.0
kernel will,1,1,1.0
class separability,1,1,1.0
separability providing,1,1,1.0
a safer,1,1,1.0
safer environment,1,1,1.0
environment for,1,1,1.0
patterns the,3,1,3.0
the last,1,1,1.0
last part,1,1,1.0
paper proposes,2,2,1.0
proposes a,2,1,2.0
uniﬁed adaptive,1,1,1.0
adaptive framework,1,1,1.0
preferential generalising,1,1,1.0
generalising several,1,1,1.0
several approaches,2,1,2.0
literature the,1,1,1.0
optimal svm,2,1,2.0
svm hyperplane,3,1,3.0
hyperplane and,3,1,3.0
and kernel,2,1,2.0
learning techniques,8,2,4.0
the synthetically,1,1,1.0
to check,2,1,2.0
check if,1,1,1.0
if some,1,1,1.0
some regions,2,1,2.0
space can,2,1,2.0
useful for,2,1,2.0
for than,3,1,3.0
others t,1,1,1.0
o test,1,1,1.0
different hypotheses,1,1,1.0
hypotheses exposed,1,1,1.0
exposed in,1,1,1.0
we perform,1,1,1.0
a thorough,3,2,1.5
thorough set,2,1,2.0
of experiments,3,2,1.5
experiments with,2,2,1.0
with binary,1,1,1.0
binary imbalanced,1,1,1.0
section ii,3,2,1.5
ii introduces,1,1,1.0
introduces some,1,1,1.0
some useful,1,1,1.0
useful notions,1,1,1.0
notions section,1,1,1.0
section iii,3,2,1.5
iii exposes,1,1,1.0
exposes how,1,1,1.0
perform sampling,1,1,1.0
efs section,1,1,1.0
section iv,2,2,1.0
iv develops,1,1,1.0
develops a,1,1,1.0
new methodology,1,1,1.0
methodology for,2,1,2.0
for kernel,2,1,2.0
learning section,1,1,1.0
section v,1,1,1.0
v proposes,1,1,1.0
general preferential,1,1,1.0
preferential framework,1,1,1.0
framework section,1,1,1.0
section vi,1,1,1.0
vi exposes,1,1,1.0
exposes the,1,1,1.0
the tal,1,1,1.0
tal study,1,1,1.0
study and,1,1,1.0
and analyses,2,1,2.0
obtained and,1,1,1.0
and ﬁnally,3,1,3.0
ﬁnally section,1,1,1.0
section vii,1,1,1.0
vii outlines,1,1,1.0
outlines some,1,1,1.0
some conclusions,1,1,1.0
conclusions and,1,1,1.0
future work,2,1,2.0
work ii,1,1,1.0
ii b,1,1,1.0
b ac,1,1,1.0
ac k,2,1,2.0
k g,1,1,1.0
g ro,1,1,1.0
ro u,1,1,1.0
u n,2,1,2.0
d this,1,1,1.0
section is,4,2,2.0
is intended,3,1,3.0
intended to,3,1,3.0
to introduce,1,1,1.0
introduce the,1,1,1.0
the notation,1,1,1.0
notation used,1,1,1.0
used throughout,1,1,1.0
throughout all,1,1,1.0
provide some,1,1,1.0
some previous,1,1,1.0
previous notions,1,1,1.0
notions about,1,1,1.0
about svm,1,1,1.0
svm classiﬁers,1,1,1.0
classiﬁers and,1,1,1.0
space consider,1,1,1.0
sample d,1,1,1.0
d xi,1,1,1.0
xi y,3,2,1.5
y i,2,2,1.0
i m,9,2,4.5
m x,2,1,2.0
x y,5,2,2.5
y generated,2,2,1.0
generated from,3,1,3.0
a unknown,1,1,1.0
unknown joint,1,1,1.0
joint distribution,1,1,1.0
distribution p,1,1,1.0
p x,1,1,1.0
y where,1,1,1.0
where x,1,1,1.0
x rd,1,1,1.0
rd y,1,1,1.0
y the,3,2,1.5
binary classiﬁcation,1,1,1.0
to assign,2,1,2.0
assign an,1,1,1.0
an input,1,1,1.0
input vector,1,1,1.0
vector x,1,1,1.0
x to,2,1,2.0
classes denote,1,1,1.0
denote by,3,1,3.0
by xtr,1,1,1.0
xtr and,2,1,2.0
and xts,1,1,1.0
xts the,1,1,1.0
and testing,4,2,2.0
testing inputs,1,1,1.0
inputs respectively,1,1,1.0
respectively furthermore,2,2,1.0
furthermore we,3,1,3.0
will mark,1,1,1.0
mark by,1,1,1.0
by subscript,1,1,1.0
subscript and,1,1,1.0
containing inputs,1,1,1.0
inputs from,1,1,1.0
class respectively,1,1,1.0
respectively for,2,2,1.0
set x,1,1,1.0
x we,1,1,1.0
we denote,2,1,2.0
by x,1,1,1.0
x the,2,2,1.0
design matrix,4,1,4.0
matrix storing,3,1,3.0
storing points,2,1,2.0
points of,2,1,2.0
of x,1,1,1.0
x as,1,1,1.0
as rows,3,1,3.0
rows reproducing,1,1,1.0
reproducing kernels,1,1,1.0
kernels often,1,1,1.0
often referred,1,1,1.0
referred as,1,1,1.0
as mercer,1,1,1.0
mercer kernels,1,1,1.0
kernels are,1,1,1.0
are functions,1,1,1.0
functions k,1,1,1.0
k x,1,1,1.0
x x,1,1,1.0
x r,1,1,1.0
r which,1,1,1.0
which for,2,1,2.0
all pattern,1,1,1.0
pattern sets,1,1,1.0
sets xm,1,1,1.0
xm give,1,1,1.0
give rise,1,1,1.0
to semideﬁnite,1,1,1.0
semideﬁnite positive,1,1,1.0
positive matrices,1,1,1.0
matrices where,2,1,2.0
where kij,1,1,1.0
kij k,1,1,1.0
k xi,8,1,8.0
xi xj,17,1,17.0
xj kernel,1,1,1.0
kernel functions,2,1,2.0
functions allow,1,1,1.0
allow us,1,1,1.0
to derive,1,1,1.0
derive nonlinear,1,1,1.0
classiﬁers by,2,2,1.0
by reducing,1,1,1.0
reducing them,1,1,1.0
to linear,1,1,1.0
linear ones,1,1,1.0
ones but,1,1,1.0
but in,1,1,1.0
some hilbert,1,1,1.0
hilbert space,2,1,2.0
space h,2,1,2.0
h nonlinearly,1,1,1.0
nonlinearly related,1,1,1.0
and furnished,1,1,1.0
furnished with,1,1,1.0
a dot,1,1,1.0
dot product,10,1,10.0
product k,1,1,1.0
xj xi,2,1,2.0
xi φ,10,1,10.0
φ xj,1,1,1.0
xj the,2,1,2.0
this kernel,3,1,3.0
function instead,1,1,1.0
product in,2,1,2.0
in rm,1,1,1.0
rm corresponds,1,1,1.0
to using,1,1,1.0
a usually,1,1,1.0
usually nonlinear,1,1,1.0
nonlinear mapping,1,1,1.0
mapping of,1,1,1.0
patterns from,1,1,1.0
from x,1,1,1.0
a or,1,1,1.0
or dimensional,1,1,1.0
dimensional hilbert,1,1,1.0
h such,1,1,1.0
that φ,1,1,1.0
φ x,2,1,2.0
x h,1,1,1.0
h where,1,1,1.0
the separation,4,2,2.0
separation would,1,1,1.0
would ideally,3,1,3.0
ideally be,1,1,1.0
be easier,1,1,1.0
easier and,1,1,1.0
and take,1,1,1.0
product there,1,1,1.0
there kernel,1,1,1.0
kernel machines,2,2,1.0
machines trained,1,1,1.0
trained on,1,1,1.0
on d,1,1,1.0
d do,1,1,1.0
not operate,1,1,1.0
whole of,1,1,1.0
of h,2,2,1.0
h but,1,1,1.0
its subset,1,1,1.0
subset f,1,1,1.0
f span,1,1,1.0
span φ,1,1,1.0
φ φ,1,1,1.0
φ xm,1,1,1.0
xm which,1,1,1.0
will refer,1,1,1.0
refer to,3,2,1.5
space such,2,1,2.0
that f,2,1,2.0
f h,2,1,2.0
h note,1,1,1.0
f is,2,1,2.0
most an,1,1,1.0
an linear,1,1,1.0
linear support,1,1,1.0
ector machines,3,1,3.0
machines svm,2,1,2.0
svm is,1,1,1.0
is perhaps,1,1,1.0
common kernel,1,1,1.0
kernel method,1,1,1.0
for statistical,1,1,1.0
statistical pattern,1,1,1.0
recognition due,1,1,1.0
its good,1,1,1.0
and freedom,1,1,1.0
freedom from,1,1,1.0
from local,1,1,1.0
local minima,1,1,1.0
minima the,1,1,1.0
separation of,2,1,2.0
classes through,1,1,1.0
a hyperplane,1,1,1.0
hyperplane which,1,1,1.0
is speciﬁed,1,1,1.0
speciﬁed by,1,1,1.0
a normal,1,1,1.0
normal vector,1,1,1.0
vector w,1,1,1.0
w and,2,1,2.0
a bias,2,1,2.0
bias the,2,2,1.0
optimal separating,1,1,1.0
separating hyperplane,3,1,3.0
hyperplane is,2,1,2.0
one which,1,1,1.0
which maximises,2,1,2.0
maximises the,2,1,2.0
the hyperplane,4,1,4.0
nearest points,1,1,1.0
classes called,1,1,1.0
called margin,1,1,1.0
margin beyond,1,1,1.0
kernel techniques,1,1,1.0
to allow,1,1,1.0
allow decision,1,1,1.0
decision discriminants,1,1,1.0
discriminants the,1,1,1.0
trick another,1,1,1.0
another generalisation,1,1,1.0
generalisation was,1,1,1.0
to replace,1,1,1.0
replace hard,1,1,1.0
hard margins,1,1,1.0
margins with,1,1,1.0
with soft,1,1,1.0
soft margins,1,1,1.0
margins using,1,1,1.0
the ξi,1,1,1.0
ξi in,1,1,1.0
with overlapping,1,1,1.0
classes therefore,1,1,1.0
therefore this,2,1,2.0
algorithm seeks,1,1,1.0
seeks for,1,1,1.0
a classiﬁer,4,2,2.0
classiﬁer f,1,1,1.0
f rd,1,1,1.0
rd r,1,1,1.0
r of,1,1,1.0
form f,1,1,1.0
f x,1,1,1.0
x w,1,1,1.0
w φ,3,1,3.0
x b,1,1,1.0
b φ,1,1,1.0
φ being,1,1,1.0
the mapping,1,1,1.0
mapping function,1,1,1.0
function induced,1,1,1.0
kernel that,2,1,2.0
that minimises,2,1,2.0
minimises the,3,1,3.0
objective function,1,1,1.0
function c,1,1,1.0
c ξi,1,1,1.0
ξi for,1,1,1.0
some parameter,1,1,1.0
parameter c,1,1,1.0
c subject,1,1,1.0
subject to,2,1,2.0
the constraints,2,1,2.0
constraints yi,2,1,2.0
yi w,2,1,2.0
φ xi,8,1,8.0
xi b,2,1,2.0
b ξi,2,1,2.0
ξi ξ,2,1,2.0
ξ i,2,1,2.0
m it,1,1,1.0
that using,1,1,1.0
using svms,1,1,1.0
svms the,2,1,2.0
the maximization,1,1,1.0
maximization paradigm,1,1,1.0
paradigm poses,2,1,2.0
hindrance for,1,1,1.0
main reason,1,1,1.0
reason for,1,1,1.0
that svm,1,1,1.0
svm optimisation,1,1,1.0
optimisation is,1,1,1.0
is focused,2,2,1.0
on overall,1,1,1.0
overall error,1,1,1.0
error therefore,1,1,1.0
therefore they,2,1,2.0
are inherently,1,1,1.0
inherently biased,1,1,1.0
toward the,4,2,2.0
worst case,1,1,1.0
a noisy,1,1,1.0
and highly,1,1,1.0
the svm,7,1,7.0
svm paradigm,3,1,3.0
paradigm is,1,1,1.0
very likely,1,1,1.0
obtain a,4,1,4.0
a trivial,2,1,2.0
trivial classiﬁer,2,1,2.0
classiﬁer the,1,1,1.0
that classiﬁes,1,1,1.0
classiﬁes all,2,2,1.0
patterns in,4,1,4.0
solution that,1,1,1.0
that as,1,1,1.0
as said,7,1,7.0
said if,1,1,1.0
is severe,1,1,1.0
severe could,1,1,1.0
could provide,1,1,1.0
provide the,2,2,1.0
minimal error,1,1,1.0
error t,1,1,1.0
issue several,1,1,1.0
several studies,1,1,1.0
studies in,1,1,1.0
learning literature,2,1,2.0
literature have,1,1,1.0
have explored,1,1,1.0
explored different,1,1,1.0
different solutions,1,1,1.0
problem considering,1,1,1.0
paradigm most,1,1,1.0
on classiﬁcation,1,1,1.0
classiﬁcation ensembles,1,1,1.0
ensembles and,1,1,1.0
kernel optimisation,2,1,2.0
optimisation techniques,2,1,2.0
techniques among,1,1,1.0
among others,1,1,1.0
others however,1,1,1.0
however some,1,1,1.0
some studies,2,2,1.0
sampling is,2,1,2.0
as effective,1,1,1.0
effective as,1,1,1.0
case because,1,1,1.0
potential loss,1,1,1.0
of information,1,1,1.0
information on,1,1,1.0
boundaries which,1,1,1.0
is crucial,1,1,1.0
crucial for,1,1,1.0
svm solution,2,1,2.0
solution synthetic,1,1,1.0
stated one,1,1,1.0
used techniques,1,1,1.0
simple the,1,1,1.0
method consists,1,1,1.0
consists on,1,1,1.0
on generating,1,1,1.0
instances on,1,1,1.0
the line,3,2,1.5
line that,1,1,1.0
that connects,1,1,1.0
connects one,1,1,1.0
one randomly,2,2,1.0
randomly chosen,3,2,1.5
chosen point,1,1,1.0
point with,1,1,1.0
nearest neighbours,3,1,3.0
class therefore,1,1,1.0
this methodology,3,1,3.0
methodology relies,1,1,1.0
a convex,2,1,2.0
two patterns,2,1,2.0
patterns note,2,1,2.0
that with,1,1,1.0
approach new,2,2,1.0
patterns could,1,1,1.0
could lie,2,1,2.0
lie inside,1,1,1.0
inside the,2,1,2.0
region although,1,1,1.0
although choosing,1,1,1.0
choosing a,1,1,1.0
a correct,1,1,1.0
correct value,1,1,1.0
value for,2,2,1.0
k parameter,1,1,1.0
neighbours method,3,1,3.0
method could,1,1,1.0
could avoid,1,1,1.0
avoid this,1,1,1.0
this to,1,1,1.0
to happen,1,1,1.0
happen in,1,1,1.0
cases empirical,1,1,1.0
efs w,1,1,1.0
e can,1,1,1.0
can endow,1,1,1.0
endow an,1,1,1.0
an r,1,1,1.0
r m,1,1,1.0
m space,1,1,1.0
space f,3,1,3.0
f with,1,1,1.0
with an,7,2,3.5
an orthonormal,3,1,3.0
orthonormal basis,3,1,3.0
basis ug,1,1,1.0
ug g,1,1,1.0
g b,2,1,2.0
b b,1,1,1.0
b r,1,1,1.0
r satisfying,1,1,1.0
satisfying orthogonality,1,1,1.0
orthogonality normalisation,1,1,1.0
normalisation and,1,1,1.0
and completeness,1,1,1.0
completeness consider,1,1,1.0
consider the,5,1,5.0
set e,2,1,2.0
e ϕ,1,1,1.0
ϕ v,3,1,3.0
v f,2,1,2.0
f where,2,1,2.0
where ϕ,1,1,1.0
v the,1,1,1.0
the map,2,1,2.0
map ϕ,1,1,1.0
ϕ is,2,1,2.0
an isometric,1,1,1.0
isometric isomorphism,1,1,1.0
isomorphism of,1,1,1.0
of f,3,1,3.0
f and,3,1,3.0
e a,3,1,3.0
a bijective,2,1,2.0
bijective linear,2,1,2.0
linear mapping,2,1,2.0
mapping such,1,1,1.0
products are,1,1,1.0
are preserved,2,1,2.0
preserved v,1,1,1.0
v ϕ,3,1,3.0
ϕ when,1,1,1.0
when f,1,1,1.0
space the,4,2,2.0
is referred,2,1,2.0
referred to,3,1,3.0
as empirical,1,1,1.0
efs consider,1,1,1.0
training points,3,1,3.0
points xi,3,1,3.0
xi m,1,1,1.0
x then,1,1,1.0
then when,1,1,1.0
when working,1,1,1.0
working with,1,1,1.0
function k,1,1,1.0
k to,2,1,2.0
to map,1,1,1.0
map the,2,1,2.0
thus obtain,1,1,1.0
a gram,1,1,1.0
gram matrix,5,1,5.0
matrix k,7,1,7.0
k with,2,1,2.0
with rank,1,1,1.0
rank r,3,1,3.0
r r,2,2,1.0
r the,1,1,1.0
map from,1,1,1.0
space φ,1,1,1.0
φ e,35,1,35.0
e r,14,2,7.0
r x,1,1,1.0
x rr,1,1,1.0
rr which,1,1,1.0
which preserves,1,1,1.0
structure is,1,1,1.0
empirical kernel,12,1,12.0
kernel map,10,1,10.0
efs e,5,1,5.0
is chosen,1,1,1.0
chosen so,1,1,1.0
so as,1,1,1.0
as to,1,1,1.0
preserve the,1,1,1.0
product information,1,1,1.0
about f,1,1,1.0
f contained,1,1,1.0
contained in,3,1,3.0
in k,1,1,1.0
be isometric,1,1,1.0
isometric isomorphic,1,1,1.0
the embedded,1,1,1.0
embedded feature,1,1,1.0
h in,1,1,1.0
this sense,2,1,2.0
sense it,1,1,1.0
map corresponds,1,1,1.0
mapping ϕ,1,1,1.0
ϕ f,1,1,1.0
f e,4,1,4.0
a graphical,2,1,2.0
graphical representation,2,1,2.0
space feature,2,1,2.0
and mappings,1,1,1.0
mappings between,1,1,1.0
between these,1,1,1.0
these spaces,1,1,1.0
spaces is,1,1,1.0
is shown,3,2,1.5
in fig,12,2,6.0
fig fig,1,1,1.0
fig representation,2,2,1.0
relation and,1,1,1.0
and mapping,1,1,1.0
mapping between,1,1,1.0
between input,1,1,1.0
space any,1,1,1.0
any given,1,1,1.0
given gram,1,1,1.0
k of,2,1,2.0
of rank,2,1,2.0
r can,2,1,2.0
be diagonalised,1,1,1.0
diagonalised as,1,1,1.0
follows λ,1,1,1.0
λ pt,3,1,3.0
pt where,1,1,1.0
where t,2,1,2.0
t is,2,1,2.0
the transpose,1,1,1.0
transpose operation,1,1,1.0
operation λ,1,1,1.0
λ is,1,1,1.0
a diagonal,1,1,1.0
diagonal matrix,1,1,1.0
matrix containing,1,1,1.0
containing the,1,1,1.0
the r,2,2,1.0
r nonzero,1,1,1.0
nonzero eigenvalues,2,1,2.0
eigenvalues of,2,1,2.0
of k,3,2,1.5
k in,1,1,1.0
in decreasing,1,1,1.0
decreasing order,1,1,1.0
order λ,1,1,1.0
λ r,1,1,1.0
r and,2,1,2.0
and p,3,1,3.0
a unitary,2,1,2.0
unitary matrix,2,1,2.0
matrix that,1,1,1.0
that consists,1,1,1.0
consists of,3,2,1.5
the eigenvectors,1,1,1.0
eigenvectors associated,1,1,1.0
associated to,6,1,6.0
to those,1,1,1.0
those r,1,1,1.0
r eigenvalues,1,1,1.0
eigenvalues ur,1,1,1.0
ur constituting,1,1,1.0
constituting an,1,1,1.0
basis of,4,1,4.0
of rr,1,1,1.0
rr then,1,1,1.0
map is,1,1,1.0
is deﬁned,4,2,2.0
deﬁned as,5,2,2.5
as φ,1,1,1.0
r xi,3,1,3.0
xi λ,1,1,1.0
pt k,2,1,2.0
xi k,2,1,2.0
xi xm,1,1,1.0
xm consider,1,1,1.0
set φ,1,1,1.0
r φ,1,1,1.0
r xm,1,1,1.0
xm of,1,1,1.0
efs images,2,1,2.0
points let,1,1,1.0
let be,1,1,1.0
storing φ,1,1,1.0
xi as,1,1,1.0
rows it,1,1,1.0
is easy,1,1,1.0
check that,1,1,1.0
standard dot,1,1,1.0
product matrix,1,1,1.0
matrix of,2,2,1.0
of φ,3,1,3.0
xi i,3,2,1.5
m evaluated,1,1,1.0
evaluated in,1,1,1.0
is k,1,1,1.0
k writing,1,1,1.0
writing z,1,1,1.0
z λ,1,1,1.0
k we,1,1,1.0
we obtain,1,1,1.0
obtain ztz,1,1,1.0
ztz pλp,1,1,1.0
pλp tpλ,1,1,1.0
tpλ t,1,1,1.0
t since,1,1,1.0
the angles,2,1,2.0
angles of,1,1,1.0
m vectors,1,1,1.0
vectors φ,1,1,1.0
m in,1,1,1.0
the note,1,1,1.0
that p,1,1,1.0
and k,1,1,1.0
k a,1,1,1.0
a symmetric,1,1,1.0
symmetric dot,1,1,1.0
product xi,1,1,1.0
xj k,2,1,2.0
xi xi,2,1,2.0
k xj,1,1,1.0
xj xj,1,1,1.0
data have,1,1,1.0
same geometrical,1,1,1.0
geometrical ture,1,1,1.0
ture in,1,1,1.0
both spaces,1,1,1.0
spaces f,1,1,1.0
and however,1,1,1.0
however recall,1,1,1.0
φ into,1,1,1.0
is nonlinear,1,1,1.0
nonlinear therefore,1,1,1.0
therefore each,1,1,1.0
the span,1,1,1.0
span of,1,1,1.0
the mapped,1,1,1.0
mapped input,1,1,1.0
input data,1,1,1.0
data would,1,1,1.0
would not,1,1,1.0
not necessarily,3,2,1.5
necessarily be,1,1,1.0
the image,1,1,1.0
image of,1,1,1.0
some input,1,1,1.0
input pattern,1,1,1.0
pattern this,1,1,1.0
the preimage,2,1,2.0
preimage problem,1,1,1.0
problem this,2,2,1.0
problem also,1,1,1.0
also appears,1,1,1.0
appears when,1,1,1.0
map because,1,1,1.0
it also,1,1,1.0
also corresponds,1,1,1.0
a nonlinear,1,1,1.0
nonlinear transformation,1,1,1.0
transformation note,1,1,1.0
the linear,3,2,1.5
linear decision,1,1,1.0
is built,1,1,1.0
built in,1,1,1.0
and if,2,1,2.0
are almost,1,1,1.0
almost linearly,1,1,1.0
separable in,1,1,1.0
space doing,1,1,1.0
doing local,1,1,1.0
local convex,1,1,1.0
combination is,2,1,2.0
is reasonable,1,1,1.0
reasonable whether,1,1,1.0
points exist,1,1,1.0
exist or,1,1,1.0
not iii,1,1,1.0
iii s,1,1,1.0
s y,2,2,1.0
y n,2,2,1.0
t h,3,1,3.0
h e,5,2,2.5
e t,2,2,1.0
c ov,1,1,1.0
ov e,3,1,3.0
a m,4,1,4.0
m p,3,1,3.0
p l,3,1,3.0
l i,3,1,3.0
b y,2,1,2.0
y c,1,1,1.0
c o,3,1,3.0
n v,1,1,1.0
v e,2,1,2.0
e x,2,1,2.0
x c,1,1,1.0
o m,1,1,1.0
m b,1,1,1.0
b i,2,2,1.0
i nat,1,1,1.0
nat i,1,1,1.0
n i,3,1,3.0
e efs,1,1,1.0
main hypothesis,1,1,1.0
hypothesis in,1,1,1.0
efs provide,1,1,1.0
provide us,2,1,2.0
us with,2,1,2.0
suitable class,1,1,1.0
for it,1,1,1.0
when classes,1,1,1.0
are nonlinearly,1,1,1.0
nonlinearly separable,3,1,3.0
separable which,1,1,1.0
which may,2,1,2.0
may occur,1,1,1.0
one should,1,1,1.0
be very,3,1,3.0
very careful,1,1,1.0
careful when,1,1,1.0
when creating,1,1,1.0
patterns by,2,1,2.0
because these,1,1,1.0
these could,1,1,1.0
region however,1,1,1.0
however if,1,1,1.0
are linearly,1,1,1.0
separable a,1,1,1.0
a statement,1,1,1.0
statement that,1,1,1.0
that will,1,1,1.0
be true,1,1,1.0
true if,1,1,1.0
underlying learning,1,1,1.0
problem by,1,1,1.0
patterns is,1,1,1.0
problem t,2,1,2.0
o illustrate,1,1,1.0
illustrate this,2,2,1.0
this consider,1,1,1.0
consider fig,1,1,1.0
fig where,1,1,1.0
a toy,1,1,1.0
toy nonlinearly,1,1,1.0
separable dataset,1,1,1.0
dataset have,1,1,1.0
been represented,1,1,1.0
the φ,2,1,2.0
e transformation,1,1,1.0
transformation using,1,1,1.0
a gaussian,1,1,1.0
gaussian kernel,19,1,19.0
kernel retaining,1,1,1.0
retaining only,1,1,1.0
only two,2,2,1.0
two dominant,1,1,1.0
dominant dimensions,8,1,8.0
dimensions imbalanced,1,1,1.0
imbalanced donut,1,1,1.0
donut toy,1,1,1.0
toy dataset,1,1,1.0
dataset separable,1,1,1.0
separable projection,1,1,1.0
projection of,1,1,1.0
imbalanced toy,1,1,1.0
toy data,1,1,1.0
map linearly,1,1,1.0
separable fig,1,1,1.0
fig synthethic,2,1,2.0
synthethic dataset,1,1,1.0
dataset representing,1,1,1.0
representing a,1,1,1.0
a linearly,1,1,1.0
separable classiﬁcation,2,1,2.0
their tion,1,1,1.0
tion to,2,2,1.0
dimensions of,3,1,3.0
e induced,1,1,1.0
the gaussian,4,1,4.0
function linearly,2,1,2.0
separable problem,2,1,2.0
problem reduced,1,1,1.0
reduced empirical,3,1,3.0
this subsection,5,1,5.0
we present,3,2,1.5
a reduced,1,1,1.0
reduced version,1,1,1.0
efs where,1,1,1.0
where we,1,1,1.0
we select,2,1,2.0
the q,3,1,3.0
q q,3,1,3.0
q r,5,1,5.0
r dominant,1,1,1.0
dimensions to,1,1,1.0
matrix in,3,2,1.5
to classiﬁcation,1,1,1.0
classiﬁcation it,1,1,1.0
been argued,1,1,1.0
most decisive,1,1,1.0
decisive information,1,1,1.0
information can,1,1,1.0
be contained,1,1,1.0
a subspace,1,1,1.0
subspace of,1,1,1.0
space under,1,1,1.0
assumption of,1,1,1.0
of smooth,1,1,1.0
smooth kernels,1,1,1.0
kernels matching,1,1,1.0
matching the,1,1,1.0
problem however,1,1,1.0
capacity control,1,1,1.0
control inclusion,1,1,1.0
inclusion of,1,1,1.0
of slacks,1,1,1.0
slacks variables,1,1,1.0
variables and,1,1,1.0
and dimensions,1,1,1.0
dimensions associated,1,1,1.0
highest eigenvalues,1,1,1.0
the gram,1,1,1.0
matrix parameter,1,1,1.0
for preventing,1,1,1.0
preventing is,1,1,1.0
is alent,1,1,1.0
alent to,1,1,1.0
to some,2,2,1.0
some form,1,1,1.0
of regularisation,1,1,1.0
regularisation so,1,1,1.0
that denoising,1,1,1.0
denoising is,1,1,1.0
not necessary,1,1,1.0
necessary although,1,1,1.0
although it,1,1,1.0
very useful,1,1,1.0
for unregularised,1,1,1.0
unregularised methods,1,1,1.0
we test,1,1,1.0
whether a,2,1,2.0
the reduced,8,1,8.0
reduced dimensionality,2,1,2.0
dimensionality efs,2,1,2.0
efs as,2,1,2.0
to in,3,2,1.5
full efs,1,1,1.0
efs can,1,1,1.0
be beneﬁcial,2,1,2.0
beneﬁcial one,1,1,1.0
one motivation,1,1,1.0
in reduced,1,1,1.0
efs is,4,1,4.0
procedure relies,1,1,1.0
on distances,2,1,2.0
efs to,1,1,1.0
the neighbourhood,1,1,1.0
neighbourhood analysis,1,1,1.0
analysis roughly,1,1,1.0
speaking these,1,1,1.0
these distances,1,1,1.0
distances have,1,1,1.0
been proven,2,1,2.0
proven to,2,1,2.0
be misleading,1,1,1.0
misleading as,1,1,1.0
data dimensionality,1,1,1.0
dimensionality increases,2,1,2.0
increases making,1,1,1.0
making more,1,1,1.0
more probable,1,1,1.0
probable that,1,1,1.0
neighbours are,1,1,1.0
are chosen,2,2,1.0
chosen in,1,1,1.0
random fashion,1,1,1.0
fashion it,1,1,1.0
any real,1,1,1.0
real symmetric,1,1,1.0
symmetric m,1,1,1.0
m m,3,1,3.0
m matrix,1,1,1.0
r we,1,1,1.0
can ﬁnd,1,1,1.0
ﬁnd its,3,2,1.5
its real,1,1,1.0
real nonzero,1,1,1.0
eigenvalues λr,1,1,1.0
λr and,1,1,1.0
corresponding orthonormal,1,1,1.0
orthonormal eigenvectors,1,1,1.0
eigenvectors ur,1,1,1.0
ur so,1,1,1.0
that k,1,1,1.0
k r,1,1,1.0
r λiuiut,1,1,1.0
λiuiut i,2,1,2.0
best q,1,1,1.0
r approximation,1,1,1.0
approximation to,1,1,1.0
to k,1,1,1.0
is kq,1,1,1.0
kq q,1,1,1.0
q λiuiut,1,1,1.0
the sense,2,1,2.0
sense that,2,1,2.0
it minimises,1,1,1.0
minimises f,1,1,1.0
f over,1,1,1.0
over all,1,1,1.0
all q,1,1,1.0
q matrices,1,1,1.0
where f,1,1,1.0
f denotes,1,1,1.0
the frobenius,1,1,1.0
frobenius norm,1,1,1.0
norm this,1,1,1.0
this concept,1,1,1.0
concept can,1,1,1.0
idea for,1,1,1.0
reduced efs,3,1,3.0
of working,1,1,1.0
working in,1,1,1.0
e we,1,1,1.0
can operate,1,1,1.0
its lower,1,1,1.0
lower dimensional,1,1,1.0
dimensional subspace,1,1,1.0
subspace e,1,1,1.0
e q,33,1,33.0
q where,1,1,1.0
matrix has,1,1,1.0
form k,1,1,1.0
k q,2,1,2.0
p q,3,1,3.0
q λ,1,1,1.0
λ q,2,1,2.0
q t,1,1,1.0
t q,1,1,1.0
r where,2,1,2.0
where p,1,1,1.0
and λ,2,1,2.0
q consist,1,1,1.0
ﬁrst q,1,1,1.0
q columns,1,1,1.0
columns of,1,1,1.0
λ respectively,1,1,1.0
respectively consider,1,1,1.0
preimage f,1,1,1.0
f q,6,1,6.0
q of,1,1,1.0
q under,1,1,1.0
the isomorphism,2,1,2.0
isomorphism ϕ,2,1,2.0
ϕ let,1,1,1.0
let uj,1,1,1.0
uj q,3,1,3.0
q be,1,1,1.0
q given,1,1,1.0
given v,1,1,1.0
f its,1,1,1.0
its projection,1,1,1.0
projection onto,1,1,1.0
onto f,1,1,1.0
is obtained,3,2,1.5
obtained as,1,1,1.0
as uj,1,1,1.0
q the,3,1,3.0
ϕ from,1,1,1.0
from f,1,1,1.0
f to,1,1,1.0
e carries,1,1,1.0
carries the,1,1,1.0
the structure,3,1,3.0
structure over,1,1,1.0
over ϕ,1,1,1.0
is projected,1,1,1.0
projected onto,1,1,1.0
onto e,1,1,1.0
q as,1,1,1.0
as v,1,1,1.0
ϕ uj,2,1,2.0
q moreover,1,1,1.0
moreover for,1,1,1.0
all j,1,1,1.0
j q,1,1,1.0
q uj,1,1,1.0
uj v,1,1,1.0
uj therefore,1,1,1.0
therefore we,4,2,2.0
we could,5,1,5.0
could deﬁne,1,1,1.0
deﬁne the,2,1,2.0
kernel associated,1,1,1.0
efs by,1,1,1.0
by k,1,1,1.0
q xi,11,1,11.0
xj φ,2,1,2.0
q xj,2,1,2.0
xj e,1,1,1.0
e which,1,1,1.0
for q,1,1,1.0
q being,1,1,1.0
the rank,2,1,2.0
rank of,3,1,3.0
k will,1,1,1.0
will correspond,2,1,2.0
to synthetic,1,1,1.0
reduced or,2,1,2.0
or rank,1,1,1.0
rank efs,1,1,1.0
efs once,1,1,1.0
once that,1,1,1.0
of efs,1,1,1.0
efs has,1,1,1.0
been introduced,1,1,1.0
introduced this,1,1,1.0
subsection will,2,1,2.0
will show,1,1,1.0
steps to,4,2,2.0
to extend,2,2,1.0
extend a,1,1,1.0
a algorithm,1,1,1.0
this space,2,1,2.0
space concerning,1,1,1.0
training phase,1,1,1.0
phase the,2,1,2.0
ﬁrst step,3,2,1.5
step of,1,1,1.0
proposed methodology,1,1,1.0
methodology corresponds,1,1,1.0
the computation,3,1,3.0
computation of,2,1,2.0
training kernel,3,1,3.0
k through,1,1,1.0
a predeﬁned,1,1,1.0
predeﬁned kernel,1,1,1.0
function then,1,1,1.0
or empirical,1,1,1.0
be computed,2,1,2.0
computed via,1,1,1.0
the eigenvector,1,1,1.0
eigenvector decomposition,1,1,1.0
decomposition of,1,1,1.0
this training,1,1,1.0
k eq,1,1,1.0
eq as,1,1,1.0
said let,1,1,1.0
let z,1,1,1.0
z be,1,1,1.0
set generated,1,1,1.0
q transformation,1,1,1.0
transformation to,2,1,2.0
training patterns,7,1,7.0
of z,1,1,1.0
z as,1,1,1.0
rows in,1,1,1.0
second step,1,1,1.0
step the,1,1,1.0
is w,1,1,1.0
e assume,1,1,1.0
the singular,1,1,1.0
singular values,1,1,1.0
are performed,1,1,1.0
performed over,1,1,1.0
class images,1,1,1.0
this z,1,1,1.0
z matrix,2,1,2.0
matrix resulting,1,1,1.0
resulting in,1,1,1.0
n new,1,1,1.0
synthetic images,1,1,1.0
images arranged,1,1,1.0
arranged in,1,1,1.0
s and,8,1,8.0
matrix more,1,1,1.0
more speciﬁcally,2,1,2.0
speciﬁcally as,1,1,1.0
standard smote,2,1,2.0
algorithm has,1,1,1.0
been chosen,1,1,1.0
chosen for,4,2,2.0
each new,1,1,1.0
instance will,1,1,1.0
a linear,3,1,3.0
between pattern,1,1,1.0
pattern xi,2,1,2.0
xi and,9,2,4.5
class at,1,1,1.0
at every,1,1,1.0
every step,1,1,1.0
step j,1,1,1.0
j n,1,1,1.0
n we,1,1,1.0
point sj,1,1,1.0
sj in,1,1,1.0
q by,1,1,1.0
by picking,1,1,1.0
picking at,1,1,1.0
at random,1,1,1.0
class point,1,1,1.0
point xi,1,1,1.0
and calculating,1,1,1.0
calculating sj,1,1,1.0
sj φ,1,1,1.0
q ˆ,5,1,5.0
ˆ xi,8,1,8.0
xi δ,2,1,2.0
δ where,2,1,2.0
where φ,1,1,1.0
xi is,5,2,2.5
neighbours for,1,1,1.0
for φ,1,1,1.0
xi in,2,1,2.0
and δ,1,1,1.0
δ is,2,1,2.0
number generated,2,1,2.0
the uniform,4,1,4.0
uniform distribution,4,2,2.0
distribution u,2,1,2.0
u for,1,1,1.0
for simplicity,2,1,2.0
simplicity we,2,1,2.0
classes become,1,1,1.0
become balanced,1,1,1.0
balanced from,1,1,1.0
the deﬁnition,1,1,1.0
deﬁnition of,1,1,1.0
efs we,1,1,1.0
we know,1,1,1.0
know that,1,1,1.0
that ϕ,1,1,1.0
ϕ sj,2,1,2.0
sj f,1,1,1.0
the representation,3,1,3.0
new pattern,1,1,1.0
pattern in,1,1,1.0
space will,1,1,1.0
be unique,1,1,1.0
unique and,1,1,1.0
and will,1,1,1.0
line between,1,1,1.0
between ϕ,1,1,1.0
ϕ xi,1,1,1.0
and ϕ,1,1,1.0
ϕ ˆ,1,1,1.0
xi ϕ,2,1,2.0
linear map,1,1,1.0
map recall,1,1,1.0
the norms,1,1,1.0
norms and,1,1,1.0
and distances,1,1,1.0
distances are,1,1,1.0
preserved e,1,1,1.0
xi ˆ,1,1,1.0
so are,1,1,1.0
angles φ,1,1,1.0
xi t,1,1,1.0
t φ,1,1,1.0
xi sj,1,1,1.0
sj xi,1,1,1.0
φ ˆ,1,1,1.0
sj as,1,1,1.0
a consequence,1,1,1.0
consequence if,1,1,1.0
if φ,3,1,3.0
neighbours of,1,1,1.0
efs this,1,1,1.0
space as,2,1,2.0
well the,1,1,1.0
third step,1,1,1.0
step is,3,2,1.5
the execution,2,1,2.0
execution of,1,1,1.0
learning machine,3,1,3.0
machine over,1,1,1.0
set ϕ,1,1,1.0
ϕ z,3,1,3.0
z s,2,1,2.0
s f,1,1,1.0
q in,1,1,1.0
case there,1,1,1.0
are two,1,1,1.0
different possibilities,1,1,1.0
possibilities to,1,1,1.0
consider first,1,1,1.0
first we,1,1,1.0
could employ,1,1,1.0
employ the,2,2,1.0
new representation,2,1,2.0
representation for,2,1,2.0
classiﬁcation algorithm,1,1,1.0
new space,1,1,1.0
as done,5,1,5.0
other works,3,1,3.0
works this,1,1,1.0
idea will,1,1,1.0
will provide,2,1,2.0
easily separable,1,1,1.0
and balanced,1,1,1.0
balanced space,1,1,1.0
space than,1,1,1.0
space which,1,1,1.0
which could,3,1,3.0
could indeed,1,1,1.0
indeed be,1,1,1.0
machine independently,1,1,1.0
independently of,2,1,2.0
being kernelized,1,1,1.0
kernelized or,1,1,1.0
not however,1,1,1.0
function it,2,1,2.0
could actually,1,1,1.0
actually be,1,1,1.0
more advisable,3,1,3.0
advisable to,2,1,2.0
to recompute,1,1,1.0
recompute the,1,1,1.0
products between,1,1,1.0
between patterns,3,1,3.0
patterns create,1,1,1.0
new sampled,1,1,1.0
sampled kernel,1,1,1.0
matrix due,1,1,1.0
the high,1,1,1.0
high number,1,1,1.0
cases will,1,1,1.0
the computational,11,2,5.5
computational cost,2,1,2.0
machine considered,1,1,1.0
considered t,1,1,1.0
o do,5,1,5.0
so synthetic,1,1,1.0
samples will,1,1,1.0
to complete,2,1,2.0
complete the,4,1,4.0
matrix by,2,1,2.0
by obtaining,1,1,1.0
obtaining their,1,1,1.0
their dot,1,1,1.0
efs with,1,1,1.0
the rest,5,2,2.5
rest of,3,2,1.5
patterns using,1,1,1.0
the sampled,3,2,1.5
sampled training,1,1,1.0
training gram,1,1,1.0
be composed,2,1,2.0
composed as,2,1,2.0
follows z,1,1,1.0
z zt,1,1,1.0
zt z,1,1,1.0
z st,1,1,1.0
st s,1,1,1.0
s zt,1,1,1.0
zt s,1,1,1.0
s st,1,1,1.0
st note,1,1,1.0
any number,1,1,1.0
of dominant,4,1,4.0
dimensions q,1,1,1.0
q for,1,1,1.0
matrix obtained,2,1,2.0
obtained will,1,1,1.0
be positive,1,1,1.0
positive semideﬁnite,2,1,2.0
semideﬁnite furthermore,1,1,1.0
furthermore since,1,1,1.0
since we,3,2,1.5
are generating,1,1,1.0
linear combination,1,1,1.0
other patterns,1,1,1.0
kernel maps,1,1,1.0
maps associated,1,1,1.0
to ϕ,2,1,2.0
z and,1,1,1.0
s can,1,1,1.0
be equivalent,1,1,1.0
equivalent for,1,1,1.0
the generalisation,2,1,2.0
generalisation phase,1,1,1.0
same steps,1,1,1.0
steps are,1,1,1.0
test kernel,2,1,2.0
matrix considering,1,1,1.0
considering that,1,1,1.0
test patterns,4,1,4.0
patterns are,5,1,5.0
are derived,1,1,1.0
derived using,1,1,1.0
same φ,1,1,1.0
q map,1,1,1.0
map considering,1,1,1.0
considering only,1,1,1.0
data note,2,1,2.0
case we,2,2,1.0
will compute,1,1,1.0
product between,3,1,3.0
between train,1,1,1.0
train and,1,1,1.0
and test,2,1,2.0
between test,1,1,1.0
test and,2,1,2.0
sampled test,1,1,1.0
test gram,1,1,1.0
follows t,1,1,1.0
t z,1,1,1.0
z tt,1,1,1.0
tt s,1,1,1.0
s tt,1,1,1.0
tt where,1,1,1.0
representation in,1,1,1.0
efs of,1,1,1.0
t corresponds,1,1,1.0
these new,1,1,1.0
new kernel,2,1,2.0
kernel matrices,6,1,6.0
matrices and,2,1,2.0
any algorithm,1,1,1.0
algorithm a,1,1,1.0
a summary,1,1,1.0
summary of,2,2,1.0
seen in,4,1,4.0
fig algorithm,1,1,1.0
algorithm synthetic,1,1,1.0
space input,1,1,1.0
input training,2,2,1.0
patterns xtr,1,1,1.0
xtr training,1,1,1.0
training targets,1,1,1.0
targets ytr,1,1,1.0
ytr and,1,1,1.0
testing patterns,2,1,2.0
patterns xts,1,1,1.0
xts output,1,1,1.0
output t,1,1,1.0
t esting,1,1,1.0
esting targets,1,1,1.0
targets yts,1,1,1.0
yts compute,1,1,1.0
compute kernel,1,1,1.0
matrix ktr,1,1,1.0
ktr for,1,1,1.0
patterns compute,1,1,1.0
q via,1,1,1.0
via ktr,1,1,1.0
ktr map,1,1,1.0
map training,1,1,1.0
efs using,3,1,3.0
using φ,2,1,2.0
and obtain,3,1,3.0
obtain their,2,1,2.0
their new,2,1,2.0
new tation,2,1,2.0
tation z,1,1,1.0
z generate,1,1,1.0
patterns s,1,1,1.0
s using,1,1,1.0
representation z,1,1,1.0
z of,1,1,1.0
patterns complete,1,1,1.0
the train,1,1,1.0
train kernel,1,1,1.0
matrix with,5,1,5.0
patterns eq,2,1,2.0
eq train,1,1,1.0
a plane,1,1,1.0
plane w,1,1,1.0
bias term,1,1,1.0
term b,1,1,1.0
b map,1,1,1.0
map testing,1,1,1.0
tation complete,1,1,1.0
eq predict,1,1,1.0
predict yts,1,1,1.0
yts using,1,1,1.0
model w,1,1,1.0
w b,1,1,1.0
b eq,1,1,1.0
eq fig,1,1,1.0
fig different,1,1,1.0
different steps,1,1,1.0
steps for,1,1,1.0
kernel algorithm,1,1,1.0
mentioned before,1,1,1.0
before our,2,1,2.0
our points,1,1,1.0
space may,1,1,1.0
have preimages,1,1,1.0
preimages in,1,1,1.0
this does,1,1,1.0
not pose,1,1,1.0
a methodological,1,1,1.0
methodological problem,1,1,1.0
problem since,1,1,1.0
separation is,1,1,1.0
is formulated,1,1,1.0
formulated in,2,2,1.0
space iv,1,1,1.0
iv o,1,1,1.0
o p,2,2,1.0
p t,2,2,1.0
m i,1,1,1.0
i s,3,2,1.5
s i,2,1,2.0
g t,1,1,1.0
e f,3,2,1.5
e at,1,1,1.0
at u,1,1,1.0
u r,1,1,1.0
r e,7,2,3.5
e s,3,1,3.0
s pac,1,1,1.0
pac e,1,1,1.0
e b,1,1,1.0
y k,1,1,1.0
k e,2,2,1.0
r n,2,1,2.0
n e,1,1,1.0
e l,8,2,4.0
l l,1,1,1.0
g f,1,1,1.0
o r,2,1,2.0
r ov,1,1,1.0
g as,2,1,2.0
stated before,4,1,4.0
our ﬁrst,1,1,1.0
ﬁrst hypothesis,1,1,1.0
hypothesis was,1,1,1.0
efs was,2,1,2.0
was more,2,1,2.0
advisable if,1,1,1.0
function matched,1,1,1.0
matched the,1,1,1.0
can asymptotically,1,1,1.0
asymptotically represent,1,1,1.0
the function,1,1,1.0
learned and,1,1,1.0
is sufﬁciently,1,1,1.0
sufﬁciently smooth,1,1,1.0
smooth in,1,1,1.0
propose a,2,1,2.0
learning that,1,1,1.0
that would,1,1,1.0
ideally provide,1,1,1.0
clearer class,1,1,1.0
separation in,1,1,1.0
analyse its,1,1,1.0
its effect,1,1,1.0
effect in,1,1,1.0
method ideally,1,1,1.0
ideally we,1,1,1.0
would like,4,2,2.0
like to,6,2,3.0
to ﬁnd,4,2,2.0
ﬁnd the,5,2,2.5
true risk,1,1,1.0
risk of,1,1,1.0
classiﬁer for,3,2,1.5
a speciﬁc,1,1,1.0
speciﬁc dataset,1,1,1.0
dataset unfortunately,1,1,1.0
unfortunately the,1,1,1.0
the risk,1,1,1.0
not accessible,1,1,1.0
accessible therefore,1,1,1.0
therefore different,2,1,2.0
different analytical,1,1,1.0
analytical bounds,1,1,1.0
bounds for,3,1,3.0
generalisation error,1,1,1.0
error have,1,1,1.0
developed in,3,1,3.0
literature with,1,1,1.0
of better,1,1,1.0
better suiting,1,1,1.0
suiting a,1,1,1.0
kernel machine,3,1,3.0
machine literature,1,1,1.0
literature a,1,1,1.0
a considerable,1,1,1.0
considerable interest,1,1,1.0
interest has,1,1,1.0
been devoted,1,1,1.0
devoted to,1,1,1.0
optimal kernel,3,1,3.0
kernel given,1,1,1.0
a ular,1,1,1.0
ular classiﬁcation,1,1,1.0
classiﬁcation task,1,1,1.0
task as,1,1,1.0
to imposing,1,1,1.0
imposing them,1,1,1.0
them one,1,1,1.0
the prominent,1,1,1.0
prominent approaches,1,1,1.0
in kernel,4,2,2.0
is centred,1,1,1.0
centred target,1,1,1.0
target alignment,2,1,2.0
alignment kt,1,1,1.0
kt a,15,1,15.0
a centred,1,1,1.0
centred kt,4,1,4.0
a is,3,1,3.0
distribution independent,1,1,1.0
independent making,1,1,1.0
it particularly,1,1,1.0
particularly suitable,1,1,1.0
classiﬁcation note,1,1,1.0
that kt,2,1,2.0
the fisher,2,1,2.0
fisher criterion,2,1,2.0
criterion which,1,1,1.0
between different,1,1,1.0
and minimises,1,1,1.0
class distance,1,1,1.0
distance this,1,1,1.0
a useful,1,1,1.0
useful property,1,1,1.0
property of,1,1,1.0
perform class,1,1,1.0
minority patterns,2,1,2.0
patterns would,1,1,1.0
be far,1,1,1.0
region and,1,1,1.0
and closely,1,1,1.0
closely clustered,1,1,1.0
together kt,1,1,1.0
a optimises,1,1,1.0
optimises the,1,1,1.0
kernel by,2,1,2.0
by aligning,1,1,1.0
aligning it,1,1,1.0
it to,1,1,1.0
ideal kernel,4,1,4.0
matrix ki,2,1,2.0
ki which,1,1,1.0
will submit,1,1,1.0
submit the,1,1,1.0
structure ki,1,1,1.0
ki xi,1,1,1.0
xj if,1,1,1.0
if yi,1,1,1.0
yi yj,1,1,1.0
yj otherwise,1,1,1.0
otherwise where,1,1,1.0
where yi,1,1,1.0
yi is,1,1,1.0
target of,1,1,1.0
of pattern,1,1,1.0
xi xtr,2,1,2.0
xtr in,1,1,1.0
sense ki,1,1,1.0
ki will,1,1,1.0
provide information,1,1,1.0
patterns should,3,1,3.0
be similar,1,1,1.0
similar when,1,1,1.0
performing a,1,1,1.0
task thus,1,1,1.0
of ﬁnding,2,1,2.0
ﬁnding an,1,1,1.0
an optimal,1,1,1.0
kernel k,1,1,1.0
is changed,1,1,1.0
changed to,1,1,1.0
ﬁnding a,1,1,1.0
good approximation,1,1,1.0
approximation k,1,1,1.0
k for,1,1,1.0
ki given,1,1,1.0
a family,1,1,1.0
family of,1,1,1.0
functions this,1,1,1.0
this formulation,1,1,1.0
formulation allows,1,1,1.0
allows to,1,1,1.0
to separate,1,1,1.0
separate the,1,1,1.0
the optimisation,12,1,12.0
optimisation from,1,1,1.0
from kernel,1,1,1.0
to reduce,2,2,1.0
reduce the,2,2,1.0
the increase,1,1,1.0
increase in,1,1,1.0
the tional,1,1,1.0
tional cost,1,1,1.0
learning more,1,1,1.0
complex kernels,2,1,2.0
kernels given,1,1,1.0
machine will,1,1,1.0
be unaffected,1,1,1.0
unaffected by,1,1,1.0
this higher,1,1,1.0
higher complexity,1,1,1.0
complexity as,1,1,1.0
said before,4,1,4.0
before concerning,1,1,1.0
concerning imbalanced,1,1,1.0
classiﬁcation ous,1,1,1.0
ous studies,1,1,1.0
studies have,2,2,1.0
have noted,1,1,1.0
noted several,1,1,1.0
in kt,1,1,1.0
a for,2,1,2.0
different pattern,1,1,1.0
pattern distributions,1,1,1.0
distributions but,1,1,1.0
but a,1,1,1.0
a recent,1,1,1.0
recent study,1,1,1.0
has shown,1,1,1.0
of centred,2,1,2.0
centred kernel,1,1,1.0
matrices the,1,1,1.0
centred alignment,1,1,1.0
alignment ac,1,1,1.0
ac between,1,1,1.0
between k,1,1,1.0
k and,3,1,3.0
and ki,1,1,1.0
ki is,1,1,1.0
as ac,1,1,1.0
k ki,1,1,1.0
ki kic,1,1,1.0
kic kic,1,1,1.0
kic where,1,1,1.0
where kc,1,1,1.0
kc denotes,1,1,1.0
the centred,1,1,1.0
centred version,1,1,1.0
is computed,1,1,1.0
computed as,1,1,1.0
as kc,1,1,1.0
kc k,1,1,1.0
k m,2,1,2.0
m k,1,1,1.0
m being,1,1,1.0
being m,1,1,1.0
a matrix,1,1,1.0
with all,1,1,1.0
all elements,1,1,1.0
elements equal,1,1,1.0
to m,1,1,1.0
m centred,1,1,1.0
is maximised,1,1,1.0
maximised when,1,1,1.0
kernel reﬂect,1,1,1.0
reﬂect the,2,2,1.0
the criminant,1,1,1.0
criminant properties,1,1,1.0
data used,1,1,1.0
to deﬁne,2,1,2.0
kernel consider,1,1,1.0
function depending,1,1,1.0
depending on,1,1,1.0
a vector,2,1,2.0
vector of,4,2,2.0
of eters,1,1,1.0
eters α,1,1,1.0
α because,1,1,1.0
the differentiability,1,1,1.0
differentiability of,1,1,1.0
of ac,1,1,1.0
ac with,1,1,1.0
to these,3,2,1.5
these kernel,2,1,2.0
kernel parameters,3,1,3.0
parameters α,3,1,3.0
α a,1,1,1.0
a gradient,1,1,1.0
gradient ascent,1,1,1.0
ascent algorithm,1,1,1.0
algorithm can,1,1,1.0
to maximise,1,1,1.0
maximise the,1,1,1.0
the alignment,3,2,1.5
alignment between,1,1,1.0
matrix constructed,1,1,1.0
constructed kα,1,1,1.0
kα and,1,1,1.0
ideal one,2,1,2.0
one ki,1,1,1.0
ki as,1,1,1.0
follows α,1,1,1.0
α arg,1,1,1.0
arg max,2,2,1.0
max α,1,1,1.0
α ac,1,1,1.0
ac kα,2,1,2.0
kα ki,2,1,2.0
ki the,1,1,1.0
alignment derivative,1,1,1.0
derivative with,1,1,1.0
α is,2,2,1.0
is ac,1,1,1.0
ki α,1,1,1.0
α kα,1,1,1.0
kα α,2,1,2.0
α kic,1,1,1.0
kic c,2,1,2.0
c kic,1,1,1.0
c kα,1,1,1.0
α c,1,1,1.0
c f,2,1,2.0
where and,1,1,1.0
for arbitrary,1,1,1.0
arbitrary matrices,1,1,1.0
it holds,1,1,1.0
holds that,1,1,1.0
which simpliﬁes,2,2,1.0
simpliﬁes the,2,2,1.0
computation in,1,1,1.0
will consider,1,1,1.0
a generalised,1,1,1.0
generalised gaussian,5,1,5.0
kernel with,5,1,5.0
with covariance,1,1,1.0
covariance structure,1,1,1.0
structure deﬁned,1,1,1.0
a positive,1,1,1.0
semideﬁnite matrix,1,1,1.0
matrix q,2,1,2.0
q k,1,1,1.0
xj exp,2,1,2.0
exp xi,3,1,3.0
xj tq,1,1,1.0
tq xi,1,1,1.0
xj as,1,1,1.0
as usual,1,1,1.0
usual the,1,1,1.0
q will,1,1,1.0
be parametrised,1,1,1.0
parametrised by,1,1,1.0
by utu,1,1,1.0
utu where,1,1,1.0
where u,1,1,1.0
u is,1,1,1.0
a d,2,2,1.0
d d,1,1,1.0
d matrix,1,1,1.0
matrix d,1,1,1.0
d being,1,1,1.0
space therefore,2,1,2.0
can equivalently,1,1,1.0
equivalently restate,1,1,1.0
restate our,1,1,1.0
our problem,1,1,1.0
as learning,1,1,1.0
best matrix,1,1,1.0
matrix u,1,1,1.0
u k,1,1,1.0
xj tutu,1,1,1.0
tutu xi,1,1,1.0
xj now,1,1,1.0
now we,1,1,1.0
can compute,1,1,1.0
the derivative,1,1,1.0
derivative of,1,1,1.0
entries of,2,1,2.0
the u,2,1,2.0
u matrix,2,1,2.0
matrix xi,1,1,1.0
xj u,1,1,1.0
u xi,1,1,1.0
xj t,1,1,1.0
t xi,1,1,1.0
xj therefore,1,1,1.0
will optimise,1,1,1.0
optimise a,3,1,3.0
α composed,1,1,1.0
to note,3,2,1.5
some attempts,1,1,1.0
attempts have,1,1,1.0
establish learning,1,1,1.0
learning bounds,2,1,2.0
with several,1,1,1.0
several parameters,2,2,1.0
parameters when,1,1,1.0
when considering,7,1,7.0
considering large,1,1,1.0
large margin,1,1,1.0
margin classiﬁers,2,1,2.0
classiﬁers these,1,1,1.0
these studies,1,1,1.0
the interaction,1,1,1.0
interaction between,1,1,1.0
the margin,1,1,1.0
margin and,1,1,1.0
complexity measure,1,1,1.0
measure of,1,1,1.0
kernel class,1,1,1.0
is plicative,1,1,1.0
plicative thus,1,1,1.0
thus discouraging,1,1,1.0
discouraging the,1,1,1.0
of techniques,1,1,1.0
optimisation of,9,1,9.0
kernels however,1,1,1.0
however recent,1,1,1.0
recent developments,1,1,1.0
developments have,1,1,1.0
this interaction,1,1,1.0
interaction is,1,1,1.0
is additive,1,1,1.0
additive up,1,1,1.0
to log,1,1,1.0
log factors,1,1,1.0
factors rather,1,1,1.0
than multiplicative,1,1,1.0
multiplicative yielding,1,1,1.0
yielding then,1,1,1.0
then stronger,1,1,1.0
stronger bounds,1,1,1.0
bounds therefore,1,1,1.0
patterns needed,1,1,1.0
needed to,3,1,3.0
obtain the,4,2,2.0
same estimation,1,1,1.0
estimation error,1,1,1.0
error with,1,1,1.0
same probability,1,1,1.0
probability for,1,1,1.0
kernel compared,1,1,1.0
a spherical,4,1,4.0
spherical one,1,1,1.0
one grows,1,1,1.0
grows slowly,1,1,1.0
slowly and,1,1,1.0
and directly,1,1,1.0
directly depends,1,1,1.0
parameters t,1,1,1.0
o demonstrate,1,1,1.0
demonstrate the,3,2,1.5
the usefulness,1,1,1.0
usefulness of,1,1,1.0
the kernels,1,1,1.0
kernels we,1,1,1.0
fig a,4,2,2.0
of three,1,1,1.0
three dimensional,1,1,1.0
dimensional toy,1,1,1.0
toy datasets,1,1,1.0
their mapping,1,1,1.0
e using,2,2,1.0
a ical,2,2,1.0
ical gaussian,1,1,1.0
with q,1,1,1.0
q id,1,1,1.0
id an,1,1,1.0
an optimised,6,1,6.0
optimised spherical,3,1,3.0
spherical gaussian,8,1,8.0
kernel obtained,1,1,1.0
obtained through,1,1,1.0
through centred,1,1,1.0
a and,1,1,1.0
optimised generalised,3,1,3.0
kernel summarising,1,1,1.0
summarising kernel,1,1,1.0
learning will,1,1,1.0
applied before,1,1,1.0
before the,2,1,2.0
learn a,1,1,1.0
a suitable,2,1,2.0
suitable kernel,1,1,1.0
kernel kα,2,1,2.0
kα for,1,1,1.0
data representation,2,2,1.0
representation after,1,1,1.0
after this,1,1,1.0
this the,3,2,1.5
efs φ,1,1,1.0
q associated,1,1,1.0
kα will,1,1,1.0
patterns for,3,1,3.0
class contained,1,1,1.0
the z,2,1,2.0
be for,1,1,1.0
for comparison,1,1,1.0
comparison purposes,1,1,1.0
purposes we,1,1,1.0
will also,1,1,1.0
also test,1,1,1.0
the optimization,1,1,1.0
optimization of,3,2,1.5
one kernel,1,1,1.0
kernel parameter,1,1,1.0
parameter via,1,1,1.0
via alignment,1,1,1.0
alignment v,1,1,1.0
v u,1,1,1.0
i fi,1,1,1.0
fi e,1,1,1.0
e d,1,1,1.0
d f,1,1,1.0
f r,1,1,1.0
m e,2,1,2.0
e wo,1,1,1.0
wo r,1,1,1.0
r k,1,1,1.0
k f,1,1,1.0
r p,1,1,1.0
p r,1,1,1.0
e n,3,1,3.0
l ov,1,1,1.0
before several,1,1,1.0
literature for,2,1,2.0
these contributions,1,1,1.0
contributions are,1,1,1.0
on analysing,1,1,1.0
analysing the,3,1,3.0
patterns which,2,1,2.0
for giving,1,1,1.0
giving rise,1,1,1.0
to approaches,1,1,1.0
approaches based,1,1,1.0
on on,1,1,1.0
boundary or,3,1,3.0
or in,1,1,1.0
safe region,1,1,1.0
region these,1,1,1.0
are commonly,1,1,1.0
commonly referred,1,1,1.0
as weighted,1,1,1.0
weighted however,1,1,1.0
however to,1,1,1.0
to our,1,1,1.0
best knowledge,1,1,1.0
knowledge there,1,1,1.0
no principled,1,1,1.0
principled method,1,1,1.0
for choosing,1,1,1.0
the region,1,1,1.0
region of,1,1,1.0
new adaptive,1,1,1.0
adaptive weighted,1,1,1.0
weighted technique,2,1,2.0
naturally spans,2,1,2.0
spans unweighted,1,1,1.0
unweighted and,1,1,1.0
and weighted,1,1,1.0
weighted methods,1,1,1.0
methods both,1,1,1.0
both on,1,1,1.0
boundary and,1,1,1.0
class t,1,1,1.0
so our,1,1,1.0
our approach,1,1,1.0
approach will,1,1,1.0
will take,1,1,1.0
the spatial,4,1,4.0
spatial distribution,6,1,6.0
optimal hyperplane,2,1,2.0
hyperplane obtained,1,1,1.0
solution knowledge,1,1,1.0
knowledge extraction,1,1,1.0
extraction spatial,1,1,1.0
w eighted,1,1,1.0
eighted techniques,1,1,1.0
patterns of,1,1,1.0
dataset are,1,1,1.0
are equally,3,1,3.0
equally important,1,1,1.0
and suitable,1,1,1.0
they should,1,1,1.0
should fig,1,1,1.0
synthethic datasets,1,1,1.0
datasets representing,1,1,1.0
representing separable,1,1,1.0
classiﬁcation problems,2,1,2.0
their transformation,1,1,1.0
efs induced,1,1,1.0
problem contribute,1,1,1.0
contribute equally,1,1,1.0
equally to,1,1,1.0
data one,1,1,1.0
ﬁrst steps,1,1,1.0
steps of,1,1,1.0
these methodologies,1,1,1.0
methodologies corresponds,1,1,1.0
the identiﬁcation,1,1,1.0
identiﬁcation of,1,1,1.0
useful patterns,1,1,1.0
literature do,1,1,1.0
so by,1,1,1.0
by analysing,1,1,1.0
analysing local,1,1,1.0
neighbourhood of,1,1,1.0
paper however,1,1,1.0
will derive,1,1,1.0
derive a,2,1,2.0
technique considering,1,1,1.0
patterns with,2,1,2.0
hyperplane in,1,1,1.0
for will,1,1,1.0
be selected,2,2,1.0
selected based,2,1,2.0
on their,2,1,2.0
their position,1,1,1.0
position and,1,1,1.0
and distance,2,2,1.0
hyperplane however,1,1,1.0
however as,1,1,1.0
serious problem,1,1,1.0
for anced,5,3,1.6666666666666667
anced datasets,1,1,1.0
datasets therefore,1,1,1.0
of weighted,3,2,1.5
weighted sampling,1,1,1.0
sampling we,1,1,1.0
approach giving,1,1,1.0
giving more,1,1,1.0
more importance,1,1,1.0
importance to,2,1,2.0
to errors,1,1,1.0
errors committed,1,1,1.0
committed by,1,1,1.0
by patterns,1,1,1.0
patterns belonging,4,1,4.0
svm approach,1,1,1.0
approach consists,1,1,1.0
introducing different,1,1,1.0
penalty factors,1,1,1.0
factors and,1,1,1.0
negative svm,1,1,1.0
svm slack,1,1,1.0
slack variables,1,1,1.0
variables during,1,1,1.0
during training,1,1,1.0
training the,1,1,1.0
the primal,1,1,1.0
primal svm,1,1,1.0
svm problem,1,1,1.0
is transformed,1,1,1.0
transformed into,1,1,1.0
into ξi,1,1,1.0
ξi ξi,1,1,1.0
ξi subject,1,1,1.0
m for,3,1,3.0
will set,1,1,1.0
set where,2,2,1.0
is assumed,1,1,1.0
ratio is,1,1,1.0
usually known,1,1,1.0
ratio as,2,2,1.0
before each,1,1,1.0
each synthetically,1,1,1.0
generated point,1,1,1.0
point sz,1,1,1.0
sz e,1,1,1.0
q z,1,1,1.0
z n,1,1,1.0
n in,3,2,1.5
class represented,1,1,1.0
by training,1,1,1.0
training samples,1,1,1.0
samples xtr,1,1,1.0
xtr is,2,1,2.0
by ﬁrst,1,1,1.0
ﬁrst picking,1,1,1.0
picking a,1,1,1.0
a pair,2,2,1.0
and xj,4,1,4.0
xj from,1,1,1.0
from xtr,1,1,1.0
then constructing,1,1,1.0
constructing their,1,1,1.0
their convex,1,1,1.0
combination in,1,1,1.0
q sz,1,1,1.0
sz φ,1,1,1.0
where δ,1,1,1.0
u optimisation,1,1,1.0
procedure the,1,1,1.0
xj will,1,1,1.0
be randomly,1,1,1.0
randomly selected,6,2,3.0
their relative,1,1,1.0
relative position,1,1,1.0
position in,1,1,1.0
space with,1,1,1.0
the separating,2,1,2.0
hyperplane because,1,1,1.0
the norm,1,1,1.0
norm of,1,1,1.0
of w,1,1,1.0
w is,1,1,1.0
the signed,1,1,1.0
signed distance,1,1,1.0
xi f,1,1,1.0
q from,1,1,1.0
by f,1,1,1.0
f xi,4,1,4.0
xi note,2,1,2.0
right side,1,1,1.0
hyperplane f,1,1,1.0
positive otherwise,1,1,1.0
otherwise it,1,1,1.0
negative w,1,1,1.0
e will,3,2,1.5
will represent,1,1,1.0
selection process,1,1,1.0
process as,1,1,1.0
as draws,1,1,1.0
draws from,1,1,1.0
a multinomial,1,1,1.0
multinomial distribution,1,1,1.0
distribution over,2,1,2.0
over xtr,2,1,2.0
xtr patterns,1,1,1.0
with natural,1,1,1.0
natural parameters,1,1,1.0
parameters µi,1,1,1.0
µi f,1,1,1.0
xi where,2,2,1.0
β r,1,1,1.0
r is,2,1,2.0
a scale,1,1,1.0
scale parameter,2,1,2.0
parameter using,2,1,2.0
the link,1,1,1.0
link function,2,1,2.0
function the,2,1,2.0
the probability,6,2,3.0
probability of,4,2,2.0
of picking,1,1,1.0
picking xi,1,1,1.0
is p,1,1,1.0
p xi,1,1,1.0
xi exp,2,1,2.0
exp x,1,1,1.0
x if,1,1,1.0
xi lies,1,1,1.0
lies on,1,1,1.0
hyperplane then,1,1,1.0
then f,1,1,1.0
when β,6,1,6.0
β points,4,1,4.0
points deep,1,1,1.0
deep within,1,1,1.0
within the,13,2,6.5
be picked,2,1,2.0
picked when,2,1,2.0
points closer,1,1,1.0
or lying,1,1,1.0
lying inside,1,1,1.0
the opposite,4,2,2.0
opposite class,1,1,1.0
are preferred,2,1,2.0
preferred and,2,1,2.0
β all,2,1,2.0
equally likely,3,2,1.5
be chosen,2,1,2.0
chosen as,1,1,1.0
as this,2,1,2.0
xtr this,1,1,1.0
approach naturally,1,1,1.0
spans different,1,1,1.0
to weighted,1,1,1.0
weighted and,1,1,1.0
and unweighted,1,1,1.0
unweighted previously,1,1,1.0
previously introduced,1,1,1.0
introduced in,2,2,1.0
selecting the,1,1,1.0
the pairs,1,1,1.0
pairs xi,1,1,1.0
xj xtr,1,1,1.0
xtr we,1,1,1.0
could use,1,1,1.0
use two,1,1,1.0
different ideas,1,1,1.0
ideas pick,1,1,1.0
pick xi,2,1,2.0
xj independently,2,1,2.0
independently with,1,1,1.0
the bution,1,1,1.0
bution of,1,1,1.0
of eq,2,1,2.0
eq pick,1,1,1.0
xi according,1,1,1.0
eq and,1,1,1.0
and select,1,1,1.0
select xj,1,1,1.0
xj using,1,1,1.0
using neighbours,1,1,1.0
the weighted,1,1,1.0
weighted approaches,1,1,1.0
literature they,1,1,1.0
they make,1,1,1.0
make use,1,1,1.0
method because,1,1,1.0
they obtain,1,1,1.0
distribution information,1,1,1.0
their neighbourhood,1,1,1.0
neighbourhood however,1,1,1.0
approach note,1,1,1.0
is actually,1,1,1.0
actually more,1,1,1.0
select xi,1,1,1.0
independently according,1,1,1.0
distribution obtained,1,1,1.0
obtained because,1,1,1.0
because otherwise,1,1,1.0
otherwise the,1,1,1.0
the preferential,1,1,1.0
preferential learning,1,1,1.0
sampling process,2,1,2.0
process could,2,1,2.0
be smoothed,1,1,1.0
smoothed picking,1,1,1.0
picking points,1,1,1.0
neighbours approach,1,1,1.0
approach may,2,2,1.0
may differ,1,1,1.0
differ to,1,1,1.0
large extent,1,1,1.0
extent to,1,1,1.0
selection made,1,1,1.0
made with,1,1,1.0
probability function,1,1,1.0
function based,1,1,1.0
the arguments,1,1,1.0
arguments in,1,1,1.0
iii of,1,1,1.0
done through,1,1,1.0
through sampling,1,1,1.0
efs note,1,1,1.0
patterns preferred,1,1,1.0
preferred for,2,1,2.0
space could,4,1,4.0
could not,1,1,1.0
the ones,1,1,1.0
ones preferred,1,1,1.0
preferred in,1,1,1.0
is needed,1,1,1.0
methodology as,1,1,1.0
well t,2,1,2.0
o optimise,1,1,1.0
optimise the,4,1,4.0
the β,1,1,1.0
β values,11,1,11.0
values as,1,1,1.0
as different,1,1,1.0
different β,5,1,5.0
values will,1,1,1.0
will induce,1,1,1.0
induce different,1,1,1.0
different synthetic,2,2,1.0
patterns we,1,1,1.0
will test,3,1,3.0
test two,1,1,1.0
two approaches,2,1,2.0
approaches the,4,2,2.0
single value,1,1,1.0
β found,1,1,1.0
found by,1,1,1.0
by over,1,1,1.0
p predeﬁned,1,1,1.0
predeﬁned β,1,1,1.0
second idea,1,1,1.0
use multiple,1,1,1.0
multiple β,1,1,1.0
framework of,1,1,1.0
multiple kernel,5,1,5.0
learning mkl,1,1,1.0
mkl a,1,1,1.0
different kernel,2,1,2.0
matrices for,1,1,1.0
a particular,1,1,1.0
particular value,1,1,1.0
β we,1,1,1.0
obtained on,1,1,1.0
the extended,1,1,1.0
extended data,1,1,1.0
sample ing,1,1,1.0
ing points,1,1,1.0
points obtained,1,1,1.0
obtained using,1,1,1.0
using β,1,1,1.0
β w,1,1,1.0
e ﬁx,1,1,1.0
ﬁx a,1,1,1.0
values β,1,1,1.0
β p,1,1,1.0
and compute,1,1,1.0
matrices p,1,1,1.0
then using,1,1,1.0
using kt,1,1,1.0
a we,2,1,2.0
could derive,1,1,1.0
matrix p,1,1,1.0
p ωk,2,1,2.0
ωk k,1,1,1.0
with ωk,1,1,1.0
ωk and,1,1,1.0
ωk convex,1,1,1.0
matrices k,1,1,1.0
k by,1,1,1.0
by multiple,1,1,1.0
techniques thus,1,1,1.0
thus this,1,1,1.0
this strategy,1,1,1.0
strategy will,1,1,1.0
more ﬂexible,6,1,6.0
ﬂexible than,1,1,1.0
the validation,1,1,1.0
validation one,1,1,1.0
one because,1,1,1.0
because we,1,1,1.0
can optimise,1,1,1.0
matrices instead,1,1,1.0
of restricting,1,1,1.0
restricting the,1,1,1.0
to only,1,1,1.0
only choosing,1,1,1.0
best performing,3,1,3.0
performing one,1,1,1.0
one for,1,1,1.0
optimisation we,1,1,1.0
will need,1,1,1.0
deﬁne an,1,1,1.0
an extended,1,1,1.0
extended ideal,1,1,1.0
by introducing,1,1,1.0
introducing the,1,1,1.0
patterns recall,1,1,1.0
these patterns,1,1,1.0
will belong,1,1,1.0
optimisation problem,1,1,1.0
solve in,1,1,1.0
case will,1,1,1.0
following max,1,1,1.0
max ω,1,1,1.0
ω c,1,1,1.0
f c,1,1,1.0
c where,1,1,1.0
where m,1,1,1.0
m ω,1,1,1.0
ω note,1,1,1.0
that since,1,1,1.0
to align,1,1,1.0
align the,1,1,1.0
real kernel,1,1,1.0
f does,1,1,1.0
not change,1,1,1.0
change and,1,1,1.0
be obviated,1,1,1.0
obviated in,1,1,1.0
optimisation process,1,1,1.0
the quadratic,1,1,1.0
quadratic programming,1,1,1.0
programming qp,1,1,1.0
qp optimization,1,1,1.0
optimization problem,1,1,1.0
problem associated,1,1,1.0
associated can,1,1,1.0
fig shows,5,2,2.5
different efs,1,1,1.0
the formation,1,1,1.0
formation φ,1,1,1.0
e original,1,1,1.0
original efs,1,1,1.0
efs efs,1,1,1.0
efs for,5,1,5.0
for β,1,1,1.0
β and,2,1,2.0
and β,1,1,1.0
and optimised,3,1,3.0
optimised through,4,1,4.0
through mkl,1,1,1.0
mkl in,1,1,1.0
the difference,8,2,4.0
difference between,3,2,1.5
between for,1,1,1.0
values could,1,1,1.0
be difﬁcult,1,1,1.0
to appreciate,1,1,1.0
appreciate however,1,1,1.0
the optimised,1,1,1.0
optimised efs,1,1,1.0
efs one,1,1,1.0
can note,1,1,1.0
separation increases,1,1,1.0
increases and,1,1,1.0
class decreases,1,1,1.0
decreases recall,1,1,1.0
a was,1,1,1.0
was related,1,1,1.0
criterion fig,1,1,1.0
fig empirical,2,1,2.0
spaces for,3,2,1.5
dataset associated,2,1,2.0
values and,3,1,3.0
optimised in,1,1,1.0
same vein,1,1,1.0
vein fig,1,1,1.0
the transformation,1,1,1.0
transformation φ,1,1,1.0
e in,1,1,1.0
difference for,4,1,4.0
procedure when,1,1,1.0
values can,1,1,1.0
easily appreciated,1,1,1.0
appreciated vi,1,1,1.0
vi e,1,1,1.0
x p,1,1,1.0
p e,1,1,1.0
r i,2,2,1.0
n ta,1,1,1.0
ta l,1,1,1.0
l r,1,1,1.0
s u,1,1,1.0
u lt,1,1,1.0
lt s,1,1,1.0
s the,2,2,1.0
proposed methodologies,1,1,1.0
methodologies have,1,1,1.0
been tested,1,1,1.0
tested considering,1,1,1.0
considering support,1,1,1.0
svm and,1,1,1.0
algorithm binary,1,1,1.0
the uci,2,2,1.0
repository with,1,1,1.0
ratios proportion,1,1,1.0
proportion of,2,2,1.0
majority patterns,1,1,1.0
ones have,1,1,1.0
different situations,1,1,1.0
situations the,1,1,1.0
the characteristics,2,2,1.0
in t,1,1,1.0
t able,18,1,18.0
able as,1,1,1.0
works some,1,1,1.0
some multiclass,1,1,1.0
multiclass datasets,1,1,1.0
been considered,1,1,1.0
considered by,1,1,1.0
by grouping,1,1,1.0
grouping some,1,1,1.0
some classes,1,1,1.0
classes represents,1,1,1.0
the ecoli,1,1,1.0
dataset when,2,1,2.0
considering class,1,1,1.0
class versus,1,1,1.0
versus the,1,1,1.0
rest and,1,1,1.0
the yeast,1,1,1.0
when grouping,1,1,1.0
grouping classes,1,1,1.0
and versus,1,1,1.0
versus classes,1,1,1.0
obtain higher,1,1,1.0
higher imbalance,1,1,1.0
ratio ir,1,1,1.0
ir values,1,1,1.0
values a,1,1,1.0
a stratiﬁed,1,1,1.0
stratiﬁed dietterich,1,1,1.0
dietterich technique,1,1,1.0
technique was,1,1,1.0
was performed,1,1,1.0
performed to,1,1,1.0
to divide,1,1,1.0
divide the,2,2,1.0
are taken,1,1,1.0
as mean,1,1,1.0
mean fig,1,1,1.0
optimised standard,1,1,1.0
deviation of,2,1,2.0
selected measures,1,1,1.0
done elsewhere,1,1,1.0
elsewhere each,1,1,1.0
each experiment,1,1,1.0
experiment over,1,1,1.0
over each,1,1,1.0
data partition,2,1,2.0
partition has,1,1,1.0
been repeated,1,1,1.0
repeated times,1,1,1.0
times using,1,1,1.0
different seed,1,1,1.0
seed to,1,1,1.0
obtain more,1,1,1.0
more robust,1,1,1.0
robust at,1,1,1.0
the end,1,1,1.0
end of,1,1,1.0
execution we,1,1,1.0
have results,1,1,1.0
results for,11,2,5.5
kernel was,1,1,1.0
kernel width,1,1,1.0
width and,1,1,1.0
cost parameter,1,1,1.0
of svm,1,1,1.0
svm were,1,1,1.0
were selected,1,1,1.0
selected within,1,1,1.0
values by,1,1,1.0
a nested,2,1,2.0
nested method,1,1,1.0
method applied,1,1,1.0
works the,1,1,1.0
patterns generated,1,1,1.0
generated was,1,1,1.0
that needed,1,1,1.0
distributions after,1,1,1.0
after applying,1,1,1.0
patterns were,1,1,1.0
same k,1,1,1.0
neighbours were,1,1,1.0
were evaluated,1,1,1.0
evaluated to,1,1,1.0
to minimise,1,1,1.0
minimise the,1,1,1.0
the chance,2,2,1.0
chance that,1,1,1.0
that synthetic,3,2,1.5
region when,1,1,1.0
technique the,1,1,1.0
been reported,1,1,1.0
reported in,2,1,2.0
two metrics,1,1,1.0
metrics one,1,1,1.0
them specially,1,1,1.0
specially designed,1,1,1.0
accuracy metric,1,1,1.0
metric acc,1,1,1.0
acc which,2,1,2.0
which sponds,1,1,1.0
sponds to,1,1,1.0
of correctly,4,2,2.0
correctly classiﬁed,4,2,2.0
classiﬁed patterns,2,1,2.0
and measures,1,1,1.0
measures overall,1,1,1.0
performance for,5,2,2.5
datasets this,2,2,1.0
this metric,2,1,2.0
metric may,1,1,1.0
best option,1,1,1.0
option since,1,1,1.0
the ﬁcation,1,1,1.0
ﬁcation of,1,1,1.0
compromised for,1,1,1.0
the sake,2,1,2.0
sake of,2,1,2.0
one it,1,1,1.0
not distinguish,1,1,1.0
the numbers,1,1,1.0
classiﬁed examples,1,1,1.0
could therefore,1,1,1.0
therefore obtain,1,1,1.0
classiﬁer always,1,1,1.0
always outputting,1,1,1.0
outputting the,1,1,1.0
mean of,2,2,1.0
the sensitivities,1,1,1.0
sensitivities gm,1,1,1.0
gm sp,1,1,1.0
sp sn,1,1,1.0
sn where,1,1,1.0
where sp,1,1,1.0
sp is,1,1,1.0
sensitivity for,2,1,2.0
patterns sidering,1,1,1.0
sidering only,1,1,1.0
only this,1,1,1.0
and sn,1,1,1.0
sn is,1,1,1.0
negative one,1,1,1.0
the measure,1,1,1.0
measure for,1,1,1.0
parameter selection,1,1,1.0
selection was,1,1,1.0
was gm,1,1,1.0
gm given,1,1,1.0
given its,1,1,1.0
its robustness,2,1,2.0
and extended,1,1,1.0
extended use,1,1,1.0
use for,1,1,1.0
metric gives,1,1,1.0
gives much,1,1,1.0
much importance,1,1,1.0
to recall,1,1,1.0
generated t,1,1,1.0
able i,1,1,1.0
i datasets,1,1,1.0
experiments n,1,1,1.0
n corresponds,1,1,1.0
the total,2,2,1.0
total number,1,1,1.0
patterns d,1,1,1.0
d to,1,1,1.0
and ir,1,1,1.0
ir to,1,1,1.0
ratio dataset,1,1,1.0
dataset n,2,1,2.0
d ir,2,1,2.0
ir dataset,1,1,1.0
ir wisconsin,1,1,1.0
wisconsin pima,1,1,1.0
pima haberman,1,1,1.0
haberman classes,1,1,1.0
classes being,1,1,1.0
being therefore,1,1,1.0
therefore sensitive,1,1,1.0
to trivial,1,1,1.0
trivial classiﬁers,1,1,1.0
classiﬁers if,1,1,1.0
if sp,1,1,1.0
sp then,1,1,1.0
then gm,1,1,1.0
gm independently,1,1,1.0
of sn,1,1,1.0
sn the,1,1,1.0
the source,1,1,1.0
source codes,1,1,1.0
codes in,1,1,1.0
in matlab,1,1,1.0
matlab for,1,1,1.0
paper are,2,2,1.0
are available,2,2,1.0
available together,1,1,1.0
together with,1,1,1.0
datasets partitions,1,1,1.0
partitions and,1,1,1.0
results on,6,2,3.0
the website,2,1,2.0
website associated,1,1,1.0
ﬁrst iment,1,1,1.0
iment is,1,1,1.0
map provides,1,1,1.0
space when,1,1,1.0
dimensions chosen,1,1,1.0
the concentration,2,1,2.0
concentration of,2,1,2.0
of spectral,1,1,1.0
spectral properties,1,1,1.0
properties the,1,1,1.0
second experimental,1,1,1.0
experimental subsection,2,1,2.0
will complement,1,1,1.0
complement the,1,1,1.0
the proach,1,1,1.0
proach proposing,1,1,1.0
proposing a,1,1,1.0
to optimise,2,1,2.0
ﬂexible kernel,4,1,4.0
function which,2,2,1.0
which would,1,1,1.0
ideally better,1,1,1.0
better ﬁt,1,1,1.0
ﬁt the,1,1,1.0
experiment is,2,2,1.0
function chosen,1,1,1.0
chosen inﬂuences,1,1,1.0
inﬂuences the,1,1,1.0
how optimising,1,1,1.0
optimising this,1,1,1.0
synthetic generated,1,1,1.0
generated data,1,1,1.0
data will,1,1,1.0
better adapted,1,1,1.0
adapted to,1,1,1.0
problem finally,1,1,1.0
third experiment,2,1,2.0
experiment focuses,1,1,1.0
weighted or,2,1,2.0
or preferential,1,1,1.0
preferential to,1,1,1.0
more prone,2,1,2.0
prone to,2,1,2.0
be and,2,2,1.0
test a,1,1,1.0
new multiple,1,1,1.0
able ii,3,1,3.0
ii contains,1,1,1.0
contains information,1,1,1.0
about all,1,1,1.0
methods used,1,1,1.0
this experimentation,1,1,1.0
experimentation and,2,1,2.0
brief summary,1,1,1.0
summary mean,1,1,1.0
mean results,2,1,2.0
obtained along,1,1,1.0
used apart,1,1,1.0
apart from,1,1,1.0
svm without,2,1,2.0
without performs,1,1,1.0
performs poorly,1,1,1.0
poorly for,1,1,1.0
seen that,15,1,15.0
deviation is,1,1,1.0
very high,1,1,1.0
high in,1,1,1.0
in gm,3,1,3.0
gm indicating,1,1,1.0
indicating large,1,1,1.0
large ﬂuctuations,1,1,1.0
ﬂuctuations in,1,1,1.0
results one,1,1,1.0
also see,2,2,1.0
by kt,2,1,2.0
a osk,1,1,1.0
osk does,1,1,1.0
not lead,1,1,1.0
to very,1,1,1.0
good results,2,1,2.0
better option,1,1,1.0
option is,1,1,1.0
use instead,1,1,1.0
instead or,1,1,1.0
ﬂexible http,1,1,1.0
http kernel,1,1,1.0
kernel as,2,1,2.0
one used,2,1,2.0
in ogk,1,1,1.0
ogk further,1,1,1.0
further information,1,1,1.0
results will,1,1,1.0
be extracted,1,1,1.0
extracted using,1,1,1.0
using statistical,1,1,1.0
statistical tests,2,1,2.0
tests the,2,1,2.0
the complete,1,1,1.0
complete results,1,1,1.0
the webpage,1,1,1.0
webpage associated,1,1,1.0
paper including,1,1,1.0
including the,2,2,1.0
individual results,1,1,1.0
of comparison,5,1,5.0
comparison we,1,1,1.0
we included,2,1,2.0
included the,1,1,1.0
class rule,2,1,2.0
rule mcr,1,1,1.0
mcr classiﬁer,1,1,1.0
classiﬁer as,1,1,1.0
baseline result,1,1,1.0
result a,1,1,1.0
a na,1,1,1.0
na rule,1,1,1.0
that classify,1,1,1.0
patterns as,3,1,3.0
as belonging,1,1,1.0
of mcr,1,1,1.0
mcr it,1,1,1.0
that acc,1,1,1.0
acc is,1,1,1.0
suitable metric,1,1,1.0
take into,1,1,1.0
account since,1,1,1.0
since this,1,1,1.0
this trivial,1,1,1.0
trivial methodology,1,1,1.0
methodology achieves,1,1,1.0
best results,1,1,1.0
cases haberman,1,1,1.0
haberman and,3,1,3.0
following subsections,1,1,1.0
subsections we,1,1,1.0
will perform,1,1,1.0
perform three,1,1,1.0
three differentiated,1,1,1.0
differentiated statistical,1,1,1.0
to validate,1,1,1.0
the previously,1,1,1.0
previously stated,1,1,1.0
stated hypotheses,1,1,1.0
hypotheses t,1,1,1.0
ii abbreviation,1,1,1.0
abbreviation for,1,1,1.0
methods considered,3,1,3.0
the experimentation,1,1,1.0
and mean,1,1,1.0
deviation results,1,1,1.0
results meansd,1,1,1.0
meansd for,1,1,1.0
datasets algorithm,1,1,1.0
algorithm acc,1,1,1.0
acc gm,6,1,6.0
gm majority,1,1,1.0
rule classiﬁer,1,1,1.0
classiﬁer mcr,1,1,1.0
mcr svm,1,1,1.0
without sv,1,1,1.0
sv m,19,1,19.0
m svm,1,1,1.0
svm applying,1,1,1.0
applying in,1,1,1.0
space ois,2,1,2.0
ois svm,1,1,1.0
svm with,8,1,8.0
space oef,1,1,1.0
oef s,33,1,33.0
s svm,2,1,2.0
space oref,1,1,1.0
oref s,16,1,16.0
spherical kernel,2,1,2.0
kernel for,2,1,2.0
sampling osk,1,1,1.0
osk svm,1,1,1.0
generalised kernel,2,1,2.0
sampling ogk,1,1,1.0
ogk svm,1,1,1.0
with via,2,1,2.0
via tial,1,1,1.0
tial learning,1,1,1.0
learning ocp,1,1,1.0
ocp l,10,1,10.0
l svm,1,1,1.0
via preferential,1,1,1.0
preferential multiple,1,1,1.0
multiple nel,1,1,1.0
nel learning,1,1,1.0
learning op,1,1,1.0
op mkl,11,1,11.0
mkl the,2,1,2.0
in bold,1,1,1.0
bold face,1,1,1.0
face and,1,1,1.0
second one,2,1,2.0
one in,1,1,1.0
italics first,1,1,1.0
first experiment,1,1,1.0
experiment in,1,1,1.0
will validate,1,1,1.0
space furthermore,1,1,1.0
whether by,1,1,1.0
by optimising,1,1,1.0
more adequate,1,1,1.0
adequate for,1,1,1.0
test four,1,1,1.0
approaches sv,1,1,1.0
m ois,5,1,5.0
ois oef,4,1,4.0
and oref,6,1,6.0
s see,2,1,2.0
see t,4,1,4.0
ii for,1,1,1.0
the meaning,1,1,1.0
meaning of,1,1,1.0
the acronyms,1,1,1.0
acronyms as,1,1,1.0
before we,1,1,1.0
we discarded,1,1,1.0
discarded all,1,1,1.0
all dimensions,1,1,1.0
dimensions that,1,1,1.0
that correspond,1,1,1.0
zero eigenvalues,1,1,1.0
eigenvalues for,1,1,1.0
for oef,3,1,3.0
s furthermore,1,1,1.0
we performed,1,1,1.0
performed a,1,1,1.0
nested validation,1,1,1.0
validation over,1,1,1.0
dimensions when,1,1,1.0
considering in,1,1,1.0
efs oref,1,1,1.0
we considered,1,1,1.0
following values,1,1,1.0
q value,1,1,1.0
where r,1,1,1.0
original rank,1,1,1.0
the ﬂoor,1,1,1.0
ﬂoor function,1,1,1.0
gm for,3,1,3.0
for sv,2,1,2.0
m are,2,2,1.0
general very,1,1,1.0
very poor,1,1,1.0
poor analyse,1,1,1.0
analyse for,1,1,1.0
the haberman,1,1,1.0
datasets concerning,1,1,1.0
the ois,2,1,2.0
ois method,2,1,2.0
of oef,1,1,1.0
s are,2,2,1.0
are much,1,1,1.0
much better,4,1,4.0
better analyse,1,1,1.0
analyse the,6,1,6.0
the result,5,2,2.5
result of,6,2,3.0
dataset where,1,1,1.0
where sv,1,1,1.0
m even,1,1,1.0
even obtained,1,1,1.0
obtained better,4,1,4.0
better results,10,1,10.0
results or,1,1,1.0
of controlling,1,1,1.0
dimensionality it,1,1,1.0
that oref,1,1,1.0
s generally,1,1,1.0
generally yielded,1,1,1.0
yielded similar,1,1,1.0
similar or,1,1,1.0
or better,1,1,1.0
than oef,3,1,3.0
datasets two,1,1,1.0
examples which,1,1,1.0
be afterwards,1,1,1.0
afterwards analysed,1,1,1.0
analysed when,1,1,1.0
when taking,1,1,1.0
taking acc,1,1,1.0
acc into,1,1,1.0
account it,1,1,1.0
the three,3,1,3.0
three methods,2,1,2.0
methods obtain,2,1,2.0
obtain very,3,1,3.0
similar values,1,1,1.0
values although,1,1,1.0
although oef,2,1,2.0
s obtain,1,1,1.0
obtain better,1,1,1.0
cases t,1,1,1.0
able iii,3,1,3.0
iii shows,2,2,1.0
test mean,1,1,1.0
mean rankings,1,1,1.0
rankings for,1,1,1.0
method and,6,2,3.0
worst for,1,1,1.0
considered in,3,1,3.0
experiment along,1,1,1.0
along all,1,1,1.0
datasets in,6,2,3.0
of acc,1,1,1.0
acc and,5,1,5.0
and gm,3,1,3.0
gm the,4,1,4.0
that sv,1,1,1.0
performing method,1,1,1.0
for acc,8,1,8.0
acc but,1,1,1.0
but the,3,2,1.5
worst performing,1,1,1.0
performing when,1,1,1.0
considering a,3,2,1.5
metric that,1,1,1.0
takes into,1,1,1.0
account the,1,1,1.0
data gm,1,1,1.0
gm furthermore,1,1,1.0
furthermore it,1,1,1.0
that both,3,1,3.0
both approaches,1,1,1.0
approaches for,1,1,1.0
efs oef,1,1,1.0
s outperfomed,1,1,1.0
outperfomed the,1,1,1.0
obtained when,1,1,1.0
when in,1,1,1.0
ois finally,1,1,1.0
finally it,2,1,2.0
that controlling,1,1,1.0
efs dimensionality,2,1,2.0
dimensionality we,1,1,1.0
we improve,1,1,1.0
cases as,1,1,1.0
the oref,3,1,3.0
s method,4,1,4.0
method obtained,2,1,2.0
better mean,1,1,1.0
results than,3,1,3.0
o quantify,1,1,1.0
quantify whether,1,1,1.0
a statistical,3,2,1.5
statistical difference,1,1,1.0
difference exists,1,1,1.0
exists among,1,1,1.0
the algorithms,4,2,2.0
algorithms a,1,1,1.0
a procedure,2,1,2.0
procedure is,3,2,1.5
compare multiple,1,1,1.0
multiple classiﬁers,1,1,1.0
classiﬁers in,3,2,1.5
in multiple,1,1,1.0
multiple datasets,1,1,1.0
datasets t,3,2,1.5
iii also,1,1,1.0
also shows,1,1,1.0
of applying,3,2,1.5
the statistical,1,1,1.0
statistical friedman,1,1,1.0
friedman s,7,1,7.0
s test,12,1,12.0
test for,2,1,2.0
a signiﬁcance,3,2,1.5
signiﬁcance level,3,2,1.5
level of,6,2,3.0
of α,2,2,1.0
α to,1,1,1.0
mean acc,1,1,1.0
gm rankings,1,1,1.0
rankings the,1,1,1.0
test rejects,1,1,1.0
rejects the,1,1,1.0
the that,3,1,3.0
all algorithms,3,2,1.5
algorithms perform,3,1,3.0
perform similarly,2,1,2.0
similarly in,1,1,1.0
in mean,1,1,1.0
mean ranking,8,1,8.0
ranking for,1,1,1.0
both metrics,1,1,1.0
metrics note,1,1,1.0
for gm,8,1,8.0
are larger,1,1,1.0
larger t,1,1,1.0
iii mean,1,1,1.0
ranking results,6,1,6.0
s ranking,1,1,1.0
ranking sv,1,1,1.0
s oref,2,1,2.0
s acc,3,1,3.0
gm friedman,3,1,3.0
test conﬁdence,3,1,3.0
conﬁdence interval,3,1,3.0
interval f,3,1,3.0
f α,3,1,3.0
α gm,3,1,3.0
gm on,1,1,1.0
this rejection,1,1,1.0
rejection and,1,1,1.0
and following,2,2,1.0
following the,2,2,1.0
the guidelines,1,1,1.0
guidelines of,1,1,1.0
of we,1,1,1.0
we consider,2,2,1.0
performing methods,1,1,1.0
two proposals,1,1,1.0
proposals oef,1,1,1.0
s as,3,1,3.0
as control,3,1,3.0
control methods,3,1,3.0
we compare,4,1,4.0
compare them,1,1,1.0
rest according,1,1,1.0
their rankings,1,1,1.0
rankings it,1,1,1.0
been noted,1,1,1.0
approach of,4,2,2.0
of paring,1,1,1.0
paring all,1,1,1.0
all classiﬁers,2,1,2.0
classiﬁers to,2,1,2.0
other in,1,1,1.0
test is,8,2,4.0
as sensitive,1,1,1.0
sensitive as,1,1,1.0
of comparing,1,1,1.0
comparing all,1,1,1.0
given classiﬁer,1,1,1.0
classiﬁer control,1,1,1.0
control method,2,1,2.0
method one,1,1,1.0
this latter,1,1,1.0
latter type,1,1,1.0
comparison is,1,1,1.0
the holm,7,1,7.0
holm s,5,1,5.0
test statistics,1,1,1.0
statistics for,2,2,1.0
for comparing,2,2,1.0
comparing the,2,2,1.0
and method,1,1,1.0
this procedure,2,2,1.0
is z,1,1,1.0
z k,1,1,1.0
k where,1,1,1.0
where k,2,2,1.0
of algorithms,1,1,1.0
algorithms n,1,1,1.0
n is,4,2,2.0
and ri,1,1,1.0
ri is,1,1,1.0
ranking of,1,1,1.0
z value,1,1,1.0
value is,1,1,1.0
corresponding probability,1,1,1.0
probability from,1,1,1.0
the table,1,1,1.0
table of,2,2,1.0
of normal,1,1,1.0
normal distribution,1,1,1.0
distribution which,2,2,1.0
is compared,2,2,1.0
an appropriate,1,1,1.0
appropriate level,1,1,1.0
of signiﬁcance,2,2,1.0
signiﬁcance α,1,1,1.0
α holm,2,1,2.0
test adjusts,1,1,1.0
adjusts the,2,2,1.0
the α,1,1,1.0
α value,1,1,1.0
value in,1,1,1.0
for multiple,3,1,3.0
multiple comparisons,3,1,3.0
comparisons this,1,1,1.0
procedure that,1,1,1.0
that sequentially,1,1,1.0
sequentially tests,1,1,1.0
the hypotheses,1,1,1.0
hypotheses ordered,1,1,1.0
ordered by,5,1,5.0
by their,1,1,1.0
their signiﬁcance,1,1,1.0
signiﬁcance w,1,1,1.0
will denote,1,1,1.0
the ordered,1,1,1.0
by p,1,1,1.0
that pk,1,1,1.0
pk holm,1,1,1.0
test compares,1,1,1.0
compares each,1,1,1.0
each pi,1,1,1.0
pi with,1,1,1.0
with α,1,1,1.0
holm from,1,1,1.0
most signiﬁcant,1,1,1.0
signiﬁcant p,1,1,1.0
p value,1,1,1.0
value if,1,1,1.0
if is,1,1,1.0
is below,1,1,1.0
below k,1,1,1.0
k the,1,1,1.0
is rejected,2,1,2.0
rejected and,1,1,1.0
compare with,1,1,1.0
k if,1,1,1.0
second hypothesis,1,1,1.0
rejected the,1,1,1.0
test proceeds,1,1,1.0
proceeds with,1,1,1.0
third and,1,1,1.0
on as,1,1,1.0
as soon,1,1,1.0
soon as,1,1,1.0
certain null,1,1,1.0
null hypothesis,3,2,1.5
hypothesis can,2,2,1.0
be rejected,1,1,1.0
rejected all,1,1,1.0
remaining hypotheses,1,1,1.0
hypotheses are,1,1,1.0
are retained,1,1,1.0
o analyse,2,1,2.0
test see,1,1,1.0
able iv,2,1,2.0
iv for,2,2,1.0
the oef,1,1,1.0
test concluded,3,1,3.0
were statistically,2,1,2.0
statistically signiﬁcant,5,1,5.0
signiﬁcant differences,7,1,7.0
differences with,1,1,1.0
with sv,2,1,2.0
acc note,1,1,1.0
case sv,1,1,1.0
m obtained,2,1,2.0
results sv,1,1,1.0
and ois,3,1,3.0
ois for,1,1,1.0
gm as,1,1,1.0
well this,1,1,1.0
this indicates,1,1,1.0
indicates that,2,2,1.0
that although,3,2,1.5
s obtained,1,1,1.0
obtained worst,1,1,1.0
worst results,1,1,1.0
acc in,1,1,1.0
in comparison,1,1,1.0
comparison with,1,1,1.0
gm are,1,1,1.0
are signiﬁcantly,1,1,1.0
signiﬁcantly better,3,2,1.5
better with,1,1,1.0
with comparison,1,1,1.0
comparison to,1,1,1.0
to sv,2,1,2.0
m and,2,1,2.0
ois therefore,1,1,1.0
therefore giving,1,1,1.0
giving evidence,1,1,1.0
sampling by,1,1,1.0
patterns concerning,1,1,1.0
same results,1,1,1.0
are obtained,1,1,1.0
obtained but,1,1,1.0
but there,1,1,1.0
also signiﬁcant,1,1,1.0
differences when,1,1,1.0
could indicate,1,1,1.0
beneﬁcial with,1,1,1.0
with other,3,2,1.5
other purposes,1,1,1.0
purposes for,1,1,1.0
for ensuring,1,1,1.0
ensuring the,1,1,1.0
boundaries t,1,1,1.0
iv results,1,1,1.0
holm procedure,3,1,3.0
procedure using,3,1,3.0
using oef,1,1,1.0
methods cms,1,1,1.0
cms when,1,1,1.0
when compared,6,2,3.0
ois corrected,1,1,1.0
corrected α,3,1,3.0
α values,3,1,3.0
values compared,3,1,3.0
compared method,3,1,3.0
them ordered,2,1,2.0
comparison i,3,1,3.0
i cm,3,1,3.0
cm oef,1,1,1.0
gm i,5,1,5.0
i α,5,1,5.0
α method,5,1,5.0
method pi,7,1,7.0
pi method,2,1,2.0
pi sv,2,1,2.0
m sv,2,1,2.0
ois ois,2,1,2.0
ois oref,1,1,1.0
s cm,1,1,1.0
cm oref,1,1,1.0
s oef,1,1,1.0
s win,2,1,2.0
win or,4,1,4.0
or lose,4,1,4.0
lose with,4,1,4.0
with statistical,4,1,4.0
statistical signiﬁcant,3,1,3.0
signiﬁcant difference,2,1,2.0
for α,5,1,5.0
α in,1,1,1.0
optimal dimensionality,3,1,3.0
efs it,1,1,1.0
the decay,1,1,1.0
decay rate,1,1,1.0
rate of,3,2,1.5
the eigenvalues,1,1,1.0
eigenvalues is,1,1,1.0
the smoothness,1,1,1.0
smoothness of,1,1,1.0
kernel and,2,1,2.0
of necessary,1,1,1.0
necessary dimensions,1,1,1.0
dimensions depends,1,1,1.0
the interplay,1,1,1.0
interplay between,1,1,1.0
mean value,2,1,2.0
the step,2,1,2.0
dimensions was,1,1,1.0
speciﬁcally fig,1,1,1.0
the histogram,3,1,3.0
histogram of,5,1,5.0
datasets tested,1,1,1.0
tested where,1,1,1.0
where it,1,1,1.0
cases is,1,1,1.0
is enough,1,1,1.0
enough to,1,1,1.0
to contain,1,1,1.0
contain all,1,1,1.0
the relevant,1,1,1.0
relevant information,1,1,1.0
dataset as,2,2,1.0
before one,1,1,1.0
hypothesis for,1,1,1.0
for controlling,1,1,1.0
the mensionality,1,1,1.0
mensionality of,1,1,1.0
that our,1,1,1.0
our algorithm,1,1,1.0
distances computed,1,1,1.0
computed in,1,1,1.0
computing nearest,1,1,1.0
and choosing,1,1,1.0
choosing which,1,1,1.0
distances which,1,1,1.0
may bear,1,1,1.0
bear less,1,1,1.0
less neighbourhood,1,1,1.0
neighbourhood information,1,1,1.0
information as,1,1,1.0
increases fig,1,1,1.0
of distances,2,1,2.0
distances between,4,2,2.0
between pairs,1,1,1.0
different values,1,1,1.0
for two,2,2,1.0
obtained much,1,1,1.0
and where,1,1,1.0
where this,2,1,2.0
this spectral,1,1,1.0
spectral erties,1,1,1.0
erties phenomenon,1,1,1.0
phenomenon can,1,1,1.0
be appreciated,1,1,1.0
appreciated note,1,1,1.0
using all,1,1,1.0
the dimensions,1,1,1.0
dimensions corresponds,1,1,1.0
an almost,1,1,1.0
almost randomly,1,1,1.0
randomly fashion,1,1,1.0
fashion r,1,1,1.0
r fig,1,1,1.0
fig histogram,3,1,3.0
mean optimal,1,1,1.0
the abscissa,2,1,2.0
abscissa axis,2,1,2.0
axis represents,2,1,2.0
value over,1,1,1.0
matrix the,1,1,1.0
the ordinate,2,1,2.0
ordinate axis,2,1,2.0
axis shows,1,1,1.0
this value,3,2,1.5
value was,2,1,2.0
was selected,2,1,2.0
selected from,2,1,2.0
step as,1,1,1.0
neighbours rule,1,1,1.0
rule will,1,1,1.0
very precise,1,1,1.0
precise since,1,1,1.0
between pair,2,1,2.0
similar fig,1,1,1.0
different dimensionality,1,1,1.0
dimensionality values,1,1,1.0
axis the,1,1,1.0
the occurrence,1,1,1.0
occurrence of,1,1,1.0
each distance,1,1,1.0
results several,1,1,1.0
several conclusions,2,1,2.0
conclusions can,2,1,2.0
be drawn,2,1,2.0
drawn firstly,1,1,1.0
firstly by,1,1,1.0
suitable in,1,1,1.0
an ideally,1,1,1.0
separable space,2,1,2.0
method obtains,2,1,2.0
obtains better,2,1,2.0
in metrics,1,1,1.0
metrics that,2,2,1.0
that consider,1,1,1.0
data without,1,1,1.0
without compromising,1,1,1.0
compromising the,1,1,1.0
overall accuracy,2,2,1.0
accuracy however,1,1,1.0
space does,1,1,1.0
not achieve,1,1,1.0
this balance,1,1,1.0
balance indicating,1,1,1.0
a possibly,1,1,1.0
possibly nonlinearly,1,1,1.0
generate patterns,1,1,1.0
in unwanted,1,1,1.0
unwanted areas,1,1,1.0
areas concerning,1,1,1.0
dimensions for,1,1,1.0
space this,1,1,1.0
methodology improves,1,1,1.0
cases thus,1,1,1.0
thus encouraging,1,1,1.0
encouraging further,1,1,1.0
further development,1,1,1.0
an analytical,2,1,2.0
analytical method,1,1,1.0
so second,1,1,1.0
second experiment,1,1,1.0
experiment inﬂuence,1,1,1.0
compare three,1,1,1.0
three different,1,1,1.0
different proposals,1,1,1.0
proposals ﬁrstly,1,1,1.0
ﬁrstly oef,1,1,1.0
s which,1,1,1.0
to if,1,1,1.0
function leads,1,1,1.0
to better,2,1,2.0
results secondly,1,1,1.0
secondly svm,1,1,1.0
kernel the,1,1,1.0
same kernel,1,1,1.0
kernel than,2,1,2.0
s but,1,1,1.0
but optimised,1,1,1.0
through kt,2,1,2.0
performing the,2,1,2.0
the in,1,1,1.0
space osk,1,1,1.0
osk and,5,1,5.0
ﬁnally svm,1,1,1.0
kernel in,2,1,2.0
space ogk,1,1,1.0
ogk in,1,1,1.0
the irprop,1,1,1.0
irprop algorithm,1,1,1.0
the aforementioned,2,2,1.0
aforementioned centred,1,1,1.0
a because,1,1,1.0
robustness the,1,1,1.0
the gradient,1,1,1.0
gradient norm,1,1,1.0
norm stopping,1,1,1.0
stopping criterion,1,1,1.0
criterion was,1,1,1.0
the maximum,1,1,1.0
maximum number,1,1,1.0
of conjugate,1,1,1.0
conjugate gradient,1,1,1.0
gradient steps,1,1,1.0
of ogk,1,1,1.0
ogk we,1,1,1.0
we also,8,2,4.0
also included,1,1,1.0
included a,2,1,2.0
a γ,1,1,1.0
γ parameter,1,1,1.0
parameter as,1,1,1.0
additional parameter,1,1,1.0
the generalised,3,1,3.0
kernel which,2,1,2.0
will indeed,1,1,1.0
indeed make,1,1,1.0
make the,2,2,1.0
the parameters,3,2,1.5
parameters initialisation,1,1,1.0
initialisation easier,1,1,1.0
easier the,1,1,1.0
the initial,1,1,1.0
initial point,1,1,1.0
point for,1,1,1.0
for γ,1,1,1.0
γ for,1,1,1.0
methods tested,1,1,1.0
tested was,1,1,1.0
was chosen,1,1,1.0
chosen from,1,1,1.0
set analysing,1,1,1.0
best result,1,1,1.0
in alignment,1,1,1.0
three values,1,1,1.0
q matrix,1,1,1.0
kernel is,2,1,2.0
is initialised,1,1,1.0
initialised as,1,1,1.0
the pseudoinverse,1,1,1.0
pseudoinverse of,1,1,1.0
the covariance,1,1,1.0
covariance of,1,1,1.0
points q,1,1,1.0
q cov,1,1,1.0
cov xtr,1,1,1.0
xtr to,1,1,1.0
of covariance,1,1,1.0
matrices once,1,1,1.0
kernel has,3,1,3.0
been optimised,1,1,1.0
optimised via,1,1,1.0
via kt,1,1,1.0
we optimise,1,1,1.0
c parameter,1,1,1.0
using validation,1,1,1.0
validation within,1,1,1.0
values this,1,1,1.0
this two,1,1,1.0
two stage,1,1,1.0
stage optimisation,1,1,1.0
optimisation method,1,1,1.0
also referred,1,1,1.0
literature as,1,1,1.0
as method,1,1,1.0
method from,1,1,1.0
results that,1,1,1.0
website one,1,1,1.0
that osk,1,1,1.0
and ogk,2,1,2.0
ogk obtained,1,1,1.0
obtained in,1,1,1.0
cases better,1,1,1.0
in acc,1,1,1.0
acc than,1,1,1.0
than sv,1,1,1.0
m this,1,1,1.0
this could,2,1,2.0
be due,1,1,1.0
optimisation through,1,1,1.0
a which,1,1,1.0
which selected,1,1,1.0
selected a,1,1,1.0
more optimal,1,1,1.0
method analysing,1,1,1.0
analysing gm,1,1,1.0
gm it,1,1,1.0
the spherical,3,1,3.0
not satisfactory,1,1,1.0
satisfactory in,1,1,1.0
in optimising,1,1,1.0
kernel a,1,1,1.0
a methodology,1,1,1.0
methodology should,1,1,1.0
be preferred,2,1,2.0
preferred to,1,1,1.0
to kt,1,1,1.0
kt t,1,1,1.0
o see,1,1,1.0
this analyse,1,1,1.0
where although,1,1,1.0
although osk,1,1,1.0
osk incorporates,1,1,1.0
incorporates a,1,1,1.0
a stage,1,1,1.0
stage sv,1,1,1.0
better gm,2,1,2.0
gm results,3,1,3.0
results finally,1,1,1.0
that ogk,1,1,1.0
ogk yielded,1,1,1.0
yielded a,1,1,1.0
better mance,1,1,1.0
mance in,2,2,1.0
cases analyse,1,1,1.0
dataset demonstrating,1,1,1.0
demonstrating therefore,1,1,1.0
therefore that,1,1,1.0
kernel combined,1,1,1.0
techniques could,3,1,3.0
could optimise,1,1,1.0
a necessary,1,1,1.0
necessary condition,1,1,1.0
condition for,2,2,1.0
done before,1,1,1.0
before t,1,1,1.0
able v,2,1,2.0
v shows,2,2,1.0
subsection and,1,1,1.0
the friedman,2,1,2.0
test accepted,2,1,2.0
accepted the,2,1,2.0
similarly for,1,1,1.0
and rejected,2,1,2.0
rejected it,2,1,2.0
it for,3,2,1.5
gm from,2,1,2.0
obtained it,3,2,1.5
in oef,1,1,1.0
s optimised,1,1,1.0
through and,2,2,1.0
and osk,1,1,1.0
osk optimised,1,1,1.0
optimised by,1,1,1.0
a the,1,1,1.0
are comparable,1,1,1.0
comparable and,1,1,1.0
similar mean,1,1,1.0
results as,1,1,1.0
metric used,1,1,1.0
parameters selection,1,1,1.0
selection stage,1,1,1.0
stage however,1,1,1.0
kernel such,2,1,2.0
one considered,1,1,1.0
the ogk,1,1,1.0
ogk method,1,1,1.0
results can,1,1,1.0
be signiﬁcantly,1,1,1.0
signiﬁcantly improved,1,1,1.0
improved note,1,1,1.0
that applying,1,1,1.0
applying to,1,1,1.0
kernel could,1,1,1.0
could possibly,1,1,1.0
possibly improve,1,1,1.0
improve gm,1,1,1.0
results but,1,1,1.0
computational task,1,1,1.0
task required,1,1,1.0
required would,1,1,1.0
be infeasible,1,1,1.0
infeasible on,1,1,1.0
of friedman,1,1,1.0
test rejection,1,1,1.0
rejection the,1,1,1.0
holm test,1,1,1.0
test t,1,1,1.0
v mean,1,1,1.0
s osk,2,1,2.0
ogk ranking,1,1,1.0
ranking oef,2,1,2.0
osk ogk,1,1,1.0
ogk acc,1,1,1.0
comparisons has,2,1,2.0
applied see,1,1,1.0
able vi,2,1,2.0
vi and,1,1,1.0
differences for,5,1,5.0
gm when,2,1,2.0
considering osk,1,1,1.0
and oef,2,1,2.0
said there,1,1,1.0
were no,1,1,1.0
no statistically,1,1,1.0
acc t,2,1,2.0
vi results,1,1,1.0
using ogk,1,1,1.0
ogk as,1,1,1.0
the control,2,1,2.0
to osk,1,1,1.0
s corrected,1,1,1.0
cm ogk,1,1,1.0
ogk gm,1,1,1.0
pi osk,1,1,1.0
osk oef,1,1,1.0
α the,2,2,1.0
subsection show,1,1,1.0
is affected,1,1,1.0
function although,1,1,1.0
although spherical,1,1,1.0
show promising,1,1,1.0
promising results,1,1,1.0
previous subsection,1,1,1.0
subsection kernel,1,1,1.0
is indeed,1,1,1.0
indeed a,1,1,1.0
a complex,1,1,1.0
complex issue,1,1,1.0
issue shows,1,1,1.0
shows much,1,1,1.0
results when,2,1,2.0
when ploying,1,1,1.0
ploying a,1,1,1.0
used therefore,1,1,1.0
be explored,2,1,2.0
explored in,2,1,2.0
the future,2,1,2.0
future for,1,1,1.0
efs third,1,1,1.0
experiment preferential,1,1,1.0
preferential this,1,1,1.0
this experimental,1,1,1.0
subsection is,1,1,1.0
test if,2,2,1.0
are patterns,1,1,1.0
general adaptive,1,1,1.0
adaptive approach,1,1,1.0
approach yielding,1,1,1.0
yielding solutions,1,1,1.0
solutions based,1,1,1.0
on unweighted,1,1,1.0
unweighted borderline,1,1,1.0
borderline weighted,1,1,1.0
or safe,1,1,1.0
safe level,1,1,1.0
level weighted,1,1,1.0
weighted could,1,1,1.0
than standard,1,1,1.0
standard unweighted,1,1,1.0
unweighted t,1,1,1.0
compare oef,1,1,1.0
s to,1,1,1.0
one based,2,1,2.0
strategy ocp,1,1,1.0
l and,5,1,5.0
on kernel,1,1,1.0
techniques op,2,1,2.0
mkl as,2,1,2.0
before to,1,1,1.0
test this,1,1,1.0
idea we,1,1,1.0
we ﬁrst,2,2,1.0
ﬁrst obtain,1,1,1.0
patterns based,1,1,1.0
a svm,1,1,1.0
a parametrised,1,1,1.0
parametrised link,1,1,1.0
function eq,1,1,1.0
eq to,1,1,1.0
assign different,1,1,1.0
different probabilities,1,1,1.0
probabilities of,1,1,1.0
being to,1,1,1.0
this spatial,1,1,1.0
distribution this,2,2,1.0
this parametrisation,1,1,1.0
parametrisation is,1,1,1.0
is made,1,1,1.0
made using,1,1,1.0
a β,1,1,1.0
β scale,1,1,1.0
parameter which,1,1,1.0
be optimised,1,1,1.0
through ocp,1,1,1.0
l within,1,1,1.0
of values,1,1,1.0
and through,1,1,1.0
through kernel,1,1,1.0
mkl for,1,1,1.0
set β,1,1,1.0
β analysing,1,1,1.0
both ocp,2,1,2.0
and op,4,1,4.0
mkl obtain,1,1,1.0
very competitive,1,1,1.0
competitive results,1,1,1.0
results both,1,1,1.0
both for,1,1,1.0
obtained are,1,1,1.0
equal since,1,1,1.0
since op,1,1,1.0
mkl also,1,1,1.0
also includes,1,1,1.0
includes the,1,1,1.0
the solutions,1,1,1.0
solutions of,1,1,1.0
of ocp,1,1,1.0
ocp once,1,1,1.0
once again,1,1,1.0
again t,1,1,1.0
able vii,2,1,2.0
vii shows,1,1,1.0
when comparing,1,1,1.0
comparing these,1,1,1.0
standard posed,1,1,1.0
posed technique,1,1,1.0
technique oef,1,1,1.0
s in,1,1,1.0
perform larly,1,1,1.0
larly for,1,1,1.0
results it,2,1,2.0
methods outperform,1,1,1.0
outperform the,1,1,1.0
standard proposal,1,1,1.0
proposal or,1,1,1.0
or at,1,1,1.0
least yield,1,1,1.0
yield similar,1,1,1.0
similar performance,1,1,1.0
considering acc,1,1,1.0
vii mean,1,1,1.0
by oef,1,1,1.0
s ocp,3,1,3.0
mkl ranking,1,1,1.0
l op,1,1,1.0
mkl acc,1,1,1.0
been also,1,1,1.0
also plied,1,1,1.0
plied see,1,1,1.0
able viii,2,1,2.0
viii for,1,1,1.0
are statistically,2,1,2.0
to oef,1,1,1.0
s indicating,1,1,1.0
that preferential,1,1,1.0
preferential is,1,1,1.0
is preferable,2,1,2.0
preferable over,1,1,1.0
uniform one,1,1,1.0
the strategy,1,1,1.0
strategy obtains,1,1,1.0
obtains very,1,1,1.0
results the,2,2,1.0
kernel strategy,1,1,1.0
strategy yields,1,1,1.0
yields slightly,1,1,1.0
slightly better,1,1,1.0
performance there,1,1,1.0
α t,2,1,2.0
viii results,1,1,1.0
using ocp,1,1,1.0
methods when,1,1,1.0
methods corrected,1,1,1.0
and ordered,1,1,1.0
cm op,1,1,1.0
mkl gm,1,1,1.0
pi oef,2,1,2.0
l cm,1,1,1.0
cm ocp,1,1,1.0
l gm,1,1,1.0
s op,1,1,1.0
mkl win,1,1,1.0
statistical cant,1,1,1.0
cant difference,1,1,1.0
α win,1,1,1.0
most appropriate,1,1,1.0
appropriate region,1,1,1.0
region for,1,1,1.0
for we,1,1,1.0
we analyse,1,1,1.0
optimal β,1,1,1.0
values obtained,1,1,1.0
from see,1,1,1.0
see fig,2,2,1.0
fig for,1,1,1.0
histogram recall,1,1,1.0
points on,2,2,1.0
or even,1,1,1.0
even on,1,1,1.0
other side,1,1,1.0
hyperplane are,1,1,1.0
chosen it,1,1,1.0
datasets within,1,1,1.0
within interior,1,1,1.0
interior of,1,1,1.0
preferable moreover,1,1,1.0
moreover note,1,1,1.0
a relatively,1,1,1.0
relatively large,1,1,1.0
choice is,1,1,1.0
is uniform,1,1,1.0
uniform however,1,1,1.0
could mean,1,1,1.0
mean that,1,1,1.0
that uniform,1,1,1.0
uniform sampling,3,2,1.5
sampling could,1,1,1.0
be feasible,1,1,1.0
feasible in,1,1,1.0
cases for,1,1,1.0
space because,1,1,1.0
the improved,2,1,2.0
improved data,1,1,1.0
data separation,1,1,1.0
separation finally,2,1,2.0
of preferential,2,1,2.0
preferential we,1,1,1.0
small comparison,1,1,1.0
of oefs,1,1,1.0
oefs and,2,1,2.0
and opmkl,1,1,1.0
opmkl using,1,1,1.0
partition the,1,1,1.0
dataset chosen,1,1,1.0
chosen is,1,1,1.0
is haberman,1,1,1.0
the time,10,2,5.0
time is,2,1,2.0
is reported,1,1,1.0
of seconds,1,1,1.0
seconds fig,1,1,1.0
mean values,1,1,1.0
the beta,1,1,1.0
beta parameter,1,1,1.0
parameter used,1,1,1.0
the x,1,1,1.0
x coordinate,1,1,1.0
coordinate represents,1,1,1.0
different mean,1,1,1.0
mean β,1,1,1.0
values chosen,1,1,1.0
dataset related,1,1,1.0
to preferential,1,1,1.0
preferential and,1,1,1.0
the y,1,1,1.0
process needed,1,1,1.0
parameters is,1,1,1.0
not considered,1,1,1.0
considered according,1,1,1.0
following for,1,1,1.0
for oefs,1,1,1.0
for opmkl,1,1,1.0
opmkl from,1,1,1.0
computational time,4,2,2.0
is affordable,1,1,1.0
affordable vii,1,1,1.0
vii c,1,1,1.0
n c,3,2,1.5
l u,1,1,1.0
u s,1,1,1.0
s this,2,2,1.0
paper explores,1,1,1.0
explores the,1,1,1.0
problems since,1,1,1.0
accessible the,1,1,1.0
space that,1,1,1.0
that preserves,1,1,1.0
is tackled,1,1,1.0
tackled by,1,1,1.0
convex tion,1,1,1.0
tion of,13,2,6.5
as usually,1,1,1.0
usually done,1,1,1.0
we focus,1,1,1.0
the paradigm,1,1,1.0
paradigm of,1,1,1.0
methods w,1,1,1.0
the ideas,1,1,1.0
ideas of,1,1,1.0
space by,1,1,1.0
by kernel,1,1,1.0
preferential which,1,1,1.0
which analyses,1,1,1.0
analyses which,2,2,1.0
be from,1,1,1.0
datasets several,1,1,1.0
drawn ﬁrstly,1,1,1.0
ﬁrstly in,1,1,1.0
yield better,1,1,1.0
space secondly,1,1,1.0
secondly the,1,1,1.0
control of,1,1,1.0
could lead,1,1,1.0
results thirdly,1,1,1.0
thirdly the,1,1,1.0
kernel used,1,1,1.0
used inﬂuence,1,1,1.0
inﬂuence the,1,1,1.0
a great,1,1,1.0
great extent,1,1,1.0
extent making,1,1,1.0
structure although,1,1,1.0
well for,1,1,1.0
for several,2,2,1.0
several cases,1,1,1.0
ﬁnally that,1,1,1.0
exist some,2,2,1.0
dataset which,1,1,1.0
which should,1,1,1.0
that multiple,1,1,1.0
techniques should,1,1,1.0
future with,1,1,1.0
authors would,1,1,1.0
would also,2,2,1.0
also like,2,2,1.0
to stress,1,1,1.0
stress several,1,1,1.0
several lines,1,1,1.0
lines of,1,1,1.0
of future,1,1,1.0
work firstly,1,1,1.0
firstly an,1,1,1.0
analytical methodology,1,1,1.0
be developed,2,1,2.0
developed with,1,1,1.0
of secondly,1,1,1.0
secondly considering,1,1,1.0
a unique,1,1,1.0
unique methodology,1,1,1.0
methodology combining,1,1,1.0
paper could,1,1,1.0
accomplished to,1,1,1.0
analyse how,1,1,1.0
how these,1,1,1.0
methods could,1,1,1.0
could complement,1,1,1.0
complement each,1,1,1.0
other furthermore,1,1,1.0
furthermore in,1,1,1.0
be incorporated,1,1,1.0
incorporated in,1,1,1.0
learning stage,1,1,1.0
stage to,1,1,1.0
to search,2,2,1.0
search the,1,1,1.0
suitable representation,1,1,1.0
the not,1,1,1.0
the better,1,1,1.0
better class,1,1,1.0
finally other,1,1,1.0
other intelligent,1,1,1.0
intelligent optimisation,1,1,1.0
patterns re,1,1,1.0
re f,1,1,1.0
c e,1,1,1.0
s he,1,1,1.0
he and,3,2,1.5
and garcia,3,2,1.5
engineering vol,3,1,3.0
japkowicz and,3,2,1.5
and stephen,1,1,1.0
stephen the,1,1,1.0
problem a,1,1,1.0
a tematic,1,1,1.0
tematic study,1,1,1.0
study intelligent,1,1,1.0
analysis vol,1,1,1.0
vol no,46,2,23.0
no pp,37,2,18.5
v chawla,8,2,4.0
chawla w,1,1,1.0
w bowyer,1,1,1.0
and w,2,1,2.0
w p,1,1,1.0
p kegelmeyer,1,1,1.0
of artiﬁcial,1,1,1.0
artiﬁcial intelligence,3,2,1.5
research vol,5,1,5.0
pp y,6,2,3.0
y t,2,2,1.0
t ang,1,1,1.0
ang y,1,1,1.0
y zhang,1,1,1.0
zhang v,1,1,1.0
chawla and,2,2,1.0
and krasser,1,1,1.0
krasser svms,1,1,1.0
svms modeling,1,1,1.0
modeling for,1,1,1.0
classiﬁcation ieee,2,1,2.0
cybernetics p,2,1,2.0
p art,2,1,2.0
art b,1,1,1.0
b cybernetics,1,1,1.0
pp nguyen,1,1,1.0
nguyen w,1,1,1.0
w cooper,1,1,1.0
cooper and,1,1,1.0
and kamei,1,1,1.0
data classiﬁcation,4,2,2.0
classiﬁcation international,1,1,1.0
data p,1,1,1.0
p aradigms,1,1,1.0
aradigms vol,1,1,1.0
pp bunkhumpornpat,1,1,1.0
sinapiromsaran and,1,1,1.0
and lursinsap,1,1,1.0
the p,2,1,2.0
p conference,1,1,1.0
mining bangkok,1,1,1.0
bangkok thailand,2,1,2.0
thailand april,1,1,1.0
april pp,2,1,2.0
pp he,2,2,1.0
y bai,2,2,1.0
garcia and,2,2,1.0
and li,3,2,1.5
networks hong,1,1,1.0
hong kong,2,1,2.0
kong china,2,1,2.0
china pp,2,1,2.0
pp barua,2,1,2.0
barua islam,2,1,2.0
islam and,1,1,1.0
and murase,2,1,2.0
murase a,1,1,1.0
novel synthetic,1,1,1.0
processing shanghai,1,1,1.0
shanghai china,1,1,1.0
china nov,2,1,2.0
nov pp,2,1,2.0
islam y,1,1,1.0
y ao,1,1,1.0
ao and,1,1,1.0
murase mwmote,1,1,1.0
mwmote majority,1,1,1.0
majority weighted,1,1,1.0
weighted minority,1,1,1.0
learning ieee,1,1,1.0
pp galar,2,1,2.0
galar fern,2,1,2.0
fern barrenechea,2,1,2.0
bustince and,1,1,1.0
and f,2,1,2.0
f herrera,2,1,2.0
art c,1,1,1.0
c vol,1,1,1.0
pp jul,2,2,1.0
jul gantner,1,1,1.0
gantner and,1,1,1.0
the national,1,1,1.0
national joint,2,2,1.0
networks barcelona,1,1,1.0
barcelona spain,1,1,1.0
spain july,1,1,1.0
july pp,3,1,3.0
pp boser,1,1,1.0
boser guyon,1,1,1.0
guyon and,1,1,1.0
and v,3,1,3.0
v v,2,1,2.0
v apnik,2,1,2.0
apnik a,1,1,1.0
training algorithm,1,1,1.0
optimal margin,1,1,1.0
fifth annual,1,1,1.0
annual acm,2,1,2.0
acm w,1,1,1.0
w orkshop,1,1,1.0
orkshop on,1,1,1.0
computational learning,1,1,1.0
theory pittsburgh,2,1,2.0
pittsburgh p,1,1,1.0
p a,1,1,1.0
a july,1,1,1.0
pp cortes,2,1,2.0
cortes and,1,1,1.0
apnik networks,1,1,1.0
learning vol,1,1,1.0
pp zeng,1,1,1.0
zeng and,1,1,1.0
and gao,1,1,1.0
gao improving,1,1,1.0
improving svm,1,1,1.0
svm classiﬁcation,1,1,1.0
classiﬁcation with,1,1,1.0
processing bangkok,1,1,1.0
thailand pp,1,1,1.0
scholkopf and,1,1,1.0
and smola,2,1,2.0
smola learning,1,1,1.0
with kernels,1,1,1.0
kernels support,1,1,1.0
machines regularization,1,1,1.0
regularization optimization,1,1,1.0
optimization and,1,1,1.0
and beyond,1,1,1.0
beyond cambridge,1,1,1.0
cambridge ma,3,2,1.5
ma usa,1,1,1.0
usa mit,1,1,1.0
mit press,3,2,1.5
press sch,1,1,1.0
sch mika,1,1,1.0
mika burges,1,1,1.0
burges p,1,1,1.0
p knirsch,1,1,1.0
knirsch m,1,1,1.0
m r,1,1,1.0
smola input,1,1,1.0
space versus,1,1,1.0
versus feature,1,1,1.0
in based,1,1,1.0
based methods,1,1,1.0
methods ieee,2,1,2.0
networks vol,15,2,7.5
pp xiong,2,1,2.0
xiong swamy,1,1,1.0
swamy and,1,1,1.0
and ahmad,1,1,1.0
ahmad optimizing,1,1,1.0
optimizing the,1,1,1.0
space ieee,1,1,1.0
pp march,1,1,1.0
march cristianini,1,1,1.0
cristianini kandola,1,1,1.0
kandola elisseeff,1,1,1.0
elisseeff and,1,1,1.0
and aylor,1,1,1.0
aylor on,1,1,1.0
on target,1,1,1.0
alignment in,1,1,1.0
systems v,2,1,2.0
v ancouver,2,1,2.0
ancouver canada,2,1,2.0
canada pp,1,1,1.0
cortes mohri,1,1,1.0
mohri and,1,1,1.0
and rostamizadeh,1,1,1.0
rostamizadeh algorithms,1,1,1.0
learning kernels,1,1,1.0
kernels based,1,1,1.0
on centered,1,1,1.0
centered alignment,1,1,1.0
alignment journal,1,1,1.0
pp akbani,1,1,1.0
akbani kwek,1,1,1.0
kwek and,1,1,1.0
japkowicz applying,1,1,1.0
applying support,1,1,1.0
machines to,1,1,1.0
the european,1,1,1.0
learning pisa,1,1,1.0
pisa italy,1,1,1.0
italy pp,1,1,1.0
y liu,2,2,1.0
liu an,1,1,1.0
an and,1,1,1.0
and huang,1,1,1.0
huang boosting,1,1,1.0
boosting prediction,1,1,1.0
p asia,1,1,1.0
asia conference,1,1,1.0
mining singapore,1,1,1.0
singapore april,1,1,1.0
p kang,1,1,1.0
kang and,1,1,1.0
and cho,1,1,1.0
cho eus,1,1,1.0
eus svms,1,1,1.0
svms ensemble,1,1,1.0
ensemble of,3,2,1.5
processing hong,1,1,1.0
pp hong,2,2,1.0
hong chen,2,2,1.0
chen and,3,2,1.5
and harris,2,2,1.0
harris a,2,2,1.0
sets ieee,2,2,1.0
and y,5,2,2.5
y chang,3,2,1.5
chang kba,2,2,1.0
kba kernel,2,2,1.0
kernel boundary,2,2,1.0
boundary alignment,2,2,1.0
alignment ering,1,1,1.0
ering imbalanced,1,1,1.0
distribution ieee,2,2,1.0
pp june,1,1,1.0
june adaptive,1,1,1.0
adaptive conformal,1,1,1.0
conformal transformation,1,1,1.0
transformation for,1,1,1.0
anced data,4,3,1.3333333333333333
data learning,1,1,1.0
the t,5,2,2.5
t wentieth,1,1,1.0
wentieth international,1,1,1.0
learning w,1,1,1.0
w ashington,1,1,1.0
ashington usa,1,1,1.0
usa pp,2,1,2.0
y uan,1,1,1.0
uan li,1,1,1.0
li and,2,2,1.0
and zhang,2,2,1.0
zhang learning,2,2,1.0
learning concepts,2,2,1.0
concepts from,2,2,1.0
from large,2,2,1.0
scale imbalanced,2,2,1.0
using support,2,2,1.0
support cluster,3,2,1.5
cluster machines,3,2,1.5
the annual,2,1,2.0
acm international,1,1,1.0
on multimedia,1,1,1.0
multimedia new,1,1,1.0
ork usa,1,1,1.0
pp t,1,1,1.0
t cover,1,1,1.0
cover and,1,1,1.0
p hart,1,1,1.0
hart nearest,1,1,1.0
neighbor pattern,1,1,1.0
pattern classiﬁcation,1,1,1.0
theory vol,1,1,1.0
w johnson,1,1,1.0
johnson and,1,1,1.0
and riess,1,1,1.0
riess numerical,1,1,1.0
analysis reading,1,1,1.0
reading mass,1,1,1.0
mass esley,1,1,1.0
esley pub,1,1,1.0
pub t,1,1,1.0
t y,1,1,1.0
y kwok,1,1,1.0
kwok and,1,1,1.0
w tsang,1,1,1.0
tsang the,1,1,1.0
pp nov,1,1,1.0
nov braun,1,1,1.0
braun buhmann,1,1,1.0
buhmann and,1,1,1.0
and m,3,2,1.5
m on,1,1,1.0
on relevant,1,1,1.0
relevant dimensions,1,1,1.0
kernel feature,2,1,2.0
spaces journal,1,1,1.0
pp smola,1,1,1.0
smola sch,1,1,1.0
sch and,1,1,1.0
the connection,1,1,1.0
connection between,1,1,1.0
between regularization,1,1,1.0
regularization operators,1,1,1.0
operators and,1,1,1.0
and support,2,2,1.0
vector kernels,1,1,1.0
kernels neural,1,1,1.0
pp jun,8,2,4.0
jun ledoux,1,1,1.0
ledoux the,1,1,1.0
of measure,1,1,1.0
measure phenomenon,1,1,1.0
phenomenon ser,1,1,1.0
ser ical,1,1,1.0
ical surveys,1,1,1.0
surveys and,1,1,1.0
and monographs,1,1,1.0
monographs american,1,1,1.0
american mathematical,1,1,1.0
mathematical society,1,1,1.0
society giannopoulos,1,1,1.0
giannopoulos and,1,1,1.0
v milman,1,1,1.0
milman concentration,1,1,1.0
concentration property,1,1,1.0
property on,1,1,1.0
on probability,1,1,1.0
probability spaces,1,1,1.0
spaces advances,1,1,1.0
in mathematics,2,2,1.0
mathematics vol,1,1,1.0
pp abe,1,1,1.0
abe and,1,1,1.0
and onishi,1,1,1.0
onishi sparse,1,1,1.0
sparse least,1,1,1.0
least squares,2,2,1.0
squares support,1,1,1.0
vector regressors,1,1,1.0
regressors trained,1,1,1.0
trained in,1,1,1.0
on artiﬁcial,1,1,1.0
artiﬁcial neural,1,1,1.0
networks san,1,1,1.0
san sebasti,1,1,1.0
sebasti spain,1,1,1.0
spain june,1,1,1.0
june pp,2,1,2.0
xiong a,1,1,1.0
for kernelization,1,1,1.0
kernelization the,1,1,1.0
the chinese,1,1,1.0
chinese conference,1,1,1.0
on p,1,1,1.0
p attern,3,1,3.0
attern recognition,3,1,3.0
recognition nanjing,1,1,1.0
nanjing china,1,1,1.0
pp ramona,1,1,1.0
ramona richard,1,1,1.0
richard and,1,1,1.0
and david,1,1,1.0
david multiclass,1,1,1.0
multiclass feature,1,1,1.0
selection with,1,1,1.0
kernel criteria,1,1,1.0
criteria ieee,1,1,1.0
pp lanckriet,1,1,1.0
lanckriet cristianini,1,1,1.0
cristianini p,1,1,1.0
p bartlett,1,1,1.0
bartlett ghaoui,1,1,1.0
ghaoui and,1,1,1.0
and jordan,1,1,1.0
jordan learning,1,1,1.0
with semideﬁnite,1,1,1.0
semideﬁnite programming,1,1,1.0
programming journal,1,1,1.0
pp srebro,1,1,1.0
srebro and,1,1,1.0
for support,1,1,1.0
vector chines,2,2,1.0
chines with,1,1,1.0
with learned,1,1,1.0
learned kernels,1,1,1.0
kernels in,1,1,1.0
annual conference,1,1,1.0
pittsburgh usa,1,1,1.0
usa june,1,1,1.0
pp lichman,1,1,1.0
repository online,2,2,1.0
online a,1,1,1.0
a vailable,1,1,1.0
vailable http,1,1,1.0
http kubat,1,1,1.0
sets selection,2,2,1.0
international ence,1,1,1.0
ence on,1,1,1.0
nashville usa,1,1,1.0
usa july,1,1,1.0
pp barandela,1,1,1.0
barandela s,1,1,1.0
s v,1,1,1.0
v garc,1,1,1.0
garc and,1,1,1.0
problems p,1,1,1.0
recognition vol,2,1,2.0
barrenechea and,1,1,1.0
herrera eusboost,1,1,1.0
eusboost enhancing,1,1,1.0
enhancing ensembles,1,1,1.0
by evolutionary,1,1,1.0
undersampling p,1,1,1.0
pp demsar,1,1,1.0
demsar statistical,1,1,1.0
statistical comparisons,2,2,1.0
comparisons of,2,2,1.0
classiﬁers over,2,2,1.0
over multiple,2,2,1.0
multiple data,2,2,1.0
sets journal,1,1,1.0
pp igel,1,1,1.0
igel and,1,1,1.0
and h,1,1,1.0
h empirical,1,1,1.0
empirical evaluation,1,1,1.0
improved rprop,1,1,1.0
rprop learning,1,1,1.0
algorithms neurocomputing,1,1,1.0
neurocomputing vol,1,1,1.0
pp chapelle,1,1,1.0
chapelle and,1,1,1.0
and rakotomamonjy,1,1,1.0
rakotomamonjy second,1,1,1.0
second order,1,1,1.0
order optimization,1,1,1.0
parameters in,1,1,1.0
in in,4,2,2.0
no october,10,1,10.0
october ramoboost,1,1,1.0
ramoboost ranked,1,1,1.0
ranked minority,11,1,11.0
boosting sheng,1,1,1.0
sheng chen,2,1,2.0
chen student,1,1,1.0
student member,1,1,1.0
ieee haibo,1,1,1.0
haibo he,2,1,2.0
he member,1,1,1.0
ieee and,1,1,1.0
and edwardo,1,1,1.0
edwardo garcia,2,1,2.0
garcia in,1,1,1.0
years learning,1,1,1.0
has attracted,2,1,2.0
attracted growing,1,1,1.0
growing attention,1,1,1.0
both academia,1,1,1.0
academia and,1,1,1.0
and industry,1,1,1.0
industry due,1,1,1.0
the explosive,1,1,1.0
explosive growth,1,1,1.0
growth of,1,1,1.0
applications that,1,1,1.0
that use,2,1,2.0
use and,1,1,1.0
and produce,1,1,1.0
produce imbalanced,1,1,1.0
data however,1,1,1.0
however because,1,1,1.0
the complex,1,1,1.0
complex characteristics,1,1,1.0
data many,1,1,1.0
many solutions,1,1,1.0
solutions struggle,1,1,1.0
struggle to,1,1,1.0
provide robust,1,1,1.0
robust efﬁciency,1,1,1.0
efﬁciency in,1,1,1.0
applications in,3,1,3.0
an effort,1,1,1.0
effort to,1,1,1.0
address this,1,1,1.0
paper presents,1,1,1.0
presents ranked,1,1,1.0
minority sampling,3,2,1.5
boosting ramoboost,1,1,1.0
ramoboost which,2,1,2.0
a ramo,2,1,2.0
ramo technique,2,1,2.0
technique based,1,1,1.0
of adaptive,1,1,1.0
an ensemble,2,1,2.0
system brieﬂy,1,1,1.0
brieﬂy ramoboost,1,1,1.0
ramoboost adaptively,2,1,2.0
adaptively ranks,1,1,1.0
ranks minority,1,1,1.0
instances at,1,1,1.0
at each,8,1,8.0
each learning,1,1,1.0
learning iteration,1,1,1.0
iteration according,1,1,1.0
a sampling,2,1,2.0
sampling probability,3,1,3.0
distribution that,1,1,1.0
underlying data,1,1,1.0
can adaptively,2,1,2.0
adaptively shift,3,1,3.0
shift the,4,1,4.0
boundary toward,4,1,4.0
toward minori,1,1,1.0
minori ty,2,2,1.0
ty and,1,1,1.0
a hypothesis,2,1,2.0
hypothesis assessment,1,1,1.0
assessment procedure,1,1,1.0
procedure simulation,1,1,1.0
simulation analysis,2,1,2.0
analysis on,2,1,2.0
datasets assessed,1,1,1.0
assessed over,1,1,1.0
various overall,1,1,1.0
and receiver,1,1,1.0
receiver operation,1,1,1.0
operation characteristic,1,1,1.0
characteristic used,1,1,1.0
to illustrate,2,1,2.0
illustrate the,4,1,4.0
the effectiveness,3,1,3.0
effectiveness of,2,1,2.0
method index,1,1,1.0
t adaptive,1,1,1.0
adaptive boosting,1,1,1.0
boosting data,1,1,1.0
mining ensemble,1,1,1.0
ensemble ing,1,1,1.0
ing imbalanced,1,1,1.0
data i,1,1,1.0
i ntroduction,1,1,1.0
ntroduction l,1,1,1.0
l earning,2,1,2.0
earning from,1,1,1.0
imbalanced da,1,1,1.0
da ta,1,1,1.0
ta imbalanced,1,1,1.0
a criti,1,1,1.0
criti cal,1,1,1.0
cal and,1,1,1.0
and signiﬁcant,1,1,1.0
signiﬁcant research,1,1,1.0
research issue,1,1,1.0
of today,1,1,1.0
today s,1,1,1.0
s applications,1,1,1.0
as ﬁnancial,1,1,1.0
ﬁnancial engineering,1,1,1.0
engineering anom,1,1,1.0
anom aly,1,1,1.0
aly detection,1,1,1.0
detection biomedical,1,1,1.0
biomedical data,2,1,2.0
and many,1,1,1.0
many others,1,1,1.0
others the,1,1,1.0
amount and,1,1,1.0
and complexity,1,1,1.0
of raw,1,1,1.0
raw data,1,1,1.0
is captured,1,1,1.0
captured to,1,1,1.0
to monitor,1,1,1.0
monitor analyze,1,1,1.0
analyze and,1,1,1.0
support making,1,1,1.0
making processes,1,1,1.0
processes continue,1,1,1.0
continue to,1,1,1.0
to grow,1,1,1.0
grow at,1,1,1.0
at an,1,1,1.0
an incredible,1,1,1.0
incredible rate,1,1,1.0
rate consequently,1,1,1.0
consequently this,1,1,1.0
this enhances,1,1,1.0
enhances the,1,1,1.0
capacity for,1,1,1.0
for computationally,1,1,1.0
computationally intelligent,1,1,1.0
intelligent methods,1,1,1.0
to play,1,1,1.0
play an,1,1,1.0
an essential,1,1,1.0
essential role,1,1,1.0
applications involving,1,1,1.0
large amounts,1,1,1.0
amounts of,1,1,1.0
data on,1,1,1.0
hand these,1,1,1.0
these opportunities,1,1,1.0
opportunities also,1,1,1.0
also raise,1,1,1.0
raise many,1,1,1.0
many new,1,1,1.0
new challenges,1,1,1.0
research community,1,1,1.0
community in,2,1,2.0
general generally,1,1,1.0
generally speaking,1,1,1.0
speaking any,1,1,1.0
any dataset,1,1,1.0
between its,1,1,1.0
its classes,1,1,1.0
considered imbalanced,1,1,1.0
applications datasets,1,1,1.0
datasets exhibiting,1,1,1.0
exhibiting severe,1,1,1.0
severe ances,1,1,1.0
ances are,1,1,1.0
of great,1,1,1.0
great interest,1,1,1.0
interest since,1,1,1.0
they generally,1,1,1.0
generally present,1,1,1.0
present icant,1,1,1.0
icant difﬁculties,1,1,1.0
difﬁculties for,1,1,1.0
learning mechanisms,1,1,1.0
mechanisms typical,1,1,1.0
typical imbalance,1,1,1.0
imbalance manuscript,1,1,1.0
manuscript received,1,1,1.0
received february,1,1,1.0
february revised,1,1,1.0
revised november,1,1,1.0
november march,1,1,1.0
march august,1,1,1.0
august and,1,1,1.0
and august,1,1,1.0
august accepted,1,1,1.0
accepted august,1,1,1.0
august date,2,1,2.0
date of,2,1,2.0
of publication,1,1,1.0
publication august,1,1,1.0
of current,1,1,1.0
current version,1,1,1.0
version october,1,1,1.0
october chen,1,1,1.0
garcia are,1,1,1.0
of electrical,5,1,5.0
electrical and,3,1,3.0
and computer,3,1,3.0
computer engineering,3,1,3.0
engineering stevens,2,1,2.0
stevens institute,5,1,5.0
institute of,5,1,5.0
technology hoboken,4,1,4.0
hoboken nj,4,1,4.0
nj usa,1,1,1.0
usa egarcia,1,1,1.0
egarcia he,1,1,1.0
he is,4,1,4.0
electrical computer,2,1,2.0
and biomedical,2,1,2.0
biomedical engineering,2,1,2.0
engineering university,2,1,2.0
of rhode,2,1,2.0
rhode island,2,1,2.0
island kingston,2,1,2.0
kingston ri,1,1,1.0
ri usa,1,1,1.0
usa he,1,1,1.0
he color,1,1,1.0
color versions,1,1,1.0
the ﬁgures,1,1,1.0
ﬁgures in,1,1,1.0
available online,1,1,1.0
online at,1,1,1.0
at http,1,1,1.0
http digital,1,1,1.0
digital object,1,1,1.0
object identiﬁer,1,1,1.0
identiﬁer ratios,1,1,1.0
ratios can,1,1,1.0
can range,1,1,1.0
range from,1,1,1.0
from in,1,1,1.0
detection problems,2,1,2.0
problems to,3,1,3.0
in physics,1,1,1.0
physics event,2,1,2.0
event classiﬁcation,2,1,2.0
classiﬁcation however,1,1,1.0
however imbalances,1,1,1.0
imbalances of,1,1,1.0
this form,1,1,1.0
form are,1,1,1.0
are just,2,1,2.0
just one,1,1,1.0
one aspect,1,1,1.0
imbalance learning,1,1,1.0
problem generally,1,1,1.0
generally manifests,1,1,1.0
manifests itself,1,1,1.0
itself in,1,1,1.0
two forms,1,1,1.0
forms relative,1,1,1.0
relative imbalances,4,1,4.0
and absolute,2,1,2.0
absolute imbalances,4,1,4.0
imbalances absolute,1,1,1.0
imbalances arise,1,1,1.0
where minority,1,1,1.0
minority exampl,1,1,1.0
exampl es,1,1,1.0
es are,1,1,1.0
are deﬁnitively,1,1,1.0
deﬁnitively scarce,1,1,1.0
scarce and,1,1,1.0
and underrepresented,1,1,1.0
underrepresented whereas,1,1,1.0
whereas relative,1,1,1.0
are indicative,1,1,1.0
indicative of,1,1,1.0
which minority,1,1,1.0
are well,1,1,1.0
well represented,1,1,1.0
represented but,1,1,1.0
but remain,1,1,1.0
remain severely,1,1,1.0
severely outnumbere,1,1,1.0
outnumbere d,1,1,1.0
d by,1,1,1.0
by majority,1,1,1.0
examples some,1,1,1.0
the degradation,1,1,1.0
degradation of,1,1,1.0
of classiﬁcation,1,1,1.0
classiﬁcation performance,6,1,6.0
performance attributed,1,1,1.0
necessarily the,1,1,1.0
of relative,1,1,1.0
rather due,1,1,1.0
the lack,1,1,1.0
of representative,1,1,1.0
representative examples,1,1,1.0
examples absolute,1,1,1.0
particular for,1,1,1.0
that contains,1,1,1.0
contains several,1,1,1.0
several the,1,1,1.0
examples over,1,1,1.0
class concepts,1,1,1.0
concepts may,1,1,1.0
yield clusters,1,1,1.0
with insufﬁcient,1,1,1.0
insufﬁcient representative,1,1,1.0
representative exa,1,1,1.0
exa mples,1,1,1.0
mples to,1,1,1.0
a classiﬁcation,2,1,2.0
classiﬁcation rule,1,1,1.0
rule this,1,1,1.0
of concept,1,1,1.0
concept data,1,1,1.0
representation within,1,1,1.0
also known,1,1,1.0
was veriﬁed,1,1,1.0
veriﬁed to,1,1,1.0
handle than,1,1,1.0
than datasets,1,1,1.0
with only,1,1,1.0
only homogeneous,1,1,1.0
homogeneous concepts,1,1,1.0
concepts for,1,1,1.0
class logically,1,1,1.0
logically it,1,1,1.0
it would,1,1,1.0
would follow,1,1,1.0
follow that,1,1,1.0
that solutions,1,1,1.0
solutions targeted,1,1,1.0
targeted at,1,1,1.0
at both,1,1,1.0
both relative,1,1,1.0
relative and,1,1,1.0
imbalances would,1,1,1.0
more adept,1,1,1.0
adept to,1,1,1.0
handling a,1,1,1.0
wide spectrum,1,1,1.0
spectrum of,1,1,1.0
learning problems,8,1,8.0
this end,3,1,3.0
end this,1,1,1.0
proposes ramoboost,1,1,1.0
technique embedded,1,1,1.0
embedded with,1,1,1.0
boosting procedure,4,1,4.0
to facilitate,4,1,4.0
facilitate learning,2,1,2.0
imbalanced datase,1,1,1.0
datase ts,2,2,1.0
ts based,1,1,1.0
an integration,1,1,1.0
learning ramoboost,1,1,1.0
ramoboost tematically,1,1,1.0
tematically generates,1,1,1.0
class ratios,1,1,1.0
ratios of,1,1,1.0
of surrounding,1,1,1.0
surrounding nearest,1,1,1.0
underlying training,1,1,1.0
distribution unlike,1,1,1.0
unlike many,1,1,1.0
many existing,1,1,1.0
existing approaches,4,1,4.0
use uniform,1,1,1.0
sampling distributions,1,1,1.0
distributions ramoboost,1,1,1.0
adaptively adjusts,1,1,1.0
sampling weights,2,1,2.0
examples according,1,1,1.0
their data,1,1,1.0
data distributions,1,1,1.0
moreover by,1,1,1.0
by integrating,1,1,1.0
integrating the,1,1,1.0
the ensemble,2,1,2.0
learning methodology,2,1,2.0
methodology ramoboost,1,1,1.0
ramoboost adopts,2,1,2.0
adopts an,1,1,1.0
iterative learning,1,1,1.0
learning dure,1,1,1.0
dure that,1,1,1.0
that assesses,1,1,1.0
assesses the,1,1,1.0
hypothesis developed,1,1,1.0
developed at,1,1,1.0
each boosting,3,1,3.0
boosting iteration,3,1,3.0
iteration to,1,1,1.0
to adaptively,3,1,3.0
on those,2,1,2.0
those instances,1,1,1.0
classes we,1,1,1.0
we organize,1,1,1.0
organize the,1,1,1.0
paper as,1,1,1.0
follows in,1,1,1.0
ii we,1,1,1.0
brief review,1,1,1.0
the art,1,1,1.0
art techniques,1,1,1.0
the community,1,1,1.0
community to,1,1,1.0
iii we,2,1,2.0
we discuss,1,1,1.0
discuss the,2,1,2.0
the motivation,1,1,1.0
motivation behind,1,1,1.0
the ramoboost,10,1,10.0
ramoboost framework,2,1,2.0
framework and,1,1,1.0
and present,1,1,1.0
present ieeechen,1,1,1.0
ieeechen et,1,1,1.0
al ranked,9,1,9.0
boosting the,2,1,2.0
computational complexity,4,1,4.0
complexity analysis,2,1,2.0
proposed ramoboost,4,1,4.0
ramoboost algorithms,1,1,1.0
algorithms is,2,1,2.0
also presented,2,1,2.0
section in,2,1,2.0
iv simulation,2,1,2.0
on world,1,1,1.0
world machine,1,1,1.0
learning datasets,1,1,1.0
are provided,2,1,2.0
provided to,1,1,1.0
method details,1,1,1.0
details of,2,1,2.0
of experimental,2,1,2.0
experimental parameters,1,1,1.0
parameters and,2,1,2.0
and evaluation,1,1,1.0
finally a,1,1,1.0
a conclusion,1,1,1.0
conclusion and,1,1,1.0
and brief,1,1,1.0
on future,1,1,1.0
future research,2,1,2.0
research directions,2,1,2.0
directions are,1,1,1.0
ii r,1,1,1.0
r elated,1,1,1.0
elated works,1,1,1.0
works a,1,1,1.0
a comprehensive,2,1,2.0
comprehensive review,1,1,1.0
data including,1,1,1.0
the rt,1,1,1.0
rt approaches,1,1,1.0
the assessment,2,1,2.0
assessment metrics,7,1,7.0
the major,2,1,2.0
major opportunities,1,1,1.0
opportunities and,1,1,1.0
challenges has,1,1,1.0
been presented,1,1,1.0
in interested,1,1,1.0
interested readers,1,1,1.0
readers can,1,1,1.0
can refer,2,1,2.0
that article,1,1,1.0
article for,1,1,1.0
details in,1,1,1.0
a focused,1,1,1.0
focused review,1,1,1.0
four major,1,1,1.0
major categories,1,1,1.0
categories of,1,1,1.0
research activity,1,1,1.0
activity in,1,1,1.0
methods building,1,1,1.0
building on,1,1,1.0
the foundation,1,1,1.0
foundation of,1,1,1.0
random simple,1,1,1.0
simple pling,1,1,1.0
pling and,1,1,1.0
undersampling techniques,1,1,1.0
techniques researchers,1,1,1.0
researchers have,1,1,1.0
have oped,1,1,1.0
oped advanced,1,1,1.0
the shortcomings,1,1,1.0
shortcomings of,1,1,1.0
these basic,1,1,1.0
as overﬁtting,1,1,1.0
overﬁtting and,1,1,1.0
information loss,1,1,1.0
loss for,1,1,1.0
oversampling nique,1,1,1.0
nique smote,1,1,1.0
algorithm was,1,1,1.0
of every,1,1,1.0
and generates,1,1,1.0
by calculating,2,1,2.0
calculating linear,1,1,1.0
linear interpolations,1,1,1.0
interpolations between,1,1,1.0
between an,1,1,1.0
an original,1,1,1.0
original minority,3,1,3.0
a randomly,1,1,1.0
selected neighbor,1,1,1.0
neighbor expanding,1,1,1.0
expanding on,1,1,1.0
smote framework,1,1,1.0
algorithm locates,2,1,2.0
locates those,1,1,1.0
those minority,2,1,2.0
that reside,1,1,1.0
reside along,1,1,1.0
the borders,1,1,1.0
borders between,1,1,1.0
classes other,1,1,1.0
other popular,1,1,1.0
popular approaches,1,1,1.0
approaches include,1,1,1.0
learning adaboost,1,1,1.0
adaboost in,1,1,1.0
in junction,1,1,1.0
junction with,1,1,1.0
and jittering,1,1,1.0
jittering of,1,1,1.0
approach and,1,1,1.0
the integration,1,1,1.0
links and,1,1,1.0
and edited,1,1,1.0
neighbor since,1,1,1.0
aforementioned sampling,1,1,1.0
sampling algorithms,1,1,1.0
algorithms solely,1,1,1.0
solely focus,1,1,1.0
on relative,1,1,1.0
imbalances various,1,1,1.0
various approaches,1,1,1.0
approaches were,1,1,1.0
were proposed,1,1,1.0
to explicitly,1,1,1.0
explicitly address,1,1,1.0
imbalance issue,1,1,1.0
issue some,1,1,1.0
the important,1,1,1.0
important works,1,1,1.0
works include,1,1,1.0
oversampling algorithm,1,1,1.0
algorithm support,1,1,1.0
and others,2,1,2.0
others learning,2,1,2.0
methods learning,1,1,1.0
methods typically,1,1,1.0
typically employ,1,1,1.0
of cost,1,1,1.0
cost matrices,1,1,1.0
matrices to,1,1,1.0
to estimate,1,1,1.0
estimate the,1,1,1.0
the costs,1,1,1.0
costs of,1,1,1.0
different classiﬁcation,1,1,1.0
classiﬁcation errors,1,1,1.0
errors these,1,1,1.0
shown great,1,1,1.0
great success,1,1,1.0
success when,1,1,1.0
when applied,1,1,1.0
was presented,1,1,1.0
presented to,2,1,2.0
to induce,2,1,2.0
induce trees,2,1,2.0
the asymmetric,1,1,1.0
asymmetric adaboost,2,1,2.0
adaboost method,1,1,1.0
handle face,1,1,1.0
face detection,1,1,1.0
the skewed,2,1,2.0
ratio can,1,1,1.0
quite high,1,1,1.0
high the,1,1,1.0
the adacost,1,1,1.0
adacost method,1,1,1.0
in combines,1,1,1.0
combines learning,1,1,1.0
with ing,1,1,1.0
ing by,1,1,1.0
by referring,1,1,1.0
referring to,1,1,1.0
matrix adacost,1,1,1.0
adacost assigns,1,1,1.0
assigns different,1,1,1.0
different cost,1,1,1.0
cost values,1,1,1.0
values to,1,1,1.0
to misclassiﬁed,1,1,1.0
misclassiﬁed minority,1,1,1.0
the trained,2,1,2.0
trained hypothesis,3,1,3.0
hypothesis at,2,1,2.0
iteration loop,5,1,5.0
loop other,1,1,1.0
other examples,2,1,2.0
learning include,1,1,1.0
cost framework,1,1,1.0
framework neural,1,1,1.0
network sensitive,1,1,1.0
sensitive support,1,1,1.0
machines svms,1,1,1.0
methods methods,1,1,1.0
have recently,1,1,1.0
recently become,1,1,1.0
become very,1,1,1.0
popular across,1,1,1.0
across various,2,1,2.0
various ﬁelds,1,1,1.0
ﬁelds including,1,1,1.0
including imbalanced,1,1,1.0
in eral,1,1,1.0
eral methods,1,1,1.0
methods fac,1,1,1.0
fac ilitate,1,1,1.0
ilitate learning,1,1,1.0
by maximizing,1,1,1.0
maximizing the,1,1,1.0
separation margin,1,1,1.0
margin between,1,1,1.0
between concepts,1,1,1.0
concepts in,1,1,1.0
in linearly,1,1,1.0
separable feature,1,1,1.0
instance the,2,1,2.0
alignment gorithm,1,1,1.0
gorithm was,1,1,1.0
which imbalanced,1,1,1.0
as information,1,1,1.0
information prior,1,1,1.0
prior to,1,1,1.0
to adjusting,1,1,1.0
facilitate svm,1,1,1.0
svm learning,1,1,1.0
for improved,2,1,2.0
improved prediction,1,1,1.0
accuracy another,1,1,1.0
another example,1,1,1.0
learning presents,1,1,1.0
presents a,1,1,1.0
kernel classiﬁer,1,1,1.0
classiﬁer construction,1,1,1.0
construction algorithm,1,1,1.0
algorithm using,1,1,1.0
using orthogonal,1,1,1.0
orthogonal ward,1,1,1.0
ward selection,1,1,1.0
selection to,1,1,1.0
to optimize,1,1,1.0
optimize the,1,1,1.0
the generalization,1,1,1.0
generalization model,1,1,1.0
problems this,1,1,1.0
the regularized,1,1,1.0
regularized orthogonal,1,1,1.0
orthogonal weighted,1,1,1.0
weighted least,1,1,1.0
squares method,1,1,1.0
model selection,1,1,1.0
selection criterion,1,1,1.0
criterion of,1,1,1.0
of maximal,1,1,1.0
maximal area,1,1,1.0
auc of,6,1,6.0
the receiver,1,1,1.0
characteristic roc,1,1,1.0
roc graph,2,1,2.0
graph active,1,1,1.0
active learning,7,1,7.0
methods active,1,1,1.0
were originally,1,1,1.0
originally developed,1,1,1.0
from datasets,1,1,1.0
with unl,1,1,1.0
unl abeled,1,1,1.0
abeled instances,1,1,1.0
instances recently,1,1,1.0
recently tive,1,1,1.0
tive learning,1,1,1.0
have found,1,1,1.0
found increased,1,1,1.0
increased use,1,1,1.0
learning applications,1,1,1.0
applications fo,1,1,1.0
fo r,2,1,2.0
r example,1,1,1.0
example an,1,1,1.0
learning approach,1,1,1.0
datasets was,1,1,1.0
locates the,1,1,1.0
most informative,2,1,2.0
informative sample,1,1,1.0
sample by,1,1,1.0
by evaluating,1,1,1.0
evaluating a,1,1,1.0
small ﬁxed,1,1,1.0
ﬁxed number,1,1,1.0
of randomly,1,1,1.0
examples instead,1,1,1.0
the entire,3,1,3.0
entire dataset,2,1,2.0
the stopping,1,1,1.0
stopping condition,1,1,1.0
for active,1,1,1.0
active l,1,1,1.0
earning applications,1,1,1.0
in word,1,1,1.0
word sense,2,1,2.0
sense disambiguation,2,1,2.0
disambiguation wsd,1,1,1.0
wsd domains,1,1,1.0
domains was,1,1,1.0
was investigated,1,1,1.0
investigated to,1,1,1.0
to alleviate,1,1,1.0
alleviate the,1,1,1.0
the complications,1,1,1.0
complications introduced,1,1,1.0
by ances,1,1,1.0
ances the,1,1,1.0
authors proposed,1,1,1.0
technique to,1,1,1.0
improve active,1,1,1.0
learning performance,6,1,6.0
anced wsd,1,1,1.0
wsd applications,1,1,1.0
applications iii,1,1,1.0
iii ramob,1,1,1.0
ramob oost,13,1,13.0
oost framework,1,1,1.0
framework i,1,1,1.0
i ntegration,1,1,1.0
ntegration of,1,1,1.0
generation and,1,1,1.0
and boosting,1,1,1.0
boosting ensemble,1,1,1.0
learning preliminaries,1,1,1.0
preliminaries for,1,1,1.0
for ramoboost,8,1,8.0
ramoboost various,1,1,1.0
techniques exist,1,1,1.0
exist for,1,1,1.0
for quantizing,1,1,1.0
quantizing the,1,1,1.0
learning culty,1,1,1.0
culty level,1,1,1.0
example accord,1,1,1.0
accord ing,2,1,2.0
majority cases,8,1,8.0
an intuitive,1,1,1.0
intuitive method,1,1,1.0
be formulated,2,1,2.0
formulated as,1,1,1.0
follows first,1,1,1.0
first l,1,1,1.0
l ocate,1,1,1.0
ocate the,1,1,1.0
the center,3,1,3.0
center mean,1,1,1.0
the featur,1,1,1.0
featur e,1,1,1.0
e space,1,1,1.0
then calculate,1,1,1.0
between each,1,1,1.0
minority example,30,1,30.0
center the,2,1,2.0
of minimal,2,1,2.0
minimal distanced,1,1,1.0
distanced minority,1,1,1.0
examples up,1,1,1.0
some threshold,1,1,1.0
threshold then,1,1,1.0
then would,1,1,1.0
would represent,1,1,1.0
informative examples,1,1,1.0
examples fig,2,1,2.0
a illustrates,1,1,1.0
illustrates this,2,1,2.0
idea in,2,1,2.0
this ﬁgure,2,1,2.0
ﬁgure the,1,1,1.0
the stars,1,1,1.0
stars and,1,1,1.0
and circles,1,1,1.0
circles represent,1,1,1.0
classes respectively,1,1,1.0
respectively and,2,1,2.0
triangle within,1,1,1.0
data sents,1,1,1.0
sents the,1,1,1.0
the approximate,1,1,1.0
approximate center,1,1,1.0
euclidean distances,1,1,1.0
examples a,2,1,2.0
d and,1,1,1.0
class center,1,1,1.0
center would,1,1,1.0
be calculated,1,1,1.0
calculated respectively,1,1,1.0
respectively as,1,1,1.0
as efﬁcient,1,1,1.0
efﬁcient as,1,1,1.0
it appears,1,1,1.0
appears this,1,1,1.0
method exhibits,1,1,1.0
exhibits a,1,1,1.0
critical ﬂaw,1,1,1.0
ﬂaw it,1,1,1.0
no disjuncts,1,1,1.0
disjuncts within,1,1,1.0
class concept,1,1,1.0
concept otherwise,1,1,1.0
otherwise there,1,1,1.0
may ex,1,1,1.0
ex ist,1,1,1.0
ist several,1,1,1.0
several for,1,1,1.0
the task,1,1,1.0
task of,1,1,1.0
distance against,1,1,1.0
the centers,1,1,1.0
centers beco,1,1,1.0
beco mes,1,1,1.0
mes complicated,1,1,1.0
complicated and,1,1,1.0
and requires,1,1,1.0
requires careful,1,1,1.0
careful examination,1,1,1.0
examination we,1,1,1.0
we highlight,2,1,2.0
highlight this,1,1,1.0
fig b,2,1,2.0
b ieee,1,1,1.0
october a,2,1,2.0
c majority,1,1,1.0
majority cluster,2,1,2.0
cluster majority,1,1,1.0
cluster fig,1,1,1.0
fig cluster,1,1,1.0
distance calculation,1,1,1.0
calculation based,1,1,1.0
on euclidean,1,1,1.0
a int,1,1,1.0
int uitive,1,1,1.0
uitive approach,1,1,1.0
to decide,2,1,2.0
decide the,3,1,3.0
the infor,1,1,1.0
infor mative,1,1,1.0
mative data,1,1,1.0
data examples,2,1,2.0
examples based,1,1,1.0
on euclid,1,1,1.0
euclid ean,1,1,1.0
ean distance,1,1,1.0
distance b,1,1,1.0
b potential,1,1,1.0
potential dilemma,1,1,1.0
dilemma by,1,1,1.0
this in,2,1,2.0
in tuitive,1,1,1.0
tuitive approach,1,1,1.0
approach c,1,1,1.0
c proposed,1,1,1.0
proposed approach,1,1,1.0
approach by,1,1,1.0
using k,1,1,1.0
and using,1,1,1.0
the weight,3,1,3.0
weight to,1,1,1.0
informative data,1,1,1.0
examples generated,1,1,1.0
instance xi,2,1,2.0
xi xiˆ,1,1,1.0
xiˆ fig,1,1,1.0
fig synthetic,1,1,1.0
generation based,1,1,1.0
class contains,1,1,1.0
contains two,1,1,1.0
two clusters,1,1,1.0
example has,2,1,2.0
has to,1,1,1.0
to locate,1,1,1.0
locate its,1,1,1.0
its respective,1,1,1.0
respective closest,1,1,1.0
closest cluster,1,1,1.0
cluster center,1,1,1.0
center before,1,1,1.0
before calculating,1,1,1.0
calculating the,3,1,3.0
the dist,1,1,1.0
dist ance,1,1,1.0
ance to,1,1,1.0
majority center,1,1,1.0
center ramoboost,1,1,1.0
ramoboost targets,1,1,1.0
targets this,2,1,2.0
this ﬂaw,2,1,2.0
ﬂaw in,1,1,1.0
following way,1,1,1.0
way stead,1,1,1.0
stead of,1,1,1.0
of searching,1,1,1.0
searching for,1,1,1.0
center s,1,1,1.0
s of,1,1,1.0
majority dataset,1,1,1.0
dataset ramoboost,5,1,5.0
ramoboost determines,1,1,1.0
and assigns,1,1,1.0
assigns this,1,1,1.0
value as,1,1,1.0
information weight,1,1,1.0
weight we,1,1,1.0
we illustrate,1,1,1.0
fig c,4,1,4.0
c here,1,1,1.0
here the,1,1,1.0
the highlighted,1,1,1.0
highlighted areas,1,1,1.0
areas surrounded,1,1,1.0
surrounded by,1,1,1.0
by dashed,1,1,1.0
dashed circles,1,1,1.0
circles represents,1,1,1.0
search area,1,1,1.0
area for,1,1,1.0
d k,1,1,1.0
k n,1,1,1.0
n this,1,1,1.0
case accordingly,1,1,1.0
accordingly ther,1,1,1.0
ther e,1,1,1.0
e are,1,1,1.0
are and,1,1,1.0
four nearest,1,1,1.0
d respectively,1,1,1.0
respectively this,2,1,2.0
means the,1,1,1.0
be located,1,1,1.0
located near,1,1,1.0
a example,1,1,1.0
samples should,1,1,1.0
for to,3,1,3.0
to force,1,1,1.0
force the,1,1,1.0
the ﬁnal,6,1,6.0
ﬁnal learning,2,1,2.0
learning hypothesis,1,1,1.0
hypothesis to,1,1,1.0
more focused,3,1,3.0
the difﬁcult,2,1,2.0
difﬁcult learning,1,1,1.0
learning regions,1,1,1.0
regions our,1,1,1.0
work adaptive,1,1,1.0
synthetic adasyn,1,1,1.0
adasyn also,5,1,5.0
applies a,1,1,1.0
similar idea,1,1,1.0
idea to,1,1,1.0
the difﬁculty,1,1,1.0
difﬁculty level,1,1,1.0
difference is,1,1,1.0
that instead,1,1,1.0
of directly,2,1,2.0
directly specifying,1,1,1.0
specifying the,1,1,1.0
example as,1,1,1.0
adasyn ramoboost,1,1,1.0
adopts this,1,1,1.0
this mechanism,1,1,1.0
mechanism to,1,1,1.0
to determi,1,1,1.0
determi ne,1,1,1.0
ne the,1,1,1.0
generating the,3,1,3.0
instances before,1,1,1.0
before presenting,1,1,1.0
presenting the,2,1,2.0
the details,2,1,2.0
ramoboost algorithm,3,1,3.0
algorithm we,2,1,2.0
we brieﬂy,1,1,1.0
brieﬂy discuss,1,1,1.0
generation mechanisms,2,1,2.0
mechanisms of,1,1,1.0
adasyn which,1,1,1.0
which motivated,1,1,1.0
motivated the,1,1,1.0
work presented,1,1,1.0
can provide,2,1,2.0
better understanding,1,1,1.0
of ramoboost,20,1,20.0
ramoboost in,1,1,1.0
data where,1,1,1.0
a parameter,1,1,1.0
parameter are,1,1,1.0
are adasyn,1,1,1.0
smote fig,1,1,1.0
instance proportions,1,1,1.0
proportions for,1,1,1.0
adasyn identiﬁed,1,1,1.0
identiﬁed for,1,1,1.0
the are,1,1,1.0
are deﬁned,1,1,1.0
k instances,1,1,1.0
instances such,1,1,1.0
example under,9,1,9.0
under consideration,11,1,11.0
consideration and,3,1,3.0
minority set,1,1,1.0
set are,1,1,1.0
minimal length,1,1,1.0
length along,1,1,1.0
space synthetic,1,1,1.0
then created,1,1,1.0
selecting one,1,1,1.0
neighbors and,1,1,1.0
and multiplying,1,1,1.0
multiplying the,1,1,1.0
corresponding feature,1,1,1.0
feature vector,3,1,3.0
vector difference,1,1,1.0
difference with,1,1,1.0
between xnew,1,1,1.0
xnew xi,1,1,1.0
xi ˆxi,1,1,1.0
ˆxi xi,1,1,1.0
where xi,1,1,1.0
consideration ˆxi,1,1,1.0
ˆxi is,1,1,1.0
neighbors minority,1,1,1.0
for xi,1,1,1.0
xi a,1,1,1.0
number therefore,1,1,1.0
resulting synthetic,1,1,1.0
instance according,1,1,1.0
to is,1,1,1.0
point along,1,1,1.0
line segment,1,1,1.0
segment joining,1,1,1.0
joining the,1,1,1.0
the example,2,1,2.0
consideration xi,1,1,1.0
the randomly,3,1,3.0
selected k,1,1,1.0
neighbor ˆxi,1,1,1.0
ˆxi fig,1,1,1.0
fig illustrates,1,1,1.0
procedure in,2,1,2.0
way smote,1,1,1.0
generates the,1,1,1.0
minority exam,1,1,1.0
exam ple,1,1,1.0
ple uniform,1,1,1.0
uniform di,1,1,1.0
di stribution,1,1,1.0
stribution adasyn,1,1,1.0
also uses,1,1,1.0
uses feature,1,1,1.0
feature int,1,1,1.0
int erpolation,1,1,1.0
erpolation to,1,1,1.0
generate thetic,1,1,1.0
thetic instances,1,1,1.0
difference from,1,1,1.0
is instead,1,1,1.0
a uniform,2,1,2.0
generation like,1,1,1.0
like smote,1,1,1.0
adasyn uses,2,1,2.0
a density,1,1,1.0
density distribution,3,1,3.0
distribution ˆri,1,1,1.0
ˆri as,1,1,1.0
a rion,1,1,1.0
rion to,1,1,1.0
to automatically,1,1,1.0
automatically decide,1,1,1.0
samples that,2,1,2.0
distribution criteria,1,1,1.0
criteria is,1,1,1.0
the normalized,1,1,1.0
normalized number,1,1,1.0
cases within,3,1,3.0
the neighbor,3,1,3.0
neighbor of,3,1,3.0
example fig,1,1,1.0
the proportion,1,1,1.0
by smote,1,1,1.0
adasyn based,1,1,1.0
on neighbors,1,1,1.0
examples through,1,1,1.0
through from,1,1,1.0
from fig,3,1,3.0
fig it,1,1,1.0
smote uniformly,1,1,1.0
uniformly assigns,1,1,1.0
assigns the,1,1,1.0
a differen,1,1,1.0
differen t,1,1,1.0
t distribution,1,1,1.0
to determinechen,1,1,1.0
determinechen et,1,1,1.0
to d,1,1,1.0
d a,1,1,1.0
a s,2,2,1.0
n ﬁrst,1,1,1.0
ﬁrst calculates,1,1,1.0
calculates the,1,1,1.0
of m,1,1,1.0
m ajority,1,1,1.0
ajority cases,1,1,1.0
of to,2,1,2.0
to ﬁrst,1,1,1.0
ﬁrst which,1,1,1.0
and normalizes,1,1,1.0
normalizes this,1,1,1.0
this into,1,1,1.0
to bias,1,1,1.0
generation pro,1,1,1.0
pro cess,1,1,1.0
cess algorithm,1,1,1.0
algorithm adasyn,1,1,1.0
adasyn one,1,1,1.0
one issue,1,1,1.0
issue worth,1,1,1.0
worth noting,1,1,1.0
noting is,1,1,1.0
that adasyn,5,1,5.0
adasyn does,2,1,2.0
not generate,1,1,1.0
generate instances,1,1,1.0
no majority,1,1,1.0
their neighbors,1,1,1.0
neighbors like,1,1,1.0
like in,1,1,1.0
fig ramoboost,1,1,1.0
ramoboost learning,3,1,3.0
objective of,1,1,1.0
ramoboost is,8,1,8.0
is twofold,1,1,1.0
twofold to,1,1,1.0
induction biases,1,1,1.0
biases introduced,1,1,1.0
adaptively learn,1,1,1.0
learn information,1,1,1.0
achieved in,1,1,1.0
two respects,1,1,1.0
first an,1,1,1.0
an adaptive,1,1,1.0
adaptive weight,1,1,1.0
weight adjustment,1,1,1.0
adjustment procedure,1,1,1.0
is embedded,1,1,1.0
embedded in,1,1,1.0
in ramoboost,5,1,5.0
ramoboost that,1,1,1.0
that shifts,1,1,1.0
shifts the,1,1,1.0
classes second,1,1,1.0
second a,1,1,1.0
a ranked,2,1,2.0
ranked sampling,1,1,1.0
skewed distribution,1,1,1.0
distribution motivated,1,1,1.0
smote smoteboost,1,1,1.0
smoteboost and,1,1,1.0
adasyn rithms,1,1,1.0
rithms ramoboost,1,1,1.0
ramoboost facilitates,1,1,1.0
facilitates imbalanced,1,1,1.0
by tively,1,1,1.0
tively developing,1,1,1.0
of hypotheses,1,1,1.0
hypotheses however,1,1,1.0
however unlike,1,1,1.0
unlike smote,1,1,1.0
smote which,1,1,1.0
which samples,1,1,1.0
samples minority,1,1,1.0
examples indiscriminately,1,1,1.0
indiscriminately and,1,1,1.0
and uniformly,1,1,1.0
uniformly ramoboost,1,1,1.0
ramoboost evaluates,1,1,1.0
evaluates the,1,1,1.0
potential learning,1,1,1.0
learning contribution,1,1,1.0
contribution of,1,1,1.0
and determines,1,1,1.0
determines their,1,1,1.0
their sampling,1,1,1.0
weights accordingly,1,1,1.0
accordingly this,1,1,1.0
any single,1,1,1.0
single minority,1,1,1.0
determine how,1,1,1.0
how greatly,1,1,1.0
greatly it,1,1,1.0
will beneﬁt,1,1,1.0
beneﬁt the,2,1,2.0
process before,2,1,2.0
before ofﬁcially,1,1,1.0
ofﬁcially describing,1,1,1.0
describing the,1,1,1.0
explain several,1,1,1.0
several terms,1,1,1.0
terms for,1,1,1.0
improved readability,1,1,1.0
readability in,1,1,1.0
in lieu,1,1,1.0
lieu of,1,1,1.0
directly operating,1,1,1.0
operating on,1,1,1.0
dataset d,2,1,2.0
d ramoboost,1,1,1.0
ramoboost follows,1,1,1.0
classic procedure,1,1,1.0
apply the,1,1,1.0
the boosting,6,1,6.0
boosting process,2,1,2.0
process on,1,1,1.0
the misclassiﬁed,1,1,1.0
misclassiﬁed dataset,1,1,1.0
problem b,1,1,1.0
b is,1,1,1.0
deﬁned to,1,1,1.0
is replicated,1,1,1.0
replicated n,1,1,1.0
n times,1,1,1.0
times with,1,1,1.0
different class,2,1,2.0
label other,1,1,1.0
other than,1,1,1.0
true one,1,1,1.0
one ˆri,1,1,1.0
ˆri serves,1,1,1.0
serves as,1,1,1.0
distribution function,6,1,6.0
function determining,1,1,1.0
determining the,3,1,3.0
probability that,1,1,1.0
in b,2,1,2.0
b are,1,1,1.0
instances it,1,1,1.0
is calculated,3,1,3.0
calculated by,1,1,1.0
by mapping,1,1,1.0
mapping the,1,1,1.0
b into,1,1,1.0
the range,2,1,2.0
range the,1,1,1.0
presented presented,1,1,1.0
algorithm according,1,1,1.0
this description,1,1,1.0
description the,2,1,2.0
algorithm includes,1,1,1.0
includes two,1,1,1.0
two mechanisms,1,1,1.0
mechanisms to,1,1,1.0
from anced,1,1,1.0
ﬁrst consists,1,1,1.0
of steps,1,1,1.0
where instances,1,1,1.0
are adaptively,1,1,1.0
adaptively generated,1,1,1.0
generated accord,1,1,1.0
their distributions,1,1,1.0
way more,1,1,1.0
are created,2,1,2.0
examples th,1,1,1.0
th at,1,1,1.0
at are,1,1,1.0
be classiﬁed,1,1,1.0
classiﬁed compared,1,1,1.0
is signiﬁcantly,2,1,2.0
signiﬁcantly different,1,1,1.0
algorithm where,1,1,1.0
has equal,1,1,1.0
equal weight,1,1,1.0
weight and,1,1,1.0
same numbers,2,1,2.0
second mechanism,1,1,1.0
mechanism steps,1,1,1.0
current hypothesis,1,1,1.0
hypothesis ht,2,1,2.0
ht to,1,1,1.0
to update,1,1,1.0
update the,1,1,1.0
sampling distribution,2,1,2.0
distribution dt,1,1,1.0
dt which,1,1,1.0
to sample,1,1,1.0
iteration as,2,1,2.0
step similar,1,1,1.0
similar to,5,1,5.0
algorithm ram,1,1,1.0
ram o,1,1,1.0
o boost,1,1,1.0
boost n,1,1,1.0
t α,1,1,1.0
α input,1,1,1.0
with m,1,1,1.0
m class,1,1,1.0
examples xm,1,1,1.0
xm ym,1,1,1.0
ym w,1,1,1.0
w h,2,1,2.0
e xi,1,1,1.0
the n,2,1,2.0
n dimensional,2,1,2.0
dimensional feature,1,1,1.0
space x,1,1,1.0
x and,1,1,1.0
and yi,1,1,1.0
yi y,1,1,1.0
y major,1,1,1.0
major minor,1,1,1.0
minor is,1,1,1.0
class identity,1,1,1.0
identity label,1,1,1.0
label associated,1,1,1.0
with instance,1,1,1.0
xi n,1,1,1.0
n number,1,1,1.0
data samples,3,1,3.0
generated at,2,1,2.0
iteration t,1,1,1.0
t number,2,1,2.0
of iterations,1,1,1.0
iterations the,1,1,1.0
of base,1,1,1.0
base classiﬁers,1,1,1.0
classiﬁers number,1,1,1.0
neighbors in,3,1,3.0
in adjusting,1,1,1.0
examples number,1,1,1.0
data instances,3,1,3.0
instances α,1,1,1.0
the scaling,2,1,2.0
scaling coefﬁcient,2,1,2.0
coefﬁcient let,1,1,1.0
let b,1,1,1.0
i y,7,1,7.0
m y,1,1,1.0
y initialize,1,1,1.0
initialize i,1,1,1.0
y for,1,1,1.0
for i,2,2,1.0
y b,2,1,2.0
b for,1,1,1.0
two class,1,1,1.0
class problems,1,1,1.0
problems m,1,1,1.0
m do,1,1,1.0
do for,1,1,1.0
for t,2,1,2.0
t t,1,1,1.0
t sample,1,1,1.0
the mislabeled,2,1,2.0
mislabeled training,1,1,1.0
with dt,2,1,2.0
dt and,2,1,2.0
and get,2,1,2.0
get back,3,1,3.0
back the,1,1,1.0
sampled dataset,1,1,1.0
dataset se,3,1,3.0
se of,1,1,1.0
of identical,1,1,1.0
identical size,1,1,1.0
size slice,1,1,1.0
slice se,1,1,1.0
se into,1,1,1.0
majority subset,1,1,1.0
subset and,1,1,1.0
minority subset,1,1,1.0
size mlt,1,1,1.0
mlt and,1,1,1.0
and mst,1,1,1.0
mst respectively,1,1,1.0
example xi,2,1,2.0
xi ﬁnd,1,1,1.0
se according,1,1,1.0
distance in,2,1,2.0
and calculate,1,1,1.0
calculate ri,1,1,1.0
ri deﬁned,1,1,1.0
as ri,1,1,1.0
ri exp,1,1,1.0
exp α,1,1,1.0
α δi,1,1,1.0
δi i,1,1,1.0
i mst,1,1,1.0
mst where,1,1,1.0
where δi,1,1,1.0
δi is,1,1,1.0
in examples,1,1,1.0
examples normalize,1,1,1.0
normalize ri,1,1,1.0
ri according,1,1,1.0
to ˆri,1,1,1.0
ˆri ri,1,1,1.0
ri ri,1,1,1.0
ri such,1,1,1.0
that ˆri,1,1,1.0
ˆri is,1,1,1.0
function mst,1,1,1.0
mst ri,1,1,1.0
ri deﬁne,1,1,1.0
deﬁne dt,1,1,1.0
dt ˆri,1,1,1.0
ˆri sample,1,1,1.0
sample with,1,1,1.0
back a,2,1,2.0
sampling minority,1,1,1.0
dataset gt,1,1,1.0
gt o,1,1,1.0
o fs,1,1,1.0
fs i,1,1,1.0
i z,1,1,1.0
z emst,1,1,1.0
emst for,1,1,1.0
xi gt,1,1,1.0
gt ﬁnd,1,1,1.0
in according,1,1,1.0
in n,1,1,1.0
use linear,1,1,1.0
linear inter,1,1,1.0
inter polation,1,1,1.0
polation to,1,1,1.0
generate n,1,1,1.0
n synthetic,2,1,2.0
samples provide,1,1,1.0
base classiﬁer,4,1,4.0
classiﬁer with,1,1,1.0
with sampling,1,1,1.0
sampling dataset,4,1,4.0
se and,1,1,1.0
samples get,1,1,1.0
ht x,2,1,2.0
y calculate,1,1,1.0
of ht,1,1,1.0
ht εt,1,1,1.0
εt i,1,1,1.0
b dt,1,1,1.0
dt i,3,1,3.0
y ht,1,1,1.0
ht xi,3,1,3.0
xi yi,2,1,2.0
yi ht,2,1,2.0
y set,1,1,1.0
set βt,1,1,1.0
βt εt,1,1,1.0
εt εt,1,1,1.0
εt update,1,1,1.0
update dt,1,1,1.0
y dt,1,1,1.0
y zt,1,1,1.0
zt β,1,1,1.0
β xi,1,1,1.0
t where,1,1,1.0
where zt,1,1,1.0
zt is,1,1,1.0
a normalization,1,1,1.0
normalization constant,1,1,1.0
constant end,1,1,1.0
end ieee,1,1,1.0
october output,1,1,1.0
output the,1,1,1.0
the output,1,1,1.0
output hypothesis,1,1,1.0
hypothesis h,1,1,1.0
h f,2,1,2.0
f inal,2,1,2.0
inal x,2,1,2.0
x is,1,1,1.0
calculated as,1,1,1.0
follows h,1,1,1.0
x arg,1,1,1.0
max log,1,1,1.0
log βt,1,1,1.0
βt ht,1,1,1.0
y algorithm,1,1,1.0
the anism,1,1,1.0
anism can,1,1,1.0
ﬁnal hypothesis,4,1,4.0
hypothesis toward,1,1,1.0
facilitate the,1,1,1.0
key components,1,1,1.0
components of,1,1,1.0
two ramoboost,1,1,1.0
ramoboost mechanisms,1,1,1.0
mechanisms are,1,1,1.0
are steps,1,1,1.0
steps and,2,1,2.0
by sampling,2,1,2.0
the updated,1,1,1.0
updated weight,1,1,1.0
weight distribution,1,1,1.0
function dt,1,1,1.0
dt at,1,1,1.0
the tth,1,1,1.0
tth iteration,1,1,1.0
step ramoboost,2,1,2.0
ramoboost can,9,1,9.0
can sively,1,1,1.0
sively shift,1,1,1.0
examples meanwhile,1,1,1.0
meanwhile in,1,1,1.0
ramoboost generates,2,1,2.0
generates more,1,1,1.0
for those,2,1,2.0
ples by,1,1,1.0
function obtained,1,1,1.0
from manipulating,1,1,1.0
manipulating the,3,1,3.0
example through,1,1,1.0
and analysis,1,1,1.0
methodology data,1,1,1.0
generation mechanism,5,1,5.0
mechanism the,1,1,1.0
proposed boost,1,1,1.0
boost algorithm,1,1,1.0
algorithm shares,1,1,1.0
shares some,1,1,1.0
generation aspects,1,1,1.0
aspects with,1,1,1.0
adding the,1,1,1.0
a randomized,1,1,1.0
randomized real,1,1,1.0
real number,2,1,2.0
number in,1,1,1.0
in multiplied,1,1,1.0
multiplied by,1,1,1.0
minority ple,1,1,1.0
ple under,1,1,1.0
chosen example,1,1,1.0
example within,1,1,1.0
within its,1,1,1.0
its k,1,1,1.0
dataset step,1,1,1.0
step in,1,1,1.0
the which,1,1,1.0
same way,1,1,1.0
way as,1,1,1.0
two however,1,1,1.0
however is,1,1,1.0
that ramoboost,8,1,8.0
ramoboost employs,1,1,1.0
a systematic,2,1,2.0
systematic method,1,1,1.0
adaptively determine,1,1,1.0
instances created,1,1,1.0
dataset according,1,1,1.0
their learnability,1,1,1.0
learnability more,1,1,1.0
instances will,1,1,1.0
those examples,1,1,1.0
examples therefore,2,1,2.0
hypothesis will,1,1,1.0
those difﬁcult,2,1,2.0
difﬁcult decision,1,1,1.0
decision regions,1,1,1.0
regions concretely,1,1,1.0
concretely ramoboost,1,1,1.0
neighbors across,1,1,1.0
whole sampling,1,1,1.0
dataset steps,1,1,1.0
the while,1,1,1.0
smote universally,1,1,1.0
universally generates,1,1,1.0
generates identical,1,1,1.0
identical number,1,1,1.0
example similar,1,1,1.0
to ramoboost,1,1,1.0
ramoboost adasyn,1,1,1.0
also aims,1,1,1.0
to cally,1,1,1.0
cally generate,1,1,1.0
the derlying,1,1,1.0
derlying data,1,1,1.0
distribution instead,1,1,1.0
distribution consequently,1,1,1.0
consequently adasyn,1,1,1.0
also has,1,1,1.0
to push,1,1,1.0
push the,2,1,2.0
difﬁcult regions,1,1,1.0
boundary however,1,1,1.0
however adasyn,1,1,1.0
an aggressive,1,1,1.0
aggressive manner,1,1,1.0
manner almost,1,1,1.0
very close,3,1,3.0
boundary this,2,1,2.0
is because,3,1,3.0
mechanism of,4,1,4.0
of adasyn,2,1,2.0
adasyn approves,1,1,1.0
approves generation,1,1,1.0
data only,1,1,1.0
only when,1,1,1.0
when there,1,1,1.0
there exists,1,1,1.0
exists at,1,1,1.0
majority case,1,1,1.0
consideration speciﬁcally,1,1,1.0
speciﬁcally the,3,1,3.0
consideration directly,1,1,1.0
directly dictates,1,1,1.0
dictates the,1,1,1.0
generated around,1,1,1.0
around it,1,1,1.0
the extreme,1,1,1.0
extreme case,1,1,1.0
case noisy,1,1,1.0
noisy examples,2,1,2.0
can have,1,1,1.0
have multiple,1,1,1.0
multiple synthetic,1,1,1.0
generated while,1,1,1.0
while examples,1,1,1.0
are relatively,1,1,1.0
relatively far,1,1,1.0
far away,1,1,1.0
boundary but,1,1,1.0
but are,1,1,1.0
are tive,1,1,1.0
tive of,1,1,1.0
target concept,2,1,2.0
concept of,1,1,1.0
not selected,1,1,1.0
selected for,1,1,1.0
for synthetic,3,1,3.0
contrast ramoboost,1,1,1.0
ramoboost ploys,1,1,1.0
ploys a,1,1,1.0
a logistic,1,1,1.0
logistic function,1,1,1.0
to ﬁrstly,1,1,1.0
ﬁrstly map,1,1,1.0
map δ,1,1,1.0
δ the,1,1,1.0
consideration to,2,1,2.0
number r,1,1,1.0
r between,1,1,1.0
and which,2,1,2.0
then normalized,1,1,1.0
normalized to,1,1,1.0
for determining,2,1,2.0
way ramoboost,1,1,1.0
ramoboost considers,1,1,1.0
considers all,1,1,1.0
synthetic generation,1,1,1.0
generation albeit,1,1,1.0
albeit at,1,1,1.0
at varied,1,1,1.0
varied levels,1,1,1.0
levels adasyn,1,1,1.0
adasyn additionally,1,1,1.0
additionally introduces,1,1,1.0
introduces complications,1,1,1.0
complications at,1,1,1.0
boundary since,3,1,3.0
since almost,1,1,1.0
are located,1,1,1.0
located in,1,1,1.0
region which,1,1,1.0
that excessively,1,1,1.0
excessively more,1,1,1.0
are probably,1,1,1.0
probably generated,1,1,1.0
for noisy,1,1,1.0
label phasizing,1,1,1.0
phasizing the,1,1,1.0
region may,1,1,1.0
may magnify,1,1,1.0
magnify the,1,1,1.0
of noise,2,1,2.0
noise within,1,1,1.0
dataset thereby,1,1,1.0
thereby leading,1,1,1.0
performance depreciation,1,1,1.0
depreciation ramoboost,1,1,1.0
ramoboost on,3,1,3.0
hand assigns,1,1,1.0
assigns high,1,1,1.0
high probabilities,1,1,1.0
probabilities for,1,1,1.0
examples close,1,1,1.0
boundary which,1,1,1.0
generated near,1,1,1.0
boundary on,1,1,1.0
relative basis,1,1,1.0
basis as,2,1,2.0
to an,1,1,1.0
an absolute,1,1,1.0
absolute basis,1,1,1.0
result the,2,1,2.0
negative impact,1,1,1.0
impact of,1,1,1.0
noise is,1,1,1.0
is attenuated,1,1,1.0
attenuated in,1,1,1.0
ramoboost as,2,1,2.0
as compared,1,1,1.0
to adasyn,1,1,1.0
adasyn in,1,1,1.0
ramoboost with,3,1,3.0
with that,2,1,2.0
adasyn we,1,1,1.0
a shows,1,1,1.0
and fig,1,1,1.0
b d,1,1,1.0
d shows,1,1,1.0
distribution data,1,1,1.0
distribution respectively,1,1,1.0
respectively in,1,1,1.0
in all,4,1,4.0
these ﬁgures,1,1,1.0
ﬁgures the,1,1,1.0
the plus,1,1,1.0
plus and,1,1,1.0
and point,1,1,1.0
point shapes,1,1,1.0
shapes represent,1,1,1.0
the inal,2,2,1.0
inal majority,1,1,1.0
data original,1,1,1.0
data respectively,1,1,1.0
furthermore for,1,1,1.0
each ﬁgure,2,1,2.0
ﬁgure we,2,1,2.0
also illustrate,1,1,1.0
classiﬁcation confusion,1,1,1.0
of instant,1,1,1.0
instant counts,1,1,1.0
counts for,2,1,2.0
for performance,1,1,1.0
performance assessment,1,1,1.0
assessment here,1,1,1.0
here we,2,1,2.0
we follow,1,1,1.0
follow the,1,1,1.0
the suggestions,1,1,1.0
suggestions of,1,1,1.0
classiﬁer used,1,1,1.0
make predictions,1,1,1.0
predictions on,1,1,1.0
on all,3,1,3.0
datasets shown,1,1,1.0
fig is,1,1,1.0
is classiﬁcation,1,1,1.0
regression tree,1,1,1.0
tree comparing,1,1,1.0
ramoboost method,5,1,5.0
can improve,1,1,1.0
improve classiﬁcation,1,1,1.0
performance speciﬁcally,1,1,1.0
improvement of,1,1,1.0
of true,1,1,1.0
tn counts,2,1,2.0
dataset changes,1,1,1.0
changes from,1,1,1.0
to while,1,1,1.0
while for,1,1,1.0
ramoboost it,1,1,1.0
it increases,1,1,1.0
increases from,1,1,1.0
example while,1,1,1.0
while in,1,1,1.0
ramoboost the,2,1,2.0
generation process,1,1,1.0
is adaptive,1,1,1.0
adaptive according,1,1,1.0
distribution from,1,1,1.0
c we,1,1,1.0
adasyn is,2,1,2.0
very aggressive,1,1,1.0
aggressive in,1,1,1.0
instances very,2,1,2.0
have two,1,1,1.0
two effects,1,1,1.0
performance it,1,1,1.0
may increasechen,1,1,1.0
increasechen et,1,1,1.0
boosting a,1,1,1.0
c d,2,1,2.0
d hypothesis,1,1,1.0
hypothesis outputy,4,1,4.0
outputy p,4,1,4.0
class n,4,1,4.0
n n,4,1,4.0
n hypothesis,3,1,3.0
n fig,1,1,1.0
fig comparison,1,1,1.0
mechanisms a,1,1,1.0
a original,1,1,1.0
data distribu,2,1,2.0
distribu tion,2,1,2.0
tion majority,1,1,1.0
and mi,1,1,1.0
mi nority,2,2,1.0
nority examples,1,1,1.0
examples b,1,1,1.0
b data,1,1,1.0
distribution after,1,1,1.0
method c,1,1,1.0
c data,1,1,1.0
tion after,1,1,1.0
after adasyn,1,1,1.0
adasyn method,1,1,1.0
method d,1,1,1.0
d data,1,1,1.0
data dis,1,1,1.0
dis tribution,1,1,1.0
tribution after,1,1,1.0
after ramoboost,1,1,1.0
classiﬁcation accuracy,2,1,2.0
it provides,1,1,1.0
good representation,1,1,1.0
distribution close,1,1,1.0
boundary thereby,1,1,1.0
thereby improving,1,1,1.0
improving the,1,1,1.0
recall performance,2,1,2.0
performance which,1,1,1.0
be discussed,1,1,1.0
discussed in,1,1,1.0
detail in,1,1,1.0
section however,1,1,1.0
also decrease,1,1,1.0
decrease the,1,1,1.0
the jority,1,1,1.0
jority class,1,1,1.0
turn deteriorates,1,1,1.0
deteriorates the,1,1,1.0
overall classiﬁcation,1,1,1.0
performance one,1,1,1.0
can observe,1,1,1.0
observe from,1,1,1.0
c that,1,1,1.0
examples under,1,1,1.0
the adasyn,1,1,1.0
adasyn technique,1,1,1.0
best among,1,1,1.0
among all,1,1,1.0
methods true,1,1,1.0
tp therefore,1,1,1.0
therefore recall,1,1,1.0
the tn,1,1,1.0
counts of,1,1,1.0
also decreases,1,1,1.0
decreases signiﬁcantly,1,1,1.0
signiﬁcantly the,1,1,1.0
the lowest,1,1,1.0
lowest of,1,1,1.0
all in,1,1,1.0
case with,1,1,1.0
with tn,1,1,1.0
tn to,1,1,1.0
end ramoboost,1,1,1.0
adasyn to,1,1,1.0
overall learning,1,1,1.0
performance our,1,1,1.0
our simulation,4,1,4.0
simulation analyses,1,1,1.0
various datasets,1,1,1.0
and various,1,1,1.0
various assessment,2,1,2.0
section also,1,1,1.0
also conﬁrm,1,1,1.0
conﬁrm this,1,1,1.0
procedure boosting,1,1,1.0
boosting has,1,1,1.0
attracted cantly,1,1,1.0
cantly increased,1,1,1.0
increased attention,1,1,1.0
attention recently,1,1,1.0
recently in,1,1,1.0
computational gence,1,1,1.0
gence community,1,1,1.0
our proposed,1,1,1.0
ramoboost approach,1,1,1.0
boosting algorithm,3,1,3.0
is essentially,1,1,1.0
essentially the,1,1,1.0
classic rather,1,1,1.0
than reducing,1,1,1.0
prediction error,1,1,1.0
error on,1,1,1.0
loop in,1,1,1.0
a stepwise,1,1,1.0
stepwise manner,1,1,1.0
manner the,1,1,1.0
algorithm of,1,1,1.0
of can,1,1,1.0
can focus,1,1,1.0
focus the,1,1,1.0
learner on,1,1,1.0
labels that,1,1,1.0
are hardest,1,1,1.0
hardest to,1,1,1.0
to discriminate,1,1,1.0
discriminate by,1,1,1.0
as deﬁned,1,1,1.0
deﬁned in,1,1,1.0
words the,1,1,1.0
emphasis of,1,1,1.0
of is,1,1,1.0
the incorrect,1,1,1.0
incorrect class,1,1,1.0
label as,1,1,1.0
as distinguishable,1,1,1.0
distinguishable as,1,1,1.0
possible from,1,1,1.0
the correct,3,1,3.0
correct class,1,1,1.0
label belonging,1,1,1.0
consideration this,1,1,1.0
mislabeled dataset,1,1,1.0
is sampled,1,1,1.0
sampled in,1,1,1.0
loop instead,1,1,1.0
correct one,1,1,1.0
one compared,1,1,1.0
to enables,1,1,1.0
enables the,1,1,1.0
make useful,1,1,1.0
useful contributions,1,1,1.0
contributions to,1,1,1.0
hypothesis even,1,1,1.0
even when,1,1,1.0
weak hypothesis,1,1,1.0
hypothesis does,1,1,1.0
not predict,1,1,1.0
correct label,1,1,1.0
label with,1,1,1.0
a probability,1,1,1.0
probability greater,1,1,1.0
way the,1,1,1.0
the iteration,1,1,1.0
loop is,1,1,1.0
not broken,1,1,1.0
broken regardless,1,1,1.0
hypothesis which,1,1,1.0
other boosting,1,1,1.0
including given,1,1,1.0
generally very,1,1,1.0
very hard,1,1,1.0
hard for,1,1,1.0
single weak,1,1,1.0
to extract,1,1,1.0
extract sufﬁcient,1,1,1.0
sufﬁcient knowledge,1,1,1.0
dataset at,1,1,1.0
one instance,1,1,1.0
ramoboost and,5,1,5.0
algorithms may,1,1,1.0
from an,1,1,1.0
an insufﬁcient,1,1,1.0
insufﬁcient number,1,1,1.0
boosting iterations,2,1,2.0
iterations thus,1,1,1.0
thus for,1,1,1.0
for an,1,1,1.0
imbalanced ieee,1,1,1.0
october of,1,1,1.0
any size,1,1,1.0
size whose,1,1,1.0
whose target,1,1,1.0
concept we,1,1,1.0
we assume,2,1,2.0
assume is,1,1,1.0
is difﬁcult,1,1,1.0
learn it,1,1,1.0
is our,2,1,2.0
our belief,1,1,1.0
belief that,1,1,1.0
be guaranteed,1,1,1.0
guaranteed satisfactory,1,1,1.0
satisfactory if,1,1,1.0
if it,1,1,1.0
can iterate,1,1,1.0
iterate for,1,1,1.0
for enough,1,1,1.0
enough epochs,1,1,1.0
epochs this,1,1,1.0
our motivation,1,1,1.0
for employing,1,1,1.0
employing the,1,1,1.0
ramoboost computational,1,1,1.0
analysis in,1,1,1.0
training stage,1,1,1.0
stage the,2,1,2.0
ramoboost arises,1,1,1.0
arises from,1,1,1.0
the construction,1,1,1.0
construction of,1,1,1.0
loop as,1,1,1.0
procedure we,1,1,1.0
assume the,1,1,1.0
following in,1,1,1.0
our analysis,1,1,1.0
analysis dimension,1,1,1.0
space number,1,1,1.0
examples ratio,1,1,1.0
dataset p,1,1,1.0
boosting epochs,1,1,1.0
epochs the,1,1,1.0
procedure of,2,1,2.0
of generatin,1,1,1.0
generatin g,1,1,1.0
g synthetic,1,1,1.0
is initialized,1,1,1.0
initialized with,1,1,1.0
the calculation,1,1,1.0
calculation of,1,1,1.0
time complexity,9,1,9.0
complexity can,1,1,1.0
be decomposed,1,1,1.0
decomposed into,1,1,1.0
three steps,1,1,1.0
steps calculating,1,1,1.0
the euc,1,1,1.0
euc lidean,1,1,1.0
lidean distance,1,1,1.0
training o,1,1,1.0
o mn,2,1,2.0
mn sorting,1,1,1.0
sorting all,1,1,1.0
all current,1,1,1.0
current euclidean,1,1,1.0
distance calculations,1,1,1.0
calculations in,1,1,1.0
in ascending,1,1,1.0
ascending o,1,1,1.0
n log,4,1,4.0
log n,8,1,8.0
n r,1,1,1.0
e v,1,1,1.0
v i,1,1,1.0
n gt,1,1,1.0
gt h,1,1,1.0
h eﬁ,1,1,1.0
eﬁ r,1,1,1.0
r s,1,1,1.0
s examples,1,1,1.0
examples corresponding,1,1,1.0
corresponding to,3,1,3.0
ﬁrst items,1,1,1.0
items in,1,1,1.0
the sorted,1,1,1.0
sorted euclidean,1,1,1.0
distance complexity,1,1,1.0
complexity o,1,1,1.0
o thus,1,1,1.0
this step,1,1,1.0
step should,1,1,1.0
be o,2,1,2.0
mn n,1,1,1.0
in typical,1,1,1.0
typical situations,1,1,1.0
situations and,1,1,1.0
are both,1,1,1.0
both signiﬁcantly,1,1,1.0
signiﬁcantly smaller,1,1,1.0
than n,1,1,1.0
n which,2,1,2.0
complexity to,1,1,1.0
to approximately,1,1,1.0
approximately o,1,1,1.0
n the,3,1,3.0
minority neighbors,1,1,1.0
this calculation,1,1,1.0
calculation is,1,1,1.0
same with,1,1,1.0
step except,1,1,1.0
except that,1,1,1.0
distance calcula,1,1,1.0
calcula tion,1,1,1.0
tion is,1,1,1.0
is between,1,1,1.0
complexity is,1,1,1.0
no greater,1,1,1.0
than o,1,1,1.0
n since,1,1,1.0
since there,3,1,3.0
are altogether,1,1,1.0
altogether np,1,1,1.0
np minority,1,1,1.0
total time,1,1,1.0
complexity should,1,1,1.0
o log,4,1,4.0
be simpliﬁed,1,1,1.0
simpliﬁed to,1,1,1.0
to o,1,1,1.0
n lastly,1,1,1.0
lastly since,1,1,1.0
since data,1,1,1.0
generation is,2,1,2.0
applied in,1,1,1.0
iteration the,1,1,1.0
generation for,1,1,1.0
is o,2,1,2.0
n for,1,1,1.0
network with,2,1,2.0
with multilayer,1,1,1.0
mlp the,1,1,1.0
process excluding,1,1,1.0
excluding thetic,1,1,1.0
thetic data,2,1,2.0
at worse,1,1,1.0
worse o,1,1,1.0
o therefore,1,1,1.0
we summarize,1,1,1.0
summarize that,1,1,1.0
training process,1,1,1.0
process time,1,1,1.0
with mlp,2,1,2.0
mlp as,1,1,1.0
classiﬁer is,1,1,1.0
testing stage,1,1,1.0
computational operation,1,1,1.0
each hypothesis,1,1,1.0
is just,1,1,1.0
just a,1,1,1.0
comparison operation,1,1,1.0
operation the,1,1,1.0
time tion,1,1,1.0
tion for,2,1,2.0
them is,1,1,1.0
small since,1,1,1.0
weighted combination,1,1,1.0
all trained,1,1,1.0
hypothesis as,1,1,1.0
of predicting,1,1,1.0
instance can,1,1,1.0
estimated as,1,1,1.0
as o,1,1,1.0
o t,1,1,1.0
t iv,1,1,1.0
simulation and,1,1,1.0
and discussion,1,1,1.0
discussion in,1,1,1.0
we conduct,1,1,1.0
conduct various,1,1,1.0
various simulations,1,1,1.0
simulations of,2,1,2.0
and compare,1,1,1.0
compare its,1,1,1.0
its performance,1,1,1.0
performance table,2,1,2.0
table i,3,2,1.5
i summary,1,1,1.0
the datas,1,1,1.0
datas et,2,2,1.0
et characteristics,1,1,1.0
characteristics sorted,1,1,1.0
sorted in,1,1,1.0
class skew,1,1,1.0
skew dataset,1,1,1.0
dataset feature,1,1,1.0
feature data,1,1,1.0
data minority,1,1,1.0
majority imbalanced,2,1,2.0
imbalanced instances,1,1,1.0
instances instances,1,1,1.0
instances ratio,1,1,1.0
ratio sonar,1,1,1.0
sonar spambase,5,1,5.0
spambase ionosphere,5,1,5.0
ionosphere pid,5,1,5.0
pid wine,5,1,5.0
wine german,5,1,5.0
german phoneme,5,1,5.0
vehicle texture,5,1,5.0
texture segment,6,1,6.0
segment satimage,6,1,6.0
satimage vow,6,1,6.0
vow e,7,1,7.0
l abalone,6,1,6.0
abalone glass,6,1,6.0
glass yeast,6,1,6.0
yeast letter,6,1,6.0
letter shuttle,5,1,5.0
shuttle with,1,1,1.0
with smoteboost,2,1,2.0
smoteboost smote,42,1,42.0
adasyn adacost,40,1,40.0
adacost linesmote,1,1,1.0
linesmote and,1,1,1.0
and across,1,1,1.0
across different,2,1,2.0
the neural,2,1,2.0
mlp is,3,1,3.0
employed as,2,1,2.0
base learner,2,1,2.0
learner the,2,1,2.0
is conﬁgured,1,1,1.0
conﬁgured as,1,1,1.0
of hidden,3,1,3.0
hidden layer,3,1,3.0
layer neurons,3,1,3.0
neurons is,3,1,3.0
is set,8,1,8.0
be four,1,1,1.0
four and,1,1,1.0
of input,1,1,1.0
input neurons,1,1,1.0
is equal,3,1,3.0
dataset similar,1,1,1.0
to most,1,1,1.0
existing imbalanced,2,1,2.0
in literature,2,1,2.0
literature we,1,1,1.0
also consider,1,1,1.0
consider only,2,1,2.0
only imbalanced,1,1,1.0
imbalanced problems,2,1,2.0
our current,2,1,2.0
study therefore,1,1,1.0
of output,1,1,1.0
output neurons,1,1,1.0
all simulations,2,1,2.0
simulations the,1,1,1.0
the sigmoid,1,1,1.0
sigmoid function,1,1,1.0
function is,1,1,1.0
the activation,1,1,1.0
activation function,1,1,1.0
function and,1,1,1.0
the inner,1,1,1.0
inner training,1,1,1.0
training epochs,1,1,1.0
epochs is,1,1,1.0
be with,1,1,1.0
learning rate,1,1,1.0
of due,1,1,1.0
the concern,1,1,1.0
concern that,1,1,1.0
the scattered,1,1,1.0
scattered feature,1,1,1.0
feature distribution,1,1,1.0
some datasets,1,1,1.0
datasets may,2,1,2.0
may hinder,1,1,1.0
hinder the,1,1,1.0
network from,1,1,1.0
from converging,1,1,1.0
converging fast,1,1,1.0
fast enough,1,1,1.0
enough for,1,1,1.0
parameter acceleration,1,1,1.0
acceleration process,1,1,1.0
before all,1,1,1.0
to t,1,1,1.0
t he,4,2,2.0
he comparative,1,1,1.0
comparative algorithms,25,1,25.0
learning we,1,1,1.0
ﬁrst use,1,1,1.0
nonlinear normalization,1,1,1.0
normalization approach,1,1,1.0
to normalize,1,1,1.0
normalize the,1,1,1.0
the features,2,2,1.0
features of,2,2,1.0
datasets to,1,1,1.0
to reside,1,1,1.0
reside in,1,1,1.0
the interval,2,1,2.0
interval dataset,1,1,1.0
dataset description,1,1,1.0
is evaluated,1,1,1.0
evaluated on,2,1,2.0
repository and,1,1,1.0
and elena,1,1,1.0
elena project,2,1,2.0
project these,1,1,1.0
datasets vary,1,1,1.0
vary in,1,1,1.0
in size,1,1,1.0
size and,1,1,1.0
distributions to,1,1,1.0
ensure a,1,1,1.0
thorough assessment,1,1,1.0
assessment of,2,1,2.0
i summarizes,1,1,1.0
simulation since,1,1,1.0
since several,1,1,1.0
several of,1,1,1.0
original datasets,1,1,1.0
are multiclass,1,1,1.0
multiclass data,1,1,1.0
data we,1,1,1.0
we modiﬁed,1,1,1.0
modiﬁed those,1,1,1.0
those datasets,1,1,1.0
datasets following,1,1,1.0
following suggestions,1,1,1.0
suggestions in,1,1,1.0
literature to,1,1,1.0
make them,1,1,1.0
them into,1,1,1.0
into datasets,1,1,1.0
table ii,2,1,2.0
ii shows,1,1,1.0
the modiﬁcations,1,1,1.0
modiﬁcations that,1,1,1.0
we used,1,1,1.0
classes assessment,1,1,1.0
metrics under,1,1,1.0
learning scenario,1,1,1.0
scenario the,1,1,1.0
the conventional,1,1,1.0
conventional assessment,1,1,1.0
assessment method,1,1,1.0
method of,1,1,1.0
single criterion,1,1,1.0
criterion such,1,1,1.0
as overallchen,1,1,1.0
overallchen et,1,1,1.0
boosting table,4,1,4.0
ii description,1,1,1.0
description of,1,1,1.0
dataset minority,1,1,1.0
class sonar,1,1,1.0
sonar class,1,1,1.0
class r,1,1,1.0
r rock,1,1,1.0
rock instances,1,1,1.0
instances class,1,1,1.0
class m,1,1,1.0
m metal,1,1,1.0
metal cylinder,1,1,1.0
cylinder instances,1,1,1.0
instances spambase,1,1,1.0
spambase spam,1,1,1.0
spam email,1,1,1.0
email legitimate,1,1,1.0
legitimate email,1,1,1.0
email ionosphere,1,1,1.0
ionosphere bad,1,1,1.0
bad radar,1,1,1.0
radar class,2,1,2.0
class good,1,1,1.0
good radar,1,1,1.0
class pid,1,1,1.0
pid positive,1,1,1.0
class negative,1,1,1.0
class wine,1,1,1.0
wine class,1,1,1.0
class classes,2,1,2.0
and german,1,1,1.0
german customers,1,1,1.0
customers with,2,1,2.0
with bad,1,1,1.0
bad credit,1,1,1.0
credit customers,1,1,1.0
with good,1,1,1.0
good credit,1,1,1.0
credit phoneme,1,1,1.0
phoneme class,1,1,1.0
of oral,1,1,1.0
sounds class,2,1,2.0
class class,1,1,1.0
of nasal,1,1,1.0
nasal sounds,1,1,1.0
class vehicle,1,1,1.0
vehicle class,1,1,1.0
of van,1,1,1.0
van classes,1,1,1.0
of opel,1,1,1.0
opel saas,1,1,1.0
saas and,1,1,1.0
and bus,1,1,1.0
bus texture,1,1,1.0
texture classes,1,1,1.0
and classes,1,1,1.0
and segment,1,1,1.0
segment class,1,1,1.0
of brickface,1,1,1.0
brickface classes,1,1,1.0
of sky,1,1,1.0
sky foliage,1,1,1.0
foliage cement,1,1,1.0
cement window,1,1,1.0
window path,1,1,1.0
path and,1,1,1.0
grass classes,1,1,1.0
of horizontal,1,1,1.0
horizontal line,1,1,1.0
line graphic,1,1,1.0
graphic class,1,1,1.0
of text,1,1,1.0
text vertical,1,1,1.0
vertical line,1,1,1.0
line and,1,1,1.0
and picture,1,1,1.0
picture satimage,1,1,1.0
satimage class,1,1,1.0
of damp,1,1,1.0
damp grey,2,1,2.0
grey soil,3,1,3.0
soil classes,1,1,1.0
of red,1,1,1.0
red soil,1,1,1.0
soil cotton,1,1,1.0
cotton crop,1,1,1.0
crop grey,1,1,1.0
soil soil,1,1,1.0
soil with,1,1,1.0
with vegetation,1,1,1.0
vegetation stubble,1,1,1.0
stubble and,1,1,1.0
very damp,1,1,1.0
soil class,1,1,1.0
of digit,1,1,1.0
digit classes,1,1,1.0
of digits,1,1,1.0
digits and,1,1,1.0
and vow,1,1,1.0
l class,1,1,1.0
to abalone,1,1,1.0
abalone class,1,1,1.0
of glass,1,1,1.0
glass class,1,1,1.0
class tableware,1,1,1.0
tableware all,1,1,1.0
classes yeast,1,1,1.0
yeast class,1,1,1.0
of pox,1,1,1.0
pox class,1,1,1.0
of cyt,1,1,1.0
cyt letter,1,1,1.0
letter class,1,1,1.0
of letter,1,1,1.0
letter z,1,1,1.0
z classes,1,1,1.0
of letters,1,1,1.0
letters a,1,1,1.0
a y,1,1,1.0
y shuttle,1,1,1.0
shuttle class,1,1,1.0
of fpv,1,1,1.0
fpv close,1,1,1.0
close classes,1,1,1.0
of rad,1,1,1.0
rad flow,1,1,1.0
flow fpv,1,1,1.0
fpv open,1,1,1.0
open high,1,1,1.0
high bypass,1,1,1.0
bypass bpv,1,1,1.0
bpv close,1,1,1.0
close and,1,1,1.0
and bpv,1,1,1.0
bpv open,1,1,1.0
open accuracy,1,1,1.0
accuracy oa,1,1,1.0
oa may,1,1,1.0
be able,1,1,1.0
comprehensive assessment,1,1,1.0
algorithm considering,1,1,1.0
simple case,1,1,1.0
with minority,1,1,1.0
a naïve,1,1,1.0
naïve approach,1,1,1.0
classifying every,1,1,1.0
every example,1,1,1.0
example to,2,1,2.0
can at,1,1,1.0
best provide,1,1,1.0
an oa,1,1,1.0
oa of,1,1,1.0
as biomedical,1,1,1.0
analysis suc,1,1,1.0
suc h,1,1,1.0
performance would,1,1,1.0
be unacceptable,1,1,1.0
unacceptable as,1,1,1.0
it misclassiﬁes,1,1,1.0
misclassiﬁes all,1,1,1.0
minority cases,1,1,1.0
cases which,1,1,1.0
which generally,1,1,1.0
generally are,1,1,1.0
important in,1,1,1.0
such situations,1,1,1.0
situations as,1,1,1.0
the oa,1,1,1.0
oa by,1,1,1.0
by itself,1,1,1.0
itself may,1,1,1.0
be sufﬁcient,1,1,1.0
sufﬁcient in,1,1,1.0
evaluating the,1,1,1.0
performance fo,1,1,1.0
r imbalanced,1,1,1.0
our simulations,1,1,1.0
simulations we,1,1,1.0
we adopt,2,1,2.0
adopt ﬁve,1,1,1.0
ﬁve major,1,1,1.0
major assessment,1,1,1.0
metrics related,1,1,1.0
for analysis,1,1,1.0
analysis oa,1,1,1.0
oa precision,4,1,4.0
the detailed,3,1,3.0
detailed discussions,1,1,1.0
discussions on,1,1,1.0
these metrics,1,1,1.0
their appli,1,1,1.0
appli cations,1,1,1.0
cations for,1,1,1.0
learning can,1,1,1.0
addition to,1,1,1.0
these singular,1,1,1.0
singular assessment,1,1,1.0
metrics we,1,1,1.0
also adopted,1,1,1.0
adopted the,2,1,2.0
graph for,1,1,1.0
for evaluation,1,1,1.0
evaluation in,1,1,1.0
paper brieﬂy,1,1,1.0
brieﬂy speaking,1,1,1.0
speaking the,1,1,1.0
established by,1,1,1.0
by plotting,1,1,1.0
plotting the,1,1,1.0
the tps,1,1,1.0
tps rate,1,1,1.0
tp over,1,1,1.0
over false,1,1,1.0
positives rate,1,1,1.0
rate fp,1,1,1.0
fp h,1,1,1.0
e roc,1,1,1.0
curves are,1,1,1.0
are normally,1,1,1.0
normally formulated,1,1,1.0
formulated by,1,1,1.0
threshold to,1,1,1.0
a seri,1,1,1.0
seri es,1,1,1.0
es of,1,1,1.0
different classiﬁers,1,1,1.0
classiﬁers performance,1,1,1.0
case one,2,1,2.0
one generally,1,1,1.0
generally uses,1,1,1.0
an evaluation,1,1,1.0
evaluation criterion,1,1,1.0
criterion a,1,1,1.0
detailed discussion,1,1,1.0
its assessment,1,1,1.0
assessment for,1,1,1.0
for classiﬁer,1,1,1.0
classiﬁer performances,1,1,1.0
performances can,1,1,1.0
are other,2,1,2.0
other metrics,1,1,1.0
can potentially,1,1,1.0
potentially be,1,1,1.0
was recently,1,1,1.0
recently presented,1,1,1.0
that h,1,1,1.0
h could,1,1,1.0
a qualiﬁed,1,1,1.0
qualiﬁed alternative,1,1,1.0
alternative metric,1,1,1.0
metric of,1,1,1.0
of auc,6,1,6.0
auc the,1,1,1.0
major motivation,1,1,1.0
motivation of,1,1,1.0
h is,1,1,1.0
that auc,1,1,1.0
auc is,1,1,1.0
is equivalent,1,1,1.0
equivalent to,1,1,1.0
to averaging,1,1,1.0
averaging the,1,1,1.0
the misclassiﬁcation,1,1,1.0
misclassiﬁcation loss,1,1,1.0
loss over,1,1,1.0
cost ratio,1,1,1.0
ratio distribution,1,1,1.0
distribution dependent,1,1,1.0
dependent on,1,1,1.0
score distributions,2,1,2.0
distributions which,1,1,1.0
are decided,1,1,1.0
decided by,1,1,1.0
classiﬁer itself,1,1,1.0
itself rather,1,1,1.0
curve averaged,1,1,1.0
averaged roc,14,1,14.0
curve roc,1,1,1.0
curve point,2,1,2.0
point on,5,1,5.0
on roc,2,1,2.0
curve l,1,1,1.0
l point,1,1,1.0
on averaged,1,1,1.0
curve x,1,1,1.0
x fig,1,1,1.0
fig vertical,1,1,1.0
vertical averaging,3,1,3.0
averaging approach,2,1,2.0
curves target,1,1,1.0
target dataset,1,1,1.0
introduce undesired,1,1,1.0
undesired subjectivity,1,1,1.0
subjectivity into,1,1,1.0
into performance,1,1,1.0
performance evaluation,1,1,1.0
evaluation h,1,1,1.0
h targets,1,1,1.0
ﬂaw by,1,1,1.0
by decoupling,1,1,1.0
decoupling the,1,1,1.0
weight function,2,1,2.0
for loss,1,1,1.0
loss calculation,1,1,1.0
calculation from,1,1,1.0
from score,1,1,1.0
distributions for,1,1,1.0
can apply,1,1,1.0
a beta,1,1,1.0
beta distribution,1,1,1.0
to simulate,1,1,1.0
simulate the,1,1,1.0
which ensures,1,1,1.0
ensures objectivity,1,1,1.0
objectivity across,1,1,1.0
algorithms under,2,1,2.0
under comparison,1,1,1.0
comparison the,1,1,1.0
the interested,1,1,1.0
interested reader,1,1,1.0
reader can,1,1,1.0
for further,1,1,1.0
further details,1,1,1.0
on h,1,1,1.0
h and,1,1,1.0
ical review,1,1,1.0
assessment met,1,1,1.0
met rics,1,1,1.0
rics for,1,1,1.0
to reﬂect,1,1,1.0
curve characteristics,1,1,1.0
characteristics for,1,1,1.0
random runs,17,1,17.0
runs we,1,1,1.0
the vertical,3,1,3.0
to plot,1,1,1.0
plot the,1,1,1.0
the averaged,12,1,12.0
curves our,1,1,1.0
averaging method,1,1,1.0
is illustrated,1,1,1.0
illustrated in,1,1,1.0
fig assume,1,1,1.0
assume one,1,1,1.0
to average,1,1,1.0
average two,1,1,1.0
two roc,1,1,1.0
curves and,1,1,1.0
is formed,1,1,1.0
formed by,1,1,1.0
a series,1,1,1.0
series of,1,1,1.0
to evenly,1,1,1.0
evenly divide,1,1,1.0
of fp,1,1,1.0
fp into,1,1,1.0
of intervals,1,1,1.0
intervals then,1,1,1.0
then at,1,1,1.0
each interval,1,1,1.0
interval ﬁnd,1,1,1.0
october table,3,1,3.0
table iii,4,1,4.0
iii eva,1,1,1.0
eva l,1,1,1.0
l uat,1,1,1.0
uat i,1,1,1.0
o nmetrics,1,1,1.0
nmetrics and,1,1,1.0
and performance,1,1,1.0
comparison dataset,1,1,1.0
dataset methods,2,1,2.0
methods oa,2,1,2.0
auc sonar,1,1,1.0
sonar ramoboost,1,1,1.0
ramoboost smoteboost,39,1,39.0
adacost borderlinesmote,37,1,37.0
borderlinesmote spambase,1,1,1.0
spambase ramoboost,1,1,1.0
borderlinesmote ionosphere,1,1,1.0
ionosphere ramoboost,1,1,1.0
borderlinesmote pid,1,1,1.0
pid ramoboost,1,1,1.0
borderlinesmote wine,1,1,1.0
wine ramoboost,1,1,1.0
borderlinesmote german,1,1,1.0
german ramoboost,1,1,1.0
borderlinesmote phoneme,1,1,1.0
phoneme ramoboost,1,1,1.0
borderlinesmote vehicle,1,1,1.0
vehicle ramoboost,1,1,1.0
borderlinesmote texture,2,1,2.0
texture ramoboost,1,1,1.0
borderlinesmote segment,1,1,1.0
segment ramoboost,1,1,1.0
borderlinesmote et,1,1,1.0
boosting dataset,1,1,1.0
auc ramoboost,1,1,1.0
borderlinesmote satimage,1,1,1.0
satimage ramoboost,1,1,1.0
borderlinesmote ramoboost,3,1,3.0
borderlinesmote vowel,1,1,1.0
vowel ramoboost,1,1,1.0
borderlinesmote abalone,1,1,1.0
abalone ramoboost,1,1,1.0
borderlinesmote glass,1,1,1.0
glass ramoboost,1,1,1.0
borderlinesmote yeast,1,1,1.0
yeast ramoboost,1,1,1.0
borderlinesmote letter,1,1,1.0
letter ramoboost,1,1,1.0
borderlinesmote shuttle,1,1,1.0
shuttle ramoboost,1,1,1.0
borderlinesmote tp,1,1,1.0
tp values,2,1,2.0
each roc,1,1,1.0
curve and,1,1,1.0
and average,1,1,1.0
average them,1,1,1.0
fig and,1,1,1.0
points from,1,1,1.0
and corresponding,1,1,1.0
interval fp,1,1,1.0
fp by,1,1,1.0
by averaging,1,1,1.0
averaging their,1,1,1.0
their tp,1,1,1.0
corresponding roc,1,1,1.0
roc point,1,1,1.0
obtained however,1,1,1.0
some roc,1,1,1.0
curves that,1,1,1.0
have corresponding,1,1,1.0
corresponding points,1,1,1.0
certain intervals,1,1,1.0
intervals in,1,1,1.0
interpolation method,1,1,1.0
roc points,1,1,1.0
points for,1,1,1.0
fig the,1,1,1.0
point corresponding,1,1,1.0
to fp,1,1,1.0
fp is,1,1,1.0
calculated based,1,1,1.0
two neighboring,1,1,1.0
neighboring points,1,1,1.0
points and,1,1,1.0
and n,1,1,1.0
c is,1,1,1.0
be averaged,1,1,1.0
averaged with,1,1,1.0
with to,1,1,1.0
to get,1,1,1.0
get the,1,1,1.0
corresponding point,1,1,1.0
curve our,1,1,1.0
our auc,1,1,1.0
auc results,3,1,3.0
results presented,2,1,2.0
section are,1,1,1.0
the average,2,1,2.0
average of,2,1,2.0
all random,1,1,1.0
runs according,1,1,1.0
vertical aver,1,1,1.0
aver aging,1,1,1.0
aging ieee,1,1,1.0
f ramoboost,1,1,1.0
borderlinesmote smote,2,1,2.0
adacost borerlinesmote,1,1,1.0
borerlinesmote ramoboost,1,1,1.0
smoteboost smoteboost,1,1,1.0
adasyn borderlinesmote,9,1,9.0
ramoboost adacost,3,1,3.0
adacost ramoboost,1,1,1.0
smote borderlinesmote,1,1,1.0
borderlinesmote adasyn,1,1,1.0
adacost smoteboost,1,1,1.0
smoteboost ramoboost,2,1,2.0
smoteboost adacostsmote,1,1,1.0
adacostsmote adasyn,1,1,1.0
ramoboost fig,1,1,1.0
fig averaged,1,1,1.0
curves for,7,1,7.0
smote adas,1,1,1.0
adas yn,1,1,1.0
yn adacost,1,1,1.0
borderlinesmote and,7,1,7.0
methods a,1,1,1.0
a averaged,1,1,1.0
for german,1,1,1.0
german dataset,1,1,1.0
dataset b,1,1,1.0
b averaged,1,1,1.0
for ionosphere,1,1,1.0
ionosphere dataset,1,1,1.0
dataset c,1,1,1.0
c averaged,1,1,1.0
for dataset,1,1,1.0
d averaged,1,1,1.0
for phoneme,1,1,1.0
dataset e,1,1,1.0
e averaged,1,1,1.0
for satim,1,1,1.0
satim age,1,1,1.0
age dataset,1,1,1.0
dataset f,1,1,1.0
f averaged,1,1,1.0
for abalone,1,1,1.0
dataset table,1,1,1.0
table iv,3,1,3.0
iv simula,1,1,1.0
simula tion,20,1,20.0
tion s,5,1,5.0
s ignificance,12,1,12.0
ignificance test,12,1,12.0
test of,12,1,12.0
of averaged,10,1,10.0
averaged auc,17,1,17.0
auc b,9,1,9.0
b etween,8,1,8.0
etween ramob,8,1,8.0
oost and,12,1,12.0
and smoteb,1,1,1.0
smoteb oost,11,1,11.0
oost dataset,1,1,1.0
smoteboost difference,1,1,1.0
difference rank,2,1,2.0
rank sonar,2,1,2.0
shuttle n,1,1,1.0
the signiﬁcance,13,1,13.0
signiﬁcance of,1,1,1.0
the simulation,7,1,7.0
simulation results,14,1,14.0
the comparative,8,1,8.0
algorithms wilcoxon,1,1,1.0
wilcoxon test,5,1,5.0
paper wilcoxon,1,1,1.0
metric statistical,1,1,1.0
statistical procedure,1,1,1.0
comparing two,1,1,1.0
two samples,1,1,1.0
are paired,1,1,1.0
paired or,1,1,1.0
or related,1,1,1.0
related it,1,1,1.0
assumes commensurability,1,1,1.0
commensurability of,1,1,1.0
of ferences,1,1,1.0
ferences but,1,1,1.0
only qualitatively,1,1,1.0
qualitatively greater,1,1,1.0
greater differences,1,1,1.0
differences still,1,1,1.0
still count,1,1,1.0
count more,1,1,1.0
more which,1,1,1.0
is probably,2,1,2.0
probably desired,1,1,1.0
desired but,1,1,1.0
the absolute,1,1,1.0
absolute magnitudes,1,1,1.0
magnitudes are,1,1,1.0
are ignored,1,1,1.0
ignored from,1,1,1.0
statistical point,2,1,2.0
of view,2,1,2.0
view the,1,1,1.0
is safer,1,1,1.0
safer since,1,1,1.0
not assume,1,1,1.0
assume normal,1,1,1.0
normal distributions,1,1,1.0
distributions also,1,1,1.0
also outliers,1,1,1.0
outliers exceptionally,1,1,1.0
exceptionally performances,1,1,1.0
performances on,1,1,1.0
few datasets,1,1,1.0
have less,1,1,1.0
less effect,1,1,1.0
the wilcoxon,1,1,1.0
wilcoxon than,1,1,1.0
the suppose,1,1,1.0
suppose there,1,1,1.0
are n,1,1,1.0
n objects,1,1,1.0
objects to,2,1,2.0
be observed,1,1,1.0
observed by,1,1,1.0
by two,1,1,1.0
two gorithms,1,1,1.0
gorithms let,1,1,1.0
let us,1,1,1.0
us denote,1,1,1.0
difference value,2,1,2.0
two table,1,1,1.0
table v,2,1,2.0
v simula,1,1,1.0
and adacost,2,1,2.0
adacost dataset,1,1,1.0
adacost difference,1,1,1.0
shuttle and,1,1,1.0
algorithms observation,1,1,1.0
observation on,1,1,1.0
the ith,1,1,1.0
ith objects,1,1,1.0
be di,1,1,1.0
di i,1,1,1.0
are ranked,1,1,1.0
ranked according,1,1,1.0
their solute,1,1,1.0
solute values,1,1,1.0
values ranks,1,1,1.0
ranks of,3,1,3.0
the tied,1,1,1.0
tied values,1,1,1.0
are averaged,1,1,1.0
averaged let,1,1,1.0
let stand,1,1,1.0
stand for,1,1,1.0
the sum,2,1,2.0
sum of,2,1,2.0
the ranks,1,1,1.0
objects on,1,1,1.0
on which,1,1,1.0
two algorithms,1,1,1.0
algorithms observations,1,1,1.0
are greater,1,1,1.0
than zero,1,1,1.0
zero and,1,1,1.0
and denote,1,1,1.0
opposite ranks,1,1,1.0
of di,1,1,1.0
di are,1,1,1.0
are evenly,1,1,1.0
evenly split,1,1,1.0
split between,1,1,1.0
and equations,1,1,1.0
equations and,1,1,1.0
and conclude,1,1,1.0
conclude the,1,1,1.0
the calculations,1,1,1.0
calculations of,1,1,1.0
and di,1,1,1.0
di rank,4,1,4.0
rank di,3,1,3.0
rank chen,1,1,1.0
chen et,1,1,1.0
table vi,2,1,2.0
vi simula,1,1,1.0
of smoteb,10,1,10.0
oost smote,10,1,10.0
adasyn a,9,1,9.0
a dacost,9,1,9.0
dacost b,9,1,9.0
b orderline,10,1,10.0
orderline smote,10,1,10.0
and omek,10,1,10.0
omek ramoboost,5,1,5.0
borderlinesmote t,6,1,6.0
t table,4,1,4.0
table vii,3,1,3.0
vii simula,1,1,1.0
tion auc,2,1,2.0
auc p,2,1,2.0
p erformance,2,1,2.0
erformance characteristics,5,1,5.0
characteristics dataset,2,1,2.0
dataset ramo,1,1,1.0
ramo smote,3,1,3.0
borderlinesmote sonar,2,1,2.0
shuttle if,1,1,1.0
we set,1,1,1.0
set t,1,1,1.0
t min,3,1,3.0
min with,1,1,1.0
α and,1,1,1.0
of observed,1,1,1.0
observed objects,1,1,1.0
objects being,1,1,1.0
being n,1,1,1.0
signiﬁcance value,2,1,2.0
value n,2,1,2.0
n that,1,1,1.0
that t,1,1,1.0
t should,3,1,3.0
equal or,1,1,1.0
or less,1,1,1.0
for rejection,1,1,1.0
rejection of,1,1,1.0
a null,3,1,3.0
be retrieved,1,1,1.0
retrieved by,1,1,1.0
by querying,1,1,1.0
querying the,1,1,1.0
the critical,2,1,2.0
critical value,3,1,3.0
value table,3,1,3.0
table which,1,1,1.0
be accessed,1,1,1.0
accessed in,1,1,1.0
section wilcoxon,1,1,1.0
is conducted,1,1,1.0
conducted between,1,1,1.0
between ramoboost,3,1,3.0
other comparative,12,1,12.0
algorithms ramoboost,1,1,1.0
adacost ramo,1,1,1.0
smote etc,1,1,1.0
etc in,1,1,1.0
all tables,1,1,1.0
tables presenting,1,1,1.0
signiﬁcance test,10,1,10.0
the symbol,1,1,1.0
symbol signiﬁes,1,1,1.0
signiﬁes that,1,1,1.0
is quantitatively,1,1,1.0
quantitatively better,1,1,1.0
comparative algorithm,2,1,2.0
algorithm under,1,1,1.0
consideration in,1,1,1.0
the speciﬁed,1,1,1.0
speciﬁed assessment,1,1,1.0
assessment metric,1,1,1.0
metric and,1,1,1.0
and denotes,1,1,1.0
opposite whenever,1,1,1.0
whenever there,1,1,1.0
signiﬁcance existing,1,1,1.0
existing we,1,1,1.0
highlight the,1,1,1.0
corresponding result,1,1,1.0
result by,1,1,1.0
by underscoring,1,1,1.0
underscoring it,1,1,1.0
it simulation,1,1,1.0
simulation we,3,1,3.0
use boosting,1,1,1.0
iterations t,1,1,1.0
t in,2,2,1.0
as suggested,2,1,2.0
suggested in,2,1,2.0
for ensemble,1,1,1.0
data generated,1,1,1.0
iteration is,1,1,1.0
to of,1,1,1.0
and respectively,3,1,3.0
respectively the,1,1,1.0
coefﬁcient α,1,1,1.0
was sen,1,1,1.0
sen using,1,1,1.0
for optimizing,1,1,1.0
optimizing boost,1,1,1.0
boost s,1,1,1.0
for smoteboost,1,1,1.0
of est,1,1,1.0
to ﬁve,1,1,1.0
ﬁve the,1,1,1.0
cost factor,1,1,1.0
factor c,1,1,1.0
c for,1,1,1.0
for adacost,1,1,1.0
adacost is,1,1,1.0
to three,1,1,1.0
three according,1,1,1.0
the suggestion,2,1,2.0
suggestion of,2,1,2.0
c should,1,1,1.0
an integer,1,1,1.0
integer between,1,1,1.0
test are,1,1,1.0
are conducted,1,1,1.0
conducted on,1,1,1.0
a pairwise,1,1,1.0
pairwise table,1,1,1.0
table viii,2,1,2.0
viii simula,1,1,1.0
adacost b,1,1,1.0
omek ramo,1,1,1.0
t manner,1,1,1.0
manner for,1,1,1.0
simulations introduced,1,1,1.0
section simulation,1,1,1.0
simulation in,2,1,2.0
this simulation,1,1,1.0
apply all,1,1,1.0
all ative,1,1,1.0
ative algorithms,1,1,1.0
datasets described,2,1,2.0
described in,4,1,4.0
of ten,6,1,6.0
ten runs,1,1,1.0
runs at,1,1,1.0
each run,1,1,1.0
run we,1,1,1.0
we randomly,1,1,1.0
randomly select,1,1,1.0
select half,1,1,1.0
as training,1,1,1.0
remaining half,1,1,1.0
half as,1,1,1.0
as testing,2,1,2.0
testing data,2,1,2.0
data fig,1,1,1.0
fig gives,1,1,1.0
gives several,1,1,1.0
several snapshots,1,1,1.0
snapshots of,1,1,1.0
roc graphs,3,1,3.0
graphs of,1,1,1.0
adasyn cost,1,1,1.0
cost borderlinesmote,1,1,1.0
methods here,1,1,1.0
here fig,1,1,1.0
a f,1,1,1.0
f represents,1,1,1.0
the german,1,1,1.0
german ionosphere,1,1,1.0
ionosphere phoneme,1,1,1.0
phoneme satimage,1,1,1.0
satimage and,1,1,1.0
and abalone,1,1,1.0
abalone datasets,1,1,1.0
datasets respectively,1,1,1.0
ﬁgure indicates,1,1,1.0
competitive when,1,1,1.0
in roc,1,1,1.0
space table,1,1,1.0
iii summarizes,1,1,1.0
best performance,3,1,3.0
each algorithm,1,1,1.0
algorithm across,1,1,1.0
across each,1,1,1.0
each evaluation,1,1,1.0
evaluation criteria,1,1,1.0
criteria i,1,1,1.0
s highlighted,1,1,1.0
highlighted from,1,1,1.0
from table,2,1,2.0
we ﬁnd,1,1,1.0
ﬁnd that,1,1,1.0
provide competitive,1,1,1.0
competitive simulation,1,1,1.0
algorithms except,1,1,1.0
except for,1,1,1.0
for recall,1,1,1.0
performance ieee,1,1,1.0
table ix,3,1,3.0
ix simula,1,1,1.0
letter table,1,1,1.0
table x,2,1,2.0
x simula,1,1,1.0
table xi,3,1,3.0
xi simula,1,1,1.0
tion time,1,1,1.0
in seconds,2,1,2.0
seconds of,2,1,2.0
of comparative,1,1,1.0
algorithms across,1,1,1.0
shuttle table,1,1,1.0
table xii,2,1,2.0
xii simula,1,1,1.0
of tuning,12,1,12.0
oversampling rati,2,1,2.0
rati o,6,1,6.0
o a,2,1,2.0
a u,4,1,4.0
u cp,4,1,4.0
cp erformance,4,1,4.0
erformance charateristics,1,1,1.0
charateristics oversampling,1,1,1.0
oversampling ratio,4,1,4.0
ratio ramoboost,2,1,2.0
borderlinesmote see,1,1,1.0
adasyn seems,1,1,1.0
better recall,1,1,1.0
recall rate,1,1,1.0
because adasyn,1,1,1.0
adasyn can,1,1,1.0
can learn,1,1,1.0
learn very,1,1,1.0
very aggressively,1,1,1.0
aggressively from,1,1,1.0
generates thetic,1,1,1.0
boundary see,1,1,1.0
c this,1,1,1.0
adasyn may,1,1,1.0
may push,1,1,1.0
minority positive,1,1,1.0
recall criteria,1,1,1.0
criteria while,1,1,1.0
performance may,1,1,1.0
not prove,1,1,1.0
prove signiﬁcantly,1,1,1.0
signiﬁcantly in,1,1,1.0
words if,1,1,1.0
one algorithm,1,1,1.0
algorithm classiﬁes,1,1,1.0
all testing,1,1,1.0
as positive,1,1,1.0
its recall,1,1,1.0
recall r,1,1,1.0
a t,3,1,3.0
t e,1,1,1.0
be maximized,1,1,1.0
maximized even,1,1,1.0
even if,1,1,1.0
is low,1,1,1.0
low the,1,1,1.0
adasyn performs,1,1,1.0
than other,2,1,2.0
recall w,1,1,1.0
h only,1,1,1.0
only stands,1,1,1.0
stands for,1,1,1.0
classiﬁed minority,1,1,1.0
but performs,1,1,1.0
performs worse,1,1,1.0
other assessment,1,1,1.0
which represent,1,1,1.0
the rithm,1,1,1.0
rithm s,1,1,1.0
s overall,1,1,1.0
performance on,1,1,1.0
datasets these,1,1,1.0
results conﬁrm,1,1,1.0
conﬁrm our,1,1,1.0
our discussions,1,1,1.0
discussions in,1,1,1.0
section regarding,1,1,1.0
different characteristics,1,1,1.0
these algorithms,1,1,1.0
applied on,1,1,1.0
results to,1,1,1.0
evaluate whether,3,1,3.0
whether ramoboost,2,1,2.0
can statistically,5,1,5.0
statistically outperformchen,1,1,1.0
outperformchen et,1,1,1.0
table xiii,2,1,2.0
xiii simula,1,1,1.0
auc based,3,1,3.0
runs between,4,1,4.0
between ramob,4,1,4.0
omek oversampling,1,1,1.0
oversampling ramoboost,1,1,1.0
ramoboost ratio,2,1,2.0
ratio smoteboost,2,1,2.0
borderlinesmote table,5,1,5.0
table xiv,2,1,2.0
xiv simula,1,1,1.0
imbalanced rati,4,1,4.0
p olicy,1,1,1.0
olicy of,1,1,1.0
a balone,1,1,1.0
balone d,1,1,1.0
d atas,1,1,1.0
atas et,1,1,1.0
et index,1,1,1.0
index minority,1,1,1.0
minority combination,1,1,1.0
combination majority,1,1,1.0
majority combination,1,1,1.0
combination minority,1,1,1.0
ratio i,1,1,1.0
i ii,1,1,1.0
ii iii,1,1,1.0
iii iv,1,1,1.0
iv v,1,1,1.0
v vi,1,1,1.0
vi vii,1,1,1.0
vii viii,1,1,1.0
viii ix,1,1,1.0
ix x,1,1,1.0
x table,1,1,1.0
table xv,3,1,3.0
xv simula,1,1,1.0
characteristics imbalanced,1,1,1.0
table xvi,2,1,2.0
xvi simula,1,1,1.0
t other,1,1,1.0
algorithms since,1,1,1.0
are datasets,1,1,1.0
than or,2,1,2.0
or equal,2,1,2.0
to to,1,1,1.0
to reject,2,1,2.0
reject a,2,1,2.0
null pothesis,1,1,1.0
pothesis in,1,1,1.0
of according,1,1,1.0
table table,1,1,1.0
iv shows,1,1,1.0
test result,1,1,1.0
smoteboost one,1,1,1.0
can conclude,1,1,1.0
conclude that,1,1,1.0
statistically outperform,6,1,6.0
outperform smoteboost,2,1,2.0
smoteboost t,1,1,1.0
min it,1,1,1.0
it proves,1,1,1.0
proves that,1,1,1.0
although ramoboost,1,1,1.0
ramoboost shares,1,1,1.0
shares the,1,1,1.0
same boosting,1,1,1.0
procedure and,1,1,1.0
generation technique,1,1,1.0
technique with,1,1,1.0
smoteboost the,1,1,1.0
the adaptive,1,1,1.0
adaptive ranking,1,1,1.0
ranking mechanism,1,1,1.0
mechanism for,1,1,1.0
example makes,1,1,1.0
makes ramoboost,1,1,1.0
ramoboost perform,1,1,1.0
than smoteboost,1,1,1.0
smoteboost table,1,1,1.0
similar result,1,1,1.0
result with,1,1,1.0
of table,2,1,2.0
for boost,2,1,2.0
boost adacost,1,1,1.0
adacost from,1,1,1.0
which however,1,1,1.0
not statistically,1,1,1.0
outperform adacost,1,1,1.0
adacost t,1,1,1.0
min for,1,1,1.0
for space,4,1,4.0
space consideration,3,1,3.0
consideration the,1,1,1.0
detailed statistical,1,1,1.0
statistical analysis,1,1,1.0
analysis for,1,1,1.0
ramoboost against,5,1,5.0
remaining comparative,1,1,1.0
is omitted,1,1,1.0
omitted instead,1,1,1.0
instead we,1,1,1.0
provide in,1,1,1.0
vi the,1,1,1.0
t of,1,1,1.0
all comparative,3,1,3.0
algorithms one,2,1,2.0
ramoboost also,1,1,1.0
also ieee,1,1,1.0
october outperforms,1,1,1.0
outperforms smote,1,1,1.0
also conducted,2,1,2.0
conducted the,2,1,2.0
the simulations,1,1,1.0
neurons for,1,1,1.0
classiﬁer mlp,1,1,1.0
be ten,2,1,2.0
ten our,1,1,1.0
that increasing,1,1,1.0
neurons does,1,1,1.0
necessarily improve,1,1,1.0
we feel,1,1,1.0
feel there,1,1,1.0
be several,1,1,1.0
several reasons,1,1,1.0
reasons for,1,1,1.0
this such,1,1,1.0
potential overﬁtting,1,1,1.0
overﬁtting issue,1,1,1.0
issue furthermore,1,1,1.0
furthermore as,1,1,1.0
strong base,1,1,1.0
classiﬁer in,2,1,2.0
ensemble approach,1,1,1.0
not beneﬁt,1,1,1.0
to increased,1,1,1.0
increased bias,1,1,1.0
bias of,1,1,1.0
such classiﬁers,1,1,1.0
classiﬁers due,1,1,1.0
to space,1,1,1.0
consideration we,2,1,2.0
we refrain,1,1,1.0
refrain from,1,1,1.0
from providing,1,1,1.0
providing the,1,1,1.0
detailed results,1,1,1.0
experiments simulation,1,1,1.0
we investigated,1,1,1.0
ramoboost compared,1,1,1.0
adasyn on,1,1,1.0
synthetic dataset,1,1,1.0
dataset shown,1,1,1.0
fig one,1,1,1.0
one interesting,1,1,1.0
interesting question,1,1,1.0
question that,1,1,1.0
that arises,1,1,1.0
arises is,1,1,1.0
is extracted,1,1,1.0
extracted and,1,1,1.0
and wrapped,1,1,1.0
wrapped up,1,1,1.0
up with,1,1,1.0
other classiﬁer,1,1,1.0
used which,1,1,1.0
be named,1,1,1.0
named as,1,1,1.0
as ramo,1,1,1.0
ramo how,1,1,1.0
how will,1,1,1.0
will the,1,1,1.0
learning formance,1,1,1.0
formance of,1,1,1.0
of ramo,1,1,1.0
ramo be,1,1,1.0
be when,1,1,1.0
other sampling,1,1,1.0
sampling approaches,1,1,1.0
end we,1,1,1.0
have conducted,1,1,1.0
conducted simulations,1,1,1.0
simulations for,1,1,1.0
for ramo,1,1,1.0
ramo against,1,1,1.0
against smote,1,1,1.0
and on,1,1,1.0
space considerations,2,1,2.0
considerations we,1,1,1.0
only provide,1,1,1.0
provide simulation,1,1,1.0
vii the,1,1,1.0
auc value,2,1,2.0
corresponding winning,1,1,1.0
winning approach,1,1,1.0
is highlighted,3,1,3.0
highlighted based,3,1,3.0
vii signiﬁcance,1,1,1.0
whether ramo,1,1,1.0
ramo can,2,1,2.0
outperform other,2,1,2.0
other existing,2,1,2.0
approaches since,1,1,1.0
in simulation,2,1,2.0
simulation is,1,1,1.0
simulation the,1,1,1.0
viii demonstrates,1,1,1.0
demonstrates the,1,1,1.0
test results,2,1,2.0
t value,3,1,3.0
each comparative,1,1,1.0
that ramo,1,1,1.0
outperform adasyn,1,1,1.0
and but,2,1,2.0
not outperform,2,1,2.0
outperform smote,2,1,2.0
case simulation,1,1,1.0
simulation another,1,1,1.0
another interesting,1,1,1.0
interesting simulation,1,1,1.0
we conducted,2,1,2.0
conducted is,1,1,1.0
compare ramoboost,1,1,1.0
algorithms when,1,1,1.0
are ten,1,1,1.0
ten for,1,1,1.0
ramoboost we,2,1,2.0
also conﬁgured,1,1,1.0
conﬁgured the,1,1,1.0
k value,1,1,1.0
of smoteboost,1,1,1.0
ten to,1,1,1.0
a fair,1,1,1.0
fair comparison,1,1,1.0
comparison all,1,1,1.0
other parameters,1,1,1.0
parameters remained,1,1,1.0
remained the,1,1,1.0
same we,1,1,1.0
we compared,1,1,1.0
algorithms against,1,1,1.0
the ten,1,1,1.0
ten datasets,2,1,2.0
largest skew,1,1,1.0
skew ratio,1,1,1.0
ratio since,1,1,1.0
more interested,1,1,1.0
interested in,1,1,1.0
in investigating,1,1,1.0
investigating how,1,1,1.0
how ramoboost,1,1,1.0
ramoboost performs,1,1,1.0
performs with,1,1,1.0
with highly,1,1,1.0
to retain,1,1,1.0
retain these,1,1,1.0
these severely,1,1,1.0
severely anced,1,1,1.0
anced ratios,1,1,1.0
ratios we,1,1,1.0
we adopted,2,1,2.0
adopted a,1,1,1.0
different way,1,1,1.0
way of,1,1,1.0
testing datasets,2,1,2.0
datasets sp,1,1,1.0
sp eciﬁcally,1,1,1.0
eciﬁcally the,1,1,1.0
was created,1,1,1.0
by consolidating,1,1,1.0
consolidating half,1,1,1.0
selected majority,1,1,1.0
and half,1,1,1.0
selected minority,1,1,1.0
r emaining,1,1,1.0
emaining examples,1,1,1.0
testing dataset,2,1,2.0
dataset one,1,1,1.0
can easily,1,1,1.0
easily verify,1,1,1.0
verify that,1,1,1.0
generated th,1,1,1.0
th is,1,1,1.0
is way,1,1,1.0
way bear,1,1,1.0
bear the,1,1,1.0
same imbalance,1,1,1.0
considerations only,1,1,1.0
provided in,1,1,1.0
ix with,1,1,1.0
the winning,1,1,1.0
winning value,1,1,1.0
value across,1,1,1.0
dataset highlighted,1,1,1.0
ix niﬁcance,1,1,1.0
niﬁcance test,1,1,1.0
approaches when,1,1,1.0
to ten,2,1,2.0
ten since,1,1,1.0
just ten,1,1,1.0
to eight,2,1,2.0
eight to,1,1,1.0
hypothesis between,1,1,1.0
two comparative,1,1,1.0
algorithms table,1,1,1.0
x presents,1,1,1.0
against other,3,1,3.0
algorithms from,2,1,2.0
which one,1,1,1.0
also statistically,1,1,1.0
smoteboost adasyn,1,1,1.0
but can,1,1,1.0
adacost in,1,1,1.0
case computational,1,1,1.0
time for,1,1,1.0
for simulation,1,1,1.0
simulation table,1,1,1.0
xi shows,1,1,1.0
datasets based,1,1,1.0
simulation ronment,1,1,1.0
ronment of,1,1,1.0
of intel,1,1,1.0
intel core,1,1,1.0
core duo,1,1,1.0
duo cpu,1,1,1.0
cpu ghz,1,1,1.0
ghz gb,1,1,1.0
ram and,1,1,1.0
and matlab,1,1,1.0
matlab version,1,1,1.0
version from,1,1,1.0
xi one,1,1,1.0
time cost,1,1,1.0
is similar,1,1,1.0
the runtime,1,1,1.0
runtime cost,1,1,1.0
cost for,1,1,1.0
for is,1,1,1.0
generally higher,1,1,1.0
higher than,1,1,1.0
algorithms especially,1,1,1.0
large letter,1,1,1.0
letter and,1,1,1.0
probably because,1,1,1.0
because tomek,1,1,1.0
tomek iterates,1,1,1.0
iterates across,1,1,1.0
entire d,1,1,1.0
d ata,2,2,1.0
ata space,1,1,1.0
space repeatedly,1,1,1.0
repeatedly until,1,1,1.0
until all,1,1,1.0
all tomek,1,1,1.0
links have,1,1,1.0
been cleared,1,1,1.0
cleared simulation,1,1,1.0
on tuning,3,1,3.0
tuning parameters,1,1,1.0
the robustness,3,1,3.0
robustness of,5,1,5.0
in diff,1,1,1.0
diff erent,1,1,1.0
erent parameter,1,1,1.0
parameter conﬁgurations,1,1,1.0
conﬁgurations and,1,1,1.0
and scenarios,1,1,1.0
scenarios simulations,1,1,1.0
simulations on,1,1,1.0
minority pling,1,1,1.0
pling ratios,1,1,1.0
ratios the,1,1,1.0
label noise,9,1,9.0
the attribute,2,1,2.0
attribute noise,4,1,4.0
noise of,2,1,2.0
been conducted,1,1,1.0
conducted for,1,1,1.0
only present,1,1,1.0
present the,2,1,2.0
dataset again,1,1,1.0
again mlp,1,1,1.0
the conﬁguration,1,1,1.0
conﬁguration scribed,1,1,1.0
scribed at,1,1,1.0
of section,1,1,1.0
also based,1,1,1.0
on ten,2,1,2.0
ten random,7,1,7.0
runs in,2,1,2.0
these random,1,1,1.0
runs half,1,1,1.0
majority datasets,1,1,1.0
chosen and,1,1,1.0
and merged,1,1,1.0
merged to,1,1,1.0
remaining part,1,1,1.0
part is,1,1,1.0
is motivated,1,1,1.0
which suggested,1,1,1.0
ratio could,1,1,1.0
could play,1,1,1.0
critical role,1,1,1.0
role for,1,1,1.0
problems here,1,1,1.0
dataset described,1,1,1.0
section as,1,1,1.0
by tuning,1,1,1.0
the overs,1,1,1.0
overs ampling,1,1,1.0
ampling ratio,1,1,1.0
ratio speciﬁcally,1,1,1.0
ratio for,1,1,1.0
he minority,1,1,1.0
is increased,1,1,1.0
increased progressively,1,1,1.0
progressively from,1,1,1.0
to with,1,1,1.0
an interval,1,1,1.0
interval of,1,1,1.0
xii displays,1,1,1.0
displays the,1,1,1.0
runs using,1,1,1.0
highlighted for,1,1,1.0
auc since,1,1,1.0
since t,1,1,1.0
t ramoboost,1,1,1.0
is undoubtedly,1,1,1.0
undoubtedly signiﬁcantly,1,1,1.0
all simulation,2,1,2.0
simulation scenarios,1,1,1.0
scenarios we,1,1,1.0
test based,2,1,2.0
case n,1,1,1.0
eight since,1,1,1.0
runs is,1,1,1.0
ten table,1,1,1.0
xiii shows,1,1,1.0
the comparison,1,1,1.0
comparison between,1,1,1.0
algorithms based,1,1,1.0
runs the,2,1,2.0
original abalone,2,1,2.0
has classes,1,1,1.0
and examples,1,1,1.0
we employed,1,1,1.0
employed only,1,1,1.0
to evaluatechen,1,1,1.0
evaluatechen et,1,1,1.0
table xvii,2,1,2.0
xvii simula,1,1,1.0
omek imbalanced,1,1,1.0
imbalanced ramoboost,1,1,1.0
table xviii,2,1,2.0
xviii simula,1,1,1.0
noise level,10,1,10.0
level a,2,1,2.0
characteristics noise,2,1,2.0
level ramoboost,2,1,2.0
table xix,1,1,1.0
xix simula,1,1,1.0
level s,4,1,4.0
table xx,1,1,1.0
xx simula,1,1,1.0
b ased,1,1,1.0
ased on,1,1,1.0
omek noise,2,1,2.0
noise ramoboost,2,1,2.0
ramoboost level,2,1,2.0
level smoteboost,2,1,2.0
borderlinesmote the,1,1,1.0
on versatile,1,1,1.0
versatile datasets,1,1,1.0
datasets as,1,1,1.0
obtain versatile,1,1,1.0
versatile imbalanced,1,1,1.0
ratio we,1,1,1.0
we manipulate,1,1,1.0
manipulate the,1,1,1.0
classes combination,1,1,1.0
form minority,1,1,1.0
xiv concludes,1,1,1.0
concludes the,1,1,1.0
for such,1,1,1.0
such combination,1,1,1.0
combination and,1,1,1.0
corresponding imbalanced,1,1,1.0
ratio table,1,1,1.0
xv presents,1,1,1.0
presents simulation,1,1,1.0
xv wilcoxon,1,1,1.0
test tells,1,1,1.0
us whether,1,1,1.0
whether any,1,1,1.0
any signiﬁcance,1,1,1.0
signiﬁcance exists,1,1,1.0
exists between,1,1,1.0
and any,1,1,1.0
algorithms which,1,1,1.0
xvi using,1,1,1.0
conducted signiﬁcance,1,1,1.0
signiﬁcance tests,2,1,2.0
are given,1,1,1.0
given in,1,1,1.0
xvii noise,1,1,1.0
noise in,2,1,2.0
may exhibit,1,1,1.0
exhibit unpredictably,1,1,1.0
unpredictably negative,1,1,1.0
to systematically,1,1,1.0
systematically investigate,1,1,1.0
investigate the,1,1,1.0
of boost,1,1,1.0
boost we,1,1,1.0
we manually,1,1,1.0
manually introduce,1,1,1.0
introduce class,1,1,1.0
and attribute,1,1,1.0
levels into,1,1,1.0
and let,1,1,1.0
let ramoboost,1,1,1.0
as other,1,1,1.0
algorithms ieee,1,1,1.0
table xxi,2,1,2.0
xxi simula,1,1,1.0
table xxii,1,1,1.0
xxii simula,1,1,1.0
table xxiii,1,1,1.0
xxiii simula,1,1,1.0
borderlinesmote from,1,1,1.0
from it,1,1,1.0
for adding,1,1,1.0
adding class,1,1,1.0
noise we,1,1,1.0
of speciﬁcally,1,1,1.0
speciﬁcally given,1,1,1.0
classes x,1,1,1.0
y and,2,1,2.0
a noise,2,1,2.0
level x,2,1,2.0
x an,1,1,1.0
its label,1,1,1.0
label x,1,1,1.0
x has,1,1,1.0
has an,1,1,1.0
an x,1,1,1.0
x chance,1,1,1.0
chance to,1,1,1.0
be corrupted,1,1,1.0
corrupted and,1,1,1.0
and mislabeled,1,1,1.0
mislabeled as,1,1,1.0
as y,1,1,1.0
so does,1,1,1.0
does an,1,1,1.0
class y,1,1,1.0
y table,1,1,1.0
xviii shows,1,1,1.0
comparative learning,1,1,1.0
under different,1,1,1.0
noise levels,1,1,1.0
levels tables,1,1,1.0
tables xix,1,1,1.0
xix and,1,1,1.0
and xx,1,1,1.0
xx show,1,1,1.0
runs attribute,1,1,1.0
noise was,1,1,1.0
was manually,1,1,1.0
manually added,1,1,1.0
added in,1,1,1.0
in accordance,1,1,1.0
accordance with,1,1,1.0
to corrupt,1,1,1.0
corrupt each,1,1,1.0
each attribute,1,1,1.0
attribute ai,1,1,1.0
ai with,1,1,1.0
ai is,1,1,1.0
is assigned,1,1,1.0
assigned a,1,1,1.0
random value,1,1,1.0
value approximately,1,1,1.0
approximately x,1,1,1.0
x of,1,1,1.0
time with,1,1,1.0
each tive,1,1,1.0
tive value,1,1,1.0
value being,1,1,1.0
being approximately,1,1,1.0
approximately equally,1,1,1.0
selected table,1,1,1.0
xxi shows,1,1,1.0
runs tables,1,1,1.0
tables xxii,1,1,1.0
xxii and,1,1,1.0
and xxiii,1,1,1.0
xxiii present,1,1,1.0
results based,1,1,1.0
runs all,1,1,1.0
section illustrate,1,1,1.0
ramoboost when,1,1,1.0
when exposed,1,1,1.0
exposed to,1,1,1.0
different internal,1,1,1.0
internal is,1,1,1.0
better tha,2,2,1.0
tha n,2,2,1.0
algorithm only,1,1,1.0
only if,1,1,1.0
corresponding table,1,1,1.0
table cell,1,1,1.0
cell is,1,1,1.0
is hi,1,1,1.0
hi ghlighted,1,1,1.0
ghlighted and,1,1,1.0
and underscored,1,1,1.0
underscored with,1,1,1.0
with symbol,1,1,1.0
symbol symbol,1,1,1.0
symbol represents,1,1,1.0
opposite oversampling,1,1,1.0
and exte,1,1,1.0
exte rnal,1,1,1.0
rnal imbalanced,1,1,1.0
imbalanced class,1,1,1.0
ratio noises,1,1,1.0
noises conﬁgurations,1,1,1.0
conﬁgurations the,1,1,1.0
tests also,1,1,1.0
also demonstrate,1,1,1.0
the competitiveness,1,1,1.0
competitiveness of,1,1,1.0
view c,1,1,1.0
c onclusion,1,1,1.0
onclusion in,1,1,1.0
we presented,1,1,1.0
presented the,1,1,1.0
for balanced,1,1,1.0
key characteristics,1,1,1.0
ramoboost are,1,1,1.0
are adaptive,1,1,1.0
adaptive learning,1,1,1.0
and reduction,1,1,1.0
reduction of,1,1,1.0
of bias,1,1,1.0
bias this,1,1,1.0
adaptively shifting,1,1,1.0
shifting the,1,1,1.0
toward those,1,1,1.0
difﬁcult examples,1,1,1.0
both minority,1,1,1.0
and systematically,1,1,1.0
systematically creating,1,1,1.0
creating minority,1,1,1.0
minority synthetic,1,1,1.0
synthetic stances,1,1,1.0
stances based,1,1,1.0
function simulation,1,1,1.0
datasets across,1,1,1.0
metrics including,1,1,1.0
including oa,1,1,1.0
recall roc,1,1,1.0
graphs and,1,1,1.0
auc demonstrate,1,1,1.0
effectiveness and,1,1,1.0
and robustness,1,1,1.0
problems there,1,1,1.0
several interesting,1,1,1.0
interesting future,1,1,1.0
directions for,1,1,1.0
boost for,1,1,1.0
instance our,1,1,1.0
study is,1,1,1.0
on handling,1,1,1.0
handling datasets,1,1,1.0
with continuous,1,1,1.0
continuous features,1,1,1.0
features it,1,1,1.0
is possible,1,1,1.0
possible to,1,1,1.0
extend ramoboost,1,1,1.0
ramoboost to,1,1,1.0
handle datasets,1,1,1.0
with nominal,1,1,1.0
nominal features,1,1,1.0
features by,1,1,1.0
by adopting,1,1,1.0
adopting the,1,1,1.0
techniques used,1,1,1.0
method et,1,1,1.0
boosting second,1,1,1.0
second in,1,1,1.0
paper ramoboost,1,1,1.0
is only,1,1,1.0
only evaluated,1,1,1.0
be generalized,1,1,1.0
generalized to,1,1,1.0
handle multiclass,1,1,1.0
improve its,1,1,1.0
its plicability,1,1,1.0
plicability in,1,1,1.0
practice third,1,1,1.0
third in,1,1,1.0
distance measure,1,1,1.0
measure however,1,1,1.0
other alternatives,1,1,1.0
alternatives that,1,1,1.0
also eligible,1,1,1.0
eligible and,1,1,1.0
and worthy,1,1,1.0
worthy of,1,1,1.0
of trying,1,1,1.0
trying and,1,1,1.0
and may,1,1,1.0
show improved,1,1,1.0
framework finally,1,1,1.0
finally similar,1,1,1.0
to many,1,1,1.0
algorithms there,1,1,1.0
parameters that,1,1,1.0
be determined,1,1,1.0
determined for,1,1,1.0
shown some,1,1,1.0
some empirical,1,1,1.0
results regarding,1,1,1.0
regarding this,1,1,1.0
also would,1,1,1.0
systematic and,1,1,1.0
and adaptive,1,1,1.0
adaptive way,1,1,1.0
to adjust,1,1,1.0
adjust those,1,1,1.0
those parameters,1,1,1.0
parameters could,1,1,1.0
a challenging,1,1,1.0
challenging but,1,1,1.0
but important,1,1,1.0
applied across,1,1,1.0
different application,1,1,1.0
domains our,1,1,1.0
our group,1,1,1.0
group is,1,1,1.0
currently investigating,1,1,1.0
investigating all,1,1,1.0
issues motivated,1,1,1.0
by our,1,1,1.0
initial results,1,1,1.0
ramoboost may,1,1,1.0
may provide,1,1,1.0
provide new,1,1,1.0
new insights,1,1,1.0
insights for,1,1,1.0
and have,1,1,1.0
potential to,1,1,1.0
a powerful,1,1,1.0
powerful tool,1,1,1.0
tool in,1,1,1.0
domains references,1,1,1.0
references he,1,1,1.0
eng vol,3,1,3.0
sets japkowicz,1,1,1.0
japkowicz ed,1,1,1.0
ed menlo,1,1,1.0
press v,1,1,1.0
chawla japkowicz,2,1,2.0
and kołcz,2,1,2.0
kołcz editorial,1,1,1.0
editorial special,1,1,1.0
sets acm,1,1,1.0
explorations vol,1,1,1.0
jun v,2,1,2.0
kołcz uncertainty,1,1,1.0
uncertainty sampling,1,1,1.0
for classiﬁers,1,1,1.0
in proc,22,1,22.0
proc int,12,1,12.0
conf mach,8,1,8.0
mach workshop,4,1,4.0
workshop learn,4,1,4.0
learn imbalanced,5,1,5.0
washington pp,2,1,2.0
proc learn,1,1,1.0
sets papers,1,1,1.0
papers aaai,1,1,1.0
workshop menlo,1,1,1.0
ca pp,3,1,3.0
robust classiﬁcation,2,1,2.0
classiﬁcation for,1,1,1.0
imprecise ments,1,1,1.0
ments mach,1,1,1.0
mach vol,2,1,2.0
pp mar,1,1,1.0
mar clearwater,1,1,1.0
clearwater and,1,1,1.0
and stern,1,1,1.0
stern a,1,1,1.0
a program,1,1,1.0
program in,1,1,1.0
high energy,1,1,1.0
energy physics,1,1,1.0
classiﬁcation comput,1,1,1.0
comput phys,1,1,1.0
phys commun,1,1,1.0
commun vol,1,1,1.0
framework acm,1,1,1.0
explorations newslett,4,1,4.0
newslett vol,4,1,4.0
jun g,1,1,1.0
t a,1,1,1.0
n dm,1,1,1.0
dm o,1,1,1.0
n a,1,1,1.0
r d,1,1,1.0
d as,1,1,1.0
as t,1,1,1.0
t u,1,1,1.0
u d,1,1,1.0
d yo,1,1,1.0
yo f,1,1,1.0
f the,1,1,1.0
for b,1,1,1.0
b alancing,1,1,1.0
alancing machine,1,1,1.0
and imbalanced,1,1,1.0
datasets investigating,1,1,1.0
investigating the,1,1,1.0
method probabilistic,1,1,1.0
probabilistic estimate,1,1,1.0
estimate and,1,1,1.0
and decision,1,1,1.0
tree structure,1,1,1.0
structure in,1,1,1.0
pp jo,1,1,1.0
imbalances small,1,1,1.0
disjuncts acm,1,1,1.0
jun weiss,1,1,1.0
induction artiﬁcial,1,1,1.0
artiﬁcial intell,5,1,5.0
intell vol,2,1,2.0
jul japkowicz,1,1,1.0
imbalances class,1,1,1.0
proc mexican,1,1,1.0
mexican int,1,1,1.0
conf artiﬁcial,2,1,2.0
artiﬁcial adv,1,1,1.0
adv artiﬁcial,1,1,1.0
intell pp,2,1,2.0
technique artiﬁcial,1,1,1.0
new sampling,1,1,1.0
intell adv,1,1,1.0
adv intell,1,1,1.0
comput pp,1,1,1.0
pp guo,1,1,1.0
and viktor,1,1,1.0
viktor learning,1,1,1.0
approach acm,1,1,1.0
jun mease,1,1,1.0
mease wyner,1,1,1.0
wyner and,1,1,1.0
and buja,1,1,1.0
buja boosted,1,1,1.0
boosted classiﬁcation,1,1,1.0
classiﬁcation trees,1,1,1.0
class estimation,1,1,1.0
estimation mach,1,1,1.0
mach learn,7,1,7.0
learn res,2,1,2.0
res vol,3,1,3.0
pp may,2,1,2.0
may yuan,1,1,1.0
yuan li,1,1,1.0
proc annu,2,1,2.0
annu acm,1,1,1.0
acm int,1,1,1.0
conf multimedia,1,1,1.0
multimedia santa,1,1,1.0
santa barbara,1,1,1.0
barbara ca,1,1,1.0
pp ting,1,1,1.0
ting an,1,1,1.0
trees ieee,1,1,1.0
pp and,1,1,1.0
and vasconcelos,1,1,1.0
vasconcelos asymmetric,1,1,1.0
asymmetric boosting,1,1,1.0
learn corvallis,1,1,1.0
corvallis or,1,1,1.0
or pp,1,1,1.0
pp viola,1,1,1.0
viola and,1,1,1.0
and jones,1,1,1.0
jones fast,1,1,1.0
and robust,1,1,1.0
classiﬁcation using,1,1,1.0
using asymmetric,1,1,1.0
adaboost and,1,1,1.0
a detector,1,1,1.0
detector cascade,1,1,1.0
cascade in,1,1,1.0
processing system,1,1,1.0
system cambridge,1,1,1.0
ma mit,2,1,2.0
pp fan,1,1,1.0
fan stolfo,1,1,1.0
stolfo zhang,1,1,1.0
zhang an,1,1,1.0
an d,1,1,1.0
d chan,1,1,1.0
chan adacost,1,1,1.0
adacost cation,1,1,1.0
cation boosting,1,1,1.0
learn pp,2,1,2.0
pp domingos,1,1,1.0
a genera,1,1,1.0
genera l,1,1,1.0
l method,1,1,1.0
making classiﬁers,1,1,1.0
classiﬁers sensitive,1,1,1.0
sensitive in,1,1,1.0
proc acm,2,1,2.0
sigkdd int,1,1,1.0
conf knowl,2,1,2.0
knowl discovery,4,1,4.0
discovery data,2,1,2.0
pp liu,1,1,1.0
liu and,2,1,2.0
and zhou,1,1,1.0
zhou training,1,1,1.0
training neural,1,1,1.0
networks with,1,1,1.0
with methods,2,1,2.0
methods addressing,1,1,1.0
problem ieee,1,1,1.0
data vol,1,1,1.0
chen face,1,1,1.0
face recognition,1,1,1.0
recognition using,1,1,1.0
using total,1,1,1.0
total adaptive,1,1,1.0
adaptive fuzzy,1,1,1.0
fuzzy support,1,1,1.0
machines ieee,2,1,2.0
trans neural,7,1,7.0
neural v,1,1,1.0
v o,1,1,1.0
l no,1,1,1.0
alignment considering,1,1,1.0
considering imbalanced,1,1,1.0
jun wu,1,1,1.0
chang aligning,1,1,1.0
aligning boundary,1,1,1.0
kernel space,1,1,1.0
proc ieee,1,1,1.0
ieee int,1,1,1.0
conf data,1,1,1.0
mining brighton,1,1,1.0
brighton pp,1,1,1.0
neural vol,4,1,4.0
pp ertekin,2,1,2.0
ertekin huang,2,1,2.0
huang bottou,1,1,1.0
bottou and,1,1,1.0
and giles,2,1,2.0
giles learning,1,1,1.0
learning on,1,1,1.0
border active,1,1,1.0
classiﬁcation in,2,1,2.0
acm conf,1,1,1.0
conf inform,1,1,1.0
inform knowl,1,1,1.0
knowl manage,1,1,1.0
manage lisbon,1,1,1.0
lisbon portugal,1,1,1.0
portugal pp,1,1,1.0
huang and,1,1,1.0
giles active,1,1,1.0
annu int,1,1,1.0
int acm,1,1,1.0
acm sigir,1,1,1.0
sigir conf,1,1,1.0
conf res,1,1,1.0
res develop,1,1,1.0
develop inform,1,1,1.0
inform retrieval,1,1,1.0
retrieval amsterdam,1,1,1.0
amsterdam the,1,1,1.0
the netherlands,1,1,1.0
netherlands pp,1,1,1.0
pp zhu,2,1,2.0
zhu and,1,1,1.0
and hovy,1,1,1.0
hovy active,1,1,1.0
for word,1,1,1.0
disambiguation with,1,1,1.0
for addressing,1,1,1.0
proc joint,1,1,1.0
joint conf,2,1,2.0
conf empirical,1,1,1.0
methods natural,1,1,1.0
natural l,1,1,1.0
l ang,1,1,1.0
ang process,1,1,1.0
process computat,1,1,1.0
computat natural,1,1,1.0
natural lang,1,1,1.0
lang prague,1,1,1.0
prague czech,1,1,1.0
czech republic,1,1,1.0
republic pp,1,1,1.0
int joint,1,1,1.0
conf neural,1,1,1.0
neural jun,1,1,1.0
jun pp,1,1,1.0
bowyer boost,1,1,1.0
boost improving,1,1,1.0
proc principles,1,1,1.0
principles knowl,1,1,1.0
discovery databases,1,1,1.0
databases croatia,1,1,1.0
y freund,2,1,2.0
schapire experiments,1,1,1.0
new boosting,1,1,1.0
boosting comput,1,1,1.0
comput syst,1,1,1.0
syst sci,1,1,1.0
sci vol,1,1,1.0
matwin addressi,1,1,1.0
addressi ng,1,1,1.0
ng the,1,1,1.0
learn nashville,1,1,1.0
nashville tn,1,1,1.0
tn pp,1,1,1.0
pp caballero,1,1,1.0
caballero martinze,1,1,1.0
martinze hervas,1,1,1.0
hervas and,1,1,1.0
and gutierrez,1,1,1.0
gutierrez sensitivity,1,1,1.0
sensitivity accuracy,1,1,1.0
in multiclass,1,1,1.0
multiclass problems,1,1,1.0
using memetic,1,1,1.0
memetic pareto,1,1,1.0
pareto evolutionary,1,1,1.0
evolutionary neural,1,1,1.0
networks ieee,1,1,1.0
neural netw,2,1,2.0
netw vol,2,1,2.0
may constructing,1,1,1.0
constructing ensembles,1,1,1.0
ensembles of,1,1,1.0
weighted instance,1,1,1.0
instance selection,1,1,1.0
selection ieee,1,1,1.0
pp muhlbaier,1,1,1.0
muhlbaier topalis,1,1,1.0
topalis and,1,1,1.0
and polikar,1,1,1.0
polikar learn,1,1,1.0
learn combining,1,1,1.0
combining ensemble,1,1,1.0
classiﬁers with,1,1,1.0
with dynamically,1,1,1.0
dynamically weighted,1,1,1.0
weighted for,1,1,1.0
for efﬁcient,1,1,1.0
efﬁcient incremental,1,1,1.0
incremental learning,1,1,1.0
learning of,1,1,1.0
new classes,1,1,1.0
classes ieee,1,1,1.0
pp ieee,1,1,1.0
october shen,1,1,1.0
shen and,1,1,1.0
li boosting,1,1,1.0
boosting through,1,1,1.0
through optimization,1,1,1.0
of margin,1,1,1.0
margin utions,1,1,1.0
utions ieee,1,1,1.0
pp apr,2,1,2.0
apr sun,1,1,1.0
sun and,1,1,1.0
and yao,1,1,1.0
yao sparse,1,1,1.0
sparse approximation,1,1,1.0
approximation through,1,1,1.0
through boosting,1,1,1.0
boosting for,1,1,1.0
learning large,1,1,1.0
scale kernel,1,1,1.0
jun hu,1,1,1.0
hu hu,1,1,1.0
hu and,1,1,1.0
and maybank,1,1,1.0
maybank algorithm,1,1,1.0
for network,1,1,1.0
detection ieee,1,1,1.0
trans man,1,1,1.0
man part,1,1,1.0
part b,1,1,1.0
b vol,1,1,1.0
apr he,1,1,1.0
and shen,1,1,1.0
shen a,1,1,1.0
ranked subspace,1,1,1.0
subspace learning,1,1,1.0
for gene,1,1,1.0
gene expression,1,1,1.0
expression data,1,1,1.0
pp asuncion,1,1,1.0
asuncion and,1,1,1.0
and newman,1,1,1.0
online available,3,1,3.0
available http,2,1,2.0
http elena,1,1,1.0
project online,1,1,1.0
available ftp,1,1,1.0
ftp fawcett,1,1,1.0
fawcett roc,1,1,1.0
graphs notes,1,1,1.0
notes and,1,1,1.0
and practical,1,1,1.0
practical considerations,1,1,1.0
considerations for,1,1,1.0
mining researchers,1,1,1.0
researchers hp,1,1,1.0
hp palo,1,1,1.0
palo alto,1,1,1.0
alto ca,1,1,1.0
ca tech,1,1,1.0
tech kubat,1,1,1.0
images mach,1,1,1.0
vol nos,1,1,1.0
nos pp,1,1,1.0
fawcett analysis,1,1,1.0
and visualization,1,1,1.0
visualization of,1,1,1.0
of classiﬁer,1,1,1.0
comparison under,1,1,1.0
under imprecise,1,1,1.0
imprecise class,1,1,1.0
mining newport,1,1,1.0
newport beach,1,1,1.0
beach ca,1,1,1.0
pp hand,1,1,1.0
hand measuring,1,1,1.0
measuring classiﬁer,1,1,1.0
performance a,1,1,1.0
a coherent,1,1,1.0
coherent alternative,1,1,1.0
curve mach,1,1,1.0
learn vol,1,1,1.0
pp corder,1,1,1.0
corder and,1,1,1.0
and foreman,1,1,1.0
foreman nonparametric,1,1,1.0
nonparametric statistics,1,1,1.0
new york,3,1,3.0
york wiley,1,1,1.0
wiley demšar,1,1,1.0
demšar statistical,1,1,1.0
sets mach,1,1,1.0
pp critical,1,1,1.0
of wilcoxon,1,1,1.0
test online,1,1,1.0
http opitz,1,1,1.0
opitz and,1,1,1.0
and maclin,1,1,1.0
maclin popular,1,1,1.0
popular ensemble,1,1,1.0
methods an,1,1,1.0
study artiﬁcial,1,1,1.0
pp breiman,1,1,1.0
breiman arcing,1,1,1.0
arcing classiﬁers,1,1,1.0
classiﬁers ann,1,1,1.0
ann vol,1,1,1.0
chawla cieslak,1,1,1.0
cieslak hall,1,1,1.0
and joshi,1,1,1.0
joshi ically,1,1,1.0
ically countering,1,1,1.0
countering imbalance,1,1,1.0
its empirical,1,1,1.0
empirical relationship,1,1,1.0
relationship to,1,1,1.0
to cost,1,1,1.0
cost data,1,1,1.0
mining knowl,1,1,1.0
discovery vol,1,1,1.0
pp anyfantis,1,1,1.0
anyfantis karagiannopoulos,1,1,1.0
karagiannopoulos kotsiantis,1,1,1.0
pintelas robustness,1,1,1.0
in handling,1,1,1.0
handling class,1,1,1.0
class noise,1,1,1.0
proc ifip,1,1,1.0
ifip int,1,1,1.0
int federation,1,1,1.0
federation inform,1,1,1.0
inform vol,1,1,1.0
y yang,1,1,1.0
yang error,1,1,1.0
error detection,1,1,1.0
and sensitive,1,1,1.0
sensitive instance,1,1,1.0
instance ranking,1,1,1.0
ranking in,1,1,1.0
in noisy,1,1,1.0
noisy datasets,1,1,1.0
in american,1,1,1.0
american tion,1,1,1.0
for artiﬁcial,1,1,1.0
intelligence cambridge,1,1,1.0
pp sheng,1,1,1.0
chen s,1,1,1.0
s received,1,1,1.0
received the,3,1,3.0
and degrees,1,1,1.0
degrees in,1,1,1.0
in control,1,1,1.0
control science,1,1,1.0
engineering from,4,1,4.0
from huazhong,2,1,2.0
huazhong university,1,1,1.0
technology wuhan,2,1,2.0
wuhan china,2,1,2.0
china in,2,1,2.0
respectively he,1,1,1.0
currently pursuing,1,1,1.0
pursuing the,1,1,1.0
degree in,4,1,4.0
the partment,2,1,2.0
partment of,2,1,2.0
nj his,2,1,2.0
his current,3,1,3.0
research interests,3,1,3.0
interests include,3,1,3.0
include machine,2,1,2.0
learning data,2,1,2.0
mining and,1,1,1.0
and computational,1,1,1.0
computational intelligent,1,1,1.0
intelligent systems,1,1,1.0
systems haibo,1,1,1.0
he m,1,1,1.0
m received,1,1,1.0
and grees,1,1,1.0
grees in,1,1,1.0
in electrical,2,1,2.0
electrical engineering,2,1,2.0
huazhong versity,1,1,1.0
versity of,1,1,1.0
from ohio,1,1,1.0
ohio university,1,1,1.0
university athens,1,1,1.0
athens in,1,1,1.0
in he,2,1,2.0
currently an,1,1,1.0
an assistant,2,1,2.0
assistant professor,2,1,2.0
professor at,2,1,2.0
kingston from,1,1,1.0
to he,1,1,1.0
he was,1,1,1.0
was an,1,1,1.0
computer neering,1,1,1.0
neering stevens,1,1,1.0
include sys,1,1,1.0
sys tems,1,1,1.0
tems machine,1,1,1.0
mining putational,1,1,1.0
putational intelligence,1,1,1.0
in critical,1,1,1.0
critical engineering,1,1,1.0
engineering ﬁelds,1,1,1.0
ﬁelds such,1,1,1.0
as smart,1,1,1.0
smart grid,2,1,2.0
grid and,1,1,1.0
and sensor,1,1,1.0
sensor networks,1,1,1.0
networks very,1,1,1.0
scale integration,1,1,1.0
integration and,1,1,1.0
and programmable,1,1,1.0
programmable gate,1,1,1.0
gate array,1,1,1.0
array design,1,1,1.0
design he,1,1,1.0
he has,1,1,1.0
has served,1,1,1.0
served regularly,1,1,1.0
regularly on,1,1,1.0
the organizing,1,1,1.0
organizing committees,1,1,1.0
committees of,1,1,1.0
of numerous,1,1,1.0
numerous international,1,1,1.0
international conferences,1,1,1.0
conferences and,1,1,1.0
also served,1,1,1.0
served as,1,1,1.0
a guest,1,1,1.0
guest editor,1,1,1.0
editor for,1,1,1.0
several journals,1,1,1.0
journals including,1,1,1.0
including applied,1,1,1.0
applied mathematics,1,1,1.0
mathematics and,1,1,1.0
and computation,1,1,1.0
computation soft,1,1,1.0
and journal,1,1,1.0
experimental theoretical,1,1,1.0
theoretical artiﬁcial,1,1,1.0
intelligence ei,1,1,1.0
ei s,1,1,1.0
s currently,1,1,1.0
the editor,1,1,1.0
editor of,2,1,2.0
ieee computational,1,1,1.0
intelligence society,1,1,1.0
society electronic,1,1,1.0
electronic letter,1,1,1.0
letter an,1,1,1.0
an editorial,1,1,1.0
editorial board,1,1,1.0
board member,1,1,1.0
of cognitive,1,1,1.0
cognitive computation,1,1,1.0
computation and,1,1,1.0
an ciate,1,1,1.0
ciate editor,1,1,1.0
of ieee,1,1,1.0
ieee t,1,1,1.0
t ransactions,1,1,1.0
ransactions on,1,1,1.0
and ieee,1,1,1.0
on smart,1,1,1.0
grid edwardo,1,1,1.0
garcia received,1,1,1.0
mathematics from,1,1,1.0
from new,1,1,1.0
york university,1,1,1.0
york and,1,1,1.0
in computer,1,1,1.0
from stevens,1,1,1.0
nj both,1,1,1.0
currently with,1,1,1.0
the stevens,1,1,1.0
technology his,1,1,1.0
learning biologically,1,1,1.0
biologically inspired,1,1,1.0
inspired intelligence,1,1,1.0
intelligence cognitive,1,1,1.0
cognitive neuroscience,1,1,1.0
neuroscience data,1,1,1.0
for medical,1,1,1.0
medical agnostics,1,1,1.0
agnostics and,1,1,1.0
and mathematical,1,1,1.0
mathematical methods,1,1,1.0
for magnetic,1,1,1.0
magnetic resonance,1,1,1.0
resonance imaging,1,1,1.0
methods erate,1,1,1.0
combines isting,1,1,1.0
isting instances,1,1,1.0
the experi,1,1,1.0
experi ments,1,1,1.0
ments also,1,1,1.0
when m,1,1,1.0
m ost,1,1,1.0
ost of,1,1,1.0
medical dia,1,1,1.0
dia gnosis,1,1,1.0
gnosis fraud,1,1,1.0
class imbal,2,1,2.0
imbal ance,1,1,1.0
ance problem,2,1,2.0
original d,1,1,1.0
ata typically,1,1,1.0
solving ance,1,1,1.0
ance issues,1,1,1.0
data augm,1,1,1.0
augm entation,1,1,1.0
entation problem,1,1,1.0
and dat,1,1,1.0
dat a,1,1,1.0
a augmentation,1,1,1.0
to sol,1,1,1.0
sol ve,1,1,1.0
ve problems,1,1,1.0
existing stances,1,1,1.0
stances thus,1,1,1.0
e xisting,1,1,1.0
xisting data,1,1,1.0
this counterfac,1,1,1.0
counterfac tual,1,1,1.0
tual method,1,1,1.0
some backs,1,1,1.0
backs since,1,1,1.0
the taset,1,1,1.0
taset and,1,1,1.0
randomly re,1,1,1.0
re moves,1,1,1.0
moves examples,1,1,1.0
the ity,1,1,1.0
ity class,1,1,1.0
augmentation lems,1,1,1.0
lems see,1,1,1.0
is c,1,1,1.0
c reated,1,1,1.0
reated by,1,1,1.0
by lating,1,1,1.0
lating between,1,1,1.0
and creat,1,1,1.0
creat es,1,1,1.0
es new,1,1,1.0
then determine,1,1,1.0
determine s,1,1,1.0
s 𝑚,1,1,1.0
𝑚 nally,1,1,1.0
nally smote,1,1,1.0
𝑝 𝛿,1,1,1.0
𝛿 𝛿,1,1,1.0
𝛿 where,1,1,1.0
any consi,1,1,1.0
consi deration,1,1,1.0
deration that,1,1,1.0
not exi,1,1,1.0
exi st,1,1,1.0
st in,1,1,1.0
on fying,1,1,1.0
fying regions,1,1,1.0
the importanc,1,1,1.0
importanc e,1,1,1.0
e of,1,1,1.0
sometimes ana,1,1,1.0
ana lyze,1,1,1.0
lyze the,1,1,1.0
more portant,1,1,1.0
portant or,1,1,1.0
instance ns,1,1,1.0
ns smote,1,1,1.0
then oversample,1,1,1.0
oversample s,1,1,1.0
s from,3,1,3.0
most stances,1,1,1.0
stances the,1,1,1.0
within ters,1,1,1.0
ters to,1,1,1.0
a spac,1,1,1.0
spac e,1,1,1.0
e and,1,1,1.0
their butions,1,1,1.0
butions generating,1,1,1.0
c lass,2,1,2.0
lass boundary,1,1,1.0
be dled,1,1,1.0
dled differently,1,1,1.0
successful classificati,1,1,1.0
classificati on,1,1,1.0
on so,1,1,1.0
minority stances,1,1,1.0
stances in,1,1,1.0
minority c,1,1,1.0
lass is,1,1,1.0
𝑚 𝑚,4,1,4.0
instances 𝑚,1,1,1.0
easily sified,1,1,1.0
sified and,1,1,1.0
if 𝑚,1,1,1.0
𝑚 then,1,1,1.0
generate synthet,1,1,1.0
synthet ic,1,1,1.0
ic instances,1,1,1.0
decision ary,1,1,1.0
ary and,1,1,1.0
s imilar,1,1,1.0
imilar vein,1,1,1.0
vein smote,1,1,1.0
smote divides,1,1,1.0
majority stances,2,1,2.0
stances see,1,1,1.0
other ods,1,1,1.0
ods explore,1,1,1.0
for insta,1,1,1.0
insta nce,1,1,1.0
nce tomek,1,1,1.0
tomek finds,1,1,1.0
neighbours smote,1,1,1.0
prove performance,1,1,1.0
a differe,1,1,1.0
differe nt,1,1,1.0
nt approach,1,1,1.0
mahalanobis distance,1,1,1.0
distance s,1,1,1.0
s requiring,1,1,1.0
related proaches,1,1,1.0
proaches finally,1,1,1.0
stances into,1,1,1.0
after sm,1,1,1.0
sm ote,1,1,1.0
ote has,1,1,1.0
to move,1,1,1.0
move generated,1,1,1.0
counterfactual planation,1,1,1.0
planation is,1,1,1.0
you quested,1,1,1.0
quested a,1,1,1.0
which out,1,1,1.0
out come,1,1,1.0
come would,1,1,1.0
diverse nam,1,1,1.0
nam es,1,1,1.0
es for,1,1,1.0
for stance,1,1,1.0
stance in,1,1,1.0
xai bec,1,1,1.0
bec ause,1,1,1.0
ause they,1,1,1.0
inal loan,1,1,1.0
sometimes randoml,1,1,1.0
randoml y,1,1,1.0
that bala,1,1,1.0
bala nces,1,1,1.0
nces proximity,1,1,1.0
counterfactual instanc,1,1,1.0
instanc e,2,1,2.0
e for,1,1,1.0
of verse,1,1,1.0
verse counterfactual,1,1,1.0
optimization m,1,1,1.0
m ethods,1,1,1.0
ethods is,1,1,1.0
the ance,1,1,1.0
class w,1,1,1.0
w ith,1,1,1.0
ith noise,1,1,1.0
with quential,1,1,1.0
quential negative,1,1,1.0
counterfactual tion,1,1,1.0
tion between,1,1,1.0
two fea,1,1,1.0
fea for,1,1,1.0
male accountant,1,1,1.0
loan sion,1,1,1.0
sion is,1,1,1.0
in zation,1,1,1.0
zation techniques,1,1,1.0
synthetic counterfact,1,1,1.0
counterfact ual,1,1,1.0
ual cases,1,1,1.0
could prove,1,1,1.0
prove the,1,1,1.0
on counte,1,1,1.0
counte factuals,1,1,1.0
factuals in,1,1,1.0
it ity,1,1,1.0
ity that,1,1,1.0
dataset howe,1,1,1.0
howe ver,2,1,2.0
ver mothilal,1,1,1.0
on generat,1,1,1.0
generat ed,1,1,1.0
ed factuals,1,1,1.0
factuals could,1,1,1.0
was t,1,1,1.0
t rained,1,1,1.0
rained and,1,1,1.0
for tual,1,1,1.0
tual data,1,1,1.0
al propos,1,1,1.0
propos ed,1,1,1.0
ed counterfactual,1,1,1.0
stitching gether,1,1,1.0
gether subsamples,1,1,1.0
counterfactual ods,1,1,1.0
ods developed,1,1,1.0
tested technique,1,1,1.0
technique s,1,1,1.0
domain invol,1,1,1.0
invol ved,2,1,2.0
ved a,1,1,1.0
the narios,1,1,1.0
narios recorded,1,1,1.0
weather variabl,1,1,1.0
variabl es,1,1,1.0
es like,1,1,1.0
solar diation,1,1,1.0
diation or,1,1,1.0
grass cordingly,1,1,1.0
cordingly the,1,1,1.0
these disrupted,1,1,1.0
disrupted months,1,1,1.0
based class,1,1,1.0
a cation,1,1,1.0
cation perspective,1,1,1.0
counterfactual m,1,1,1.0
m ethod,1,1,1.0
ethod to,1,1,1.0
these mi,1,1,1.0
nority interestingly,1,1,1.0
the ods,1,1,1.0
ods did,1,1,1.0
specifically t,1,1,1.0
he dice,1,1,1.0
other probl,1,1,1.0
probl em,1,1,1.0
em domains,1,1,1.0
to ulating,1,1,1.0
ulating the,1,1,1.0
the cation,1,1,1.0
cation of,1,1,1.0
the observati,1,1,1.0
observati on,1,1,1.0
on that,1,1,1.0
the eva,1,1,1.0
eva luation,1,1,1.0
luation metrics,1,1,1.0
d reasoning,1,1,1.0
reasoning proach,1,1,1.0
proach to,1,1,1.0
binary fication,1,1,1.0
fication problems,1,1,1.0
and history,1,1,1.0
history that,1,1,1.0
different ory,1,1,1.0
ory they,1,1,1.0
dataset usi,1,1,1.0
usi ng,2,1,2.0
ng our,1,1,1.0
by thi,1,1,1.0
thi s,1,1,1.0
s known,1,1,1.0
minority instanc,1,1,1.0
the ature,1,1,1.0
ature from,1,1,1.0
is counterfac,1,1,1.0
counterfac tually,1,1,1.0
tually related,1,1,1.0
this i,1,1,1.0
i teratively,1,1,1.0
teratively for,1,1,1.0
cfa a,1,1,1.0
n unpaired,1,1,1.0
𝒑 low,1,1,1.0
low box,1,1,1.0
counterfactual 𝒑,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑋,1,1,1.0
𝑋 𝑥,1,1,1.0
𝑥 𝑐𝑓,2,1,2.0
𝑝 𝑃,1,1,1.0
𝑃 𝑝,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑝,1,1,1.0
𝑝 𝑝j,2,1,2.0
𝑝 𝑐𝑓,1,1,1.0
𝑥 𝑡𝑎𝑟𝑔𝑒𝑡,1,1,1.0
𝑡𝑎𝑟𝑔𝑒𝑡 𝑝,1,1,1.0
𝑃 𝑥,1,1,1.0
a taset,1,1,1.0
taset 𝑇,1,1,1.0
native cause,1,1,1.0
cause in,1,1,1.0
each paired,1,1,1.0
paired stance,1,1,1.0
stance involved,1,1,1.0
notably t,1,1,1.0
t his,1,1,1.0
his means,1,1,1.0
ed de,1,1,1.0
de 𝑝,1,1,1.0
𝑝 𝑞,1,1,1.0
𝑞 k,1,1,1.0
e temraz,1,1,1.0
a didate,1,1,1.0
didate native,1,1,1.0
synthetic tual,1,1,1.0
tual instance,1,1,1.0
finding matc,1,1,1.0
matc and,1,1,1.0
cfa c,1,1,1.0
c omputes,1,1,1.0
omputes a,1,1,1.0
a ance,1,1,1.0
ance by,1,1,1.0
the mea,1,1,1.0
mea n,1,1,1.0
n all,2,1,2.0
the al,1,1,1.0
al gorithm,1,1,1.0
gorithm from,1,1,1.0
good factual,1,1,1.0
factual pairing,1,1,1.0
and sm,1,1,1.0
sm yth,1,1,1.0
yth defined,1,1,1.0
more temraz,1,1,1.0
synthetic us,1,1,1.0
us ing,2,1,2.0
ing these,1,1,1.0
critical ence,1,1,1.0
ence between,1,1,1.0
counterfactual s,1,1,1.0
class literature,1,1,1.0
native terfactuals,2,1,2.0
terfactuals in,3,1,3.0
minority i,1,1,1.0
i nstances,1,1,1.0
nstances and,1,1,1.0
those invol,1,1,1.0
ved in,1,1,1.0
counterfactual me,1,1,1.0
me thod,1,1,1.0
thod is,1,1,1.0
interpolation be,1,1,1.0
be tween,1,1,1.0
tween nority,1,1,1.0
nority instances,1,1,1.0
six oversam,1,1,1.0
oversam pling,1,1,1.0
pling methods,1,1,1.0
smote rsb,1,1,1.0
rsb these,1,1,1.0
as tions,1,1,1.0
tions the,1,1,1.0
representative selecti,1,1,1.0
selecti on,1,1,1.0
on of,1,1,1.0
because ferent,1,1,1.0
ferent models,1,1,1.0
mance of,1,1,1.0
these dataset,1,1,1.0
dataset s,2,1,2.0
and rest,1,1,1.0
rest ovr,1,1,1.0
a gainst,1,1,1.0
gainst all,1,1,1.0
modified usi,1,1,1.0
ng both,1,1,1.0
chemical ysis,1,1,1.0
ysis consisting,1,1,1.0
with classe,1,1,1.0
classe the,1,1,1.0
features tracted,1,1,1.0
tracted from,1,1,1.0
each subs,1,1,1.0
subs et,1,1,1.0
et included,1,1,1.0
included mately,1,1,1.0
mately equal,1,1,1.0
set into,1,1,1.0
generated dataset,1,1,1.0
the datase,1,1,1.0
ts generated,1,1,1.0
our ments,1,1,1.0
ments we,1,1,1.0
data augmentati,2,1,2.0
augmentati on,2,1,2.0
on methods,1,1,1.0
was als,1,1,1.0
als o,1,1,1.0
o run,1,1,1.0
each tion,1,1,1.0
tion method,1,1,1.0
rf 𝑛𝑡𝑟𝑒𝑒,1,1,1.0
𝑛𝑡𝑟𝑒𝑒 𝑚𝑎𝑥r,1,1,1.0
𝑚𝑎𝑥r stu,1,1,1.0
stu neighbors,1,1,1.0
t 𝐶,1,1,1.0
𝐶 𝑠𝑜𝑙𝑣𝑒𝑟,1,1,1.0
𝑠𝑜𝑙𝑣𝑒𝑟 multilayer,1,1,1.0
mlp 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛,1,1,1.0
𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 𝑎𝑙𝑝ℎ𝑎,1,1,1.0
𝑎𝑙𝑝ℎ𝑎 𝑠𝑜𝑙𝑣𝑒𝑟,1,1,1.0
𝑠𝑜𝑙𝑣𝑒𝑟 oversampling,1,1,1.0
or negati,1,1,1.0
negati ve,1,1,1.0
ve so,1,1,1.0
given fication,1,1,1.0
fication a,1,1,1.0
fn curacy,1,1,1.0
curacy was,1,1,1.0
it ca,1,1,1.0
ca n,1,1,1.0
n be,1,1,1.0
i balanced,1,1,1.0
our experime,1,1,1.0
experime nts,1,1,1.0
nts were,1,1,1.0
follows 𝑅𝑒𝑐𝑎𝑙𝑙,1,1,1.0
𝑅𝑒𝑐𝑎𝑙𝑙 𝑇𝑃,1,1,1.0
𝑇𝑃 𝐹𝑁,1,1,1.0
𝐹𝑁 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑃,1,1,1.0
𝑇𝑃 𝐹𝑃,1,1,1.0
𝐹𝑃 𝐹j,1,1,1.0
𝐹j 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙,2,1,2.0
𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝑟𝑒𝑐𝑎𝑙𝑙 𝐴𝑈𝐶,1,1,1.0
𝐴𝑈𝐶 𝑇𝑃,1,1,1.0
𝑇𝑃 𝑇𝑁,1,1,1.0
𝑇𝑁 table,1,1,1.0
classifiers test,1,1,1.0
test ed,1,1,1.0
ed see,1,1,1.0
see bles,1,1,1.0
bles recall,1,1,1.0
adasyn mote,1,1,1.0
mote smote,1,1,1.0
for e,1,1,1.0
e ach,1,1,1.0
ach sifier,1,1,1.0
sifier on,1,1,1.0
smote being,1,1,1.0
greater provement,1,1,1.0
provement in,1,1,1.0
the ne,1,1,1.0
ne xt,1,1,1.0
xt best,1,1,1.0
highest au,1,1,1.0
au in,1,1,1.0
the rsb,1,1,1.0
rsb being,1,1,1.0
with e,1,1,1.0
e being,1,1,1.0
the highes,1,1,1.0
highes t,1,1,1.0
contribution i,1,1,1.0
i t,2,1,2.0
t seems,1,1,1.0
the sion,1,1,1.0
sion score,1,1,1.0
n of,1,1,1.0
for line,1,1,1.0
line for,1,1,1.0
it ha,1,1,1.0
ha s,1,1,1.0
methods smot,1,1,1.0
smot e,1,1,1.0
e smote,1,1,1.0
these rithms,1,1,1.0
rithms achieved,1,1,1.0
most i,1,1,1.0
i nteresting,1,1,1.0
nteresting result,1,1,1.0
tion and,1,1,1.0
best mance,1,1,1.0
s ets,1,1,1.0
ets finally,1,1,1.0
and s,1,1,1.0
s pecificity,1,1,1.0
pecificity are,1,1,1.0
cfa formed,1,1,1.0
formed methods,1,1,1.0
on ent,1,1,1.0
ent data,1,1,1.0
data augmenta,2,1,2.0
augmenta tion,2,1,2.0
tion methods,1,1,1.0
these re,1,1,1.0
re sults,1,1,1.0
sults support,1,1,1.0
hand fi,1,1,1.0
fi gure,1,1,1.0
gure shows,1,1,1.0
methods tained,1,1,1.0
tained for,1,1,1.0
for planatory,1,1,1.0
planatory purposes,1,1,1.0
explanatory tuals,1,1,1.0
tuals are,1,1,1.0
tion as,1,1,1.0
problem s,1,1,1.0
s howed,1,1,1.0
howed that,1,1,1.0
minority cla,1,1,1.0
cla ss,1,1,1.0
ss improved,1,1,1.0
it ates,1,1,1.0
ates minority,1,1,1.0
known minority,1,1,1.0
minority but,1,1,1.0
by ing,1,1,1.0
ing similar,1,1,1.0
a apa,1,1,1.0
apa rtment,1,1,1.0
rtment with,1,1,1.0
with bathrooms,1,1,1.0
bathrooms and,1,1,1.0
with hrooms,1,1,1.0
hrooms the,1,1,1.0
historical c,1,1,1.0
c ase,1,1,1.0
ase and,1,1,1.0
from number,1,1,1.0
number differences,1,1,1.0
the ke,1,1,1.0
ke y,1,1,1.0
y that,1,1,1.0
minority insta,1,1,1.0
insta nces,1,1,1.0
nces they,1,1,1.0
though t,1,1,1.0
t hey,1,1,1.0
hey lack,1,1,1.0
same rences,1,1,1.0
rences as,1,1,1.0
local member,1,1,1.0
member cfa,1,1,1.0
the ship,1,1,1.0
ship is,1,1,1.0
already ve,1,1,1.0
ve ry,1,1,1.0
ry similar,1,1,1.0
fail howe,1,1,1.0
ver there,1,1,1.0
he quality,1,1,1.0
of datas,1,1,1.0
et differences,1,1,1.0
synthetic dat,1,1,1.0
dat apoints,1,1,1.0
apoints will,1,1,1.0
be verely,1,1,1.0
verely hampered,1,1,1.0
systematically tes,1,1,1.0
tes ted,1,1,1.0
ted how,1,1,1.0
very minim,1,1,1.0
minim ferent,1,1,1.0
ferent counterfactual,1,1,1.0
explored us,1,1,1.0
ing and,1,1,1.0
improve tive,1,1,1.0
tive importance,1,1,1.0
useful minori,1,1,1.0
ty instances,1,1,1.0
of differences,1,1,1.0
differences were,1,1,1.0
that fe,1,1,1.0
fe ature,1,1,1.0
ature this,1,1,1.0
this ance,1,1,1.0
ance was,1,1,1.0
more cated,1,1,1.0
cated tolerance,1,1,1.0
fewer tuals,1,1,1.0
tuals would,1,1,1.0
so clearl,1,1,1.0
clearl y,1,1,1.0
likely t,1,1,1.0
o disimprove,1,1,1.0
a soning,1,1,1.0
soning approach,1,1,1.0
m l,1,1,1.0
l models,1,1,1.0
leveraging know,1,1,1.0
know n,1,1,1.0
n terfactuals,1,1,1.0
smote ants,1,1,1.0
ants on,1,1,1.0
of rithms,1,1,1.0
rithms and,1,1,1.0
and formation,1,1,1.0
formation systems,1,1,1.0
lursinsap smote,1,1,1.0
smote minority,1,1,1.0
imbal anced,1,1,1.0
anced problem,1,1,1.0
dbsmote based,1,1,1.0
based synthetic,1,1,1.0
b counterfactua,1,1,1.0
counterfactua l,1,1,1.0
l explanations,1,1,1.0
twentieth national,1,1,1.0
optimal neare,1,1,1.0
neare st,1,1,1.0
st neighbor,1,1,1.0
counterfactual tions,1,1,1.0
tions for,1,1,1.0
a tic,1,1,1.0
tic oversampling,1,1,1.0
in ceedings,1,1,1.0
ceedings of,1,1,1.0
synthetic sampl,1,1,1.0
sampl ing,1,1,1.0
ing approach,1,1,1.0
chines ieee,1,1,1.0
classification mance,1,1,1.0
mance when,1,1,1.0
data classific,1,1,1.0
classific ation,1,1,1.0
ation based,1,1,1.0
and evolutional,1,1,1.0
evolutional ly,1,1,1.0
ly underdamping,1,1,1.0
feature lection,1,1,1.0
lection and,1,1,1.0
guided brid,1,1,1.0
brid sampling,1,1,1.0
on based,1,1,1.0
based reasoning,1,1,1.0

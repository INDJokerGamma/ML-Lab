,gram_count,document_count,Average_freq
temraz keane counterfactual,82,2,41.0
keane counterfactual data,94,2,47.0
counterfactual data augmentation,98,2,49.0
data augmentation abstract,2,2,1.0
augmentation abstract learning,2,2,1.0
abstract learning from,2,2,1.0
learning from class,2,2,1.0
from class imbalanced,2,2,1.0
class imbalanced datasets,2,2,1.0
imbalanced datasets poses,2,2,1.0
datasets poses challenges,2,2,1.0
poses challenges for,2,2,1.0
challenges for many,2,2,1.0
for many machine,2,2,1.0
many machine learning,2,2,1.0
machine learning algorithms,9,3,3.0
learning algorithms many,2,2,1.0
algorithms many domains,2,2,1.0
many domains are,2,2,1.0
domains are by,2,2,1.0
are by definition,2,2,1.0
by definition class,2,2,1.0
definition class imbalanced,2,2,1.0
class imbalanced by,2,2,1.0
imbalanced by virtue,2,2,1.0
by virtue of,2,2,1.0
virtue of having,2,2,1.0
of having a,2,2,1.0
having a majority,2,2,1.0
a majority class,5,3,1.6666666666666667
majority class that,5,3,1.6666666666666667
class that naturally,2,2,1.0
that naturally has,2,2,1.0
naturally has many,2,2,1.0
has many more,2,2,1.0
many more instances,4,2,2.0
more instances than,4,2,2.0
instances than its,2,2,1.0
than its minority,2,2,1.0
its minority class,2,2,1.0
minority class genuine,2,2,1.0
class genuine bank,2,2,1.0
genuine bank transactions,2,2,1.0
bank transactions occur,2,2,1.0
transactions occur much,2,2,1.0
occur much more,2,2,1.0
much more often,2,2,1.0
more often than,2,2,1.0
often than fraudulent,2,2,1.0
than fraudulent ones,4,2,2.0
fraudulent ones many,2,2,1.0
ones many methods,2,2,1.0
many methods have,2,2,1.0
methods have been,8,4,2.0
have been proposed,3,3,1.0
been proposed to,2,2,1.0
proposed to solve,4,2,2.0
to solve the,7,3,2.3333333333333335
solve the class,4,2,2.0
the class imbalance,44,6,7.333333333333333
class imbalance problem,48,6,8.0
imbalance problem among,2,2,1.0
problem among the,2,2,1.0
among the most,2,2,1.0
the most popular,5,3,1.6666666666666667
most popular being,2,2,1.0
popular being oversampling,2,2,1.0
being oversampling techniques,2,2,1.0
oversampling techniques such,2,2,1.0
techniques such as,3,3,1.0
such as smote,4,3,1.3333333333333333
as smote these,2,2,1.0
smote these methods,2,2,1.0
these methods synthetic,1,1,1.0
methods synthetic instances,1,1,1.0
synthetic instances in,5,2,2.5
instances in the,26,2,13.0
in the minority,55,4,13.75
the minority class,157,5,31.4
minority class to,7,5,1.4
class to balance,2,2,1.0
to balance the,4,4,1.0
balance the dataset,2,2,1.0
the dataset performing,2,2,1.0
dataset performing data,2,2,1.0
performing data augmentations,2,2,1.0
data augmentations that,2,2,1.0
augmentations that improve,2,2,1.0
that improve the,4,2,2.0
improve the performance,4,2,2.0
the performance of,25,6,4.166666666666667
performance of predictive,2,2,1.0
of predictive machine,2,2,1.0
predictive machine learning,2,2,1.0
machine learning ml,4,2,2.0
learning ml models,2,2,1.0
ml models in,2,2,1.0
models in this,2,2,1.0
in this paper,28,5,5.6
this paper we,11,5,2.2
paper we advance,2,2,1.0
we advance a,2,2,1.0
advance a novel,2,2,1.0
a novel data,2,2,1.0
novel data augmentation,2,2,1.0
data augmentation method,16,2,8.0
augmentation method adapted,2,2,1.0
method adapted from,2,2,1.0
adapted from explainable,2,2,1.0
from explainable ai,2,2,1.0
explainable ai that,2,2,1.0
ai that generates,2,2,1.0
that generates synthetic,4,2,2.0
generates synthetic counterfactual,6,2,3.0
synthetic counterfactual instances,8,2,4.0
counterfactual instances in,6,2,3.0
minority class unlike,2,2,1.0
class unlike other,2,2,1.0
unlike other oversampling,2,2,1.0
other oversampling techniques,2,2,1.0
oversampling techniques this,2,2,1.0
techniques this method,2,2,1.0
this method adaptively,2,2,1.0
method adaptively combines,2,2,1.0
adaptively combines instances,1,1,1.0
combines instances from,1,1,1.0
instances from the,4,2,2.0
from the dataset,4,2,2.0
the dataset using,3,3,1.0
dataset using actual,2,2,1.0
using actual rather,2,2,1.0
actual rather than,2,2,1.0
rather than interpolating,2,2,1.0
than interpolating values,2,2,1.0
interpolating values between,2,2,1.0
values between instances,4,2,2.0
between instances several,2,2,1.0
instances several experiments,2,2,1.0
several experiments using,2,2,1.0
experiments using four,2,2,1.0
using four different,2,2,1.0
four different classifiers,2,2,1.0
different classifiers and,2,2,1.0
classifiers and solving,2,2,1.0
and solving the,2,2,1.0
solving the class,2,2,1.0
imbalance problem using,2,2,1.0
problem using a,2,2,1.0
using a counterfactual,2,2,1.0
a counterfactual method,4,2,2.0
counterfactual method for,4,2,2.0
method for data,6,2,3.0
for data augmentation,16,2,8.0
data augmentation mohammed,2,2,1.0
augmentation mohammed mark,2,2,1.0
mohammed mark school,2,2,1.0
mark school of,2,2,1.0
school of computer,5,4,1.25
of computer science,11,4,2.75
computer science university,4,2,2.0
science university college,2,2,1.0
university college dublin,6,2,3.0
college dublin belfield,6,2,3.0
dublin belfield dublin,6,2,3.0
belfield dublin ireland,6,2,3.0
dublin ireland insight,2,2,1.0
ireland insight centre,2,2,1.0
insight centre for,4,2,2.0
centre for data,4,2,2.0
for data analytics,4,2,2.0
data analytics university,2,2,1.0
analytics university college,2,2,1.0
dublin ireland vistamilk,2,2,1.0
ireland vistamilk sfi,2,2,1.0
vistamilk sfi research,4,2,2.0
sfi research centre,4,2,2.0
research centre university,2,2,1.0
centre university college,2,2,1.0
dublin ireland temraz,2,2,1.0
ireland temraz keane,2,2,1.0
data augmentation datasets,4,2,2.0
augmentation datasets are,2,2,1.0
datasets are reported,2,2,1.0
are reported which,2,2,1.0
reported which show,2,2,1.0
which show that,2,2,1.0
show that this,2,2,1.0
that this counterfactual,4,2,2.0
this counterfactual augmentation,2,2,1.0
counterfactual augmentation method,2,2,1.0
augmentation method cfa,2,2,1.0
method cfa generates,2,2,1.0
cfa generates useful,2,2,1.0
generates useful synthetic,2,2,1.0
useful synthetic datapoints,2,2,1.0
synthetic datapoints in,2,2,1.0
datapoints in the,2,2,1.0
minority class the,7,4,1.75
class the experiments,1,1,1.0
the experiments also,1,1,1.0
experiments also show,1,1,1.0
also show that,4,2,2.0
show that cfa,4,2,2.0
that cfa is,2,2,1.0
cfa is competitive,2,2,1.0
is competitive with,2,2,1.0
competitive with many,2,2,1.0
with many other,2,2,1.0
many other oversampling,2,2,1.0
other oversampling methods,2,2,1.0
oversampling methods many,2,2,1.0
methods many of,2,2,1.0
many of which,2,2,1.0
of which are,2,2,1.0
which are variants,2,2,1.0
are variants of,2,2,1.0
variants of smote,4,2,2.0
of smote the,4,3,1.3333333333333333
smote the basis,2,2,1.0
the basis for,2,2,1.0
basis for cfa,2,2,1.0
for cfa s,2,2,1.0
cfa s performance,2,2,1.0
s performance is,2,2,1.0
performance is discussed,2,2,1.0
is discussed along,2,2,1.0
discussed along with,2,2,1.0
along with the,4,2,2.0
with the conditions,2,2,1.0
the conditions under,4,2,2.0
conditions under which,6,2,3.0
under which it,2,2,1.0
which it is,2,2,1.0
it is likely,2,2,1.0
is likely to,8,3,2.6666666666666665
likely to perform,2,2,1.0
to perform better,4,3,1.3333333333333333
perform better or,2,2,1.0
better or worse,2,2,1.0
or worse in,2,2,1.0
worse in future,2,2,1.0
in future tests,2,2,1.0
future tests keywords,2,2,1.0
tests keywords counterfactual,2,2,1.0
keywords counterfactual class,2,2,1.0
counterfactual class imbalance,2,2,1.0
imbalance problem reasoning,2,2,1.0
problem reasoning data,2,2,1.0
reasoning data augmentation,2,2,1.0
data augmentation explainable,2,2,1.0
augmentation explainable ai,2,2,1.0
explainable ai introduction,2,2,1.0
ai introduction imbalanced,2,2,1.0
introduction imbalanced datasets,2,2,1.0
imbalanced datasets create,2,2,1.0
datasets create significant,2,2,1.0
create significant problems,2,2,1.0
significant problems for,2,2,1.0
problems for machine,2,2,1.0
for machine learning,4,3,1.3333333333333333
learning ml in,2,2,1.0
ml in classification,2,2,1.0
in classification tasks,2,2,1.0
classification tasks classically,2,2,1.0
tasks classically this,2,2,1.0
classically this problem,2,2,1.0
this problem arises,2,2,1.0
problem arises in,2,2,1.0
arises in binary,2,2,1.0
in binary classification,4,2,2.0
binary classification tasks,4,2,2.0
classification tasks when,2,2,1.0
tasks when most,1,1,1.0
when most of,1,1,1.0
most of data,1,1,1.0
of data comes,2,2,1.0
data comes from,2,2,1.0
comes from one,2,2,1.0
from one class,2,2,1.0
one class the,2,2,1.0
class the majority,2,2,1.0
the majority class,96,6,16.0
majority class and,18,4,4.5
class and less,2,2,1.0
and less comes,2,2,1.0
less comes from,2,2,1.0
comes from the,2,2,1.0
from the other,2,2,1.0
the other class,4,3,1.3333333333333333
other class the,2,2,1.0
class the minority,2,2,1.0
minority class for,7,4,1.75
class for instance,3,2,1.5
for instance in,3,3,1.0
instance in datasets,2,2,1.0
in datasets always,2,2,1.0
datasets always have,2,2,1.0
always have many,2,2,1.0
have many more,2,2,1.0
instances than fraudulent,2,2,1.0
fraudulent ones simply,2,2,1.0
ones simply because,2,2,1.0
simply because the,2,2,1.0
because the latter,2,2,1.0
the latter are,2,2,1.0
latter are rarer,2,2,1.0
are rarer than,2,2,1.0
rarer than the,2,2,1.0
than the former,2,2,1.0
the former when,2,2,1.0
former when a,2,2,1.0
when a given,3,3,1.0
a given class,2,2,1.0
given class is,2,2,1.0
class is in,2,2,1.0
is in the,2,2,1.0
in the dataset,26,4,6.5
the dataset in,6,4,1.5
dataset in this,3,3,1.0
in this way,7,4,1.75
this way a,2,2,1.0
way a classifier,2,2,1.0
a classifier s,4,2,2.0
classifier s performance,5,3,1.6666666666666667
s performance can,2,2,1.0
performance can be,2,2,1.0
can be compromised,2,2,1.0
be compromised in,2,2,1.0
compromised in several,2,2,1.0
in several ways,2,2,1.0
several ways for,2,2,1.0
ways for instance,2,2,1.0
for instance it,4,3,1.3333333333333333
instance it may,2,2,1.0
it may show,2,2,1.0
may show poor,2,2,1.0
show poor accuracy,2,2,1.0
poor accuracy in,2,2,1.0
accuracy in predicting,2,2,1.0
in predicting the,2,2,1.0
predicting the minority,2,2,1.0
minority class or,6,4,1.5
class or spuriously,2,2,1.0
or spuriously high,2,2,1.0
spuriously high accuracy,2,2,1.0
high accuracy for,2,2,1.0
accuracy for the,2,2,1.0
for the classifier,6,2,3.0
the classifier as,2,2,1.0
classifier as a,2,2,1.0
as a whole,2,2,1.0
a whole based,2,2,1.0
whole based only,2,2,1.0
based only on,2,2,1.0
only on its,2,2,1.0
on its success,2,2,1.0
its success with,2,2,1.0
success with the,2,2,1.0
with the majority,9,3,3.0
majority class it,3,3,1.0
class it can,2,2,1.0
it can result,2,2,1.0
can result in,2,2,1.0
result in poor,2,2,1.0
in poor rule,2,2,1.0
poor rule induction,2,2,1.0
rule induction for,2,2,1.0
induction for decision,2,2,1.0
for decision trees,2,2,1.0
decision trees this,2,2,1.0
trees this problem,2,2,1.0
this problem has,2,2,1.0
problem has been,5,3,1.6666666666666667
has been recognized,4,3,1.3333333333333333
been recognized in,3,3,1.0
recognized in many,3,3,1.0
in many application,3,3,1.0
many application domains,4,4,1.0
application domains such,3,3,1.0
domains such as,3,3,1.0
such as medical,3,3,1.0
as medical diagnosis,2,2,1.0
medical diagnosis fraud,1,1,1.0
diagnosis fraud detection,1,1,1.0
fraud detection text,2,2,1.0
detection text classification,2,2,1.0
text classification and,2,2,1.0
classification and detection,2,2,1.0
and detection of,2,2,1.0
detection of oil,6,4,1.5
of oil spills,6,4,1.5
oil spills in,6,4,1.5
spills in satellite,6,4,1.5
in satellite radar,6,4,1.5
satellite radar images,7,4,1.75
radar images notably,2,2,1.0
images notably recently,2,2,1.0
notably recently some,2,2,1.0
recently some of,2,2,1.0
some of the,8,5,1.6
of the techniques,2,2,1.0
the techniques proposed,3,3,1.0
techniques proposed to,2,2,1.0
imbalance problem have,1,1,1.0
problem have also,2,2,1.0
have also proved,2,2,1.0
also proved useful,2,2,1.0
proved useful in,2,2,1.0
useful in data,2,2,1.0
in data augmentation,9,2,4.5
data augmentation for,8,2,4.0
augmentation for deep,6,2,3.0
for deep learning,6,2,3.0
deep learning models,2,2,1.0
learning models when,2,2,1.0
models when new,2,2,1.0
when new synthetic,2,2,1.0
new synthetic keane,2,2,1.0
synthetic keane counterfactual,2,2,1.0
data augmentation points,4,2,2.0
augmentation points need,2,2,1.0
points need to,2,2,1.0
need to be,11,4,2.75
to be generated,6,4,1.5
be generated to,2,2,1.0
generated to create,2,2,1.0
to create the,5,3,1.6666666666666667
create the large,2,2,1.0
the large labelled,2,2,1.0
large labelled datasets,2,2,1.0
labelled datasets required,2,2,1.0
datasets required for,2,2,1.0
required for better,2,2,1.0
for better performance,2,2,1.0
better performance in,2,2,1.0
performance in this,4,3,1.3333333333333333
in this literature,2,2,1.0
this literature several,2,2,1.0
literature several important,2,2,1.0
several important approaches,2,2,1.0
important approaches have,2,2,1.0
approaches have emerged,2,2,1.0
have emerged to,2,2,1.0
emerged to deal,2,2,1.0
to deal with,8,4,2.0
deal with this,2,2,1.0
with this problem,2,2,1.0
this problem based,2,2,1.0
problem based on,2,2,1.0
based on solutions,2,2,1.0
on solutions at,2,2,1.0
solutions at the,2,2,1.0
at the data,2,2,1.0
the data or,2,2,1.0
data or algorithm,2,2,1.0
or algorithm levels,2,2,1.0
algorithm levels data,2,2,1.0
levels data level,2,2,1.0
data level solutions,4,2,2.0
level solutions attempt,2,2,1.0
solutions attempt to,2,2,1.0
attempt to change,2,2,1.0
to change the,2,2,1.0
change the distribution,2,2,1.0
the distribution of,4,4,1.0
distribution of the,6,3,2.0
of the imbalanced,4,4,1.0
the imbalanced data,2,2,1.0
imbalanced data by,4,2,2.0
data by the,3,3,1.0
by the original,2,2,1.0
the original data,6,4,1.5
original data typically,1,1,1.0
data typically these,1,1,1.0
typically these techniques,4,2,2.0
these techniques either,2,2,1.0
techniques either oversample,2,2,1.0
either oversample the,2,2,1.0
oversample the minority,6,2,3.0
class or undersample,2,2,1.0
or undersample the,2,2,1.0
undersample the majority,4,2,2.0
majority class or,4,2,2.0
class or sample,2,2,1.0
or sample using,2,2,1.0
sample using some,2,2,1.0
using some combination,2,2,1.0
some combination of,2,2,1.0
combination of both,3,3,1.0
of both of,2,2,1.0
both of these,2,2,1.0
of these methods,4,2,2.0
these methods specifically,2,2,1.0
methods specifically the,2,2,1.0
specifically the synthetic,2,2,1.0
the synthetic minority,4,4,1.0
synthetic minority technique,12,5,2.4
minority technique smote,5,3,1.6666666666666667
technique smote has,2,2,1.0
smote has become,2,2,1.0
has become a,3,3,1.0
become a very,2,2,1.0
a very popular,2,2,1.0
very popular method,2,2,1.0
popular method for,4,2,2.0
method for solving,2,2,1.0
for solving issues,1,1,1.0
solving issues in,1,1,1.0
issues in traditional,2,2,1.0
in traditional ml,2,2,1.0
traditional ml and,2,2,1.0
ml and has,2,2,1.0
and has also,2,2,1.0
has also been,3,3,1.0
also been applied,2,2,1.0
been applied to,6,2,3.0
applied to the,5,3,1.6666666666666667
to the data,7,5,1.4
the data augmentation,4,2,2.0
data augmentation problem,1,1,1.0
augmentation problem in,1,1,1.0
problem in deep,2,2,1.0
in deep learners,2,2,1.0
deep learners algorithm,2,2,1.0
learners algorithm level,2,2,1.0
algorithm level solutions,2,2,1.0
level solutions aim,2,2,1.0
solutions aim to,2,2,1.0
aim to modify,2,2,1.0
to modify the,3,3,1.0
modify the machine,2,2,1.0
the machine learning,4,3,1.3333333333333333
learning algorithms used,2,2,1.0
algorithms used to,2,2,1.0
used to mitigate,2,2,1.0
to mitigate their,2,2,1.0
mitigate their bias,2,2,1.0
their bias towards,2,2,1.0
bias towards majority,2,2,1.0
towards majority groups,2,2,1.0
majority groups typically,2,2,1.0
groups typically these,2,2,1.0
these techniques involve,2,2,1.0
techniques involve the,2,2,1.0
involve the use,2,2,1.0
the use of,18,6,3.0
use of and,2,2,1.0
of and ensemble,2,2,1.0
and ensemble methods,2,2,1.0
ensemble methods in,2,2,1.0
methods in this,3,3,1.0
paper we explore,2,2,1.0
we explore a,2,2,1.0
explore a novel,2,2,1.0
a novel approach,2,2,1.0
novel approach to,2,2,1.0
approach to both,2,2,1.0
to both the,2,2,1.0
both the class,2,2,1.0
class imbalance and,7,4,1.75
imbalance and data,1,1,1.0
and data augmentation,5,2,2.5
data augmentation problems,5,2,2.5
augmentation problems using,2,2,1.0
problems using an,2,2,1.0
using an counterfactual,2,2,1.0
an counterfactual method,2,2,1.0
counterfactual method that,2,2,1.0
method that generates,2,2,1.0
generates synthetic in,2,2,1.0
synthetic in the,6,3,2.0
minority class interestingly,2,2,1.0
class interestingly this,2,2,1.0
interestingly this method,2,2,1.0
this method was,2,2,1.0
method was previously,2,2,1.0
was previously developed,2,2,1.0
previously developed to,2,2,1.0
developed to solve,1,1,1.0
to solve problems,5,2,2.5
solve problems in,1,1,1.0
problems in explainable,2,2,1.0
in explainable ai,2,2,1.0
explainable ai xai,4,2,2.0
ai xai for,2,2,1.0
xai for reviews,2,2,1.0
for reviews see,4,2,2.0
reviews see in,2,2,1.0
see in logic,2,2,1.0
in logic lewis,2,2,1.0
logic lewis proposed,2,2,1.0
lewis proposed that,2,2,1.0
proposed that counterfactuals,2,2,1.0
that counterfactuals are,2,2,1.0
counterfactuals are the,2,2,1.0
are the closest,2,2,1.0
the closest possible,2,2,1.0
closest possible world,2,2,1.0
possible world to,2,2,1.0
world to the,2,2,1.0
to the current,4,2,2.0
the current world,2,2,1.0
current world in,2,2,1.0
world in which,4,2,2.0
in which the,10,4,2.5
which the outcome,4,2,2.0
the outcome is,2,2,1.0
outcome is different,2,2,1.0
is different hence,2,2,1.0
different hence the,2,2,1.0
hence the intuition,2,2,1.0
the intuition behind,2,2,1.0
intuition behind the,2,2,1.0
behind the current,2,2,1.0
the current technique,2,2,1.0
current technique is,2,2,1.0
technique is that,2,2,1.0
is that it,9,4,2.25
that it generates,2,2,1.0
it generates synthetic,3,3,1.0
counterfactual instances using,2,2,1.0
instances using the,4,2,2.0
using the actual,2,2,1.0
the actual of,2,2,1.0
actual of instances,2,2,1.0
of instances not,2,2,1.0
instances not interpolated,2,2,1.0
not interpolated values,2,2,1.0
interpolated values that,4,2,2.0
values that are,2,2,1.0
that are the,2,2,1.0
are the close,2,2,1.0
the close to,2,2,1.0
close to existing,6,2,3.0
to existing thus,1,1,1.0
existing thus populating,1,1,1.0
thus populating the,2,2,1.0
populating the minority,2,2,1.0
minority class with,6,3,2.0
class with plausible,2,2,1.0
with plausible adaptations,2,2,1.0
plausible adaptations of,2,2,1.0
adaptations of existing,1,1,1.0
of existing data,1,1,1.0
existing data if,1,1,1.0
data if this,2,2,1.0
if this intuition,2,2,1.0
this intuition is,2,2,1.0
intuition is correct,2,2,1.0
is correct then,2,2,1.0
correct then the,2,2,1.0
then the synthetic,2,2,1.0
the synthetic instances,7,4,1.75
synthetic instances generated,6,3,2.0
instances generated by,3,3,1.0
generated by this,2,2,1.0
by this counterfactual,1,1,1.0
this counterfactual method,8,2,4.0
counterfactual method should,1,1,1.0
method should improve,2,2,1.0
should improve ml,2,2,1.0
improve ml performance,2,2,1.0
ml performance perhaps,2,2,1.0
performance perhaps to,2,2,1.0
perhaps to a,2,2,1.0
to a level,2,2,1.0
a level that,2,2,1.0
level that advances,2,2,1.0
that advances current,2,2,1.0
advances current techniques,2,2,1.0
current techniques temraz,2,2,1.0
techniques temraz keane,2,2,1.0
data augmentation related,2,2,1.0
augmentation related work,2,2,1.0
related work the,2,2,1.0
work the related,2,2,1.0
the related work,2,2,1.0
related work to,2,2,1.0
work to the,2,2,1.0
to the present,4,2,2.0
the present research,2,2,1.0
present research comes,2,2,1.0
research comes from,2,2,1.0
comes from two,2,2,1.0
from two different,2,2,1.0
two different strands,2,2,1.0
different strands of,2,2,1.0
strands of ai,2,2,1.0
of ai research,2,2,1.0
ai research from,2,2,1.0
research from i,2,2,1.0
from i sampling,2,2,1.0
i sampling techniques,2,2,1.0
sampling techniques for,2,2,1.0
techniques for the,4,4,1.0
for the class,10,4,2.5
imbalance problem and,9,5,1.8
problem and ii,2,2,1.0
and ii counterfactual,2,2,1.0
ii counterfactual methods,2,2,1.0
counterfactual methods for,4,2,2.0
methods for explainable,2,2,1.0
for explainable ai,2,2,1.0
ai xai data,2,2,1.0
xai data level,2,2,1.0
level solutions to,2,2,1.0
solutions to the,5,3,1.6666666666666667
to the class,8,4,2.0
imbalance problem are,2,2,1.0
problem are dominated,2,2,1.0
are dominated by,2,2,1.0
dominated by three,2,2,1.0
by three main,2,2,1.0
three main approaches,2,2,1.0
main approaches random,2,2,1.0
approaches random ros,2,2,1.0
random ros random,2,2,1.0
ros random rus,2,2,1.0
random rus and,2,2,1.0
rus and synthetic,2,2,1.0
and synthetic minority,2,2,1.0
technique smote in,2,2,1.0
smote in ros,2,2,1.0
in ros the,2,2,1.0
ros the class,2,2,1.0
the class distribution,4,3,1.3333333333333333
class distribution is,3,3,1.0
distribution is balanced,2,2,1.0
is balanced by,2,2,1.0
balanced by randomly,2,2,1.0
by randomly adding,2,2,1.0
randomly adding multiple,2,2,1.0
adding multiple copies,2,2,1.0
multiple copies of,2,2,1.0
copies of some,2,2,1.0
of some of,2,2,1.0
of the minority,40,6,6.666666666666667
the minority classes,4,4,1.0
minority classes to,2,2,1.0
classes to the,2,2,1.0
to the training,4,3,1.3333333333333333
the training data,17,4,4.25
training data whereas,2,2,1.0
data whereas with,2,2,1.0
whereas with rus,2,2,1.0
with rus a,2,2,1.0
rus a certain,2,2,1.0
a certain number,2,2,1.0
certain number of,2,2,1.0
number of examples,2,2,1.0
of examples of,2,2,1.0
examples of the,6,4,1.5
of the majority,24,6,4.0
majority class are,3,3,1.0
class are randomly,2,2,1.0
are randomly removed,2,2,1.0
randomly removed from,2,2,1.0
removed from the,2,2,1.0
from the original,6,2,3.0
the original dataset,14,3,4.666666666666667
original dataset although,2,2,1.0
dataset although these,2,2,1.0
although these methods,2,2,1.0
these methods can,2,2,1.0
methods can the,2,2,1.0
can the original,2,2,1.0
original dataset they,2,2,1.0
dataset they have,2,2,1.0
they have some,2,2,1.0
have some since,1,1,1.0
some since ros,1,1,1.0
since ros merely,2,2,1.0
ros merely copies,2,2,1.0
merely copies instances,2,2,1.0
copies instances no,2,2,1.0
instances no new,2,2,1.0
no new information,2,2,1.0
new information is,2,2,1.0
information is added,2,2,1.0
is added to,2,2,1.0
added to the,6,2,3.0
to the and,1,1,1.0
the and hence,1,1,1.0
and hence it,2,2,1.0
hence it can,2,2,1.0
it can lead,2,2,1.0
can lead to,2,2,1.0
lead to overfitting,2,2,1.0
to overfitting on,2,2,1.0
overfitting on the,2,2,1.0
on the other,9,6,1.5
the other hand,8,5,1.6
other hand since,2,2,1.0
hand since rus,2,2,1.0
since rus randomly,2,2,1.0
rus randomly removes,1,1,1.0
randomly removes examples,1,1,1.0
removes examples from,1,1,1.0
examples from the,5,3,1.6666666666666667
from the majority,8,4,2.0
majority class data,2,2,1.0
class data can,2,2,1.0
data can be,3,3,1.0
can be discarded,2,2,1.0
be discarded that,2,2,1.0
discarded that may,2,2,1.0
that may be,2,2,1.0
may be important,2,2,1.0
be important the,2,2,1.0
important the third,2,2,1.0
the third option,2,2,1.0
third option smote,2,2,1.0
option smote adopts,2,2,1.0
smote adopts a,2,2,1.0
adopts a somewhat,2,2,1.0
a somewhat different,2,2,1.0
somewhat different approach,2,2,1.0
different approach based,2,2,1.0
approach based on,5,3,1.6666666666666667
based on oversampling,4,2,2.0
on oversampling from,2,2,1.0
oversampling from the,2,2,1.0
from the class,4,4,1.0
the class as,3,2,1.5
class as smote,2,2,1.0
as smote is,2,2,1.0
smote is the,2,2,1.0
is the baseline,2,2,1.0
the baseline method,2,2,1.0
baseline method used,2,2,1.0
method used for,2,2,1.0
used for comparisons,2,2,1.0
for comparisons in,2,2,1.0
comparisons in the,2,2,1.0
in the present,2,2,1.0
the present experiments,2,2,1.0
present experiments we,2,2,1.0
experiments we briefly,2,2,1.0
we briefly describe,2,2,1.0
briefly describe it,2,2,1.0
describe it in,2,2,1.0
it in more,2,2,1.0
in more detail,2,2,1.0
more detail here,2,2,1.0
detail here see,2,2,1.0
here see section,2,2,1.0
see section along,2,2,1.0
section along with,2,2,1.0
along with important,2,2,1.0
with important smote,2,2,1.0
important smote variants,2,2,1.0
smote variants see,2,2,1.0
variants see section,2,2,1.0
see section before,2,2,1.0
section before going,2,2,1.0
before going on,2,2,1.0
going on to,2,2,1.0
on to describe,2,2,1.0
to describe the,2,2,1.0
describe the counterfactual,2,2,1.0
the counterfactual method,11,2,5.5
counterfactual method we,2,2,1.0
method we have,2,2,1.0
we have adapted,2,2,1.0
have adapted from,2,2,1.0
adapted from the,2,2,1.0
from the xai,6,2,3.0
the xai literature,6,2,3.0
xai literature see,2,2,1.0
literature see section,2,2,1.0
see section finally,2,2,1.0
section finally we,2,2,1.0
finally we briefly,2,2,1.0
we briefly sketch,4,2,2.0
briefly sketch the,2,2,1.0
sketch the recent,2,2,1.0
the recent and,2,2,1.0
recent and very,2,2,1.0
and very small,2,2,1.0
very small literature,2,2,1.0
small literature that,2,2,1.0
literature that has,2,2,1.0
that has begun,2,2,1.0
has begun to,2,2,1.0
begun to apply,2,2,1.0
to apply these,2,2,1.0
apply these counterfactual,2,2,1.0
these counterfactual xai,2,2,1.0
counterfactual xai methods,2,2,1.0
xai methods to,2,2,1.0
methods to and,2,2,1.0
to and data,2,2,1.0
data augmentation see,1,1,1.0
augmentation see section,1,1,1.0
see section temraz,2,2,1.0
section temraz keane,2,2,1.0
data augmentation data,2,2,1.0
augmentation data sampling,2,2,1.0
data sampling methods,2,2,1.0
sampling methods for,2,2,1.0
methods for the,4,3,1.3333333333333333
imbalance problem smote,2,2,1.0
problem smote synthetic,2,2,1.0
smote synthetic minority,11,6,1.8333333333333333
synthetic minority oversampling,11,6,1.8333333333333333
minority oversampling technique,12,5,2.4
oversampling technique smote,3,3,1.0
technique smote oversamples,2,2,1.0
smote oversamples the,2,2,1.0
oversamples the minority,2,2,1.0
minority class by,4,4,1.0
class by creating,2,2,1.0
by creating synthetic,2,2,1.0
creating synthetic instances,2,2,1.0
synthetic instances rather,2,2,1.0
instances rather than,4,2,2.0
rather than by,2,2,1.0
than by oversampling,2,2,1.0
by oversampling using,2,2,1.0
oversampling using replacement,2,2,1.0
using replacement it,2,2,1.0
replacement it is,2,2,1.0
it is one,2,2,1.0
is one of,6,4,1.5
one of the,25,5,5.0
of the most,9,4,2.25
the most widely,4,3,1.3333333333333333
most widely used,4,3,1.3333333333333333
widely used solutions,2,2,1.0
used solutions to,2,2,1.0
to the problem,5,3,1.6666666666666667
the problem google,2,2,1.0
problem google scholar,2,2,1.0
google scholar lists,2,2,1.0
scholar lists over,2,2,1.0
lists over citations,2,2,1.0
over citations to,2,2,1.0
citations to the,2,2,1.0
to the original,9,4,2.25
the original paper,2,2,1.0
original paper in,2,2,1.0
paper in smote,2,2,1.0
in smote the,3,3,1.0
smote the new,2,2,1.0
the new example,2,2,1.0
new example in,2,2,1.0
example in the,5,4,1.25
minority class is,12,6,2.0
class is created,1,1,1.0
is created by,1,1,1.0
created by between,1,1,1.0
by between several,1,1,1.0
between several minority,3,3,1.0
several minority class,3,3,1.0
minority class instances,5,3,1.6666666666666667
class instances by,3,3,1.0
instances by interpolating,2,2,1.0
by interpolating instead,3,3,1.0
interpolating instead of,3,3,1.0
instead of copying,2,2,1.0
of copying instances,2,2,1.0
copying instances smote,2,2,1.0
instances smote avoids,2,2,1.0
smote avoids the,3,3,1.0
avoids the problem,3,3,1.0
the problem and,3,3,1.0
problem and creates,1,1,1.0
and creates new,1,1,1.0
creates new synthetic,1,1,1.0
new synthetic instances,2,2,1.0
instances in neighborhoods,2,2,1.0
in neighborhoods surrounding,2,2,1.0
neighborhoods surrounding instances,2,2,1.0
surrounding instances in,2,2,1.0
minority class briefly,2,2,1.0
class briefly the,2,2,1.0
briefly the algorithm,2,2,1.0
the algorithm works,2,2,1.0
algorithm works as,2,2,1.0
works as follows,2,2,1.0
as follows assume,2,2,1.0
follows assume that,2,2,1.0
assume that the,7,4,1.75
that the minority,5,3,1.6666666666666667
class is 𝑃,3,2,1.5
is 𝑃 and,4,2,2.0
𝑃 and the,4,2,2.0
and the majority,5,3,1.6666666666666667
majority class is,6,3,2.0
class is smote,2,2,1.0
is smote starts,2,2,1.0
smote starts by,2,2,1.0
starts by randomly,2,2,1.0
by randomly selecting,3,3,1.0
randomly selecting a,2,2,1.0
selecting a minority,2,2,1.0
a minority instance,6,2,3.0
minority instance 𝑝,4,2,2.0
instance 𝑝 from,2,2,1.0
𝑝 from the,4,2,2.0
from the minority,8,4,2.0
minority class 𝑃,6,2,3.0
class 𝑃 and,2,2,1.0
𝑃 and then,2,2,1.0
and then determines,1,1,1.0
then determines 𝑚,1,1,1.0
determines 𝑚 as,1,1,1.0
𝑚 as the,2,2,1.0
as the nearest,2,2,1.0
the nearest neighbors,9,5,1.8
nearest neighbors of,17,4,4.25
neighbors of 𝑝,6,2,3.0
of 𝑝 after,2,2,1.0
𝑝 after determining,2,2,1.0
after determining 𝑚,2,2,1.0
determining 𝑚 nearest,2,2,1.0
𝑚 nearest neighbors,8,2,4.0
of 𝑝 it,2,2,1.0
𝑝 it selects,2,2,1.0
it selects a,2,2,1.0
selects a random,2,2,1.0
a random neighbor,2,2,1.0
random neighbor 𝑚,2,2,1.0
neighbor 𝑚 where,2,2,1.0
𝑚 where 𝑚,2,2,1.0
where 𝑚 smote,1,1,1.0
𝑚 smote creates,1,1,1.0
smote creates a,2,2,1.0
creates a new,2,2,1.0
a new instance,2,2,1.0
new instance 𝑝,2,2,1.0
instance 𝑝 using,2,2,1.0
𝑝 using the,2,2,1.0
using the following,2,2,1.0
the following formula,2,2,1.0
following formula 𝑝,2,2,1.0
formula 𝑝 𝑝,2,2,1.0
𝑝 𝑝 𝑚,2,2,1.0
𝑝 𝑚 where,1,1,1.0
𝑚 where 𝛿,1,1,1.0
where 𝛿 is,2,2,1.0
𝛿 is a,2,2,1.0
is a random,5,4,1.25
a random number,6,4,1.5
random number between,4,4,1.0
number between and,3,3,1.0
between and this,2,2,1.0
and this new,2,2,1.0
this new instance,6,2,3.0
new instance is,2,2,1.0
instance is then,2,2,1.0
is then added,4,2,2.0
then added to,4,2,2.0
to the dataset,5,3,1.6666666666666667
the dataset for,4,2,2.0
dataset for the,3,3,1.0
for the minority,7,4,1.75
minority class one,2,2,1.0
class one of,2,2,1.0
of the potential,3,3,1.0
the potential problems,2,2,1.0
potential problems with,2,2,1.0
problems with smote,2,2,1.0
with smote is,2,2,1.0
smote is that,2,2,1.0
is that its,2,2,1.0
that its generation,2,2,1.0
its generation of,2,2,1.0
generation of minority,4,2,2.0
of minority instances,4,2,2.0
minority instances is,2,2,1.0
instances is done,2,2,1.0
is done without,2,2,1.0
done without reference,2,2,1.0
without reference to,2,2,1.0
reference to the,2,2,1.0
to the majority,15,5,3.0
class or indeed,2,2,1.0
or indeed any,2,2,1.0
indeed any consideration,1,1,1.0
any consideration that,1,1,1.0
consideration that some,1,1,1.0
that some minority,2,2,1.0
some minority instances,4,2,2.0
minority instances may,2,2,1.0
instances may be,2,2,1.0
may be better,2,2,1.0
be better than,2,2,1.0
better than others,2,2,1.0
than others to,2,2,1.0
others to use,2,2,1.0
to use in,2,2,1.0
use in this,2,2,1.0
in this process,2,2,1.0
this process another,2,2,1.0
process another issue,2,2,1.0
another issue is,2,2,1.0
issue is that,2,2,1.0
that it may,2,2,1.0
it may introduce,2,2,1.0
may introduce noise,2,2,1.0
introduce noise by,2,2,1.0
noise by generating,2,2,1.0
by generating interpolated,2,2,1.0
generating interpolated values,2,2,1.0
values that do,2,2,1.0
that do not,5,3,1.6666666666666667
do not exist,1,1,1.0
not exist in,1,1,1.0
exist in the,3,2,1.5
in the domain,2,2,1.0
the domain the,2,2,1.0
domain the interpolated,2,2,1.0
the interpolated value,2,2,1.0
interpolated value could,2,2,1.0
value could be,2,2,1.0
could be accordingly,2,2,1.0
be accordingly many,2,2,1.0
accordingly many extensions,2,2,1.0
many extensions have,2,2,1.0
extensions have been,2,2,1.0
have been made,3,3,1.0
been made to,3,3,1.0
made to smote,2,2,1.0
to smote that,2,2,1.0
smote that improve,4,2,2.0
that improve on,4,2,2.0
improve on its,2,2,1.0
on its operation,2,2,1.0
its operation in,2,2,1.0
operation in the,2,2,1.0
in the following,4,4,1.0
the following we,2,2,1.0
following we review,2,2,1.0
we review the,2,2,1.0
review the smote,2,2,1.0
the smote variants,2,2,1.0
smote variants that,4,2,2.0
variants that are,2,2,1.0
that are closest,2,2,1.0
are closest to,2,2,1.0
closest to the,2,2,1.0
the current method,2,2,1.0
current method proposed,2,2,1.0
method proposed to,2,2,1.0
proposed to reveal,2,2,1.0
to reveal how,2,2,1.0
reveal how it,2,2,1.0
how it differs,2,2,1.0
it differs temraz,2,2,1.0
differs temraz keane,2,2,1.0
data augmentation smote,8,2,4.0
augmentation smote variants,2,2,1.0
smote variants three,2,2,1.0
variants three key,2,2,1.0
three key insights,4,2,2.0
key insights there,2,2,1.0
insights there are,2,2,1.0
there are many,2,2,1.0
are many variants,2,2,1.0
many variants of,2,2,1.0
of smote that,2,2,1.0
improve on the,2,2,1.0
on the original,5,3,1.6666666666666667
the original s,2,2,1.0
original s performance,2,2,1.0
s performance based,2,2,1.0
performance based on,2,2,1.0
based on several,2,2,1.0
on several insights,2,2,1.0
several insights about,2,2,1.0
insights about how,2,2,1.0
about how to,2,2,1.0
how to solve,2,2,1.0
solve the problem,2,2,1.0
the problem so,2,2,1.0
problem so these,2,2,1.0
so these variants,2,2,1.0
these variants often,2,2,1.0
variants often hinge,2,2,1.0
often hinge on,2,2,1.0
hinge on regions,1,1,1.0
on regions in,1,1,1.0
regions in the,8,2,4.0
minority class they,4,2,2.0
class they emphasise,2,2,1.0
they emphasise the,2,2,1.0
emphasise the importance,1,1,1.0
the importance of,7,2,3.5
importance of focusing,1,1,1.0
of focusing on,2,2,1.0
focusing on the,5,5,1.0
on the border,3,3,1.0
the border region,2,2,1.0
border region between,2,2,1.0
region between the,2,2,1.0
between the majority,4,2,2.0
the majority and,6,4,1.5
majority and minority,16,5,3.2
and minority classes,6,4,1.5
minority classes and,2,2,1.0
classes and sometimes,2,2,1.0
and sometimes analyze,1,1,1.0
sometimes analyze the,1,1,1.0
analyze the majority,1,1,1.0
majority class with,2,2,1.0
class with respect,2,2,1.0
with respect to,18,5,3.6
respect to the,10,4,2.5
to the minority,20,5,4.0
the minority to,2,2,1.0
minority to guide,2,2,1.0
to guide smote,4,2,2.0
guide smote in,2,2,1.0
smote in selected,2,2,1.0
in selected regions,2,2,1.0
selected regions one,2,2,1.0
regions one critical,2,2,1.0
one critical improvement,2,2,1.0
critical improvement to,2,2,1.0
improvement to the,2,2,1.0
the original smote,4,2,2.0
original smote method,4,2,2.0
smote method hinges,2,2,1.0
method hinges on,2,2,1.0
hinges on the,2,2,1.0
on the insight,2,2,1.0
the insight that,2,2,1.0
insight that not,2,2,1.0
that not all,3,3,1.0
not all regions,2,2,1.0
all regions in,2,2,1.0
minority class are,4,4,1.0
class are equal,2,2,1.0
are equal some,2,2,1.0
equal some may,2,2,1.0
some may be,2,2,1.0
may be more,4,2,2.0
be more or,1,1,1.0
more or safer,1,1,1.0
or safer than,2,2,1.0
safer than others,2,2,1.0
than others within,2,2,1.0
others within which,2,2,1.0
within which to,4,2,2.0
which to apply,2,2,1.0
to apply smote,2,2,1.0
apply smote for,2,2,1.0
smote for instance,2,2,1.0
for instance smote,1,1,1.0
instance smote clusters,1,1,1.0
smote clusters minority,2,2,1.0
clusters minority instances,2,2,1.0
minority instances into,4,2,2.0
instances into k,2,2,1.0
into k clusters,2,2,1.0
k clusters and,2,2,1.0
clusters and then,2,2,1.0
and then oversamples,1,1,1.0
then oversamples from,1,1,1.0
oversamples from clusters,1,1,1.0
from clusters with,2,2,1.0
clusters with the,3,3,1.0
with the most,2,2,1.0
the most the,1,1,1.0
most the assumption,1,1,1.0
the assumption being,2,2,1.0
assumption being that,2,2,1.0
being that these,2,2,1.0
that these are,2,2,1.0
these are safer,2,2,1.0
are safer regions,2,2,1.0
safer regions and,2,2,1.0
regions and are,2,2,1.0
and are less,2,2,1.0
are less likely,2,2,1.0
less likely to,4,2,2.0
likely to generate,4,2,2.0
to generate noise,2,2,1.0
generate noise see,2,2,1.0
noise see for,2,2,1.0
see for a,2,2,1.0
for a related,2,2,1.0
a related solution,2,2,1.0
related solution other,2,2,1.0
solution other versions,2,2,1.0
other versions of,2,2,1.0
versions of this,2,2,1.0
of this approach,3,3,1.0
this approach have,2,2,1.0
approach have used,2,2,1.0
have used dbscan,2,2,1.0
used dbscan a,2,2,1.0
dbscan a based,2,2,1.0
a based clustering,2,2,1.0
based clustering algorithm,2,2,1.0
clustering algorithm to,2,2,1.0
algorithm to identify,2,2,1.0
to identify safe,2,2,1.0
identify safe regions,2,2,1.0
safe regions or,2,2,1.0
regions or use,2,2,1.0
or use representative,2,2,1.0
use representative points,2,2,1.0
representative points within,2,2,1.0
points within to,1,1,1.0
within to guide,1,1,1.0
guide smote some,2,2,1.0
smote some methods,2,2,1.0
some methods project,2,2,1.0
methods project the,2,2,1.0
project the minority,2,2,1.0
minority class into,2,2,1.0
class into a,2,2,1.0
into a lower,4,2,2.0
a lower dimension,4,2,2.0
lower dimension before,2,2,1.0
dimension before applying,2,2,1.0
before applying smote,2,2,1.0
applying smote to,4,2,2.0
smote to the,4,2,2.0
to the clusters,2,2,1.0
the clusters found,2,2,1.0
clusters found for,2,2,1.0
found for instance,2,2,1.0
for instance somo,2,2,1.0
instance somo uses,2,2,1.0
somo uses a,2,2,1.0
uses a map,2,2,1.0
a map to,2,2,1.0
map to transform,2,2,1.0
to transform datasets,2,2,1.0
transform datasets into,2,2,1.0
datasets into a,2,2,1.0
into a space,1,1,1.0
a space and,1,1,1.0
space and uses,1,1,1.0
and uses a,2,2,1.0
uses a embedding,2,2,1.0
a embedding algorithm,2,2,1.0
embedding algorithm to,2,2,1.0
algorithm to project,2,2,1.0
to project into,2,2,1.0
project into a,2,2,1.0
lower dimension where,2,2,1.0
dimension where the,2,2,1.0
where the datasets,2,2,1.0
the datasets are,2,2,1.0
datasets are more,2,2,1.0
are more separable,2,2,1.0
more separable still,2,2,1.0
separable still others,2,2,1.0
still others such,2,2,1.0
others such as,2,2,1.0
such as and,2,2,1.0
as and adasyn,1,1,1.0
and adasyn explore,2,2,1.0
adasyn explore different,2,2,1.0
explore different ways,2,2,1.0
different ways to,2,2,1.0
ways to identify,2,2,1.0
to identify regions,2,2,1.0
identify regions within,2,2,1.0
regions within which,2,2,1.0
which to generate,2,2,1.0
to generate minority,2,2,1.0
generate minority instances,2,2,1.0
minority instances defines,2,2,1.0
instances defines a,2,2,1.0
defines a geometric,2,2,1.0
a geometric region,2,2,1.0
geometric region around,2,2,1.0
region around each,2,2,1.0
around each minority,2,2,1.0
each minority class,3,3,1.0
minority class instance,3,3,1.0
class instance for,2,2,1.0
instance for generating,2,2,1.0
for generating synthetic,5,3,1.6666666666666667
generating synthetic datapoints,2,2,1.0
synthetic datapoints adasyn,2,2,1.0
datapoints adasyn proposed,2,2,1.0
adasyn proposed by,2,2,1.0
proposed by he,2,2,1.0
by he et,2,2,1.0
he et al,2,2,1.0
et al generates,2,2,1.0
al generates minority,2,2,1.0
generates minority class,2,2,1.0
class instances according,2,2,1.0
instances according to,3,3,1.0
according to their,7,4,1.75
to their generating,1,1,1.0
their generating more,1,1,1.0
generating more synthetic,2,2,1.0
more synthetic data,4,2,2.0
synthetic data from,2,2,1.0
data from minority,2,2,1.0
from minority instances,2,2,1.0
minority instances that,8,2,4.0
instances that are,15,3,5.0
that are harder,2,2,1.0
are harder to,2,2,1.0
harder to learn,2,2,1.0
to learn compared,2,2,1.0
learn compared to,2,2,1.0
compared to minority,3,3,1.0
to minority instances,4,2,2.0
that are easier,2,2,1.0
are easier to,4,2,2.0
easier to learn,2,2,1.0
to learn where,2,2,1.0
learn where is,2,2,1.0
where is related,2,2,1.0
is related to,5,4,1.25
related to the,11,6,1.8333333333333333
to the number,5,5,1.0
the number of,66,6,11.0
number of temraz,2,2,1.0
of temraz keane,2,2,1.0
data augmentation instances,4,2,2.0
augmentation instances in,4,2,2.0
in the neighbors,3,3,1.0
the neighbors that,2,2,1.0
neighbors that belong,3,3,1.0
that belong to,3,3,1.0
belong to the,6,4,1.5
majority class however,4,2,2.0
class however these,2,2,1.0
however these solutions,2,2,1.0
these solutions owe,2,2,1.0
solutions owe a,2,2,1.0
owe a lot,2,2,1.0
a lot to,4,2,2.0
lot to another,2,2,1.0
to another key,2,2,1.0
another key insight,2,2,1.0
key insight namely,2,2,1.0
insight namely that,2,2,1.0
namely that regions,2,2,1.0
that regions close,2,2,1.0
regions close to,4,2,2.0
close to the,21,3,7.0
the class boundary,5,3,1.6666666666666667
class boundary are,1,1,1.0
boundary are particularly,2,2,1.0
are particularly important,4,2,2.0
particularly important for,2,2,1.0
important for instance,2,2,1.0
for instance generation,2,2,1.0
instance generation smote,3,2,1.5
generation smote on,2,2,1.0
smote on the,3,3,1.0
on the borderline,2,2,1.0
the borderline the,2,2,1.0
borderline the idea,2,2,1.0
the idea that,5,3,1.6666666666666667
idea that different,2,2,1.0
that different regions,2,2,1.0
different regions in,2,2,1.0
the dataset need,2,2,1.0
dataset need to,2,2,1.0
to be differently,1,1,1.0
be differently owes,1,1,1.0
differently owes a,2,2,1.0
owes a lot,2,2,1.0
lot to the,2,2,1.0
to the intuition,2,2,1.0
the intuition that,2,2,1.0
intuition that minority,2,2,1.0
that minority instances,2,2,1.0
minority instances close,4,2,2.0
instances close to,4,2,2.0
to the decision,16,3,5.333333333333333
the decision boundary,32,3,10.666666666666666
decision boundary of,2,2,1.0
boundary of the,3,3,1.0
of the classifier,2,2,1.0
the classifier are,2,2,1.0
classifier are particularly,2,2,1.0
particularly important to,2,2,1.0
important to successful,2,2,1.0
to successful classification,1,1,1.0
successful classification so,1,1,1.0
classification so generating,1,1,1.0
so generating minority,2,2,1.0
generating minority in,1,1,1.0
minority in this,1,1,1.0
in this boundary,2,2,1.0
this boundary region,2,2,1.0
boundary region should,2,2,1.0
region should help,2,2,1.0
should help performance,2,2,1.0
help performance more,2,2,1.0
performance more proposed,2,2,1.0
more proposed by,2,2,1.0
proposed by han,2,2,1.0
by han et,2,2,1.0
han et al,3,3,1.0
et al realized,2,2,1.0
al realized this,2,2,1.0
realized this idea,2,2,1.0
this idea by,2,2,1.0
idea by creating,2,2,1.0
by creating instances,2,2,1.0
creating instances using,2,2,1.0
instances using only,2,2,1.0
using only minority,2,2,1.0
only minority instances,2,2,1.0
that are close,4,2,2.0
are close to,4,2,2.0
decision boundary so,4,2,2.0
boundary so again,2,2,1.0
so again assume,2,2,1.0
again assume that,2,2,1.0
class is 𝑋,2,2,1.0
is 𝑋 and,2,2,1.0
𝑋 and the,2,2,1.0
and the whole,2,2,1.0
the whole training,2,2,1.0
whole training set,2,2,1.0
training set is,4,4,1.0
set is in,2,2,1.0
is in for,2,2,1.0
in for every,2,2,1.0
for every minority,2,2,1.0
every minority instance,3,3,1.0
instance 𝑝 in,2,2,1.0
𝑝 in the,4,2,2.0
class 𝑃 the,2,2,1.0
𝑃 the method,2,2,1.0
the method calculates,2,2,1.0
method calculates its,2,2,1.0
calculates its 𝑚,2,2,1.0
its 𝑚 nearest,2,2,1.0
nearest neighbors from,4,4,1.0
neighbors from the,4,4,1.0
from the training,2,2,1.0
the training set,9,4,2.25
training set it,2,2,1.0
set it should,2,2,1.0
it should be,10,3,3.3333333333333335
should be noted,8,3,2.6666666666666665
be noted that,8,3,2.6666666666666665
noted that the,3,3,1.0
that the number,3,3,1.0
number of majority,11,4,2.75
of majority instances,10,2,5.0
majority instances among,2,2,1.0
instances among the,2,2,1.0
among the 𝑚,2,2,1.0
the 𝑚 nearest,4,2,2.0
nearest neighbors is,4,2,2.0
neighbors is called,2,2,1.0
is called as,2,2,1.0
called as 𝑚,2,2,1.0
as 𝑚 in,1,1,1.0
𝑚 in step,2,2,1.0
in step if,2,2,1.0
step if all,2,2,1.0
if all the,2,2,1.0
all the 𝑚,2,2,1.0
of 𝑝 are,2,2,1.0
𝑝 are majority,2,2,1.0
are majority instances,2,2,1.0
majority instances 𝑝,1,1,1.0
instances 𝑝 is,1,1,1.0
𝑝 is considered,2,2,1.0
is considered as,3,3,1.0
considered as noise,2,2,1.0
as noise and,3,3,1.0
noise and is,2,2,1.0
and is not,2,2,1.0
is not used,2,2,1.0
not used in,2,2,1.0
used in the,8,5,1.6
in the next,8,4,2.0
the next step,3,3,1.0
next step if,2,2,1.0
step if the,2,2,1.0
if the set,4,2,2.0
the set of,9,3,3.0
set of 𝑝,2,2,1.0
of 𝑝 s,2,2,1.0
𝑝 s majority,2,2,1.0
s majority nearest,2,2,1.0
majority nearest neighbors,2,2,1.0
neighbors is larger,2,2,1.0
is larger than,2,2,1.0
larger than that,2,2,1.0
than that of,2,2,1.0
that of its,4,2,2.0
of its minority,2,2,1.0
its minority ones,2,2,1.0
minority ones 𝑚,2,2,1.0
ones 𝑚 𝑝,1,1,1.0
𝑚 𝑝 will,2,2,1.0
𝑝 will be,2,2,1.0
will be easily,2,2,1.0
be easily and,1,1,1.0
easily and put,1,1,1.0
and put into,2,2,1.0
put into a,2,2,1.0
into a danger,2,2,1.0
a danger set,2,2,1.0
danger set if,2,2,1.0
set if then,1,1,1.0
if then 𝑝,1,1,1.0
then 𝑝 is,2,2,1.0
𝑝 is safe,2,2,1.0
is safe and,2,2,1.0
safe and does,2,2,1.0
and does not,5,3,1.6666666666666667
does not participate,2,2,1.0
not participate in,2,2,1.0
participate in the,2,2,1.0
in the subsequent,2,2,1.0
the subsequent steps,2,2,1.0
subsequent steps this,2,2,1.0
steps this danger,2,2,1.0
this danger set,2,2,1.0
danger set contains,2,2,1.0
set contains the,2,2,1.0
contains the borderline,2,2,1.0
the borderline instances,2,2,1.0
borderline instances of,2,2,1.0
instances of the,2,2,1.0
minority class finally,2,2,1.0
class finally for,2,2,1.0
finally for each,2,2,1.0
for each instance,3,2,1.5
each instance in,2,2,1.0
instance in the,10,2,5.0
in the danger,2,2,1.0
the danger set,2,2,1.0
danger set the,2,2,1.0
set the neighbors,2,2,1.0
the neighbors from,2,2,1.0
neighbors from 𝑃,2,2,1.0
from 𝑃 are,2,2,1.0
𝑃 are found,2,2,1.0
are found and,2,2,1.0
found and the,6,2,3.0
and the steps,2,2,1.0
the steps from,2,2,1.0
steps from the,2,2,1.0
smote method are,2,2,1.0
method are applied,2,2,1.0
are applied to,2,2,1.0
applied to them,2,2,1.0
to them to,2,2,1.0
them to generate,4,2,2.0
to generate synthetic,15,5,3.0
generate synthetic instances,5,2,2.5
minority class this,4,2,2.0
class this insight,2,2,1.0
this insight about,2,2,1.0
insight about the,2,2,1.0
about the importance,2,2,1.0
importance of the,4,2,2.0
of the boundary,2,2,1.0
the boundary regions,2,2,1.0
boundary regions has,2,2,1.0
regions has been,2,2,1.0
has been exploited,2,2,1.0
been exploited in,2,2,1.0
exploited in different,2,2,1.0
in different ways,3,3,1.0
different ways for,2,2,1.0
ways for example,2,2,1.0
for example uses,2,2,1.0
example uses an,2,2,1.0
uses an svm,2,2,1.0
an svm to,2,2,1.0
svm to approximate,2,2,1.0
to approximate the,3,3,1.0
approximate the decision,2,2,1.0
the decision and,1,1,1.0
decision and then,1,1,1.0
and then generates,2,2,1.0
then generates new,2,2,1.0
generates new synthetic,2,2,1.0
new synthetic data,4,3,1.3333333333333333
synthetic data along,2,2,1.0
data along the,2,2,1.0
along the lines,2,2,1.0
the lines joining,2,2,1.0
lines joining each,2,2,1.0
joining each temraz,2,2,1.0
each temraz keane,2,2,1.0
data augmentation with,2,2,1.0
augmentation with its,2,2,1.0
with its nearest,2,2,1.0
its nearest neighbors,4,3,1.3333333333333333
nearest neighbors using,2,2,1.0
neighbors using interpolation,2,2,1.0
using interpolation or,2,2,1.0
interpolation or extrapolation,2,2,1.0
or extrapolation techniques,2,2,1.0
extrapolation techniques in,2,2,1.0
techniques in a,2,2,1.0
in a similar,1,1,1.0
a similar vein,1,1,1.0
similar vein divides,1,1,1.0
vein divides minority,1,1,1.0
divides minority instances,2,2,1.0
instances into three,2,2,1.0
into three groups,2,2,1.0
three groups security,2,2,1.0
groups security instances,2,2,1.0
security instances border,2,2,1.0
instances border instances,2,2,1.0
border instances and,2,2,1.0
instances and latent,2,2,1.0
and latent noise,2,2,1.0
latent noise instances,2,2,1.0
noise instances and,2,2,1.0
instances and then,2,2,1.0
and then treats,2,2,1.0
then treats these,2,2,1.0
treats these groups,2,2,1.0
these groups differently,2,2,1.0
groups differently when,2,2,1.0
differently when generating,2,2,1.0
when generating instances,2,2,1.0
generating instances other,2,2,1.0
instances other variants,2,2,1.0
other variants in,2,2,1.0
variants in this,2,2,1.0
in this vein,2,2,1.0
this vein adjust,2,2,1.0
vein adjust the,2,2,1.0
adjust the sampling,2,2,1.0
the sampling rate,2,2,1.0
sampling rate for,2,2,1.0
rate for some,2,2,1.0
for some minority,2,2,1.0
minority instances those,2,2,1.0
instances those close,2,2,1.0
those close to,2,2,1.0
to the boundary,3,3,1.0
the boundary to,2,2,1.0
boundary to improve,2,2,1.0
to improve these,2,2,1.0
improve these methods,2,2,1.0
these methods further,2,2,1.0
methods further see,2,2,1.0
further see and,2,2,1.0
see and this,2,2,1.0
and this use,2,2,1.0
this use of,2,2,1.0
use of boundary,2,2,1.0
of boundary regions,2,2,1.0
boundary regions in,2,2,1.0
minority class also,2,2,1.0
class also raises,2,2,1.0
also raises questions,2,2,1.0
raises questions about,2,2,1.0
questions about the,2,2,1.0
about the relationship,2,2,1.0
the relationship of,2,2,1.0
relationship of majority,2,2,1.0
majority instances to,4,2,2.0
instances to minority,2,2,1.0
minority instances leading,2,2,1.0
instances leading to,2,2,1.0
leading to a,2,2,1.0
to a third,2,2,1.0
a third insight,2,2,1.0
third insight underlying,2,2,1.0
insight underlying smote,2,2,1.0
underlying smote variants,2,2,1.0
smote variants namely,2,2,1.0
variants namely that,2,2,1.0
namely that oversampling,2,2,1.0
that oversampling in,2,2,1.0
oversampling in the,2,2,1.0
minority class can,5,3,1.6666666666666667
class can be,2,2,1.0
can be by,2,2,1.0
be by considering,2,2,1.0
by considering the,3,3,1.0
considering the majority,2,2,1.0
majority class using,4,2,2.0
class using the,3,3,1.0
using the majority,2,2,1.0
majority class a,3,3,1.0
class a third,2,2,1.0
a third important,2,2,1.0
third important insight,2,2,1.0
important insight in,2,2,1.0
insight in this,2,2,1.0
in this research,2,2,1.0
this research area,2,2,1.0
research area which,2,2,1.0
area which becomes,2,2,1.0
which becomes more,2,2,1.0
becomes more apparent,2,2,1.0
more apparent when,2,2,1.0
apparent when borderlines,2,2,1.0
when borderlines are,2,2,1.0
borderlines are explored,2,2,1.0
are explored is,2,2,1.0
explored is the,2,2,1.0
is the idea,2,2,1.0
idea that the,2,2,1.0
that the relationship,2,2,1.0
the relationship between,7,3,2.3333333333333335
relationship between the,2,2,1.0
class and the,9,5,1.8
and the minority,6,4,1.5
class can also,2,2,1.0
can also help,2,2,1.0
also help guide,2,2,1.0
help guide smote,2,2,1.0
guide smote earlier,2,2,1.0
smote earlier we,2,2,1.0
earlier we saw,2,2,1.0
we saw that,2,2,1.0
saw that does,2,2,1.0
that does not,2,2,1.0
does not interpolate,2,2,1.0
not interpolate instances,2,2,1.0
interpolate instances when,2,2,1.0
instances when the,2,2,1.0
when the neighbors,2,2,1.0
the neighbors show,2,2,1.0
neighbors show a,2,2,1.0
show a preponderance,2,2,1.0
a preponderance of,2,2,1.0
preponderance of majority,2,2,1.0
of majority see,1,1,1.0
majority see also,1,1,1.0
see also adysyn,2,2,1.0
also adysyn this,2,2,1.0
adysyn this is,2,2,1.0
this is one,2,2,1.0
is one way,2,2,1.0
one way to,2,2,1.0
way to take,2,2,1.0
to take the,2,2,1.0
take the majority,2,2,1.0
majority class into,2,2,1.0
class into account,2,2,1.0
into account other,2,2,1.0
account other explore,1,1,1.0
other explore the,1,1,1.0
explore the relationship,2,2,1.0
relationship between classes,2,2,1.0
between classes to,2,2,1.0
classes to undersample,2,2,1.0
to undersample the,2,2,1.0
class using techniques,2,2,1.0
using techniques or,2,2,1.0
techniques or to,2,2,1.0
or to guide,2,2,1.0
to guide the,4,2,2.0
guide the oversampling,2,2,1.0
the oversampling of,2,2,1.0
oversampling of the,2,2,1.0
for instance finds,1,1,1.0
instance finds pairs,1,1,1.0
finds pairs of,2,2,1.0
pairs of instances,4,2,2.0
of instances between,2,2,1.0
instances between the,2,2,1.0
between the minority,4,3,1.3333333333333333
the minority and,5,3,1.6666666666666667
minority and majority,11,3,3.6666666666666665
and majority classes,5,3,1.6666666666666667
majority classes that,2,2,1.0
classes that are,2,2,1.0
that are very,2,2,1.0
are very similar,2,2,1.0
very similar low,2,2,1.0
similar low euclidean,2,2,1.0
low euclidean distance,2,2,1.0
euclidean distance a,3,3,1.0
distance a tomek,2,2,1.0
a tomek link,4,3,1.3333333333333333
tomek link and,2,2,1.0
link and then,2,2,1.0
and then removes,2,2,1.0
then removes the,2,2,1.0
removes the majority,2,2,1.0
the majority instance,2,2,1.0
majority instance in,2,2,1.0
in the pair,2,2,1.0
the pair by,2,2,1.0
pair by removing,2,2,1.0
by removing these,2,2,1.0
removing these and,2,2,1.0
these and applying,2,2,1.0
and applying smote,2,2,1.0
minority class it,2,2,1.0
class it attempts,2,2,1.0
it attempts to,2,2,1.0
attempts to the,2,2,1.0
to the classes,2,2,1.0
the classes uses,2,2,1.0
classes uses a,2,2,1.0
uses a related,2,2,1.0
a related approach,2,2,1.0
related approach involving,2,2,1.0
approach involving the,2,2,1.0
involving the edited,2,2,1.0
the edited nearest,2,2,1.0
edited nearest neighbour,2,2,1.0
nearest neighbour method,2,2,1.0
neighbour method other,2,2,1.0
method other methods,2,2,1.0
other methods and,2,2,1.0
methods and swim,2,2,1.0
and swim perform,2,2,1.0
swim perform explicit,2,2,1.0
perform explicit analyses,2,2,1.0
explicit analyses of,2,2,1.0
analyses of the,2,2,1.0
class and use,2,2,1.0
and use this,2,2,1.0
use this analysis,2,2,1.0
this analysis to,2,2,1.0
analysis to minority,2,2,1.0
to minority instance,2,2,1.0
minority instance generation,2,2,1.0
instance generation does,1,1,1.0
generation does this,1,1,1.0
does this by,2,2,1.0
this by computing,2,2,1.0
by computing a,2,2,1.0
computing a score,2,2,1.0
a score for,2,2,1.0
score for each,2,2,1.0
for each minority,12,3,4.0
each minority instance,2,2,1.0
minority instance where,2,2,1.0
instance where safety,2,2,1.0
where safety is,2,2,1.0
safety is based,2,2,1.0
is based on,5,4,1.25
based on the,32,6,5.333333333333333
on the frequency,2,2,1.0
the frequency of,3,3,1.0
frequency of majority,2,2,1.0
majority instances in,4,2,2.0
in the neighbours,2,2,1.0
the neighbours and,2,2,1.0
neighbours and a,2,2,1.0
and a temraz,2,2,1.0
a temraz keane,2,2,1.0
data augmentation ratio,2,2,1.0
augmentation ratio based,2,2,1.0
ratio based on,2,2,1.0
on the score,3,3,1.0
the score of,2,2,1.0
score of a,2,2,1.0
of a minority,2,2,1.0
minority instance over,2,2,1.0
instance over that,2,2,1.0
over that of,2,2,1.0
of its neighbours,4,3,1.3333333333333333
its neighbours s,1,1,1.0
neighbours s finer,1,1,1.0
s finer analysis,2,2,1.0
finer analysis of,2,2,1.0
analysis of the,7,4,1.75
of the relationship,2,2,1.0
relationship between majority,2,2,1.0
between majority and,7,3,2.3333333333333335
and minority has,2,2,1.0
minority has been,2,2,1.0
has been shown,8,3,2.6666666666666665
been shown to,6,3,2.0
shown to performance,1,1,1.0
to performance over,1,1,1.0
performance over sampling,2,2,1.0
over sampling with,2,2,1.0
sampling with the,2,2,1.0
the majority swim,2,2,1.0
majority swim adopt,2,2,1.0
swim adopt a,2,2,1.0
adopt a different,1,1,1.0
a different approach,1,1,1.0
different approach leveraging,1,1,1.0
approach leveraging information,2,2,1.0
leveraging information about,2,2,1.0
information about the,4,3,1.3333333333333333
about the density,2,2,1.0
the density of,2,2,1.0
density of majority,2,2,1.0
majority instances using,2,2,1.0
using the mahalanobis,2,2,1.0
the mahalanobis distances,1,1,1.0
mahalanobis distances requiring,1,1,1.0
distances requiring generated,1,1,1.0
requiring generated minority,2,2,1.0
generated minority instances,2,2,1.0
minority instances to,3,3,1.0
instances to have,2,2,1.0
to have similar,2,2,1.0
have similar distances,2,2,1.0
similar distances to,2,2,1.0
distances to their,2,2,1.0
to their minority,2,2,1.0
their minority seeds,2,2,1.0
minority seeds so,2,2,1.0
seeds so swim,2,2,1.0
so swim essentially,2,2,1.0
swim essentially analyses,2,2,1.0
essentially analyses the,2,2,1.0
analyses the topology,2,2,1.0
the topology of,4,2,2.0
topology of the,4,2,2.0
majority class to,2,2,1.0
class to guide,2,2,1.0
guide the generation,2,2,1.0
the generation of,10,4,2.5
minority instances see,2,2,1.0
instances see and,2,2,1.0
see and for,1,1,1.0
and for related,1,1,1.0
for related finally,1,1,1.0
related finally is,1,1,1.0
finally is another,2,2,1.0
is another method,3,3,1.0
another method that,2,2,1.0
method that takes,2,2,1.0
that takes similarities,2,2,1.0
takes similarities to,2,2,1.0
similarities to majority,2,2,1.0
to majority into,1,1,1.0
majority into account,1,1,1.0
into account in,2,2,1.0
account in computing,2,2,1.0
in computing rough,2,2,1.0
computing rough sets,2,2,1.0
rough sets over,2,2,1.0
sets over the,2,2,1.0
over the minority,4,4,1.0
minority class after,2,2,1.0
class after smote,1,1,1.0
after smote has,1,1,1.0
smote has been,1,1,1.0
has been applied,3,3,1.0
applied to generate,2,2,1.0
to generate additional,2,2,1.0
generate additional minority,2,2,1.0
additional minority instances,2,2,1.0
minority instances this,2,2,1.0
instances this method,2,2,1.0
this method acts,2,2,1.0
method acts like,2,2,1.0
acts like a,2,2,1.0
like a step,2,2,1.0
a step to,2,2,1.0
step to generated,1,1,1.0
to generated instances,1,1,1.0
generated instances that,2,2,1.0
instances that might,2,2,1.0
that might be,2,2,1.0
might be noise,2,2,1.0
be noise many,2,2,1.0
noise many of,2,2,1.0
many of these,2,2,1.0
these methods improve,2,2,1.0
methods improve on,2,2,1.0
improve on s,2,2,1.0
on s performance,2,2,1.0
s performance and,2,2,1.0
performance and as,2,2,1.0
and as such,4,2,2.0
as such show,2,2,1.0
such show that,2,2,1.0
show that paying,2,2,1.0
that paying more,2,2,1.0
paying more attention,2,2,1.0
more attention to,3,3,1.0
attention to the,3,3,1.0
majority class can,3,3,1.0
class can play,2,2,1.0
can play a,2,2,1.0
play a key,2,2,1.0
a key role,2,2,1.0
key role in,2,2,1.0
role in instance,2,2,1.0
in instance generation,2,2,1.0
instance generation in,2,2,1.0
generation in the,2,2,1.0
minority class we,2,2,1.0
class we will,2,2,1.0
we will see,2,2,1.0
will see later,2,2,1.0
see later that,2,2,1.0
later that while,2,2,1.0
that while the,3,3,1.0
while the current,2,2,1.0
the current counterfactual,2,2,1.0
current counterfactual methods,2,2,1.0
counterfactual methods reflect,2,2,1.0
methods reflect these,2,2,1.0
reflect these three,2,2,1.0
these three key,2,2,1.0
key insights about,2,2,1.0
insights about out,2,2,1.0
about out to,2,2,1.0
out to improve,2,2,1.0
to improve on,2,2,1.0
improve on smote,2,2,1.0
on smote it,2,2,1.0
smote it is,2,2,1.0
it is quite,2,2,1.0
is quite different,4,2,2.0
quite different from,4,2,2.0
different from all,2,2,1.0
from all of,2,2,1.0
all of the,13,5,2.6
of the above,2,2,1.0
the above methods,2,2,1.0
above methods in,2,2,1.0
methods in how,2,2,1.0
in how it,2,2,1.0
how it operates,2,2,1.0
it operates see,2,2,1.0
operates see section,2,2,1.0
see section but,2,2,1.0
section but before,2,2,1.0
but before considering,2,2,1.0
before considering this,2,2,1.0
considering this counterfactual,2,2,1.0
counterfactual method in,2,2,1.0
method in detail,2,2,1.0
in detail we,2,2,1.0
detail we first,2,2,1.0
we first briefly,2,2,1.0
first briefly review,2,2,1.0
briefly review how,2,2,1.0
review how it,2,2,1.0
how it has,2,2,1.0
it has emerged,2,2,1.0
has emerged in,2,2,1.0
emerged in xai,2,2,1.0
in xai counterfactual,4,2,2.0
xai counterfactual generation,2,2,1.0
counterfactual generation in,2,2,1.0
generation in xai,2,2,1.0
in xai in,2,2,1.0
xai in this,2,2,1.0
paper we deploy,2,2,1.0
we deploy a,2,2,1.0
deploy a counterfactual,2,2,1.0
counterfactual method to,9,2,4.5
method to generate,4,2,2.0
synthetic instances counterfactual,2,2,1.0
instances counterfactual methods,2,2,1.0
counterfactual methods have,4,2,2.0
have been developed,5,3,1.6666666666666667
been developed to,3,3,1.0
developed to generate,2,2,1.0
to generate examples,2,2,1.0
generate examples to,2,2,1.0
examples to explain,2,2,1.0
to explain the,2,2,1.0
explain the predictions,2,2,1.0
the predictions of,3,3,1.0
predictions of ml,2,2,1.0
of ml models,2,2,1.0
ml models and,4,2,2.0
models and to,2,2,1.0
and to provide,3,3,1.0
to provide algorithmic,2,2,1.0
provide algorithmic recourse,2,2,1.0
algorithmic recourse for,2,2,1.0
recourse for temraz,2,2,1.0
for temraz keane,2,2,1.0
data augmentation trying,2,2,1.0
augmentation trying to,2,2,1.0
trying to mitigate,2,2,1.0
to mitigate automated,2,2,1.0
mitigate automated decisions,2,2,1.0
automated decisions for,2,2,1.0
decisions for reviews,2,2,1.0
reviews see the,2,2,1.0
see the classic,2,2,1.0
the classic counterfactual,2,2,1.0
classic counterfactual is,1,1,1.0
counterfactual is one,1,1,1.0
is one that,2,2,1.0
one that is,2,2,1.0
that is given,2,2,1.0
is given when,2,2,1.0
given when an,2,2,1.0
when an automated,2,2,1.0
an automated system,2,2,1.0
automated system refuses,2,2,1.0
system refuses a,2,2,1.0
refuses a person,2,2,1.0
a person on,2,2,1.0
person on a,2,2,1.0
on a loan,2,2,1.0
a loan application,2,2,1.0
loan application when,2,2,1.0
application when the,2,2,1.0
when the asks,2,2,1.0
the asks why,2,2,1.0
asks why the,2,2,1.0
why the system,2,2,1.0
the system might,2,2,1.0
system might counterfactually,2,2,1.0
might counterfactually explain,2,2,1.0
counterfactually explain that,2,2,1.0
explain that if,2,2,1.0
that if you,2,2,1.0
if you a,1,1,1.0
you a loan,1,1,1.0
a loan for,2,2,1.0
loan for less,2,2,1.0
for less over,2,2,1.0
less over a,2,2,1.0
over a shorter,2,2,1.0
a shorter term,2,2,1.0
shorter term then,2,2,1.0
term then you,2,2,1.0
then you would,2,2,1.0
you would have,2,2,1.0
would have been,2,2,1.0
have been granted,2,2,1.0
been granted the,2,2,1.0
granted the loan,2,2,1.0
the loan that,2,2,1.0
loan that is,4,2,2.0
that is the,3,3,1.0
is the counterfactual,2,2,1.0
the counterfactual explanation,2,2,1.0
counterfactual explanation tells,2,2,1.0
explanation tells users,2,2,1.0
tells users about,2,2,1.0
users about the,2,2,1.0
about the conditions,2,2,1.0
under which outcome,1,1,1.0
which outcome would,1,1,1.0
outcome would change,1,1,1.0
would change the,2,2,1.0
change the closest,2,2,1.0
the closest world,2,2,1.0
closest world to,2,2,1.0
world to their,2,2,1.0
to their world,2,2,1.0
their world in,2,2,1.0
the outcome would,2,2,1.0
outcome would be,2,2,1.0
would be what,2,2,1.0
be what they,2,2,1.0
what they desire,2,2,1.0
they desire counterfactuals,2,2,1.0
desire counterfactuals have,2,2,1.0
counterfactuals have been,2,2,1.0
have been researched,2,2,1.0
been researched for,2,2,1.0
researched for some,2,2,1.0
for some time,2,2,1.0
some time in,2,2,1.0
time in ai,2,2,1.0
in ai under,2,2,1.0
ai under diverse,2,2,1.0
under diverse names,1,1,1.0
diverse names for,1,1,1.0
names for in,1,1,1.0
for in the,2,2,1.0
in the past,2,2,1.0
the past they,2,2,1.0
past they have,2,2,1.0
they have been,2,2,1.0
have been called,2,2,1.0
been called nearest,2,2,1.0
called nearest unlike,2,2,1.0
nearest unlike neighbours,2,2,1.0
unlike neighbours nuns,2,2,1.0
neighbours nuns or,2,2,1.0
nuns or inverse,2,2,1.0
or inverse classifications,2,2,1.0
inverse classifications recently,2,2,1.0
classifications recently they,2,2,1.0
recently they have,2,2,1.0
they have emerged,2,2,1.0
have emerged as,2,2,1.0
emerged as a,2,2,1.0
as a hot,2,2,1.0
a hot topic,4,3,1.3333333333333333
hot topic in,3,3,1.0
topic in xai,2,2,1.0
in xai because,1,1,1.0
xai because they,1,1,1.0
because they appear,1,1,1.0
they appear to,2,2,1.0
appear to have,2,2,1.0
to have psychological,2,2,1.0
have psychological benefits,2,2,1.0
psychological benefits people,2,2,1.0
benefits people naturally,2,2,1.0
people naturally understand,2,2,1.0
naturally understand them,2,2,1.0
understand them and,2,2,1.0
them and legal,2,2,1.0
and legal benefits,2,2,1.0
legal benefits they,2,2,1.0
benefits they are,2,2,1.0
they are said,2,2,1.0
are said to,2,2,1.0
said to be,4,3,1.3333333333333333
to be gdpr,2,2,1.0
be gdpr compliant,2,2,1.0
gdpr compliant optimization,2,2,1.0
compliant optimization techniques,2,2,1.0
optimization techniques are,2,2,1.0
techniques are currently,2,2,1.0
are currently the,2,2,1.0
currently the most,2,2,1.0
most popular method,2,2,1.0
method for computing,2,2,1.0
for computing counterfactuals,2,2,1.0
computing counterfactuals given,2,2,1.0
counterfactuals given a,2,2,1.0
given a test,2,2,1.0
a test instance,2,2,1.0
test instance one,2,2,1.0
instance one encoding,2,2,1.0
one encoding the,2,2,1.0
encoding the loan,1,1,1.0
the loan refusal,1,1,1.0
loan refusal these,2,2,1.0
refusal these optimization,2,2,1.0
these optimization methods,3,2,1.5
optimization methods search,2,2,1.0
methods search a,2,2,1.0
search a sometimes,2,2,1.0
a sometimes randomly,1,1,1.0
sometimes randomly generated,1,1,1.0
randomly generated space,1,1,1.0
generated space of,2,2,1.0
space of perturbations,2,2,1.0
of perturbations of,2,2,1.0
perturbations of the,2,2,1.0
of the query,4,2,2.0
the query synthetic,2,2,1.0
query synthetic instances,2,2,1.0
synthetic instances under,2,2,1.0
instances under a,2,2,1.0
under a loss,2,2,1.0
a loss function,2,2,1.0
loss function that,2,2,1.0
function that balances,1,1,1.0
that balances proximity,1,1,1.0
balances proximity to,1,1,1.0
proximity to the,4,2,2.0
to the against,2,2,1.0
the against proximity,2,2,1.0
against proximity to,2,2,1.0
decision boundary for,2,2,1.0
boundary for the,2,2,1.0
for the counterfactual,2,2,1.0
the counterfactual class,2,2,1.0
counterfactual class the,2,2,1.0
class the class,2,2,1.0
the class that,2,2,1.0
class that counters,2,2,1.0
that counters that,2,2,1.0
counters that of,2,2,1.0
that of the,6,4,1.5
the query using,2,2,1.0
query using a,2,2,1.0
using a scaled,2,2,1.0
a scaled wachter,2,2,1.0
scaled wachter et,2,2,1.0
wachter et s,4,2,2.0
et s seminal,2,2,1.0
s seminal method,2,2,1.0
seminal method uses,2,2,1.0
method uses gradient,2,2,1.0
uses gradient descent,2,2,1.0
gradient descent to,2,2,1.0
descent to find,2,2,1.0
to find the,6,3,2.0
find the best,2,2,1.0
the best counterfactual,2,2,1.0
best counterfactual instance,1,1,1.0
counterfactual instance for,1,1,1.0
instance for a,1,1,1.0
for a query,2,2,1.0
a query though,2,2,1.0
query though later,2,2,1.0
though later models,2,2,1.0
later models have,2,2,1.0
models have used,2,2,1.0
have used other,2,2,1.0
used other techniques,2,2,1.0
other techniques genetic,2,2,1.0
techniques genetic algorithms,2,2,1.0
genetic algorithms mothilal,2,2,1.0
algorithms mothilal et,2,2,1.0
mothilal et al,6,2,3.0
et al proposed,6,3,2.0
al proposed the,4,2,2.0
proposed the diverse,2,2,1.0
the diverse counterfactual,2,2,1.0
diverse counterfactual explanations,6,2,3.0
counterfactual explanations dice,2,2,1.0
explanations dice method,2,2,1.0
dice method as,2,2,1.0
method as an,2,2,1.0
as an extension,2,2,1.0
an extension to,3,3,1.0
extension to generate,2,2,1.0
to generate a,5,3,1.6666666666666667
generate a set,4,2,2.0
a set of,16,5,3.2
set of counterfactual,1,1,1.0
of counterfactual candidates,1,1,1.0
counterfactual candidates avoiding,2,2,1.0
candidates avoiding the,2,2,1.0
avoiding the problem,2,2,1.0
the problem of,15,5,3.0
problem of generating,2,2,1.0
of generating sets,2,2,1.0
generating sets of,2,2,1.0
sets of candidates,2,2,1.0
of candidates that,2,2,1.0
candidates that were,2,2,1.0
that were trivial,2,2,1.0
were trivial variants,2,2,1.0
trivial variants on,2,2,1.0
variants on one,2,2,1.0
on one another,2,2,1.0
one another the,2,2,1.0
another the main,2,2,1.0
the main problem,2,2,1.0
main problem with,2,2,1.0
problem with these,4,2,2.0
with these optimization,2,2,1.0
optimization methods is,1,1,1.0
methods is that,1,1,1.0
is that given,2,2,1.0
that given their,2,2,1.0
given their blind,2,2,1.0
their blind perturbation,2,2,1.0
blind perturbation of,2,2,1.0
perturbation of they,2,2,1.0
of they sometimes,2,2,1.0
they sometimes generate,2,2,1.0
sometimes generate invalid,2,2,1.0
generate invalid keane,2,2,1.0
invalid keane counterfactual,2,2,1.0
augmentation points this,2,2,1.0
points this defect,2,2,1.0
this defect has,2,2,1.0
defect has potentially,2,2,1.0
has potentially serious,2,2,1.0
potentially serious for,2,2,1.0
serious for their,2,2,1.0
for their use,2,2,1.0
their use in,4,2,2.0
use in the,2,2,1.0
in the problem,1,1,1.0
the problem as,1,1,1.0
problem as it,2,2,1.0
as it suggests,2,2,1.0
it suggests that,2,2,1.0
suggests that they,2,2,1.0
that they might,2,2,1.0
they might populate,2,2,1.0
might populate the,2,2,1.0
populate the minority,2,2,1.0
class with noise,1,1,1.0
with noise with,1,1,1.0
noise with negative,1,1,1.0
with negative effects,1,1,1.0
negative effects on,3,3,1.0
effects on a,2,2,1.0
on a classifier,2,2,1.0
s performance however,2,2,1.0
performance however a,2,2,1.0
however a very,2,2,1.0
a very different,2,2,1.0
very different approach,2,2,1.0
different approach to,2,2,1.0
approach to counterfactual,2,2,1.0
to counterfactual generation,2,2,1.0
counterfactual generation has,2,2,1.0
generation has recently,2,2,1.0
has recently been,2,2,1.0
recently been proposed,2,2,1.0
been proposed this,2,2,1.0
proposed this method,2,2,1.0
this method finds,2,2,1.0
method finds the,2,2,1.0
finds the s,2,2,1.0
the s that,2,2,1.0
s that takes,2,2,1.0
that takes part,2,2,1.0
takes part in,2,2,1.0
part in a,6,2,3.0
in a explanation,2,2,1.0
a explanation case,2,2,1.0
explanation case xc,2,2,1.0
case xc an,2,2,1.0
xc an explanation,2,2,1.0
an explanation case,2,2,1.0
explanation case captures,2,2,1.0
case captures a,2,2,1.0
captures a counterfactual,2,2,1.0
a counterfactual between,1,1,1.0
counterfactual between existing,1,1,1.0
between existing instances,2,2,1.0
existing instances in,2,2,1.0
the dataset that,2,2,1.0
dataset that are,2,2,1.0
that are in,2,2,1.0
are in opposing,2,2,1.0
in opposing classes,2,2,1.0
opposing classes either,2,2,1.0
classes either side,2,2,1.0
either side of,6,2,3.0
side of a,4,2,2.0
of a decision,2,2,1.0
a decision boundary,2,2,1.0
decision boundary with,4,2,2.0
boundary with the,4,2,2.0
with the constraint,2,2,1.0
the constraint that,4,2,2.0
constraint that the,4,2,2.0
that the pair,2,2,1.0
the pair of,2,2,1.0
pair of instances,2,2,1.0
of instances differ,2,2,1.0
instances differ by,2,2,1.0
differ by at,2,2,1.0
by at most,2,2,1.0
at most two,2,2,1.0
most two for,1,1,1.0
two for example,1,1,1.0
for example the,5,5,1.0
example the loan,2,2,1.0
the loan dataset,2,2,1.0
loan dataset could,2,2,1.0
dataset could contain,2,2,1.0
could contain two,2,2,1.0
contain two existing,2,2,1.0
two existing cases,2,2,1.0
existing cases that,2,2,1.0
cases that are,2,2,1.0
that are counterfactually,2,2,1.0
are counterfactually related,2,2,1.0
counterfactually related one,2,2,1.0
related one about,2,2,1.0
one about a,2,2,1.0
about a old,2,2,1.0
a old female,2,2,1.0
old female accountant,2,2,1.0
female accountant earning,2,2,1.0
accountant earning who,4,2,2.0
earning who was,6,2,3.0
who was refused,4,2,2.0
was refused a,4,2,2.0
refused a loan,4,2,2.0
a loan that,2,2,1.0
that is counterfactually,3,2,1.5
is counterfactually related,3,2,1.5
counterfactually related to,5,2,2.5
related to another,2,2,1.0
to another instance,2,2,1.0
another instance with,2,2,1.0
instance with a,2,2,1.0
with a different,5,3,1.6666666666666667
a different outcome,2,2,1.0
different outcome namely,2,2,1.0
outcome namely a,2,2,1.0
namely a old,2,2,1.0
a old accountant,1,1,1.0
old accountant earning,1,1,1.0
who was granted,2,2,1.0
was granted a,2,2,1.0
granted a loan,2,2,1.0
a loan differences,2,2,1.0
loan differences shown,2,2,1.0
differences shown in,2,2,1.0
shown in italics,2,2,1.0
in italics this,2,2,1.0
italics this explanatory,2,2,1.0
this explanatory case,2,2,1.0
explanatory case implicitly,2,2,1.0
case implicitly suggests,2,2,1.0
implicitly suggests that,2,2,1.0
suggests that if,2,2,1.0
that if one,2,2,1.0
if one earns,2,2,1.0
one earns rather,2,2,1.0
earns rather than,2,2,1.0
rather than then,4,2,2.0
than then the,4,2,2.0
then the loan,4,2,2.0
the loan is,1,1,1.0
loan is likely,1,1,1.0
likely to be,20,5,4.0
to be granted,2,2,1.0
be granted rather,2,2,1.0
granted rather than,2,2,1.0
rather than refused,2,2,1.0
than refused so,2,2,1.0
refused so if,2,2,1.0
so if i,4,2,2.0
if i am,4,2,2.0
i am a,2,2,1.0
am a old,2,2,1.0
a old male,3,2,1.5
old male teacher,2,2,1.0
male teacher earning,2,2,1.0
teacher earning who,2,2,1.0
a loan then,2,2,1.0
loan then this,2,2,1.0
then this algorithm,2,2,1.0
this algorithm could,2,2,1.0
algorithm could find,2,2,1.0
could find this,2,2,1.0
find this as,2,2,1.0
this as a,2,2,1.0
as a nearest,2,2,1.0
a nearest neighbour,4,2,2.0
nearest neighbour and,2,2,1.0
neighbour and suggest,2,2,1.0
and suggest that,2,2,1.0
suggest that if,2,2,1.0
that if this,2,2,1.0
if this earned,2,2,1.0
this earned more,2,2,1.0
earned more rather,2,2,1.0
more rather than,2,2,1.0
the loan would,2,2,1.0
loan would be,2,2,1.0
would be granted,2,2,1.0
be granted in,2,2,1.0
granted in the,2,2,1.0
in the xai,2,2,1.0
the xai context,2,2,1.0
xai context this,2,2,1.0
context this method,2,2,1.0
this method has,2,2,1.0
method has been,2,2,1.0
shown to generate,2,2,1.0
to generate close,2,2,1.0
generate close plausible,2,2,1.0
close plausible counterfactuals,2,2,1.0
plausible counterfactuals and,2,2,1.0
counterfactuals and appears,2,2,1.0
and appears to,2,2,1.0
appears to avoid,2,2,1.0
to avoid the,2,2,1.0
avoid the pitfalls,2,2,1.0
the pitfalls that,2,2,1.0
pitfalls that arise,2,2,1.0
that arise in,2,2,1.0
arise in techniques,1,1,1.0
in techniques see,1,1,1.0
techniques see from,2,2,1.0
see from a,2,2,1.0
from a data,4,2,2.0
a data augmentation,4,2,2.0
data augmentation perspective,4,2,2.0
augmentation perspective this,2,2,1.0
perspective this method,2,2,1.0
this method can,7,3,2.3333333333333335
method can be,4,4,1.0
can be seen,22,3,7.333333333333333
be seen as,3,3,1.0
seen as supporting,2,2,1.0
as supporting the,2,2,1.0
supporting the creation,2,2,1.0
the creation of,2,2,1.0
creation of synthetic,2,2,1.0
of synthetic in,3,3,1.0
minority class using,10,3,3.3333333333333335
class using information,2,2,1.0
using information from,2,2,1.0
information from these,2,2,1.0
from these known,2,2,1.0
these known counterfactual,2,2,1.0
known counterfactual pairs,2,2,1.0
counterfactual pairs however,2,2,1.0
pairs however few,2,2,1.0
however few xai,2,2,1.0
few xai techniques,2,2,1.0
xai techniques have,2,2,1.0
techniques have been,3,3,1.0
have been applied,2,2,1.0
applied to data,2,2,1.0
to data augmentation,7,2,3.5
augmentation problems in,2,2,1.0
problems in the,2,2,1.0
the next subsection,2,2,1.0
next subsection we,2,2,1.0
subsection we briefly,2,2,1.0
briefly sketch this,2,2,1.0
sketch this small,2,2,1.0
this small literature,2,2,1.0
small literature on,2,2,1.0
literature on the,2,2,1.0
on the topic,2,2,1.0
the topic temraz,2,2,1.0
topic temraz keane,2,2,1.0
data augmentation using,4,2,2.0
augmentation using counterfactuals,2,2,1.0
using counterfactuals for,2,2,1.0
counterfactuals for data,2,2,1.0
data augmentation beyond,2,2,1.0
augmentation beyond xai,2,2,1.0
beyond xai our,2,2,1.0
xai our hypothesis,2,2,1.0
our hypothesis is,2,2,1.0
hypothesis is that,2,2,1.0
is that counterfactual,2,2,1.0
that counterfactual methods,2,2,1.0
counterfactual methods can,2,2,1.0
methods can also,2,2,1.0
can also play,2,2,1.0
also play a,2,2,1.0
play a role,2,2,1.0
a role in,2,2,1.0
role in data,2,2,1.0
in data to,1,1,1.0
data to solve,3,2,1.5
solve problems that,2,2,1.0
problems that generated,2,2,1.0
that generated synthetic,2,2,1.0
generated synthetic counterfactual,1,1,1.0
synthetic counterfactual cases,1,1,1.0
counterfactual cases could,1,1,1.0
cases could the,1,1,1.0
could the predictive,1,1,1.0
the predictive accuracy,2,2,1.0
predictive accuracy of,2,2,1.0
accuracy of ai,2,2,1.0
of ai models,2,2,1.0
ai models although,2,2,1.0
models although there,2,2,1.0
although there are,2,2,1.0
there are now,2,2,1.0
are now of,2,2,1.0
now of papers,2,2,1.0
of papers on,2,2,1.0
papers on in,1,1,1.0
on in xai,1,1,1.0
in xai only,2,2,1.0
xai only a,2,2,1.0
only a handful,2,2,1.0
a handful of,2,2,1.0
handful of papers,2,2,1.0
of papers consider,2,2,1.0
papers consider their,2,2,1.0
consider their use,2,2,1.0
use in data,2,2,1.0
data augmentation in,5,2,2.5
augmentation in evaluating,2,2,1.0
in evaluating xai,2,2,1.0
evaluating xai counterfactual,2,2,1.0
xai counterfactual methods,4,2,2.0
counterfactual methods mothilal,2,2,1.0
methods mothilal et,2,2,1.0
et al suggested,3,3,1.0
al suggested that,3,3,1.0
suggested that a,2,2,1.0
that a good,3,3,1.0
a good method,2,2,1.0
good method should,2,2,1.0
method should generate,2,2,1.0
should generate a,2,2,1.0
set of counterfactuals,2,2,1.0
of counterfactuals that,2,2,1.0
counterfactuals that can,2,2,1.0
that can substitute,2,2,1.0
can substitute for,2,2,1.0
substitute for the,2,2,1.0
for the original,2,2,1.0
original dataset calling,2,2,1.0
dataset calling it,2,2,1.0
calling it that,1,1,1.0
it that is,1,1,1.0
that is if,2,2,1.0
is if the,3,3,1.0
set of generated,2,2,1.0
of generated counterfactuals,2,2,1.0
generated counterfactuals were,2,2,1.0
counterfactuals were plausible,2,2,1.0
were plausible and,2,2,1.0
plausible and close,2,2,1.0
and close to,6,2,3.0
original data then,2,2,1.0
data then their,2,2,1.0
then their predictive,2,2,1.0
their predictive performance,2,2,1.0
predictive performance should,2,2,1.0
performance should parallel,2,2,1.0
should parallel that,2,2,1.0
parallel that of,2,2,1.0
of the original,13,5,2.6
original dataset however,1,1,1.0
dataset however mothilal,1,1,1.0
however mothilal et,1,1,1.0
et al did,2,2,1.0
al did not,2,2,1.0
did not consider,2,2,1.0
not consider using,2,2,1.0
consider using their,2,2,1.0
using their counterfactual,2,2,1.0
their counterfactual method,2,2,1.0
augmentation in a,2,2,1.0
in a student,2,2,1.0
a student project,2,2,1.0
student project hasan,2,2,1.0
project hasan did,2,2,1.0
hasan did and,2,2,1.0
did and tried,2,2,1.0
and tried to,2,2,1.0
tried to determine,2,2,1.0
to determine whether,2,2,1.0
determine whether an,2,2,1.0
whether an augmented,2,2,1.0
an augmented dataset,2,2,1.0
augmented dataset based,2,2,1.0
dataset based on,2,2,1.0
based on generated,1,1,1.0
on generated could,1,1,1.0
generated could act,1,1,1.0
could act as,2,2,1.0
act as a,2,2,1.0
as a proxy,2,2,1.0
a proxy dataset,2,2,1.0
proxy dataset but,2,2,1.0
dataset but only,2,2,1.0
but only found,2,2,1.0
only found modest,2,2,1.0
found modest success,2,2,1.0
modest success a,2,2,1.0
success a selection,2,2,1.0
a selection of,2,2,1.0
selection of other,2,2,1.0
of other papers,2,2,1.0
other papers in,3,3,1.0
papers in diverse,2,2,1.0
in diverse areas,2,2,1.0
diverse areas have,2,2,1.0
areas have also,2,2,1.0
have also circled,2,2,1.0
also circled the,2,2,1.0
circled the issue,2,2,1.0
the issue of,3,3,1.0
issue of using,2,2,1.0
of using counterfactual,2,2,1.0
using counterfactual techniques,2,2,1.0
counterfactual techniques for,2,2,1.0
techniques for data,2,2,1.0
data augmentation subbaswamy,2,2,1.0
augmentation subbaswamy and,2,2,1.0
subbaswamy and saria,2,2,1.0
and saria considered,2,2,1.0
saria considered the,2,2,1.0
considered the problem,2,2,1.0
problem of dataset,2,2,1.0
of dataset shift,2,2,1.0
dataset shift where,2,2,1.0
shift where there,2,2,1.0
where there is,2,2,1.0
there is a,8,5,1.6
is a divergence,2,2,1.0
a divergence between,2,2,1.0
divergence between the,2,2,1.0
between the context,2,2,1.0
the context in,2,2,1.0
context in which,2,2,1.0
in which a,3,3,1.0
which a model,2,2,1.0
a model was,2,2,1.0
model was trained,1,1,1.0
was trained and,1,1,1.0
trained and tested,1,1,1.0
and tested they,2,2,1.0
tested they use,2,2,1.0
they use the,2,2,1.0
use the notion,2,2,1.0
the notion of,9,4,2.25
notion of counterfactual,2,2,1.0
of counterfactual risk,2,2,1.0
counterfactual risk to,2,2,1.0
risk to diagnose,2,2,1.0
to diagnose this,2,2,1.0
diagnose this problem,2,2,1.0
this problem using,2,2,1.0
problem using causal,2,2,1.0
using causal models,2,2,1.0
causal models zeng,2,2,1.0
models zeng et,2,2,1.0
zeng et al,2,2,1.0
proposed the counterfactual,2,2,1.0
the counterfactual generator,2,2,1.0
counterfactual generator which,2,2,1.0
generator which generates,2,2,1.0
which generates counterfactual,2,2,1.0
generates counterfactual examples,2,2,1.0
counterfactual examples for,2,2,1.0
examples for data,1,1,1.0
for data and,1,1,1.0
data and found,2,2,1.0
and found that,3,3,1.0
found that generated,2,2,1.0
that generated counterfactuals,4,2,2.0
generated counterfactuals improved,2,2,1.0
counterfactuals improved the,2,2,1.0
improved the generalizability,2,2,1.0
the generalizability of,2,2,1.0
generalizability of models,2,2,1.0
of models under,2,2,1.0
models under limited,2,2,1.0
under limited observational,2,2,1.0
limited observational examples,2,2,1.0
observational examples pitis,2,2,1.0
examples pitis et,2,2,1.0
pitis et al,2,2,1.0
al proposed counterfactual,1,1,1.0
proposed counterfactual data,1,1,1.0
data augmentation coda,2,2,1.0
augmentation coda for,2,2,1.0
coda for generating,2,2,1.0
for generating counterfactual,2,2,1.0
generating counterfactual experiences,2,2,1.0
counterfactual experiences in,2,2,1.0
experiences in reinforcement,2,2,1.0
in reinforcement learning,2,2,1.0
reinforcement learning rl,2,2,1.0
learning rl in,2,2,1.0
rl in which,2,2,1.0
which the method,2,2,1.0
the method increases,2,2,1.0
method increases the,2,2,1.0
increases the size,2,2,1.0
the size of,6,6,1.0
size of available,2,2,1.0
of available training,2,2,1.0
available training data,2,2,1.0
training data with,3,3,1.0
data with counterfactual,2,2,1.0
with counterfactual examples,2,2,1.0
counterfactual examples by,2,2,1.0
examples by stitching,2,2,1.0
by stitching subsamples,1,1,1.0
stitching subsamples from,1,1,1.0
subsamples from the,2,2,1.0
from the environment,2,2,1.0
the environment they,2,2,1.0
environment they found,2,2,1.0
they found that,2,2,1.0
found that coda,2,2,1.0
that coda significantly,2,2,1.0
coda significantly improved,2,2,1.0
significantly improved the,2,2,1.0
improved the performance,2,2,1.0
performance of rl,2,2,1.0
of rl agents,2,2,1.0
rl agents in,2,2,1.0
agents in tasks,2,2,1.0
in tasks for,2,2,1.0
tasks for and,2,2,1.0
for and keane,2,2,1.0
and keane counterfactual,2,2,1.0
data augmentation conditioned,2,2,1.0
augmentation conditioned settings,2,2,1.0
conditioned settings the,2,2,1.0
settings the problem,2,2,1.0
the problem with,2,2,1.0
with these papers,2,2,1.0
these papers is,2,2,1.0
papers is that,2,2,1.0
is that they,4,2,2.0
that they use,2,2,1.0
they use bespoke,2,2,1.0
use bespoke counterfactual,2,2,1.0
bespoke counterfactual developed,1,1,1.0
counterfactual developed for,1,1,1.0
developed for specific,2,2,1.0
for specific task,2,2,1.0
specific task domains,2,2,1.0
task domains rather,2,2,1.0
domains rather than,2,2,1.0
rather than the,3,3,1.0
than the tried,2,2,1.0
the tried and,2,2,1.0
tried and tested,2,2,1.0
and tested techniques,1,1,1.0
tested techniques from,1,1,1.0
techniques from the,1,1,1.0
xai literature therefore,2,2,1.0
literature therefore their,2,2,1.0
therefore their performance,2,2,1.0
their performance robustness,2,2,1.0
performance robustness and,2,2,1.0
robustness and generalizability,2,2,1.0
and generalizability is,2,2,1.0
generalizability is at,2,2,1.0
is at best,2,2,1.0
at best however,2,2,1.0
best however one,2,2,1.0
however one study,2,2,1.0
one study has,2,2,1.0
study has applied,2,2,1.0
has applied to,2,2,1.0
applied to a,2,2,1.0
to a xai,2,2,1.0
a xai counterfactual,2,2,1.0
xai counterfactual method,2,2,1.0
method to the,4,2,2.0
problem of data,2,2,1.0
of data augmentation,4,2,2.0
data augmentation temraz,2,2,1.0
augmentation temraz et,2,2,1.0
temraz et al,8,2,4.0
et al used,2,2,1.0
al used the,2,2,1.0
used the present,2,2,1.0
the present counterfactual,2,2,1.0
present counterfactual method,2,2,1.0
generate synthetic data,4,3,1.3333333333333333
synthetic data for,3,3,1.0
data for a,2,2,1.0
for a prediction,2,2,1.0
a prediction problem,4,2,2.0
prediction problem their,2,2,1.0
problem their problem,2,2,1.0
their problem domain,2,2,1.0
problem domain involved,1,1,1.0
domain involved a,1,1,1.0
involved a model,1,1,1.0
a model for,2,2,1.0
model for grass,2,2,1.0
for grass growth,2,2,1.0
grass growth prediction,2,2,1.0
growth prediction that,2,2,1.0
prediction that relies,2,2,1.0
that relies on,2,2,1.0
relies on an,2,2,1.0
on an historical,2,2,1.0
an historical dataset,2,2,1.0
historical dataset of,2,2,1.0
dataset of specific,2,2,1.0
of specific measurements,2,2,1.0
specific measurements of,2,2,1.0
measurements of climate,2,2,1.0
of climate and,2,2,1.0
climate and grass,2,2,1.0
and grass growth,2,2,1.0
grass growth on,2,2,1.0
growth on dairy,2,2,1.0
on dairy farms,2,2,1.0
dairy farms in,2,2,1.0
farms in ireland,2,2,1.0
in ireland covering,2,2,1.0
ireland covering the,2,2,1.0
covering the model,2,2,1.0
the model does,4,2,2.0
model does reasonably,2,2,1.0
does reasonably well,2,2,1.0
reasonably well at,2,2,1.0
well at predicting,4,2,2.0
at predicting grass,4,2,2.0
predicting grass growth,4,2,2.0
grass growth for,4,2,2.0
growth for individual,2,2,1.0
for individual farms,2,2,1.0
individual farms in,2,2,1.0
farms in the,2,2,1.0
in the coming,2,2,1.0
the coming week,2,2,1.0
coming week using,2,2,1.0
week using this,2,2,1.0
using this historical,2,2,1.0
this historical data,2,2,1.0
historical data but,2,2,1.0
data but with,2,2,1.0
but with climate,2,2,1.0
with climate change,2,2,1.0
climate change there,2,2,1.0
change there are,2,2,1.0
there are an,2,2,1.0
are an increasing,2,2,1.0
an increasing number,2,2,1.0
increasing number of,2,2,1.0
number of events,2,2,1.0
of events events,2,2,1.0
events events that,2,2,1.0
events that diverge,2,2,1.0
that diverge significantly,2,2,1.0
diverge significantly from,2,2,1.0
significantly from the,2,2,1.0
from the recorded,1,1,1.0
the recorded in,1,1,1.0
recorded in the,2,2,1.0
in the historical,2,2,1.0
the historical data,2,2,1.0
historical data extreme,2,2,1.0
data extreme values,2,2,1.0
extreme values for,2,2,1.0
values for key,2,2,1.0
for key weather,2,2,1.0
key weather variables,1,1,1.0
weather variables like,1,1,1.0
variables like solar,1,1,1.0
like solar or,1,1,1.0
solar or soil,1,1,1.0
or soil moisture,2,2,1.0
soil moisture for,2,2,1.0
moisture for example,2,2,1.0
for example in,3,3,1.0
example in there,2,2,1.0
in there was,2,2,1.0
there was a,2,2,1.0
was a significant,2,2,1.0
a significant drought,2,2,1.0
significant drought across,2,2,1.0
drought across europe,2,2,1.0
across europe that,2,2,1.0
europe that effectively,2,2,1.0
that effectively halted,2,2,1.0
effectively halted grass,2,2,1.0
halted grass growth,2,2,1.0
grass growth in,2,2,1.0
growth in ireland,2,2,1.0
in ireland during,2,2,1.0
ireland during what,2,2,1.0
during what is,2,2,1.0
what is usually,2,2,1.0
is usually the,2,2,1.0
usually the of,2,2,1.0
the of july,2,2,1.0
of july if,2,2,1.0
july if soil,2,2,1.0
if soil moisture,2,2,1.0
soil moisture drops,2,2,1.0
moisture drops then,2,2,1.0
drops then grass,2,2,1.0
then grass stops,2,2,1.0
grass stops growing,2,2,1.0
stops growing indeed,2,2,1.0
growing indeed high,2,2,1.0
indeed high solar,2,2,1.0
high solar radiation,2,2,1.0
solar radiation will,2,2,1.0
radiation will burn,2,2,1.0
will burn grass,2,2,1.0
burn grass the,1,1,1.0
grass the model,1,1,1.0
model does not,2,2,1.0
does not do,2,2,1.0
not do very,2,2,1.0
do very well,2,2,1.0
very well at,2,2,1.0
growth for these,2,2,1.0
for these months,1,1,1.0
these months of,1,1,1.0
months of because,2,2,1.0
of because they,2,2,1.0
because they are,4,2,2.0
they are historically,2,2,1.0
are historically unique,2,2,1.0
historically unique temraz,2,2,1.0
unique temraz et,2,2,1.0
et al defined,2,2,1.0
al defined a,2,2,1.0
defined a class,1,1,1.0
a class boundary,1,1,1.0
class boundary in,2,2,1.0
boundary in the,2,2,1.0
the dataset creating,2,2,1.0
dataset creating a,2,2,1.0
creating a division,2,2,1.0
a division between,2,2,1.0
division between normal,2,2,1.0
between normal cases,2,2,1.0
normal cases with,2,2,1.0
cases with values,4,2,2.0
with values within,2,2,1.0
values within standard,2,2,1.0
within standard deviations,2,2,1.0
standard deviations of,4,2,2.0
deviations of historical,4,2,2.0
of historical means,4,2,2.0
historical means and,2,2,1.0
means and cases,2,2,1.0
and cases with,2,2,1.0
with values standard,2,2,1.0
values standard deviations,2,2,1.0
historical means from,2,2,1.0
means from a,2,2,1.0
from a perspective,1,1,1.0
a perspective these,1,1,1.0
perspective these normal,2,2,1.0
these normal cases,2,2,1.0
normal cases were,2,2,1.0
cases were the,2,2,1.0
were the majority,2,2,1.0
and the cases,2,2,1.0
the cases an,2,2,1.0
cases an unpublished,2,2,1.0
an unpublished paper,2,2,1.0
unpublished paper reports,2,2,1.0
paper reports an,2,2,1.0
reports an identical,2,2,1.0
an identical method,2,2,1.0
identical method to,2,2,1.0
method to wachter,2,2,1.0
to wachter et,2,2,1.0
et s counterfactual,2,2,1.0
s counterfactual optimization,2,2,1.0
counterfactual optimization method,2,2,1.0
optimization method for,2,2,1.0
data augmentation however,2,2,1.0
augmentation however this,2,2,1.0
however this paper,2,2,1.0
this paper does,2,2,1.0
paper does not,2,2,1.0
does not reference,2,2,1.0
not reference wachter,2,2,1.0
reference wachter et,2,2,1.0
wachter et al,2,2,1.0
et al or,2,2,1.0
al or any,2,2,1.0
or any of,2,2,1.0
any of the,4,4,1.0
of the xai,2,2,1.0
xai literature temraz,2,2,1.0
literature temraz keane,2,2,1.0
data augmentation are,2,2,1.0
augmentation are the,2,2,1.0
are the minority,2,2,1.0
class they then,2,2,1.0
they then used,2,2,1.0
then used the,2,2,1.0
used the counterfactual,2,2,1.0
method to create,1,1,1.0
to create new,2,2,1.0
create new synthetic,2,2,1.0
new synthetic cases,2,2,1.0
synthetic cases and,2,2,1.0
cases and showed,2,2,1.0
and showed that,2,2,1.0
showed that the,2,2,1.0
that the model,2,2,1.0
the model s,2,2,1.0
model s performance,2,2,1.0
s performance specifically,2,2,1.0
performance specifically improved,2,2,1.0
specifically improved on,2,2,1.0
improved on predicting,2,2,1.0
on predicting events,2,2,1.0
predicting events in,2,2,1.0
events in using,2,2,1.0
in using these,2,2,1.0
using these minority,1,1,1.0
these minority interestingly,1,1,1.0
minority interestingly temraz,1,1,1.0
interestingly temraz et,2,2,1.0
et al s,3,3,1.0
al s experiments,2,2,1.0
s experiments showed,2,2,1.0
experiments showed that,3,3,1.0
showed that that,2,2,1.0
that that the,2,2,1.0
that the did,1,1,1.0
the did better,1,1,1.0
did better than,2,2,1.0
better than in,2,2,1.0
than in this,3,3,1.0
in this problem,2,2,1.0
this problem domain,2,2,1.0
problem domain specifically,2,2,1.0
domain specifically the,1,1,1.0
specifically the dice,1,1,1.0
the dice method,1,1,1.0
dice method however,2,2,1.0
method however this,2,2,1.0
however this work,2,2,1.0
this work only,2,2,1.0
work only considers,2,2,1.0
only considers one,2,2,1.0
considers one specific,2,2,1.0
one specific problem,2,2,1.0
specific problem domain,2,2,1.0
problem domain classifier,2,2,1.0
domain classifier and,2,2,1.0
classifier and dataset,2,2,1.0
and dataset it,2,2,1.0
dataset it remains,2,2,1.0
it remains to,2,2,1.0
remains to be,2,2,1.0
to be seen,2,2,1.0
be seen whether,2,2,1.0
seen whether this,2,2,1.0
whether this counterfactual,2,2,1.0
this counterfactual approach,2,2,1.0
counterfactual approach generalizes,2,2,1.0
approach generalizes to,2,2,1.0
generalizes to other,2,2,1.0
to other problem,1,1,1.0
other problem domains,1,1,1.0
problem domains classifiers,1,1,1.0
domains classifiers and,2,2,1.0
classifiers and datasets,2,2,1.0
and datasets and,2,2,1.0
datasets and specifically,2,2,1.0
and specifically to,2,2,1.0
specifically to datasets,2,2,1.0
to datasets where,2,2,1.0
datasets where problems,2,2,1.0
where problems arise,2,2,1.0
problems arise hence,2,2,1.0
arise hence this,2,2,1.0
hence this is,2,2,1.0
this is the,3,3,1.0
is the aim,2,2,1.0
the aim for,2,2,1.0
aim for the,2,2,1.0
for the remainder,2,2,1.0
the remainder of,3,3,1.0
remainder of this,3,3,1.0
of this paper,4,4,1.0
this paper the,5,3,1.6666666666666667
paper the counterfactual,2,2,1.0
the counterfactual augmentation,4,2,2.0
counterfactual augmentation algorithm,2,2,1.0
augmentation algorithm cfa,2,2,1.0
algorithm cfa this,2,2,1.0
cfa this paper,2,2,1.0
this paper advances,2,2,1.0
paper advances the,2,2,1.0
advances the use,2,2,1.0
use of counterfactual,2,2,1.0
of counterfactual methods,2,2,1.0
methods for data,2,2,1.0
data augmentation as,2,2,1.0
augmentation as a,2,2,1.0
as a solution,2,2,1.0
a solution to,2,2,1.0
solution to the,1,1,1.0
class with more,2,2,1.0
with more synthetic,2,2,1.0
synthetic data to,3,3,1.0
solve problems the,2,2,1.0
problems the of,1,1,1.0
the of this,1,1,1.0
of this xai,2,2,1.0
this xai method,2,2,1.0
xai method to,2,2,1.0
method to data,2,2,1.0
data augmentation was,2,2,1.0
augmentation was motivated,2,2,1.0
was motivated by,2,2,1.0
motivated by the,3,3,1.0
by the observation,1,1,1.0
the observation that,1,1,1.0
observation that it,1,1,1.0
that it seemed,2,2,1.0
it seemed to,2,2,1.0
seemed to generate,2,2,1.0
to generate plausible,4,2,2.0
generate plausible synthetic,2,2,1.0
plausible synthetic datapoints,4,2,2.0
synthetic datapoints for,4,2,2.0
datapoints for explanatory,2,2,1.0
for explanatory purposes,2,2,1.0
explanatory purposes furthermore,2,2,1.0
purposes furthermore the,2,2,1.0
furthermore the evaluation,1,1,1.0
the evaluation metrics,3,2,1.5
evaluation metrics in,1,1,1.0
metrics in xai,4,2,2.0
in xai showed,2,2,1.0
xai showed that,2,2,1.0
showed that these,2,2,1.0
that these datapoints,2,2,1.0
these datapoints were,2,2,1.0
datapoints were generally,2,2,1.0
were generally valid,2,2,1.0
generally valid and,4,2,2.0
valid and close,4,2,2.0
to existing datapoints,2,2,1.0
existing datapoints accordingly,2,2,1.0
datapoints accordingly the,2,2,1.0
accordingly the extension,2,2,1.0
the extension of,2,2,1.0
extension of these,2,2,1.0
of these techniques,2,2,1.0
these techniques to,2,2,1.0
techniques to data,2,2,1.0
augmentation problems seemed,2,2,1.0
problems seemed like,2,2,1.0
seemed like a,2,2,1.0
like a promising,2,2,1.0
a promising avenue,2,2,1.0
promising avenue of,2,2,1.0
avenue of research,2,2,1.0
of research in,3,3,1.0
research in this,2,2,1.0
in this section,12,4,3.0
this section we,7,4,1.75
section we describe,2,2,1.0
we describe a,2,2,1.0
describe a new,2,2,1.0
a new oversampling,2,2,1.0
new oversampling method,2,2,1.0
oversampling method using,2,2,1.0
method using a,2,2,1.0
using a reasoning,1,1,1.0
a reasoning to,1,1,1.0
reasoning to generating,1,1,1.0
to generating synthetic,4,2,2.0
generating synthetic counterfactuals,4,2,2.0
synthetic counterfactuals in,4,2,2.0
counterfactuals in the,10,2,5.0
class to be,3,3,1.0
to be applied,3,3,1.0
be applied to,4,2,2.0
applied to binary,2,2,1.0
to binary problems,1,1,1.0
binary problems consider,1,1,1.0
problems consider a,2,2,1.0
consider a simple,2,2,1.0
a simple scenario,2,2,1.0
simple scenario to,2,2,1.0
scenario to show,2,2,1.0
to show how,2,2,1.0
show how this,2,2,1.0
how this method,2,2,1.0
this method operates,2,2,1.0
method operates temraz,2,2,1.0
operates temraz keane,2,2,1.0
data augmentation a,4,2,2.0
augmentation a counterfactual,2,2,1.0
a counterfactual example,2,2,1.0
counterfactual example for,2,2,1.0
example for class,2,2,1.0
for class imbalances,2,2,1.0
class imbalances imagine,2,2,1.0
imbalances imagine an,2,2,1.0
imagine an ml,2,2,1.0
an ml classifier,2,2,1.0
ml classifier being,2,2,1.0
classifier being used,2,2,1.0
being used to,2,2,1.0
used to predict,8,2,4.0
to predict whether,4,2,2.0
predict whether farm,2,2,1.0
whether farm animals,2,2,1.0
farm animals are,2,2,1.0
animals are likely,2,2,1.0
are likely to,2,2,1.0
to be healthy,4,2,2.0
be healthy or,2,2,1.0
healthy or fall,2,2,1.0
or fall ill,2,2,1.0
fall ill mastitis,2,2,1.0
ill mastitis in,2,2,1.0
mastitis in cows,2,2,1.0
in cows see,2,2,1.0
cows see the,2,2,1.0
see the dataset,2,2,1.0
the dataset recording,2,2,1.0
dataset recording a,2,2,1.0
recording a herd,2,2,1.0
a herd of,2,2,1.0
herd of cows,2,2,1.0
of cows on,2,2,1.0
cows on most,2,2,1.0
on most farms,2,2,1.0
most farms will,2,2,1.0
farms will be,2,2,1.0
will be imbalanced,2,2,1.0
be imbalanced in,2,2,1.0
imbalanced in that,2,2,1.0
in that most,2,2,1.0
that most cows,2,2,1.0
most cows will,2,2,1.0
cows will tend,2,2,1.0
will tend to,3,3,1.0
tend to be,6,3,2.0
be healthy rather,2,2,1.0
healthy rather than,2,2,1.0
rather than an,2,2,1.0
than an analysis,2,2,1.0
an analysis of,4,4,1.0
analysis of this,2,2,1.0
of this dataset,2,2,1.0
this dataset shows,2,2,1.0
dataset shows that,2,2,1.0
shows that some,2,2,1.0
that some pairs,2,2,1.0
some pairs of,2,2,1.0
of instances instance,2,2,1.0
instances instance pairs,2,2,1.0
instance pairs be,2,2,1.0
pairs be counterfactually,2,2,1.0
be counterfactually related,2,2,1.0
related to one,2,2,1.0
to one another,3,3,1.0
one another for,2,2,1.0
another for example,2,2,1.0
for example a,2,2,1.0
example a majority,2,2,1.0
a majority instance,2,2,1.0
majority instance of,2,2,1.0
instance of a,2,2,1.0
of a certain,2,2,1.0
a certain breed,2,2,1.0
certain breed age,2,2,1.0
breed age and,4,2,2.0
age and that,1,1,1.0
and that is,1,1,1.0
that is classed,4,2,2.0
is classed as,4,2,2.0
classed as healthy,2,2,1.0
as healthy can,2,2,1.0
healthy can be,2,2,1.0
can be counterfactually,2,2,1.0
be counterfactually paired,2,2,1.0
counterfactually paired with,2,2,1.0
paired with a,2,2,1.0
with a minority,2,2,1.0
minority instance that,4,2,2.0
instance that is,4,2,2.0
that is of,2,2,1.0
is of the,2,2,1.0
of the same,4,2,2.0
the same breed,2,2,1.0
same breed age,2,2,1.0
breed age but,2,2,1.0
age but with,2,2,1.0
but with a,2,2,1.0
a different they,1,1,1.0
different they have,1,1,1.0
they have ill,2,2,1.0
have ill several,2,2,1.0
ill several times,2,2,1.0
several times that,2,2,1.0
times that is,2,2,1.0
classed as likely,2,2,1.0
as likely to,2,2,1.0
to be this,3,3,1.0
be this which,2,2,1.0
this which we,2,2,1.0
which we call,2,2,1.0
we call a,2,2,1.0
call a native,2,2,1.0
a native counterfactual,11,2,5.5
native counterfactual tells,2,2,1.0
counterfactual tells us,2,2,1.0
tells us that,2,2,1.0
us that a,2,2,1.0
that a difference,2,2,1.0
a difference in,2,2,1.0
difference in the,4,2,2.0
in the feature,23,4,5.75
the feature can,2,2,1.0
feature can change,2,2,1.0
can change the,2,2,1.0
change the class,2,2,1.0
the class of,5,3,1.6666666666666667
class of a,2,2,1.0
of a cow,2,2,1.0
a cow from,2,2,1.0
cow from healthy,2,2,1.0
from healthy to,2,2,1.0
healthy to so,2,2,1.0
to so if,2,2,1.0
so if we,2,2,1.0
if we want,2,2,1.0
we want to,2,2,1.0
want to fix,2,2,1.0
to fix the,2,2,1.0
fix the class,2,2,1.0
class imbalance in,5,4,1.25
imbalance in this,3,3,1.0
in this dataset,2,2,1.0
this dataset using,1,1,1.0
dataset using our,1,1,1.0
using our counterfactual,1,1,1.0
our counterfactual method,2,2,1.0
counterfactual method then,2,2,1.0
method then one,2,2,1.0
then one can,2,2,1.0
one can generate,2,2,1.0
can generate a,4,2,2.0
generate a new,6,2,3.0
a new minority,4,2,2.0
new minority instance,4,2,2.0
minority instance by,2,2,1.0
instance by this,1,1,1.0
by this known,1,1,1.0
this known counterfactual,1,1,1.0
known counterfactual relationship,2,2,1.0
counterfactual relationship imagine,2,2,1.0
relationship imagine we,2,2,1.0
imagine we pick,2,2,1.0
we pick another,2,2,1.0
pick another majority,2,2,1.0
another majority instance,2,2,1.0
majority instance a,2,2,1.0
instance a cow,2,2,1.0
a cow that,2,2,1.0
cow that has,2,2,1.0
that has no,2,2,1.0
has no existing,2,2,1.0
no existing counterfactual,2,2,1.0
existing counterfactual pair,4,2,2.0
counterfactual pair and,2,2,1.0
pair and find,2,2,1.0
and find a,2,2,1.0
find a nearest,2,2,1.0
nearest neighbour to,2,2,1.0
neighbour to that,2,2,1.0
to that is,2,2,1.0
that is part,2,2,1.0
is part of,3,3,1.0
part of known,2,2,1.0
of known the,2,2,1.0
known the pair,2,2,1.0
the pair using,2,2,1.0
pair using this,2,2,1.0
using this and,2,2,1.0
this and the,2,2,1.0
and the native,2,2,1.0
the native we,2,2,1.0
native we can,2,2,1.0
we can generate,2,2,1.0
a new synthetic,6,2,3.0
new synthetic minority,4,2,2.0
synthetic minority instance,1,1,1.0
minority instance using,1,1,1.0
instance using the,1,1,1.0
using the of,1,1,1.0
the of and,1,1,1.0
of and the,2,2,1.0
and the from,3,2,1.5
the from so,2,2,1.0
from so this,2,2,1.0
so this new,2,2,1.0
new instance would,2,2,1.0
instance would have,2,2,1.0
would have the,2,2,1.0
have the of,2,2,1.0
the of for,2,2,1.0
of for breed,2,2,1.0
for breed age,2,2,1.0
age and and,2,2,1.0
and and the,2,2,1.0
the from for,1,1,1.0
from for along,2,2,1.0
for along with,2,2,1.0
with the prediction,2,2,1.0
the prediction that,2,2,1.0
prediction that it,2,2,1.0
that it will,2,2,1.0
it will be,2,2,1.0
will be so,3,3,1.0
be so we,2,2,1.0
so we have,2,2,1.0
we have now,2,2,1.0
have now created,2,2,1.0
now created a,2,2,1.0
created a new,2,2,1.0
related to where,2,2,1.0
to where the,2,2,1.0
where the class,2,2,1.0
class of this,2,2,1.0
of this new,2,2,1.0
new instance needs,2,2,1.0
instance needs to,2,2,1.0
needs to be,6,2,3.0
to be verified,2,2,1.0
be verified by,2,2,1.0
verified by the,2,2,1.0
by the underlying,2,2,1.0
the underlying ml,2,2,1.0
underlying ml model,2,2,1.0
ml model this,2,2,1.0
model this example,2,2,1.0
this example keane,2,2,1.0
example keane smyth,2,2,1.0
keane smyth have,2,2,1.0
smyth have shown,2,2,1.0
have shown good,2,2,1.0
shown good native,2,2,1.0
good native those,2,2,1.0
native those with,2,2,1.0
those with differences,2,2,1.0
with differences between,2,2,1.0
differences between them,2,2,1.0
between them are,2,2,1.0
them are quite,2,2,1.0
are quite rare,2,2,1.0
quite rare in,2,2,1.0
rare in most,2,2,1.0
in most datasets,2,2,1.0
most datasets of,2,2,1.0
datasets of all,2,2,1.0
of all instances,2,2,1.0
all instances but,2,2,1.0
instances but with,2,2,1.0
but with some,2,2,1.0
with some tolerance,2,2,1.0
some tolerance in,2,2,1.0
tolerance in they,2,2,1.0
in they can,2,2,1.0
they can be,3,3,1.0
can be increased,3,3,1.0
be increased to,2,2,1.0
increased to temraz,2,2,1.0
to temraz keane,2,2,1.0
data augmentation describes,2,2,1.0
augmentation describes the,2,2,1.0
describes the generation,2,2,1.0
generation of one,2,2,1.0
of one minority,2,2,1.0
one minority instance,2,2,1.0
minority instance in,2,2,1.0
instance in our,2,2,1.0
in our experiments,3,2,1.5
our experiments we,2,2,1.0
experiments we do,2,2,1.0
we do this,2,2,1.0
do this iteratively,1,1,1.0
this iteratively for,1,1,1.0
iteratively for all,1,1,1.0
for all those,2,2,1.0
all those majority,2,2,1.0
those majority instances,2,2,1.0
majority instances that,2,2,1.0
that are not,4,2,2.0
are not paired,2,2,1.0
not paired in,2,2,1.0
paired in native,2,2,1.0
in native counterfactuals,3,2,1.5
native counterfactuals a,2,2,1.0
counterfactuals a step,2,2,1.0
a step that,2,2,1.0
step that results,2,2,1.0
that results in,2,2,1.0
results in the,3,3,1.0
in the generation,3,3,1.0
generation of many,2,2,1.0
of many more,2,2,1.0
many more minority,2,2,1.0
more minority instances,2,2,1.0
majority class in,5,4,1.25
class in the,8,3,2.6666666666666665
the next we,2,2,1.0
next we describe,2,2,1.0
we describe the,2,2,1.0
describe the algorithm,2,2,1.0
the algorithm more,2,2,1.0
algorithm more formally,2,2,1.0
more formally the,2,2,1.0
formally the method,2,2,1.0
the method counterfactual,2,2,1.0
method counterfactual augmentation,4,2,2.0
counterfactual augmentation cfa,8,2,4.0
augmentation cfa the,2,2,1.0
cfa the counterfactual,2,2,1.0
augmentation cfa is,2,2,1.0
cfa is a,2,2,1.0
is a technique,2,2,1.0
a technique for,2,2,1.0
technique for generating,2,2,1.0
generating synthetic examples,2,2,1.0
synthetic examples in,2,2,1.0
examples in the,12,4,3.0
class using counterfactual,2,2,1.0
using counterfactual methods,2,2,1.0
counterfactual methods see,2,2,1.0
methods see for,2,2,1.0
see for the,2,2,1.0
for the method,2,2,1.0
the method used,2,2,1.0
method used in,2,2,1.0
used in xai,2,2,1.0
in xai the,4,2,2.0
xai the cfa,2,2,1.0
the cfa method,6,2,3.0
cfa method generates,2,2,1.0
method generates synthetic,2,2,1.0
instances in three,2,2,1.0
in three main,2,2,1.0
three main steps,2,2,1.0
main steps see,2,2,1.0
steps see figure,2,2,1.0
see figure figure,2,2,1.0
figure figure counterfactual,2,2,1.0
figure counterfactual augmentation,2,2,1.0
augmentation cfa an,1,1,1.0
cfa an unpaired,1,1,1.0
an unpaired instance,5,2,2.5
unpaired instance 𝒙,6,2,3.0
instance 𝒙 grey,4,2,2.0
𝒙 grey circle,4,2,2.0
grey circle finds,2,2,1.0
circle finds a,2,2,1.0
finds a nearest,2,2,1.0
a nearest neighbor,2,2,1.0
nearest neighbor 𝒙,2,2,1.0
neighbor 𝒙 blue,2,2,1.0
𝒙 blue circle,2,2,1.0
blue circle taking,2,2,1.0
circle taking part,2,2,1.0
taking part in,4,2,2.0
in a good,2,2,1.0
a good native,2,2,1.0
good native in,2,2,1.0
native in the,4,2,2.0
the dataset 𝒄𝒇,4,2,2.0
dataset 𝒄𝒇 𝒙,4,2,2.0
𝒄𝒇 𝒙 𝒑,6,2,3.0
𝒙 𝒑 pairing,2,2,1.0
𝒑 pairing of,2,2,1.0
pairing of blue,2,2,1.0
of blue circle,2,2,1.0
blue circle and,2,2,1.0
circle and yellow,2,2,1.0
and yellow box,2,2,1.0
yellow box and,2,2,1.0
box and then,2,2,1.0
and then uses,2,2,1.0
then uses the,2,2,1.0
uses the of,2,2,1.0
the of the,10,4,2.5
of the 𝒑,2,2,1.0
the 𝒑 box,1,1,1.0
𝒑 box to,1,1,1.0
box to generate,2,2,1.0
new synthetic 𝒑,1,1,1.0
synthetic 𝒑 green,1,1,1.0
𝒑 green box,4,2,2.0
green box combining,2,2,1.0
box combining them,2,2,1.0
combining them with,2,2,1.0
them with the,2,2,1.0
with the of,2,2,1.0
the original unpaired,4,2,2.0
original unpaired instance,4,2,2.0
grey circle the,2,2,1.0
circle the generated,2,2,1.0
the generated synthetic,4,3,1.3333333333333333
generated synthetic instance,3,3,1.0
synthetic instance 𝒑,2,2,1.0
instance 𝒑 green,2,2,1.0
green box is,2,2,1.0
box is then,2,2,1.0
the dataset to,3,3,1.0
dataset to improve,2,2,1.0
to improve future,2,2,1.0
improve future prediction,2,2,1.0
future prediction temraz,2,2,1.0
prediction temraz keane,2,2,1.0
data augmentation i,2,2,1.0
augmentation i good,2,2,1.0
i good native,2,2,1.0
good native counterfactuals,4,2,2.0
native counterfactuals 𝑐𝑓,4,2,2.0
counterfactuals 𝑐𝑓 𝑥,4,2,2.0
𝑐𝑓 𝑥 𝑝,17,2,8.5
𝑥 𝑝 are,2,2,1.0
𝑝 are initially,2,2,1.0
are initially computed,2,2,1.0
initially computed over,2,2,1.0
computed over the,2,2,1.0
over the whole,2,2,1.0
the whole dataset,2,2,1.0
whole dataset 𝑇,2,2,1.0
dataset 𝑇 identifying,2,2,1.0
𝑇 identifying combinations,2,2,1.0
identifying combinations of,2,2,1.0
combinations of instances,2,2,1.0
of instances 𝑥,2,2,1.0
instances 𝑥 from,2,2,1.0
𝑥 from the,2,2,1.0
class and 𝑝,2,2,1.0
and 𝑝 from,2,2,1.0
minority class ii,2,2,1.0
class ii given,2,2,1.0
ii given an,2,2,1.0
given an unpaired,2,2,1.0
unpaired instance 𝑥,4,2,2.0
instance 𝑥 its,2,2,1.0
𝑥 its paired,2,2,1.0
its paired instance,4,2,2.0
paired instance 𝑥,3,2,1.5
instance 𝑥 is,2,2,1.0
𝑥 is found,2,2,1.0
is found and,2,2,1.0
found and used,2,2,1.0
and used to,2,2,1.0
used to identify,2,2,1.0
to identify a,3,3,1.0
identify a close,2,2,1.0
a close existing,2,2,1.0
close existing counterfactual,2,2,1.0
counterfactual pair 𝑐𝑓,4,2,2.0
pair 𝑐𝑓 𝑥,4,2,2.0
𝑥 𝑝 and,2,2,1.0
𝑝 and then,2,2,1.0
and then iii,2,2,1.0
then iii a,2,2,1.0
iii a new,2,2,1.0
new synthetic counterfactual,3,2,1.5
synthetic counterfactual instance,4,2,2.0
counterfactual instance 𝑝,2,2,1.0
instance 𝑝 is,2,2,1.0
𝑝 is produced,2,2,1.0
is produced in,2,2,1.0
produced in the,2,2,1.0
class using features,2,2,1.0
using features from,2,2,1.0
features from the,3,2,1.5
instance 𝑥 and,2,2,1.0
𝑥 and values,2,2,1.0
and values from,2,2,1.0
values from more,2,2,1.0
from more formally,2,2,1.0
more formally definitions,2,2,1.0
formally definitions 𝑋,2,2,1.0
definitions 𝑋 is,2,2,1.0
𝑋 is a,2,2,1.0
is a majority,2,2,1.0
majority class 𝑐𝑙𝑎𝑠𝑠efg,2,2,1.0
class 𝑐𝑙𝑎𝑠𝑠efg 𝑃,2,2,1.0
𝑐𝑙𝑎𝑠𝑠efg 𝑃 is,2,2,1.0
𝑃 is a,2,2,1.0
is a minority,2,2,1.0
a minority class,4,3,1.3333333333333333
minority class 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
class 𝑐𝑙𝑎𝑠𝑠e 𝑥,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑥 𝑥,1,1,1.0
𝑥 𝑥 where,2,2,1.0
𝑥 where 𝑥,4,2,2.0
where 𝑥 is,2,2,1.0
𝑥 is a,4,2,2.0
is a paired,4,2,2.0
a paired instance,4,2,2.0
paired instance in,2,2,1.0
instance in 𝑐𝑙𝑎𝑠𝑠efg,4,2,2.0
in 𝑐𝑙𝑎𝑠𝑠efg 𝑥,2,2,1.0
𝑐𝑙𝑎𝑠𝑠efg 𝑥 𝑥j,1,1,1.0
𝑥 𝑥j 𝑥k,3,2,1.5
𝑥j 𝑥k 𝑥l,4,2,2.0
𝑥k 𝑥l 𝑥,4,2,2.0
𝑥l 𝑥 where,2,2,1.0
where 𝑥 𝑥,2,1,2.0
𝑥 𝑥 𝑝,2,1,2.0
𝑥 𝑝 𝑥,2,2,1.0
𝑝 𝑥 is,2,2,1.0
𝑥 is an,2,2,1.0
is an unpaired,2,2,1.0
unpaired instance in,2,2,1.0
in 𝑐𝑙𝑎𝑠𝑠efg where,2,2,1.0
𝑐𝑙𝑎𝑠𝑠efg where 𝑥,2,2,1.0
𝑥 𝑝 𝑝,3,2,1.5
𝑝 𝑝 𝑝,1,1,1.0
𝑝 𝑝 where,2,2,1.0
𝑝 where 𝑝,3,2,1.5
where 𝑝 is,2,2,1.0
𝑝 is a,6,2,3.0
is a counterfactual,4,2,2.0
a counterfactual instance,4,2,2.0
counterfactual instance in,4,2,2.0
instance in 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
in 𝑐𝑙𝑎𝑠𝑠e 𝑝j,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑝j 𝑝k,1,1,1.0
𝑝j 𝑝k 𝑝l,4,2,2.0
𝑝k 𝑝l 𝑝,4,2,2.0
𝑝l 𝑝 where,2,2,1.0
𝑝 where 𝑥,1,1,1.0
where 𝑥 𝑝,1,1,1.0
𝑝 𝑝 is,2,2,1.0
is a synthetic,2,2,1.0
a synthetic counterfactual,2,2,1.0
counterfactual instance generated,2,2,1.0
instance generated to,2,2,1.0
generated to be,2,2,1.0
to be added,2,2,1.0
be added to,2,2,1.0
added to 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
to 𝑐𝑙𝑎𝑠𝑠e is,2,2,1.0
𝑐𝑙𝑎𝑠𝑠e is 𝑐𝑓,2,2,1.0
is 𝑐𝑓 𝑥,2,2,1.0
𝑥 𝑝 𝑡𝑎𝑟𝑔𝑒𝑡,2,2,1.0
𝑝 𝑡𝑎𝑟𝑔𝑒𝑡 𝑥,2,2,1.0
𝑡𝑎𝑟𝑔𝑒𝑡 𝑥 𝑝,1,1,1.0
𝑥 𝑝 neighbors,1,1,1.0
𝑝 neighbors assume,2,2,1.0
neighbors assume that,2,2,1.0
assume that 𝑥,2,2,1.0
that 𝑥 is,2,2,1.0
paired instance which,2,2,1.0
instance which belongs,4,2,2.0
which belongs to,4,2,2.0
belongs to the,8,3,2.6666666666666665
majority class 𝑋,2,2,1.0
class 𝑋 and,2,2,1.0
𝑋 and 𝑝,2,2,1.0
and 𝑝 is,2,2,1.0
counterfactual instance which,2,2,1.0
class 𝑃 𝑥j,1,1,1.0
𝑃 𝑥j 𝑥k,1,1,1.0
𝑥l 𝑥 𝑝j,1,1,1.0
𝑥 𝑝j 𝑝k,1,1,1.0
𝑝l 𝑝 the,2,2,1.0
𝑝 the procedure,2,2,1.0
the procedure for,3,3,1.0
procedure for cfa,2,2,1.0
for cfa is,4,2,2.0
cfa is as,2,2,1.0
is as follows,2,2,1.0
as follows temraz,2,2,1.0
follows temraz keane,2,2,1.0
data augmentation step,4,2,2.0
augmentation step compute,2,2,1.0
step compute the,2,2,1.0
compute the for,2,2,1.0
the for the,2,2,1.0
for the dataset,7,4,1.75
𝒙 𝒑 cfa,2,2,1.0
𝒑 cfa first,2,2,1.0
cfa first finds,2,2,1.0
first finds all,2,2,1.0
finds all possible,2,2,1.0
all possible good,2,2,1.0
possible good native,2,2,1.0
good native counterfactual,2,2,1.0
native counterfactual pairs,4,2,2.0
counterfactual pairs 𝑐𝑓,2,2,1.0
pairs 𝑐𝑓 𝑥,2,2,1.0
𝑥 𝑝 between,2,2,1.0
𝑝 between instances,2,2,1.0
between instances that,2,2,1.0
instances that already,2,2,1.0
that already exist,2,2,1.0
already exist in,4,2,2.0
exist in a,2,2,1.0
in a 𝑇,1,1,1.0
a 𝑇 these,1,1,1.0
𝑇 these native,2,2,1.0
these native counterfactuals,6,2,3.0
native counterfactuals pair,2,2,1.0
counterfactuals pair an,2,2,1.0
pair an instance,2,2,1.0
an instance in,2,2,1.0
in the majority,8,3,2.6666666666666665
the majority space,2,2,1.0
majority space called,2,2,1.0
space called the,4,2,2.0
called the paired,2,2,1.0
the paired instance,2,2,1.0
paired instance and,2,2,1.0
instance and its,2,2,1.0
and its instance,2,2,1.0
its instance in,2,2,1.0
the minority space,2,2,1.0
minority space called,2,2,1.0
called the counterfactual,2,2,1.0
the counterfactual instance,2,2,1.0
instance in other,2,2,1.0
in other words,4,3,1.3333333333333333
other words for,2,2,1.0
words for every,2,2,1.0
for every 𝑥,2,2,1.0
every 𝑥 in,2,2,1.0
𝑥 in the,2,2,1.0
majority class 𝑥,2,2,1.0
class 𝑥 we,2,2,1.0
𝑥 we find,2,2,1.0
we find its,2,2,1.0
find its counterfactual,2,2,1.0
its counterfactual 𝑝,2,2,1.0
counterfactual 𝑝 from,2,2,1.0
𝑝 from 𝑝,2,2,1.0
from 𝑝 in,2,2,1.0
minority class these,2,2,1.0
class these native,2,2,1.0
𝑥 𝑝 pair,2,2,1.0
𝑝 pair instances,2,2,1.0
pair instances either,2,2,1.0
instances either side,4,2,2.0
side of decision,2,2,1.0
of decision boundary,2,2,1.0
decision boundary they,2,2,1.0
boundary they are,2,2,1.0
they are called,2,2,1.0
are called native,2,2,1.0
called native in,1,1,1.0
native in one,1,1,1.0
in one sense,2,2,1.0
one sense they,2,2,1.0
sense they already,2,2,1.0
they already exist,2,2,1.0
the dataset each,2,2,1.0
dataset each of,2,2,1.0
each of these,5,4,1.25
of these native,2,2,1.0
these native pairs,2,2,1.0
native pairs has,2,2,1.0
pairs has a,2,2,1.0
has a set,2,2,1.0
set of and,2,2,1.0
of and a,2,2,1.0
and a set,2,2,1.0
set of where,2,2,1.0
of where the,2,2,1.0
where the differences,2,2,1.0
the differences determine,2,2,1.0
differences determine the,2,2,1.0
determine the class,2,2,1.0
the class change,2,2,1.0
class change over,2,2,1.0
change over the,2,2,1.0
over the decision,2,2,1.0
decision boundary step,2,2,1.0
boundary step for,2,2,1.0
step for each,2,2,1.0
for each unpaired,2,2,1.0
each unpaired instance,2,2,1.0
instance 𝒙 from,2,2,1.0
𝒙 from the,2,2,1.0
majority class find,2,2,1.0
class find its,2,2,1.0
find its paired,2,2,1.0
paired instance x,2,2,1.0
instance x taking,2,2,1.0
x taking part,2,2,1.0
in a native,4,2,2.0
native counterfactual 𝒄𝒇,2,2,1.0
counterfactual 𝒄𝒇 𝒙,2,2,1.0
𝒙 𝒑 for,2,2,1.0
𝒑 for each,2,2,1.0
each instance 𝑥,1,1,1.0
instance 𝑥 cfa,2,2,1.0
𝑥 cfa uses,2,2,1.0
cfa uses a,4,2,2.0
uses a to,2,2,1.0
a to find,2,2,1.0
to find its,2,2,1.0
find its nearest,2,2,1.0
its nearest neighboring,2,2,1.0
nearest neighboring 𝑥,2,2,1.0
neighboring 𝑥 a,2,2,1.0
𝑥 a paired,2,2,1.0
a paired involved,1,1,1.0
paired involved in,1,1,1.0
involved in a,2,2,1.0
native counterfactual pair,2,2,1.0
𝑥 𝑝 by,2,2,1.0
𝑝 by definition,2,2,1.0
by definition 𝑥,2,2,1.0
definition 𝑥 belongs,2,2,1.0
𝑥 belongs to,2,2,1.0
class and does,2,2,1.0
does not occur,2,2,1.0
not occur any,2,2,1.0
occur any native,2,2,1.0
any native counterfactual,2,2,1.0
counterfactual pairs notably,2,2,1.0
pairs notably this,1,1,1.0
notably this means,1,1,1.0
this means that,2,2,1.0
means that all,2,2,1.0
that all the,5,4,1.25
all the synthetic,2,2,1.0
the synthetic datapoints,2,2,1.0
synthetic datapoints generated,2,2,1.0
datapoints generated by,2,2,1.0
generated by cfa,2,2,1.0
by cfa come,2,2,1.0
cfa come from,2,2,1.0
come from these,2,2,1.0
from these instances,2,2,1.0
these instances in,2,2,1.0
class that are,4,3,1.3333333333333333
are not already,2,2,1.0
not already to,2,2,1.0
already to instances,2,2,1.0
to instances in,2,2,1.0
minority class euclidean,2,2,1.0
class euclidean distance,2,2,1.0
euclidean distance is,3,3,1.0
distance is used,2,2,1.0
is used in,3,3,1.0
used in finding,2,2,1.0
in finding these,2,2,1.0
finding these nearest,2,2,1.0
these nearest neighbors,2,2,1.0
nearest neighbors euclidean,2,2,1.0
neighbors euclidean distance,2,2,1.0
euclidean distance ed,2,2,1.0
distance ed 𝑝,1,1,1.0
ed 𝑝 ke,1,1,1.0
𝑝 ke temraz,1,1,1.0
ke temraz keane,1,1,1.0
augmentation step transfer,2,2,1.0
step transfer from,2,2,1.0
transfer from 𝒑,2,2,1.0
from 𝒑 to,2,2,1.0
𝒑 to 𝒑,2,2,1.0
to 𝒑 and,2,2,1.0
𝒑 and from,2,2,1.0
and from 𝒙,2,2,1.0
from 𝒙 𝐭o,2,2,1.0
𝒙 𝐭o 𝒑,2,2,1.0
𝐭o 𝒑 having,2,2,1.0
𝒑 having identified,2,2,1.0
having identified a,2,2,1.0
identified a native,1,1,1.0
native counterfactual 𝑐𝑓,2,2,1.0
counterfactual 𝑐𝑓 𝑥,2,2,1.0
𝑥 𝑝 for,2,2,1.0
𝑝 for 𝑥,2,2,1.0
for 𝑥 cfa,2,2,1.0
𝑥 cfa generates,2,2,1.0
cfa generates a,2,2,1.0
generates a synthetic,2,2,1.0
a synthetic instance,2,2,1.0
synthetic instance in,1,1,1.0
minority class 𝑝,2,2,1.0
class 𝑝 using,2,2,1.0
𝑝 using from,2,2,1.0
using from and,2,2,1.0
from and 𝑝,2,2,1.0
and 𝑝 such,2,2,1.0
𝑝 such that,2,2,1.0
such that for,2,2,1.0
that for each,2,2,1.0
for each of,10,4,2.5
each of the,11,3,3.6666666666666665
of the between,4,2,2.0
the between 𝑥,4,2,2.0
between 𝑥 and,4,2,2.0
𝑥 and 𝑝,4,2,2.0
and 𝑝 take,4,2,2.0
𝑝 take the,4,2,2.0
take the values,4,2,2.0
the values from,4,2,2.0
values from 𝑝,2,2,1.0
from 𝑝 into,2,2,1.0
𝑝 into the,2,2,1.0
into the synthetic,2,2,1.0
the synthetic counterfactual,4,2,2.0
synthetic counterfactual case,2,2,1.0
counterfactual case 𝑝,4,2,2.0
case 𝑝 for,2,2,1.0
𝑝 for each,2,2,1.0
values from 𝑥,2,2,1.0
from 𝑥 into,2,2,1.0
𝑥 into the,2,2,1.0
into the new,2,2,1.0
the new counterfactual,2,2,1.0
new counterfactual case,2,2,1.0
case 𝑝 it,2,2,1.0
𝑝 it should,2,2,1.0
noted that tolerance,2,2,1.0
that tolerance is,2,2,1.0
tolerance is one,2,2,1.0
is one parameter,2,2,1.0
one parameter in,2,2,1.0
parameter in cfa,4,2,2.0
in cfa algorithm,2,2,1.0
cfa algorithm which,2,2,1.0
algorithm which is,2,2,1.0
which is used,4,3,1.3333333333333333
is used to,8,5,1.6
used to improve,2,2,1.0
to improve the,5,4,1.25
improve the availability,2,2,1.0
the availability of,2,2,1.0
availability of good,2,2,1.0
of good native,2,2,1.0
native counterfactuals in,6,2,3.0
the dataset without,2,2,1.0
dataset without tolerance,2,2,1.0
without tolerance fewer,4,2,2.0
tolerance fewer counterfactuals,2,2,1.0
fewer counterfactuals would,2,2,1.0
counterfactuals would be,2,2,1.0
would be found,6,2,3.0
be found and,4,2,2.0
and the generative,4,2,2.0
the generative benefits,4,2,2.0
generative benefits of,4,2,2.0
benefits of them,4,2,2.0
of them would,4,2,2.0
them would likely,4,2,2.0
would likely diminish,4,2,2.0
likely diminish in,2,2,1.0
diminish in finding,2,2,1.0
in finding and,3,2,1.5
finding and between,3,2,1.5
and between two,4,2,2.0
between two instances,4,2,2.0
two instances for,4,2,2.0
instances for a,4,2,2.0
for a native,4,2,2.0
native counterfactual cfa,2,2,1.0
counterfactual cfa computes,1,1,1.0
cfa computes a,1,1,1.0
computes a by,1,1,1.0
a by finding,1,1,1.0
by finding the,2,2,1.0
finding the mean,2,2,1.0
the mean 𝜇,2,2,1.0
mean 𝜇 and,2,2,1.0
𝜇 and standard,2,2,1.0
and standard deviation,4,3,1.3333333333333333
standard deviation 𝜎,2,2,1.0
deviation 𝜎 for,2,2,1.0
𝜎 for each,2,2,1.0
for each feature,4,2,2.0
each feature then,2,2,1.0
feature then it,2,2,1.0
then it allows,2,2,1.0
it allows features,2,2,1.0
allows features to,2,2,1.0
features to match,4,2,2.0
to match if,4,2,2.0
match if their,4,2,2.0
if their values,4,2,2.0
their values are,4,2,2.0
values are within,4,2,2.0
are within of,4,2,2.0
within of the,4,2,2.0
of the standard,4,2,2.0
the standard deviation,5,3,1.6666666666666667
standard deviation from,4,2,2.0
deviation from the,4,2,2.0
from the mean,3,2,1.5
the mean all,3,2,1.5
mean all the,3,2,1.5
all the values,4,2,2.0
the values for,4,2,2.0
values for that,4,2,2.0
for that feature,3,2,1.5
that feature two,2,2,1.0
feature two subtle,2,2,1.0
two subtle differences,2,2,1.0
subtle differences that,2,2,1.0
differences that distinguish,2,2,1.0
that distinguish this,2,2,1.0
distinguish this data,2,2,1.0
this data augmentation,2,2,1.0
data augmentation version,2,2,1.0
augmentation version of,2,2,1.0
version of the,3,3,1.0
of the algorithm,6,2,3.0
the algorithm from,1,1,1.0
algorithm from its,1,1,1.0
from its xai,2,2,1.0
its xai counterpart,2,2,1.0
xai counterpart first,2,2,1.0
counterpart first although,2,2,1.0
first although both,2,2,1.0
although both algorithms,2,2,1.0
both algorithms adopt,2,2,1.0
algorithms adopt the,2,2,1.0
adopt the same,2,2,1.0
the same definition,2,2,1.0
same definition of,2,2,1.0
definition of a,2,2,1.0
of a good,2,2,1.0
a good pairing,1,1,1.0
good pairing the,1,1,1.0
pairing the do,2,2,1.0
the do so,2,2,1.0
do so for,2,2,1.0
so for different,2,2,1.0
for different reasons,2,2,1.0
different reasons on,2,2,1.0
reasons on psychological,2,2,1.0
on psychological grounds,4,2,2.0
psychological grounds keane,2,2,1.0
grounds keane and,2,2,1.0
keane and smyth,1,1,1.0
and smyth defined,1,1,1.0
smyth defined a,1,1,1.0
defined a good,2,2,1.0
a good counterfactual,4,2,2.0
good counterfactual to,2,2,1.0
counterfactual to be,2,2,1.0
to be one,2,2,1.0
be one with,2,2,1.0
one with no,2,2,1.0
with no more,2,2,1.0
no more than,4,2,2.0
more than two,4,2,2.0
than two from,2,2,1.0
two from the,2,2,1.0
the xai perspective,2,2,1.0
xai perspective researchers,2,2,1.0
perspective researchers argue,2,2,1.0
researchers argue that,2,2,1.0
argue that sparse,2,2,1.0
that sparse counterfactuals,2,2,1.0
sparse counterfactuals with,4,2,2.0
counterfactuals with fewer,2,2,1.0
with fewer feature,2,2,1.0
fewer feature differences,2,2,1.0
feature differences are,2,2,1.0
differences are better,2,2,1.0
are better because,2,2,1.0
better because people,2,2,1.0
because people find,2,2,1.0
people find them,2,2,1.0
find them more,2,2,1.0
them more understandable,2,2,1.0
more understandable confirmed,2,2,1.0
understandable confirmed by,2,2,1.0
confirmed by user,2,2,1.0
by user studies,2,2,1.0
user studies from,2,2,1.0
studies from a,2,2,1.0
augmentation perspective basing,2,2,1.0
perspective basing synthetic,2,2,1.0
basing synthetic counterfactuals,2,2,1.0
synthetic counterfactuals on,2,2,1.0
counterfactuals on sparse,2,2,1.0
on sparse pairs,2,2,1.0
sparse pairs also,2,2,1.0
pairs also makes,2,2,1.0
also makes sense,2,2,1.0
makes sense because,2,2,1.0
sense because the,2,2,1.0
because the implicit,2,2,1.0
the implicit causal,2,2,1.0
implicit causal dependencies,2,2,1.0
causal dependencies between,2,2,1.0
dependencies between matched,2,2,1.0
between matched and,2,2,1.0
matched and difference,2,2,1.0
and difference features,2,2,1.0
difference features are,2,2,1.0
features are more,2,2,1.0
are more decision,1,1,1.0
more decision boundary,1,1,1.0
decision boundary temraz,1,1,1.0
boundary temraz keane,1,1,1.0
data augmentation likely,2,2,1.0
augmentation likely to,2,2,1.0
to be preserved,2,2,1.0
be preserved in,2,2,1.0
preserved in hence,2,2,1.0
in hence generated,2,2,1.0
hence generated synthetic,2,2,1.0
generated synthetic using,1,1,1.0
synthetic using these,1,1,1.0
using these sparse,1,1,1.0
these sparse pairs,2,2,1.0
sparse pairs should,2,2,1.0
pairs should be,2,2,1.0
should be more,4,3,1.3333333333333333
be more likely,4,2,2.0
more likely to,9,5,1.8
to be valid,2,2,1.0
be valid and,2,2,1.0
valid and second,2,2,1.0
and second there,2,2,1.0
second there is,2,2,1.0
is a critical,2,2,1.0
a critical between,1,1,1.0
critical between the,1,1,1.0
between the xai,2,2,1.0
the xai and,2,2,1.0
xai and data,2,2,1.0
data augmentation contexts,2,2,1.0
augmentation contexts with,2,2,1.0
contexts with respect,2,2,1.0
to the selection,3,3,1.0
the selection of,2,2,1.0
selection of test,2,2,1.0
of test instances,2,2,1.0
test instances in,2,2,1.0
instances in xai,2,2,1.0
xai the test,2,2,1.0
the test instance,4,2,2.0
test instance is,4,2,2.0
instance is typically,2,2,1.0
is typically a,2,2,1.0
typically a novel,2,2,1.0
a novel problem,2,2,1.0
novel problem for,2,2,1.0
problem for which,2,2,1.0
for which a,2,2,1.0
which a classifier,2,2,1.0
a classifier has,2,2,1.0
classifier has made,2,2,1.0
has made a,2,2,1.0
made a prediction,2,2,1.0
a prediction a,2,2,1.0
prediction a prediction,2,2,1.0
a prediction that,2,2,1.0
prediction that needs,2,2,1.0
that needs to,2,2,1.0
to be counterfactually,2,2,1.0
be counterfactually explained,2,2,1.0
counterfactually explained hence,2,2,1.0
explained hence typically,2,2,1.0
hence typically the,2,2,1.0
typically the test,2,2,1.0
instance is not,2,2,1.0
is not already,2,2,1.0
not already in,2,2,1.0
already in the,2,2,1.0
in the training,17,5,3.4
training data in,2,2,1.0
data in data,2,2,1.0
data augmentation the,4,2,2.0
augmentation the test,2,2,1.0
the test instances,6,2,3.0
test instances used,2,2,1.0
instances used to,4,2,2.0
used to generate,7,4,1.75
generate synthetic have,2,2,1.0
synthetic have to,2,2,1.0
have to be,2,2,1.0
to be in,4,3,1.3333333333333333
be in the,3,3,1.0
the training dataset,11,3,3.6666666666666665
training dataset specifically,2,2,1.0
dataset specifically they,2,2,1.0
specifically they are,2,2,1.0
they are all,2,2,1.0
are all the,2,2,1.0
all the majority,2,2,1.0
majority class instances,3,3,1.0
class instances in,2,2,1.0
training data that,3,3,1.0
data that do,2,2,1.0
do not take,2,2,1.0
not take part,2,2,1.0
take part in,2,2,1.0
part in native,2,2,1.0
native counterfactuals this,1,1,1.0
counterfactuals this is,1,1,1.0
this is why,3,3,1.0
is why the,3,3,1.0
why the test,2,2,1.0
test instances are,4,2,2.0
instances are called,2,2,1.0
are called unpaired,2,2,1.0
called unpaired instances,2,2,1.0
unpaired instances for,2,2,1.0
instances for data,2,2,1.0
data augmentation purposes,2,2,1.0
augmentation purposes the,2,2,1.0
purposes the test,2,2,1.0
instances are a,2,2,1.0
are a residual,2,2,1.0
a residual set,2,2,1.0
residual set of,2,2,1.0
set of majority,2,2,1.0
majority instances left,2,2,1.0
instances left after,2,2,1.0
left after the,2,2,1.0
after the native,2,2,1.0
the native have,2,2,1.0
native have been,2,2,1.0
have been identified,2,2,1.0
been identified how,2,2,1.0
identified how counterfactuals,2,2,1.0
how counterfactuals differ,2,2,1.0
counterfactuals differ from,2,2,1.0
differ from smote,2,2,1.0
from smote variants,2,2,1.0
smote variants it,2,2,1.0
variants it should,2,2,1.0
should be apparent,2,2,1.0
be apparent that,2,2,1.0
apparent that this,2,2,1.0
counterfactual method is,3,2,1.5
method is quite,3,2,1.5
different from smote,2,2,1.0
from smote and,2,2,1.0
smote and its,2,2,1.0
and its variants,2,2,1.0
its variants though,2,2,1.0
variants though it,2,2,1.0
though it is,2,2,1.0
it is consistent,2,2,1.0
is consistent with,2,2,1.0
consistent with many,2,2,1.0
with many insights,2,2,1.0
many insights from,2,2,1.0
insights from the,2,2,1.0
from the literature,1,1,1.0
the literature first,1,1,1.0
literature first by,2,2,1.0
first by definition,2,2,1.0
by definition the,2,2,1.0
definition the counterfactual,2,2,1.0
counterfactual method addresses,2,2,1.0
method addresses regions,2,2,1.0
addresses regions close,2,2,1.0
decision boundary a,2,2,1.0
boundary a good,2,2,1.0
good counterfactual records,2,2,1.0
counterfactual records the,2,2,1.0
records the minimal,2,2,1.0
the minimal that,2,2,1.0
minimal that result,2,2,1.0
that result in,2,2,1.0
result in a,2,2,1.0
in a class,2,2,1.0
a class change,2,2,1.0
class change as,2,2,1.0
change as in,2,2,1.0
as in and,2,2,1.0
in and second,2,2,1.0
and second this,2,2,1.0
second this method,2,2,1.0
this method relies,2,2,1.0
method relies on,2,2,1.0
relies on native,2,2,1.0
on native in,1,1,1.0
the dataset pairings,2,2,1.0
dataset pairings between,2,2,1.0
pairings between existing,2,2,1.0
between existing majority,2,2,1.0
existing majority and,2,2,1.0
and minority instances,5,2,2.5
minority instances and,1,1,1.0
instances and as,1,1,1.0
as such is,2,2,1.0
such is exploiting,2,2,1.0
is exploiting relationships,2,2,1.0
exploiting relationships between,2,2,1.0
relationships between both,2,2,1.0
between both classes,2,2,1.0
both classes as,2,2,1.0
classes as in,2,2,1.0
as in adasyn,3,3,1.0
in adasyn third,2,2,1.0
adasyn third we,2,2,1.0
third we are,2,2,1.0
we are highly,2,2,1.0
are highly selective,2,2,1.0
highly selective in,2,2,1.0
selective in the,2,2,1.0
the minority instances,3,3,1.0
minority instances used,2,2,1.0
synthetic instances as,2,2,1.0
instances as in,2,2,1.0
as in the,2,2,1.0
in the many,2,2,1.0
the many smote,2,2,1.0
many smote variants,2,2,1.0
variants that is,2,2,1.0
that is we,3,3,1.0
is we only,2,2,1.0
we only work,2,2,1.0
only work of,2,2,1.0
work of those,2,2,1.0
of those involved,1,1,1.0
those involved in,1,1,1.0
involved in known,1,1,1.0
in known counterfactuals,2,2,1.0
known counterfactuals with,2,2,1.0
counterfactuals with two,2,2,1.0
with two feature,2,2,1.0
two feature differences,2,2,1.0
feature differences however,2,2,1.0
differences however this,2,2,1.0
however this counterfactual,2,2,1.0
is quite keane,2,2,1.0
quite keane counterfactual,2,2,1.0
data augmentation ferent,2,2,1.0
augmentation ferent in,2,2,1.0
ferent in many,2,2,1.0
in many other,2,2,1.0
many other significant,2,2,1.0
other significant respects,2,2,1.0
significant respects first,2,2,1.0
respects first it,2,2,1.0
first it does,2,2,1.0
it does not,7,5,1.4
does not use,2,2,1.0
not use interpolation,2,2,1.0
use interpolation between,1,1,1.0
interpolation between instances,1,1,1.0
between instances but,1,1,1.0
instances but rather,2,2,1.0
but rather uses,2,2,1.0
rather uses the,2,2,1.0
uses the as,2,2,1.0
the as a,2,2,1.0
as a template,2,2,1.0
a template for,2,2,1.0
template for generating,2,2,1.0
for generating new,2,2,1.0
generating new minority,2,2,1.0
new minority instances,2,2,1.0
minority instances ii,2,2,1.0
instances ii it,2,2,1.0
ii it does,2,2,1.0
does not rely,4,2,2.0
not rely on,4,2,2.0
rely on the,2,2,1.0
on the topology,2,2,1.0
majority class as,4,4,1.0
class as in,2,2,1.0
as in swim,2,2,1.0
in swim but,2,2,1.0
swim but acts,2,2,1.0
but acts in,2,2,1.0
acts in a,2,2,1.0
in a very,2,2,1.0
a very local,2,2,1.0
very local way,2,2,1.0
local way using,2,2,1.0
way using the,2,2,1.0
using the counterfactual,2,2,1.0
the counterfactual relation,2,2,1.0
counterfactual relation between,2,2,1.0
relation between a,2,2,1.0
between a single,2,2,1.0
a single majority,2,2,1.0
single majority instance,2,2,1.0
majority instance and,2,2,1.0
instance and a,3,3,1.0
and a minority,2,2,1.0
a minority one,2,2,1.0
minority one iii,2,2,1.0
one iii does,2,2,1.0
iii does not,2,2,1.0
rely on any,2,2,1.0
on any clustering,2,2,1.0
any clustering analysis,2,2,1.0
clustering analysis of,2,2,1.0
minority classes as,2,2,1.0
classes as such,2,2,1.0
as such it,2,2,1.0
such it represents,2,2,1.0
it represents quite,2,2,1.0
represents quite a,2,2,1.0
quite a novel,2,2,1.0
a novel departure,2,2,1.0
novel departure relative,2,2,1.0
departure relative to,2,2,1.0
relative to existing,2,2,1.0
to existing smote,2,2,1.0
existing smote variants,2,2,1.0
smote variants competitive,2,2,1.0
variants competitive tests,2,2,1.0
competitive tests of,2,2,1.0
tests of data,2,2,1.0
data augmentation methods,6,2,3.0
augmentation methods in,2,2,1.0
methods in the,2,2,1.0
in the current,3,3,1.0
the current study,2,2,1.0
current study we,2,2,1.0
study we competitively,2,2,1.0
we competitively test,2,2,1.0
competitively test the,2,2,1.0
test the counterfactual,2,2,1.0
counterfactual method cfa,4,2,2.0
method cfa against,2,2,1.0
cfa against the,2,2,1.0
against the benchmark,2,2,1.0
the benchmark techniques,2,2,1.0
benchmark techniques in,2,2,1.0
techniques in the,2,2,1.0
in the literature,9,3,3.0
the literature using,2,2,1.0
literature using six,2,2,1.0
using six oversampling,1,1,1.0
six oversampling methods,1,1,1.0
oversampling methods smote,5,2,2.5
methods smote adasyn,5,2,2.5
smote adasyn these,1,1,1.0
adasyn these specific,1,1,1.0
these specific methods,2,2,1.0
specific methods were,2,2,1.0
methods were chosen,2,2,1.0
were chosen based,2,2,1.0
chosen based of,2,2,1.0
based of their,2,2,1.0
of their conceptual,2,2,1.0
their conceptual closeness,2,2,1.0
conceptual closeness to,2,2,1.0
closeness to the,2,2,1.0
to the cfa,2,2,1.0
cfa method their,2,2,1.0
method their popularity,2,2,1.0
their popularity amongst,2,2,1.0
popularity amongst smote,2,2,1.0
amongst smote variants,2,2,1.0
smote variants and,2,2,1.0
variants and their,2,2,1.0
and their as,2,2,1.0
their as the,1,1,1.0
as the six,1,1,1.0
the six techniques,2,2,1.0
six techniques were,2,2,1.0
techniques were tested,2,2,1.0
were tested on,4,2,2.0
tested on a,2,2,1.0
on a representative,2,2,1.0
a representative selection,1,1,1.0
representative selection of,1,1,1.0
selection of keel,1,1,1.0
of keel datasets,2,2,1.0
keel datasets from,2,2,1.0
datasets from which,2,2,1.0
from which were,2,2,1.0
which were produced,2,2,1.0
were produced with,2,2,1.0
produced with four,2,2,1.0
with four different,2,2,1.0
four different ml,2,2,1.0
different ml classifiers,2,2,1.0
ml classifiers including,2,2,1.0
classifiers including random,2,2,1.0
including random forest,2,2,1.0
random forest rf,4,2,2.0
forest rf neighbor,2,2,1.0
rf neighbor logistic,2,2,1.0
neighbor logistic regression,2,2,1.0
logistic regression lr,4,2,2.0
regression lr and,2,2,1.0
lr and multilayer,2,2,1.0
and multilayer perceptron,2,2,1.0
multilayer perceptron mlp,5,3,1.6666666666666667
perceptron mlp models,2,2,1.0
mlp models several,2,2,1.0
models several alternative,2,2,1.0
several alternative ml,2,2,1.0
alternative ml models,2,2,1.0
ml models were,2,2,1.0
models were used,2,2,1.0
were used because,2,2,1.0
used because models,1,1,1.0
because models find,1,1,1.0
models find different,2,2,1.0
find different decision,2,2,1.0
different decision boundaries,2,2,1.0
decision boundaries for,3,3,1.0
boundaries for a,2,2,1.0
for a given,11,3,3.6666666666666665
a given dataset,7,4,1.75
given dataset differences,2,2,1.0
dataset differences that,2,2,1.0
differences that could,2,2,1.0
that could impact,2,2,1.0
could impact the,2,2,1.0
impact the success,2,2,1.0
the success of,4,2,2.0
success of the,4,2,2.0
of the counterfactual,2,2,1.0
counterfactual method as,2,2,1.0
method as it,2,2,1.0
as it relies,2,2,1.0
it relies heavily,2,2,1.0
relies heavily on,3,3,1.0
heavily on a,2,2,1.0
on a model,2,2,1.0
a model s,2,2,1.0
model s decision,2,2,1.0
s decision boundary,2,2,1.0
decision boundary the,2,2,1.0
boundary the for,2,2,1.0
the for a,2,2,1.0
a given classifier,4,2,2.0
given classifier recorded,2,2,1.0
classifier recorded the,2,2,1.0
recorded the performance,2,2,1.0
performance of the,16,6,2.6666666666666665
of the model,3,3,1.0
the model on,2,2,1.0
model on a,2,2,1.0
on a given,2,2,1.0
given dataset without,2,2,1.0
dataset without any,2,2,1.0
without any data,2,2,1.0
any data augmentation,4,2,2.0
data augmentation applied,2,2,1.0
augmentation applied several,2,2,1.0
applied several standard,2,2,1.0
several standard measures,2,2,1.0
standard measures were,2,2,1.0
measures were used,2,2,1.0
were used to,4,2,2.0
used to assess,4,4,1.0
to assess the,4,3,1.3333333333333333
assess the of,1,1,1.0
of the four,2,2,1.0
the four methods,4,2,2.0
four methods namely,2,2,1.0
methods namely precision,2,2,1.0
namely precision recall,2,2,1.0
precision recall and,4,3,1.3333333333333333
recall and plots,2,2,1.0
and plots of,2,2,1.0
plots of roc,2,2,1.0
of roc curves,9,3,3.0
roc curves temraz,2,2,1.0
curves temraz keane,2,2,1.0
augmentation method datasets,2,2,1.0
method datasets setup,2,2,1.0
datasets setup table,2,2,1.0
setup table shows,2,2,1.0
table shows the,3,3,1.0
shows the main,2,2,1.0
the main characteristics,2,2,1.0
main characteristics of,2,2,1.0
characteristics of the,3,3,1.0
of the datasets,9,4,2.25
the datasets drawn,2,2,1.0
datasets drawn from,2,2,1.0
drawn from both,2,2,1.0
from both uci,2,2,1.0
both uci and,2,2,1.0
uci and keel,2,2,1.0
and keel repositories,2,2,1.0
keel repositories as,2,2,1.0
repositories as the,2,2,1.0
as the focus,2,2,1.0
the focus is,2,2,1.0
focus is on,2,2,1.0
is on binary,2,2,1.0
on binary classification,3,3,1.0
binary classification problems,4,2,2.0
classification problems and,3,3,1.0
problems and some,2,2,1.0
and some of,2,2,1.0
some of these,3,3,1.0
of these datasets,3,3,1.0
these datasets are,1,1,1.0
datasets are they,1,1,1.0
are they were,2,2,1.0
they were converted,2,2,1.0
were converted to,4,2,2.0
converted to binary,4,2,2.0
to binary classes,2,2,1.0
binary classes the,2,2,1.0
classes the ovo,2,2,1.0
the ovo or,2,2,1.0
ovo or and,2,2,1.0
or and ovr,1,1,1.0
and ovr or,1,1,1.0
ovr or methods,2,2,1.0
or methods were,2,2,1.0
methods were used,2,2,1.0
used to do,2,2,1.0
to do this,2,2,1.0
do this conversion,2,2,1.0
this conversion the,2,2,1.0
conversion the ovo,2,2,1.0
the ovo method,2,2,1.0
ovo method splits,2,2,1.0
method splits a,2,2,1.0
splits a classification,2,2,1.0
a classification into,2,2,1.0
classification into one,2,2,1.0
into one binary,2,2,1.0
one binary classification,2,2,1.0
binary classification dataset,2,2,1.0
classification dataset for,2,2,1.0
dataset for each,6,2,3.0
for each pair,2,2,1.0
each pair of,2,2,1.0
pair of classes,3,3,1.0
of classes whereas,2,2,1.0
classes whereas the,2,2,1.0
whereas the ovr,2,2,1.0
the ovr approach,2,2,1.0
ovr approach selects,2,2,1.0
approach selects one,2,2,1.0
selects one of,2,2,1.0
of the multiple,2,2,1.0
the multiple classes,2,2,1.0
multiple classes and,2,2,1.0
classes and predicts,2,2,1.0
and predicts it,2,2,1.0
predicts it against,1,1,1.0
it against all,1,1,1.0
against all other,1,1,1.0
all other classes,9,3,3.0
other classes so,2,2,1.0
classes so that,2,2,1.0
so that one,2,2,1.0
that one of,2,2,1.0
of the classes,5,5,1.0
the classes is,2,2,1.0
classes is treated,2,2,1.0
is treated as,4,2,2.0
treated as the,10,2,5.0
as the positive,3,3,1.0
the positive minority,2,2,1.0
positive minority class,4,4,1.0
minority class and,10,4,2.5
class and all,6,2,3.0
and all other,6,2,3.0
other classes are,6,2,3.0
classes are treated,6,2,3.0
are treated as,6,2,3.0
as the negative,7,3,2.3333333333333335
the negative class,7,3,2.3333333333333335
negative class majority,2,2,1.0
class majority in,2,2,1.0
majority in this,2,2,1.0
paper the datasets,2,2,1.0
the datasets were,4,2,2.0
datasets were modified,2,2,1.0
were modified using,1,1,1.0
modified using both,1,1,1.0
using both methods,1,1,1.0
both methods one,2,2,1.0
methods one method,2,2,1.0
one method per,2,2,1.0
method per dataset,2,2,1.0
per dataset to,2,2,1.0
dataset to vary,2,2,1.0
to vary the,2,2,1.0
vary the class,2,2,1.0
the class ratio,4,2,2.0
class ratio of,3,3,1.0
ratio of class,2,2,1.0
of class imbalance,7,4,1.75
class imbalance among,2,2,1.0
imbalance among the,2,2,1.0
among the datasets,2,2,1.0
the datasets see,4,2,2.0
datasets see table,2,2,1.0
see table the,4,2,2.0
table the base,2,2,1.0
the base datasets,2,2,1.0
base datasets were,2,2,1.0
datasets were abalone,2,2,1.0
were abalone dataset,2,2,1.0
abalone dataset a,2,2,1.0
dataset a dataset,14,2,7.0
a dataset analyzed,2,2,1.0
dataset analyzed to,2,2,1.0
analyzed to find,2,2,1.0
find the age,2,2,1.0
the age of,2,2,1.0
age of abalone,2,2,1.0
of abalone from,2,2,1.0
abalone from physical,2,2,1.0
from physical measurements,2,2,1.0
physical measurements consisting,2,2,1.0
measurements consisting of,2,2,1.0
consisting of classes,6,2,3.0
of classes that,6,2,3.0
classes that was,2,2,1.0
that was modified,2,2,1.0
was modified using,4,2,2.0
modified using and,4,2,2.0
using and glass,2,2,1.0
and glass dataset,2,2,1.0
glass dataset a,2,2,1.0
a dataset used,10,2,5.0
dataset used to,10,2,5.0
used to classify,4,2,2.0
to classify based,2,2,1.0
classify based on,2,2,1.0
on the chemical,2,2,1.0
the chemical consisting,1,1,1.0
chemical consisting of,1,1,1.0
classes that modified,4,2,2.0
that modified using,4,2,2.0
modified using to,2,2,1.0
using to treat,2,2,1.0
to treat the,2,2,1.0
treat the class,2,2,1.0
class as the,4,3,1.3333333333333333
as the minority,5,3,1.6666666666666667
negative class yeast,2,2,1.0
class yeast dataset,2,2,1.0
yeast dataset a,2,2,1.0
to predict the,3,3,1.0
predict the cellular,2,2,1.0
the cellular localization,2,2,1.0
cellular localization sites,2,2,1.0
localization sites of,2,2,1.0
sites of proteins,2,2,1.0
of proteins consisting,2,2,1.0
proteins consisting of,2,2,1.0
using and pima,2,2,1.0
and pima indians,2,2,1.0
pima indians diabetes,2,2,1.0
indians diabetes dataset,2,2,1.0
diabetes dataset a,2,2,1.0
predict whether or,2,2,1.0
whether or not,2,2,1.0
or not a,2,2,1.0
not a patient,2,2,1.0
a patient has,2,2,1.0
patient has diabetes,2,2,1.0
has diabetes based,2,2,1.0
diabetes based on,2,2,1.0
based on certain,2,2,1.0
on certain diagnostic,2,2,1.0
certain diagnostic measurements,2,2,1.0
diagnostic measurements included,2,2,1.0
measurements included in,2,2,1.0
included in the,2,2,1.0
the dataset phoneme,2,2,1.0
dataset phoneme dataset,2,2,1.0
phoneme dataset a,2,2,1.0
used to distinguish,2,2,1.0
to distinguish between,2,2,1.0
distinguish between nasal,2,2,1.0
between nasal and,2,2,1.0
nasal and oral,2,2,1.0
and oral sounds,2,2,1.0
oral sounds temraz,2,2,1.0
sounds temraz keane,2,2,1.0
data augmentation vehicle,2,2,1.0
augmentation vehicle dataset,2,2,1.0
vehicle dataset the,2,2,1.0
dataset the vehicle,2,2,1.0
the vehicle data,2,2,1.0
vehicle data set,2,2,1.0
data set is,4,4,1.0
set is a,2,2,1.0
is a dataset,4,2,2.0
a dataset with,7,3,2.3333333333333335
dataset with classes,5,2,2.5
with classes the,3,2,1.5
classes the problem,3,2,1.5
the problem is,9,3,3.0
problem is to,7,3,2.3333333333333335
is to classify,6,2,3.0
to classify a,2,2,1.0
classify a given,2,2,1.0
a given silhouette,2,2,1.0
given silhouette as,2,2,1.0
silhouette as one,2,2,1.0
as one of,2,2,1.0
one of four,2,2,1.0
of four types,2,2,1.0
four types of,2,2,1.0
types of vehicle,2,2,1.0
of vehicle using,2,2,1.0
vehicle using a,2,2,1.0
using a set,2,2,1.0
set of features,2,2,1.0
of features from,1,1,1.0
from the silhouette,2,2,1.0
the silhouette again,2,2,1.0
silhouette again this,2,2,1.0
again this dataset,2,2,1.0
this dataset was,2,2,1.0
dataset was modified,2,2,1.0
modified using so,2,2,1.0
using so that,2,2,1.0
so that the,5,4,1.25
that the class,5,4,1.25
the class van,2,2,1.0
class van is,2,2,1.0
van is treated,2,2,1.0
negative class ecoli,2,2,1.0
class ecoli dataset,2,2,1.0
ecoli dataset this,2,2,1.0
dataset this dataset,2,2,1.0
this dataset is,2,2,1.0
dataset is a,2,2,1.0
to classify ecoli,2,2,1.0
classify ecoli proteins,2,2,1.0
ecoli proteins using,2,2,1.0
proteins using their,2,2,1.0
using their amino,2,2,1.0
their amino acid,2,2,1.0
amino acid sequences,2,2,1.0
acid sequences in,2,2,1.0
sequences in their,2,2,1.0
in their cell,2,2,1.0
their cell localization,2,2,1.0
cell localization sites,2,2,1.0
localization sites dataset,2,2,1.0
sites dataset a,2,2,1.0
to classify all,2,2,1.0
classify all the,3,3,1.0
all the blocks,2,2,1.0
the blocks of,2,2,1.0
blocks of the,2,2,1.0
of the page,2,2,1.0
the page layout,2,2,1.0
page layout of,2,2,1.0
layout of a,2,2,1.0
of a document,2,2,1.0
a document that,2,2,1.0
document that has,2,2,1.0
that has been,2,2,1.0
has been detected,2,2,1.0
been detected by,2,2,1.0
detected by a,2,2,1.0
by a segmentation,2,2,1.0
a segmentation process,2,2,1.0
segmentation process wine,2,2,1.0
process wine quality,2,2,1.0
wine quality dataset,2,2,1.0
quality dataset red,2,2,1.0
dataset red and,2,2,1.0
red and white,4,2,2.0
and white two,2,2,1.0
white two datasets,2,2,1.0
two datasets related,2,2,1.0
datasets related to,2,2,1.0
related to red,2,2,1.0
to red and,2,2,1.0
and white vinho,2,2,1.0
white vinho verde,2,2,1.0
vinho verde wine,2,2,1.0
verde wine samples,2,2,1.0
wine samples the,2,2,1.0
samples the problem,2,2,1.0
to classify wine,2,2,1.0
classify wine quality,2,2,1.0
wine quality based,2,2,1.0
quality based on,2,2,1.0
based on physicochemical,2,2,1.0
on physicochemical tests,2,2,1.0
physicochemical tests poker,2,2,1.0
tests poker dataset,2,2,1.0
poker dataset a,2,2,1.0
with classes used,2,2,1.0
classes used to,2,2,1.0
to predict poker,2,2,1.0
predict poker hands,2,2,1.0
poker hands the,2,2,1.0
hands the overall,2,2,1.0
the overall performance,4,3,1.3333333333333333
overall performance of,2,2,1.0
performance of each,3,3,1.0
of each classifier,2,2,1.0
each classifier was,2,2,1.0
classifier was tested,2,2,1.0
was tested using,2,2,1.0
tested using cross,2,2,1.0
using cross validation,2,2,1.0
cross validation with,2,2,1.0
validation with k,2,2,1.0
with k so,2,2,1.0
k so each,2,2,1.0
so each dataset,2,2,1.0
each dataset is,3,3,1.0
dataset is randomly,2,2,1.0
is randomly partitioned,2,2,1.0
randomly partitioned into,2,2,1.0
partitioned into disjoint,2,2,1.0
into disjoint subsets,2,2,1.0
disjoint subsets where,2,2,1.0
subsets where each,2,2,1.0
where each subset,1,1,1.0
each subset included,1,1,1.0
subset included equal,1,1,1.0
included equal size,1,1,1.0
equal size of,2,2,1.0
size of data,2,2,1.0
of data then,2,2,1.0
data then a,2,2,1.0
then a single,2,2,1.0
a single subset,2,2,1.0
single subset was,2,2,1.0
subset was retained,2,2,1.0
was retained as,2,2,1.0
retained as a,2,2,1.0
as a test,2,2,1.0
a test set,2,2,1.0
test set with,2,2,1.0
set with the,2,2,1.0
with the remaining,2,2,1.0
the remaining subsets,2,2,1.0
remaining subsets being,2,2,1.0
subsets being used,2,2,1.0
being used as,2,2,1.0
used as the,5,3,1.6666666666666667
as the training,2,2,1.0
training data different,2,2,1.0
data different datasets,2,2,1.0
different datasets have,2,2,1.0
datasets have minority,2,2,1.0
have minority classes,2,2,1.0
minority classes see,2,2,1.0
classes see table,2,2,1.0
see table for,4,2,2.0
table for details,2,2,1.0
for details for,2,2,1.0
details for each,2,2,1.0
of the smote,2,2,1.0
the smote methods,2,2,1.0
smote methods we,2,2,1.0
methods we split,2,2,1.0
we split each,2,2,1.0
split each dataset,1,1,1.0
each dataset into,1,1,1.0
dataset into training,1,1,1.0
into training and,2,2,1.0
training and validation,2,2,1.0
and validation folds,2,2,1.0
validation folds then,2,2,1.0
folds then on,2,2,1.0
then on each,2,2,1.0
on each fold,2,2,1.0
each fold we,2,2,1.0
fold we oversample,2,2,1.0
we oversample the,4,2,2.0
minority class train,2,2,1.0
class train the,2,2,1.0
train the classifier,2,2,1.0
the classifier on,4,2,2.0
classifier on the,8,2,4.0
on the training,4,4,1.0
the training folds,2,2,1.0
training folds and,2,2,1.0
folds and finally,2,2,1.0
and finally validate,2,2,1.0
finally validate the,2,2,1.0
validate the classifier,2,2,1.0
on the remaining,2,2,1.0
the remaining fold,2,2,1.0
remaining fold for,2,2,1.0
fold for cfa,2,2,1.0
for cfa the,2,2,1.0
cfa the native,2,2,1.0
the native in,1,1,1.0
training data were,2,2,1.0
data were computed,2,2,1.0
were computed and,2,2,1.0
computed and then,3,3,1.0
and then all,2,2,1.0
then all the,2,2,1.0
all the remaining,3,3,1.0
the remaining unpaired,2,2,1.0
remaining unpaired were,2,2,1.0
unpaired were run,2,2,1.0
were run through,2,2,1.0
run through cfa,2,2,1.0
through cfa to,2,2,1.0
cfa to create,2,2,1.0
create the synthetic,2,2,1.0
class this augmented,2,2,1.0
this augmented dataset,2,2,1.0
augmented dataset was,2,2,1.0
dataset was then,2,2,1.0
was then used,2,2,1.0
then used for,2,2,1.0
used for testing,2,2,1.0
for testing these,2,2,1.0
testing these generated,2,2,1.0
these generated datasets,1,1,1.0
generated datasets from,1,1,1.0
datasets from cfa,1,1,1.0
from cfa were,2,2,1.0
cfa were compared,2,2,1.0
were compared with,2,2,1.0
compared with the,2,2,1.0
with the original,2,2,1.0
original dataset without,2,2,1.0
dataset without data,2,2,1.0
without data augmentation,2,2,1.0
data augmentation and,2,2,1.0
augmentation and the,2,2,1.0
and the datasets,1,1,1.0
the datasets generated,1,1,1.0
datasets generated by,1,1,1.0
generated by temraz,2,2,1.0
by temraz keane,2,2,1.0
augmentation smote adasyn,6,2,3.0
smote adasyn and,9,2,4.5
adasyn and for,2,2,1.0
and for our,2,2,1.0
for our we,1,1,1.0
our we oversample,1,1,1.0
class using each,2,2,1.0
using each of,2,2,1.0
of the data,12,6,2.0
augmentation methods until,1,1,1.0
methods until we,2,2,1.0
until we have,2,2,1.0
we have the,2,2,1.0
have the same,3,3,1.0
the same number,3,3,1.0
same number of,3,3,1.0
number of instances,2,2,1.0
of instances in,2,2,1.0
instances in each,2,2,1.0
in each class,2,2,1.0
each class as,2,2,1.0
class as a,2,2,1.0
as a result,4,3,1.3333333333333333
a result fully,2,2,1.0
result fully balanced,2,2,1.0
fully balanced datasets,2,2,1.0
balanced datasets were,2,2,1.0
datasets were created,2,2,1.0
were created for,2,2,1.0
created for each,4,3,1.3333333333333333
for each classifier,4,2,2.0
each classifier based,2,2,1.0
classifier based on,2,2,1.0
the original imbalanced,3,3,1.0
original imbalanced dataset,2,2,1.0
imbalanced dataset was,2,2,1.0
dataset was also,1,1,1.0
was also run,1,1,1.0
also run as,1,1,1.0
run as a,2,2,1.0
as a baseline,4,3,1.3333333333333333
a baseline without,2,2,1.0
baseline without using,2,2,1.0
without using any,2,2,1.0
using any data,2,2,1.0
augmentation method finally,2,2,1.0
method finally to,2,2,1.0
finally to determine,2,2,1.0
to determine the,4,2,2.0
determine the optimal,2,2,1.0
the optimal values,2,2,1.0
optimal values for,2,2,1.0
values for all,2,2,1.0
for all classifiers,2,2,1.0
all classifiers we,2,2,1.0
classifiers we applied,2,2,1.0
we applied hyperparameters,2,2,1.0
applied hyperparameters tuning,2,2,1.0
hyperparameters tuning using,2,2,1.0
tuning using gridsearchcv,2,2,1.0
using gridsearchcv function,2,2,1.0
gridsearchcv function from,2,2,1.0
function from gridsearchcv,2,2,1.0
from gridsearchcv performs,2,2,1.0
gridsearchcv performs across,2,2,1.0
performs across all,2,2,1.0
across all hyperparameter,2,2,1.0
all hyperparameter combinations,2,2,1.0
hyperparameter combinations and,2,2,1.0
combinations and finds,2,2,1.0
and finds the,2,2,1.0
finds the best,2,2,1.0
the best score,2,2,1.0
best score for,2,2,1.0
score for a,2,2,1.0
given classifier to,2,2,1.0
classifier to achieve,2,2,1.0
to achieve this,2,2,1.0
achieve this we,2,2,1.0
this we defined,2,2,1.0
we defined our,2,2,1.0
defined our grid,2,2,1.0
our grid of,2,2,1.0
grid of parameters,2,2,1.0
of parameters for,2,2,1.0
parameters for each,2,2,1.0
for each method,7,2,3.5
each method rf,1,1,1.0
method rf lr,2,2,1.0
rf lr mlp,10,2,5.0
lr mlp and,2,2,1.0
mlp and oversampling,2,2,1.0
and oversampling methods,4,2,2.0
methods smote its,4,2,2.0
smote its variants,4,2,2.0
its variants and,2,2,1.0
variants and then,2,2,1.0
and then ran,2,2,1.0
then ran the,2,2,1.0
ran the grid,2,2,1.0
the grid search,2,2,1.0
grid search see,2,2,1.0
search see table,2,2,1.0
table for a,2,2,1.0
for a full,4,2,2.0
a full description,4,2,2.0
full description table,2,2,1.0
description table a,2,2,1.0
table a grid,2,2,1.0
a grid of,2,2,1.0
grid of hyperparameter,2,2,1.0
of hyperparameter values,2,2,1.0
hyperparameter values for,2,2,1.0
values for each,2,2,1.0
each classifier and,2,2,1.0
classifier and oversampling,2,2,1.0
oversampling methods learner,2,2,1.0
methods learner parameter,2,2,1.0
learner parameter variants,2,2,1.0
parameter variants random,2,2,1.0
variants random forest,2,2,1.0
forest rf 𝑚𝑎𝑥r,1,1,1.0
rf 𝑚𝑎𝑥r neighbors,1,1,1.0
𝑚𝑎𝑥r neighbors 𝑛,1,1,1.0
neighbors 𝑛 xuyz,2,2,1.0
𝑛 xuyz logistic,2,2,1.0
xuyz logistic regression,2,2,1.0
regression lr 𝑚𝑎𝑥,2,2,1.0
lr 𝑚𝑎𝑥 t,2,2,1.0
𝑚𝑎𝑥 t multilayer,1,1,1.0
t multilayer perceptron,1,1,1.0
perceptron mlp oversampling,1,1,1.0
mlp oversampling methods,1,1,1.0
its variants 𝑘,2,2,1.0
variants 𝑘 f,2,2,1.0
𝑘 f xuyz,2,2,1.0
f xuyz temraz,2,2,1.0
xuyz temraz keane,2,2,1.0
data augmentation table,8,2,4.0
augmentation table datasets,2,2,1.0
table datasets dataset,2,2,1.0
datasets dataset variants,2,2,1.0
dataset variants used,2,2,1.0
variants used in,2,2,1.0
in the experiment,2,2,1.0
the experiment imbalance,2,2,1.0
experiment imbalance ratio,2,2,1.0
imbalance ratio id,2,2,1.0
ratio id dataset,2,2,1.0
id dataset features,2,2,1.0
dataset features instances,2,2,1.0
features instances minority,2,2,1.0
instances minority majority,2,2,1.0
minority majority ir,2,2,1.0
majority ir pima,2,2,1.0
ir pima phoneme,2,2,1.0
pima phoneme vehicle,2,2,1.0
phoneme vehicle metrics,2,2,1.0
vehicle metrics measures,2,2,1.0
metrics measures in,2,2,1.0
measures in binary,2,2,1.0
classification problems the,2,2,1.0
problems the labels,2,2,1.0
the labels can,2,2,1.0
labels can be,2,2,1.0
can be either,2,2,1.0
be either positive,2,2,1.0
either positive or,2,2,1.0
positive or negative,1,1,1.0
or negative so,1,1,1.0
negative so the,1,1,1.0
so the prediction,2,2,1.0
the prediction made,2,2,1.0
prediction made by,2,2,1.0
made by the,2,2,1.0
by the classifier,2,2,1.0
the classifier is,2,2,1.0
classifier is represented,2,2,1.0
is represented as,2,2,1.0
represented as a,2,2,1.0
as a confusion,2,2,1.0
a confusion matrix,2,2,1.0
confusion matrix see,2,2,1.0
matrix see table,2,2,1.0
table the confusion,2,2,1.0
the confusion temraz,2,2,1.0
confusion temraz keane,2,2,1.0
data augmentation matrix,2,2,1.0
augmentation matrix summarizes,2,2,1.0
matrix summarizes the,2,2,1.0
summarizes the performance,3,3,1.0
performance of classifiers,4,3,1.3333333333333333
of classifiers for,2,2,1.0
classifiers for the,2,2,1.0
for the four,6,2,3.0
the four possible,2,2,1.0
four possible outcomes,2,2,1.0
possible outcomes of,2,2,1.0
outcomes of a,2,2,1.0
of a given,3,3,1.0
a given a,1,1,1.0
given a true,1,1,1.0
a true positive,2,2,1.0
true positive tp,7,3,2.3333333333333335
positive tp true,3,3,1.0
tp true negative,2,2,1.0
true negative tn,5,3,1.6666666666666667
negative tn false,2,2,1.0
tn false positive,2,2,1.0
false positive fp,6,2,3.0
positive fp and,2,2,1.0
fp and false,2,2,1.0
and false negative,2,2,1.0
false negative fn,4,2,2.0
negative fn was,1,1,1.0
fn was not,1,1,1.0
was not used,2,2,1.0
not used as,2,2,1.0
used as a,4,4,1.0
as a measure,2,2,1.0
a measure because,2,2,1.0
measure because as,2,2,1.0
because as discussed,2,2,1.0
as discussed earlier,2,2,1.0
discussed earlier it,2,2,1.0
earlier it can,1,1,1.0
it can be,24,4,6.0
can be spuriously,1,1,1.0
be spuriously high,2,2,1.0
spuriously high for,2,2,1.0
high for datasets,1,1,1.0
for datasets it,1,1,1.0
datasets it should,2,2,1.0
noted that all,3,3,1.0
that all datasets,2,2,1.0
all datasets used,2,2,1.0
datasets used in,4,3,1.3333333333333333
used in our,3,3,1.0
our experiments were,1,1,1.0
experiments were converted,1,1,1.0
to binary datasets,2,2,1.0
binary datasets using,2,2,1.0
datasets using two,2,2,1.0
using two of,2,2,1.0
two of the,2,2,1.0
the most strategies,2,2,1.0
most strategies and,2,2,1.0
strategies and see,2,2,1.0
and see section,2,2,1.0
see section for,2,2,1.0
section for a,2,2,1.0
full description hence,2,2,1.0
description hence the,2,2,1.0
hence the evaluation,2,2,1.0
evaluation metrics used,2,2,1.0
metrics used were,2,2,1.0
used were as,2,2,1.0
were as precision,2,2,1.0
as precision recall,2,2,1.0
precision recall auc,4,3,1.3333333333333333
recall auc and,2,2,1.0
auc and defined,2,2,1.0
and defined as,2,2,1.0
defined as follows,2,2,1.0
as follows 𝑇𝑃,1,1,1.0
follows 𝑇𝑃 𝑇𝑃,1,1,1.0
𝑇𝑃 𝑇𝑃 𝐴𝑈𝐶,1,1,1.0
𝑇𝑃 𝐴𝑈𝐶 table,1,1,1.0
𝐴𝑈𝐶 table confusion,1,1,1.0
table confusion matrix,4,4,1.0
confusion matrix for,4,4,1.0
matrix for classifications,2,2,1.0
for classifications receiver,2,2,1.0
classifications receiver operating,2,2,1.0
receiver operating characteristic,4,4,1.0
operating characteristic curves,2,2,1.0
characteristic curves roc,2,2,1.0
curves roc curve,2,2,1.0
roc curve were,2,2,1.0
curve were reported,2,2,1.0
were reported as,2,2,1.0
reported as they,4,2,2.0
as they are,8,2,4.0
they are often,2,2,1.0
are often used,2,2,1.0
often used to,2,2,1.0
used to evaluate,3,3,1.0
to evaluate classification,2,2,1.0
evaluate classification models,2,2,1.0
classification models for,2,2,1.0
models for imbalanced,2,2,1.0
for imbalanced data,16,5,3.2
imbalanced data sets,53,6,8.833333333333334
data sets the,4,3,1.3333333333333333
sets the roc,2,2,1.0
the roc curve,11,5,2.2
roc curve is,4,4,1.0
curve is a,3,3,1.0
is a graph,3,3,1.0
a graph in,3,3,1.0
graph in which,3,3,1.0
in which true,2,2,1.0
which true positive,2,2,1.0
positive tp rate,2,2,1.0
tp rate is,4,3,1.3333333333333333
rate is plotted,6,3,2.0
is plotted on,6,3,2.0
plotted on the,6,3,2.0
on the and,3,3,1.0
the and false,2,2,1.0
and false positive,2,2,1.0
positive fp rate,2,2,1.0
fp rate is,3,3,1.0
on the one,2,2,1.0
the one advantage,2,2,1.0
one advantage of,2,2,1.0
advantage of roc,2,2,1.0
roc curves is,2,2,1.0
curves is that,2,2,1.0
that they are,3,3,1.0
they are not,4,2,2.0
are not affected,2,2,1.0
not affected by,2,2,1.0
affected by the,3,3,1.0
by the class,2,2,1.0
class ratio between,2,2,1.0
ratio between minority,2,2,1.0
between minority and,3,3,1.0
and majority instances,2,2,1.0
in the datasets,2,2,1.0
datasets see figures,2,2,1.0
see figures for,2,2,1.0
figures for results,2,2,1.0
for results area,2,2,1.0
results area under,2,2,1.0
area under curve,3,3,1.0
under curve auc,3,3,1.0
curve auc predicted,2,2,1.0
auc predicted class,2,2,1.0
predicted class actual,2,2,1.0
class actual class,2,2,1.0
actual class p,2,2,1.0
class p n,2,2,1.0
p n p,2,2,1.0
n p true,2,2,1.0
p true positive,2,2,1.0
positive tp false,2,2,1.0
tp false negative,2,2,1.0
negative fn n,2,2,1.0
fn n false,2,2,1.0
n false positive,2,2,1.0
positive fp true,2,2,1.0
fp true negative,2,2,1.0
negative tn temraz,2,2,1.0
tn temraz keane,2,2,1.0
data augmentation scores,2,2,1.0
augmentation scores were,2,2,1.0
scores were also,2,2,1.0
were also reported,2,2,1.0
also reported as,2,2,1.0
they are used,2,2,1.0
are used to,3,3,1.0
used to measure,2,2,1.0
to measure the,3,3,1.0
measure the area,2,2,1.0
the area that,2,2,1.0
area that lies,2,2,1.0
that lies under,2,2,1.0
lies under the,2,2,1.0
under the roc,6,4,1.5
roc curve results,2,2,1.0
curve results discussion,2,2,1.0
results discussion overall,2,2,1.0
discussion overall the,2,2,1.0
overall the counterfactual,2,2,1.0
method cfa performs,2,2,1.0
cfa performs better,2,2,1.0
performs better than,3,3,1.0
better than all,8,3,2.6666666666666665
than all other,2,2,1.0
all other methods,2,2,1.0
other methods on,2,2,1.0
methods on the,2,2,1.0
on the main,2,2,1.0
the main metrics,2,2,1.0
main metrics reported,2,2,1.0
metrics reported for,2,2,1.0
reported for most,2,2,1.0
for most of,3,3,1.0
most of the,15,5,3.0
of the classifiers,3,3,1.0
the classifiers tested,1,1,1.0
classifiers tested see,1,1,1.0
tested see recall,1,1,1.0
see recall the,1,1,1.0
recall the datasets,2,2,1.0
datasets were tested,2,2,1.0
tested on four,2,2,1.0
on four classifiers,2,2,1.0
four classifiers rf,4,2,2.0
classifiers rf lr,4,2,2.0
lr mlp with,2,2,1.0
mlp with the,3,3,1.0
with the baseline,2,2,1.0
the baseline no,2,2,1.0
baseline no data,2,2,1.0
no data augmentation,2,2,1.0
adasyn and cfa,5,2,2.5
and cfa tables,2,2,1.0
cfa tables report,2,2,1.0
tables report the,2,2,1.0
report the main,2,2,1.0
the main metric,2,2,1.0
main metric for,2,2,1.0
metric for each,1,1,1.0
for each on,1,1,1.0
each on the,1,1,1.0
on the datasets,3,3,1.0
the datasets for,3,3,1.0
datasets for the,5,3,1.6666666666666667
for the rf,4,2,2.0
the rf classifier,6,2,3.0
rf classifier table,2,2,1.0
classifier table the,4,2,2.0
table the results,4,2,2.0
the results show,3,3,1.0
results show that,3,3,1.0
that cfa does,8,2,4.0
cfa does better,6,2,3.0
does better than,5,2,2.5
than all the,5,2,2.5
all the other,7,3,2.3333333333333335
the other methods,6,2,3.0
other methods in,7,3,2.3333333333333335
methods in datasets,2,2,1.0
in datasets out,2,2,1.0
datasets out of,2,2,1.0
out of with,2,2,1.0
of with being,1,1,1.0
with being the,2,1,2.0
being the next,7,2,3.5
the next best,7,2,3.5
next best in,2,2,1.0
best in only,2,2,1.0
in only datasets,4,2,2.0
only datasets both,2,2,1.0
datasets both adasyn,2,2,1.0
both adasyn and,2,2,1.0
adasyn and had,2,2,1.0
and had the,4,2,2.0
had the highest,5,2,2.5
the highest for,2,2,1.0
highest for one,2,2,1.0
for one dataset,2,2,1.0
one dataset for,2,2,1.0
for each for,2,2,1.0
each for the,2,2,1.0
the classifier table,2,2,1.0
classifier table it,2,2,1.0
table it is,2,2,1.0
it is observed,2,2,1.0
is observed that,2,2,1.0
observed that cfa,4,2,2.0
that cfa also,2,2,1.0
cfa also achieved,2,2,1.0
also achieved a,2,2,1.0
achieved a greater,2,2,1.0
a greater in,1,1,1.0
greater in the,1,1,1.0
in the results,3,3,1.0
the results also,3,3,1.0
results also show,2,2,1.0
show that for,4,2,2.0
that for cfa,2,2,1.0
for cfa does,2,2,1.0
methods in out,4,2,2.0
in out of,12,2,6.0
out of datasets,14,2,7.0
of datasets with,6,2,3.0
datasets with adasyn,2,2,1.0
with adasyn being,2,2,1.0
adasyn being the,2,2,1.0
next best with,5,2,2.5
best with datasets,6,2,3.0
with datasets smote,2,2,1.0
datasets smote and,2,2,1.0
smote and had,2,2,1.0
the highest in,2,1,2.0
highest in dataset,1,1,1.0
in dataset for,2,2,1.0
each method for,2,2,1.0
method for the,2,2,1.0
for the lr,4,2,2.0
the lr classifier,4,2,2.0
lr classifier table,2,2,1.0
the results are,6,4,1.5
results are quite,2,2,1.0
are quite different,2,2,1.0
quite different in,2,2,1.0
different in that,2,2,1.0
in that they,2,2,1.0
that they show,2,2,1.0
they show that,2,2,1.0
that for metric,2,2,1.0
for metric cfa,2,2,1.0
metric cfa doing,2,2,1.0
cfa doing better,2,2,1.0
doing better in,6,2,3.0
better in only,4,2,2.0
in only out,2,2,1.0
only out of,2,2,1.0
datasets with the,3,3,1.0
with the being,1,1,1.0
the being the,1,1,1.0
with datasets whereas,2,2,1.0
datasets whereas baseline,2,2,1.0
whereas baseline adasyn,2,2,1.0
baseline adasyn and,2,2,1.0
adasyn and doing,2,2,1.0
and doing better,2,2,1.0
better in datasets,2,2,1.0
in datasets finally,2,2,1.0
datasets finally for,2,2,1.0
finally for the,2,2,1.0
for the mlp,4,2,2.0
the mlp classifier,6,2,3.0
mlp classifier table,2,2,1.0
classifier table again,2,2,1.0
table again the,2,2,1.0
again the results,2,2,1.0
the results showed,2,2,1.0
results showed that,2,2,1.0
showed that cfa,2,2,1.0
of datasets whereas,2,2,1.0
datasets whereas doing,2,2,1.0
whereas doing better,2,2,1.0
only datasets with,2,2,1.0
datasets with being,1,1,1.0
with datasets baseline,2,2,1.0
datasets baseline and,2,2,1.0
baseline and adasyn,2,2,1.0
and adasyn had,2,2,1.0
adasyn had the,2,2,1.0
highest in temraz,1,1,1.0
in temraz keane,2,2,1.0
augmentation datasets for,2,2,1.0
datasets for each,4,2,2.0
each method notably,2,2,1.0
method notably these,2,2,1.0
notably these results,2,2,1.0
these results show,2,2,1.0
results show for,2,2,1.0
show for certain,2,2,1.0
for certain datasets,2,2,1.0
certain datasets and,2,2,1.0
datasets and classifiers,2,2,1.0
and classifiers the,2,2,1.0
classifiers the does,2,2,1.0
the does quite,2,2,1.0
does quite well,2,2,1.0
quite well however,2,2,1.0
well however when,2,2,1.0
however when data,2,2,1.0
when data augmentation,2,2,1.0
data augmentation can,2,2,1.0
augmentation can make,2,2,1.0
can make a,2,2,1.0
make a contribution,2,2,1.0
a contribution it,1,1,1.0
contribution it seems,1,1,1.0
it seems to,1,1,1.0
seems to be,2,2,1.0
to be cfa,2,2,1.0
be cfa that,2,2,1.0
cfa that contributes,2,2,1.0
that contributes the,2,2,1.0
contributes the most,2,2,1.0
the most to,2,2,1.0
most to performance,2,2,1.0
to performance improvements,2,2,1.0
performance improvements table,2,2,1.0
improvements table auc,2,2,1.0
table auc values,8,2,4.0
auc values for,8,2,4.0
values for the,12,3,4.0
rf classifier for,2,2,1.0
classifier for each,8,2,4.0
for each data,8,2,4.0
each data augmentation,8,2,4.0
augmentation method dataset,8,2,4.0
method dataset baseline,8,2,4.0
dataset baseline smote,8,2,4.0
baseline smote adasyn,8,2,4.0
smote adasyn smote,13,2,6.5
adasyn smote smote,10,2,5.0
smote smote smote,10,2,5.0
smote smote cfa,10,2,5.0
smote cfa total,8,2,4.0
cfa total temraz,8,2,4.0
total temraz keane,10,2,5.0
augmentation table auc,6,2,3.0
the classifier for,2,2,1.0
lr classifier for,2,2,1.0
mlp classifier for,2,2,1.0
data augmentation notably,2,2,1.0
augmentation notably if,2,2,1.0
notably if we,2,2,1.0
if we assess,2,2,1.0
we assess overall,2,2,1.0
assess overall performance,2,2,1.0
overall performance by,2,2,1.0
performance by noting,2,2,1.0
by noting occasions,2,2,1.0
noting occasions for,2,2,1.0
occasions for a,2,2,1.0
a given method,4,2,2.0
given method when,2,2,1.0
method when the,2,2,1.0
when the score,1,1,1.0
the score is,1,1,1.0
score is highest,2,2,1.0
is highest we,2,2,1.0
highest we see,2,2,1.0
we see that,3,3,1.0
see that cfa,4,2,2.0
that cfa scores,4,2,2.0
cfa scores best,4,2,2.0
scores best see,2,2,1.0
best see table,2,2,1.0
see table in,1,1,1.0
table in of,1,1,1.0
in of cases,3,2,1.5
of cases it,4,2,2.0
cases it has,3,2,1.5
it has the,3,2,1.5
has the highest,3,2,1.5
the highest precision,4,2,2.0
highest precision score,2,2,1.0
precision score as,2,2,1.0
score as opposed,4,2,2.0
as opposed to,8,4,2.0
opposed to for,4,2,2.0
to for for,2,2,1.0
for for for,4,2,2.0
for for adasyn,2,2,1.0
for adasyn for,4,2,2.0
adasyn for smote,2,2,1.0
for smote and,2,2,1.0
smote and it,2,2,1.0
and it is,2,2,1.0
it is also,6,3,2.0
is also observed,2,2,1.0
also observed that,2,2,1.0
scores best in,2,2,1.0
best in of,2,2,1.0
the highest recall,2,2,1.0
highest recall score,2,2,1.0
recall score as,2,2,1.0
to for adasyn,2,2,1.0
adasyn for both,2,2,1.0
for both smote,2,2,1.0
both smote and,3,3,1.0
smote and for,2,2,1.0
and for for,2,2,1.0
for for and,2,2,1.0
for and for,2,2,1.0
and for baseline,2,2,1.0
for baseline table,2,2,1.0
baseline table the,2,2,1.0
table the number,2,2,1.0
number of datasets,7,4,1.75
of datasets for,2,2,1.0
each method showing,2,2,1.0
method showing the,2,2,1.0
showing the highest,2,2,1.0
highest precision and,2,2,1.0
precision and recall,9,3,3.0
and recall scores,2,2,1.0
recall scores for,2,2,1.0
scores for a,2,2,1.0
given method smote,2,2,1.0
method smote adasyn,2,2,1.0
and cfa on,4,2,2.0
cfa on a,2,2,1.0
on a selected,2,2,1.0
a selected classifier,2,2,1.0
selected classifier precision,2,2,1.0
classifier precision classifier,2,2,1.0
precision classifier baseline,2,2,1.0
classifier baseline smote,4,2,2.0
adasyn smote cfa,2,1,2.0
smote cfa rf,4,2,2.0
cfa rf lr,4,2,2.0
lr mlp total,4,2,2.0
mlp total recall,2,2,1.0
total recall classifier,2,2,1.0
recall classifier baseline,2,2,1.0
mlp total temraz,2,2,1.0
augmentation a rf,2,2,1.0
a rf classifier,6,2,3.0
rf classifier b,4,2,2.0
classifier b classifier,2,2,1.0
b classifier c,2,2,1.0
classifier c lr,2,2,1.0
c lr classifier,2,2,1.0
lr classifier d,2,2,1.0
classifier d mlp,2,2,1.0
d mlp classifier,2,2,1.0
mlp classifier figure,2,2,1.0
classifier figure values,2,2,1.0
figure values for,2,2,1.0
for the different,2,2,1.0
the different conditions,2,2,1.0
different conditions across,2,2,1.0
conditions across datasets,2,2,1.0
across datasets for,2,2,1.0
the four classifiers,10,2,5.0
four classifiers a,2,2,1.0
classifiers a rf,2,2,1.0
classifier b c,2,2,1.0
b c lr,2,2,1.0
c lr and,2,2,1.0
lr and d,2,2,1.0
and d mlp,2,2,1.0
d mlp figure,2,2,1.0
mlp figure shows,2,2,1.0
figure shows the,2,2,1.0
shows the comparisons,2,2,1.0
the comparisons for,2,2,1.0
comparisons for each,2,2,1.0
augmentation methods smote,2,1,2.0
cfa on datasets,2,2,1.0
on datasets using,2,2,1.0
datasets using the,2,2,1.0
using the four,8,2,4.0
lr mlp in,2,2,1.0
mlp in general,2,2,1.0
in general higher,2,2,1.0
general higher values,2,2,1.0
higher values indicate,2,2,1.0
values indicate better,2,2,1.0
indicate better classifier,2,2,1.0
better classifier performance,2,2,1.0
classifier performance overall,2,2,1.0
performance overall there,2,2,1.0
overall there is,2,2,1.0
there is no,4,4,1.0
is no significant,2,2,1.0
no significant difference,2,2,1.0
significant difference in,2,2,1.0
in the values,2,2,1.0
the values between,2,2,1.0
values between smote,2,2,1.0
between smote variants,2,2,1.0
smote variants keane,2,2,1.0
variants keane counterfactual,2,2,1.0
adasyn and and,2,2,1.0
and and these,2,2,1.0
and these achieved,1,1,1.0
these achieved lower,1,1,1.0
achieved lower values,2,2,1.0
lower values than,2,2,1.0
values than cfa,2,2,1.0
than cfa in,2,2,1.0
cfa in most,2,2,1.0
in most cases,3,3,1.0
most cases perhaps,2,2,1.0
cases perhaps the,2,2,1.0
perhaps the most,4,4,1.0
the most interesting,1,1,1.0
most interesting result,1,1,1.0
interesting result is,1,1,1.0
result is that,2,2,1.0
is that cfa,2,2,1.0
that cfa achieved,2,2,1.0
cfa achieved higher,2,2,1.0
achieved higher values,2,2,1.0
higher values in,2,2,1.0
values in out,2,2,1.0
of datasets when,4,2,2.0
datasets when using,4,2,2.0
when using the,4,3,1.3333333333333333
using the rf,2,2,1.0
rf classifier and,2,2,1.0
classifier and in,2,2,1.0
and in out,2,2,1.0
when using classifier,2,2,1.0
using classifier it,2,2,1.0
classifier it outperformed,2,2,1.0
it outperformed the,2,2,1.0
outperformed the baseline,2,2,1.0
the baseline with,2,2,1.0
baseline with no,2,2,1.0
with no data,2,2,1.0
no data and,1,1,1.0
data and smote,1,1,1.0
and smote variants,2,2,1.0
smote variants similarly,2,2,1.0
variants similarly when,2,2,1.0
similarly when applying,2,2,1.0
when applying lr,2,2,1.0
applying lr classifier,2,2,1.0
lr classifier cfa,2,2,1.0
classifier cfa achieved,2,2,1.0
cfa achieved the,2,2,1.0
achieved the best,2,2,1.0
the best in,1,1,1.0
best in out,1,1,1.0
of datasets although,2,2,1.0
datasets although the,2,2,1.0
although the improvement,2,2,1.0
the improvement varied,2,2,1.0
improvement varied with,2,2,1.0
varied with different,2,2,1.0
with different data,2,2,1.0
different data sets,5,2,2.5
data sets finally,1,1,1.0
sets finally in,1,1,1.0
finally in the,2,2,1.0
in the mlp,2,2,1.0
mlp classifier cfa,2,2,1.0
classifier cfa still,2,2,1.0
cfa still achieved,2,2,1.0
still achieved highest,2,2,1.0
achieved highest auc,2,2,1.0
highest auc in,2,2,1.0
auc in out,2,2,1.0
of datasets finally,2,2,1.0
datasets finally the,2,2,1.0
finally the roc,2,2,1.0
the roc curves,3,3,1.0
roc curves which,2,2,1.0
curves which show,2,2,1.0
which show the,2,2,1.0
show the between,2,2,1.0
the between sensitivity,2,2,1.0
between sensitivity and,2,2,1.0
sensitivity and specificity,1,1,1.0
and specificity are,1,1,1.0
specificity are presented,1,1,1.0
are presented in,4,4,1.0
presented in figure,2,2,1.0
in figure and,2,2,1.0
figure and figure,2,2,1.0
and figure shows,2,2,1.0
figure shows selected,2,2,1.0
shows selected examples,2,2,1.0
selected examples of,2,2,1.0
examples of roc,4,2,2.0
roc curves where,8,2,4.0
curves where cfa,4,2,2.0
where cfa methods,1,1,1.0
cfa methods obtained,1,1,1.0
methods obtained for,2,2,1.0
obtained for the,4,2,2.0
four methods using,2,2,1.0
methods using the,6,2,3.0
four classifiers on,6,2,3.0
classifiers on data,1,1,1.0
on data sets,2,2,1.0
data sets according,2,2,1.0
sets according to,2,2,1.0
according to figure,2,2,1.0
to figure cfa,2,2,1.0
figure cfa clearly,2,2,1.0
cfa clearly outperformed,2,2,1.0
clearly outperformed other,2,2,1.0
outperformed other data,2,2,1.0
other data augmentation,1,1,1.0
smote adasyn for,2,2,1.0
adasyn for example,2,2,1.0
for example when,4,2,2.0
example when running,4,2,2.0
when running a,4,2,2.0
running a rf,2,2,1.0
rf classifier on,2,2,1.0
on the dataset,2,2,1.0
the dataset we,2,2,1.0
dataset we can,4,2,2.0
we can see,6,2,3.0
can see that,10,4,2.5
that cfa had,2,2,1.0
cfa had better,2,2,1.0
had better performance,4,2,2.0
better performance than,6,3,2.0
performance than methods,2,2,1.0
than methods with,2,2,1.0
methods with respect,2,2,1.0
respect to roc,2,2,1.0
to roc curve,2,2,1.0
roc curve figure,2,2,1.0
curve figure these,2,2,1.0
figure these results,1,1,1.0
these results support,1,1,1.0
results support our,1,1,1.0
support our previous,2,2,1.0
our previous results,2,2,1.0
previous results which,2,2,1.0
results which were,2,2,1.0
which were showing,2,2,1.0
were showing that,2,2,1.0
showing that cfa,2,2,1.0
that cfa can,2,2,1.0
cfa can work,2,2,1.0
can work as,2,2,1.0
work as a,2,2,1.0
as a successful,2,2,1.0
a successful technique,2,2,1.0
successful technique for,2,2,1.0
technique for data,2,2,1.0
data augmentation to,3,2,1.5
augmentation to handling,2,2,1.0
to handling the,2,2,1.0
handling the class,5,3,1.6666666666666667
the class imbalanced,5,3,1.6666666666666667
class imbalanced problem,5,3,1.6666666666666667
imbalanced problem on,2,2,1.0
problem on the,2,2,1.0
other hand figure,1,1,1.0
hand figure shows,1,1,1.0
figure shows several,1,1,1.0
shows several examples,2,2,1.0
several examples of,2,2,1.0
curves where smote,4,2,2.0
where smote variants,4,2,2.0
smote variants smote,2,2,1.0
variants smote adasyn,2,2,1.0
smote adasyn do,2,2,1.0
adasyn do better,2,2,1.0
do better than,2,2,1.0
better than cfa,2,2,1.0
than cfa for,2,2,1.0
cfa for example,2,2,1.0
running a lr,2,2,1.0
a lr classifier,2,2,1.0
lr classifier on,2,2,1.0
on the pima,2,2,1.0
the pima dataset,2,2,1.0
pima dataset we,2,2,1.0
see that methods,2,2,1.0
that methods had,2,2,1.0
methods had better,2,2,1.0
performance than cfa,2,2,1.0
than cfa figure,2,2,1.0
cfa figure temraz,2,2,1.0
figure temraz keane,2,2,1.0
data augmentation figure,4,2,2.0
augmentation figure selected,4,2,2.0
figure selected examples,4,2,2.0
selected examples for,4,2,2.0
examples for roc,4,2,2.0
for roc curves,4,2,2.0
where cfa outperformed,2,2,1.0
cfa outperformed methods,2,2,1.0
outperformed methods for,1,1,1.0
for the six,4,2,2.0
the six methods,4,2,2.0
six methods using,4,2,2.0
classifiers on different,4,2,2.0
on different data,4,2,2.0
data sets temraz,4,2,2.0
sets temraz keane,4,2,2.0
smote variants outperformed,2,2,1.0
variants outperformed cfa,2,2,1.0
outperformed cfa obtained,2,2,1.0
cfa obtained for,2,2,1.0
data augmentation why,2,2,1.0
augmentation why does,2,2,1.0
why does cfa,2,2,1.0
does cfa work,2,2,1.0
cfa work in,2,2,1.0
work in xai,2,2,1.0
have been found,2,2,1.0
been found to,2,2,1.0
found to create,2,2,1.0
to create plausible,2,2,1.0
create plausible synthetic,2,2,1.0
datapoints for purposes,1,1,1.0
for purposes indeed,1,1,1.0
purposes indeed the,2,2,1.0
indeed the evaluative,2,2,1.0
the evaluative metrics,2,2,1.0
evaluative metrics in,2,2,1.0
in xai show,2,2,1.0
xai show that,2,2,1.0
show that these,2,2,1.0
that these explanatory,2,2,1.0
these explanatory are,1,1,1.0
explanatory are generally,1,1,1.0
are generally valid,2,2,1.0
to existing this,2,2,1.0
existing this experience,2,2,1.0
this experience in,2,2,1.0
experience in xai,2,2,1.0
in xai is,2,2,1.0
xai is the,2,2,1.0
is the backdrop,2,2,1.0
the backdrop and,2,2,1.0
backdrop and motivation,2,2,1.0
and motivation for,2,2,1.0
motivation for applying,2,2,1.0
for applying this,2,2,1.0
applying this counterfactual,2,2,1.0
the data as,1,1,1.0
data as we,1,1,1.0
as we saw,2,2,1.0
we saw earlier,2,2,1.0
saw earlier initial,2,2,1.0
earlier initial tests,2,2,1.0
initial tests on,2,2,1.0
tests on a,2,2,1.0
on a prediction,2,2,1.0
prediction problem showed,1,1,1.0
problem showed that,1,1,1.0
showed that generated,1,1,1.0
generated counterfactuals in,2,2,1.0
minority class improved,1,1,1.0
class improved performance,1,1,1.0
improved performance specifically,2,2,1.0
performance specifically dealing,2,2,1.0
specifically dealing with,2,2,1.0
dealing with the,7,3,2.3333333333333335
with the dataset,2,2,1.0
the dataset drift,2,2,1.0
dataset drift caused,2,2,1.0
drift caused by,2,2,1.0
caused by climate,2,2,1.0
by climate change,2,2,1.0
climate change see,2,2,1.0
change see this,2,2,1.0
see this experience,2,2,1.0
this experience led,2,2,1.0
experience led to,2,2,1.0
led to the,2,2,1.0
the present tests,2,2,1.0
present tests to,2,2,1.0
tests to determine,2,2,1.0
determine the generality,2,2,1.0
the generality of,2,2,1.0
generality of these,2,2,1.0
of these effects,2,2,1.0
these effects as,2,2,1.0
effects as we,2,2,1.0
as we can,2,2,1.0
can see the,2,2,1.0
see the cfa,2,2,1.0
cfa method seems,2,2,1.0
method seems to,2,2,1.0
seems to work,2,2,1.0
to work well,2,2,1.0
work well across,2,2,1.0
well across wide,2,2,1.0
across wide range,2,2,1.0
wide range of,4,2,2.0
range of datasets,4,2,2.0
of datasets ml,2,2,1.0
datasets ml models,2,2,1.0
models and different,2,2,1.0
and different imbalance,2,2,1.0
different imbalance ratios,3,3,1.0
imbalance ratios but,2,2,1.0
ratios but why,2,2,1.0
but why does,2,2,1.0
why does it,2,2,1.0
does it work,2,2,1.0
it work so,2,2,1.0
work so well,2,2,1.0
so well from,2,2,1.0
well from the,2,2,1.0
from the climate,2,2,1.0
the climate example,2,2,1.0
climate example our,2,2,1.0
example our initial,2,2,1.0
our initial explanation,2,2,1.0
initial explanation was,2,2,1.0
explanation was that,2,2,1.0
was that cfa,2,2,1.0
cfa does well,2,2,1.0
does well because,2,2,1.0
well because it,2,2,1.0
because it minority,1,1,1.0
it minority instances,1,1,1.0
that are counterfactual,2,2,1.0
are counterfactual offsets,2,2,1.0
counterfactual offsets from,2,2,1.0
offsets from known,2,2,1.0
from known but,1,1,1.0
known but this,1,1,1.0
but this account,2,2,1.0
this account does,2,2,1.0
account does not,2,2,1.0
does not answer,2,2,1.0
not answer why,2,2,1.0
answer why the,2,2,1.0
why the offsets,2,2,1.0
the offsets tend,2,2,1.0
offsets tend to,2,2,1.0
to be useful,2,2,1.0
be useful our,2,2,1.0
useful our best,2,2,1.0
our best account,2,2,1.0
best account hinges,2,2,1.0
account hinges on,2,2,1.0
hinges on ideas,2,2,1.0
on ideas from,2,2,1.0
ideas from how,2,2,1.0
from how reasoning,2,2,1.0
how reasoning cbr,2,2,1.0
reasoning cbr systems,2,2,1.0
cbr systems operate,2,2,1.0
systems operate in,2,2,1.0
operate in cbr,2,2,1.0
in cbr target,2,2,1.0
cbr target problems,2,2,1.0
target problems are,2,2,1.0
problems are solved,2,2,1.0
are solved by,2,2,1.0
solved by similar,1,1,1.0
by similar cases,1,1,1.0
similar cases and,2,2,1.0
cases and sometimes,2,2,1.0
and sometimes adapting,2,2,1.0
sometimes adapting them,2,2,1.0
adapting them to,2,2,1.0
to generate predictions,2,2,1.0
generate predictions so,2,2,1.0
predictions so if,2,2,1.0
i am trying,2,2,1.0
am trying to,2,2,1.0
trying to predict,2,2,1.0
to predict in,2,2,1.0
predict in a,2,2,1.0
in a city,2,2,1.0
a city and,2,2,1.0
city and my,2,2,1.0
and my cbr,2,2,1.0
my cbr system,2,2,1.0
cbr system is,2,2,1.0
system is presented,2,2,1.0
is presented with,2,2,1.0
presented with a,2,2,1.0
with a apartment,1,1,1.0
a apartment with,3,2,1.5
apartment with and,1,1,1.0
with and the,1,1,1.0
and the closest,2,2,1.0
the closest retrieved,2,2,1.0
closest retrieved case,2,2,1.0
retrieved case is,2,2,1.0
case is a,2,2,1.0
is a apartment,2,2,1.0
apartment with the,1,1,1.0
with the system,1,1,1.0
the system could,2,2,1.0
system could have,2,2,1.0
could have an,2,2,1.0
have an adaptation,2,2,1.0
an adaptation rule,4,2,2.0
adaptation rule than,2,2,1.0
rule than can,2,2,1.0
than can bridges,2,2,1.0
can bridges the,2,2,1.0
bridges the gap,2,2,1.0
the gap between,2,2,1.0
gap between the,2,2,1.0
between the historical,2,2,1.0
the historical case,1,1,1.0
historical case and,1,1,1.0
case and the,1,1,1.0
and the target,3,3,1.0
the target case,4,2,2.0
target case for,2,2,1.0
case for instance,2,2,1.0
for instance there,2,2,1.0
instance there may,2,2,1.0
there may be,2,2,1.0
may be an,2,2,1.0
be an adaptation,2,2,1.0
adaptation rule that,2,2,1.0
rule that says,2,2,1.0
that says in,2,2,1.0
says in general,2,2,1.0
in general an,2,2,1.0
general an additional,2,2,1.0
an additional bathroom,2,2,1.0
additional bathroom is,2,2,1.0
bathroom is worth,2,2,1.0
is worth more,2,2,1.0
worth more so,2,2,1.0
more so in,2,2,1.0
so in a,2,2,1.0
in a typical,2,2,1.0
a typical this,2,2,1.0
typical this rule,2,2,1.0
this rule would,2,2,1.0
rule would be,2,2,1.0
would be applied,2,2,1.0
to the retrieved,2,2,1.0
the retrieved case,2,2,1.0
retrieved case to,2,2,1.0
case to bring,2,2,1.0
to bring it,2,2,1.0
bring it closer,2,2,1.0
it closer to,2,2,1.0
closer to the,4,4,1.0
to the target,2,2,1.0
target case and,2,2,1.0
case and improve,2,2,1.0
and improve the,2,2,1.0
improve the prediction,2,2,1.0
the prediction in,2,2,1.0
prediction in cbr,2,2,1.0
in cbr adaptation,2,2,1.0
cbr adaptation rules,2,2,1.0
adaptation rules were,2,2,1.0
rules were often,2,2,1.0
were often but,2,2,1.0
often but they,2,2,1.0
but they may,2,2,1.0
they may also,2,2,1.0
may also be,3,3,1.0
also be learned,2,2,1.0
be learned from,4,2,2.0
learned from analyses,2,2,1.0
from analyses of,2,2,1.0
analyses of patterns,2,2,1.0
of patterns between,2,2,1.0
patterns between temraz,2,2,1.0
between temraz keane,2,2,1.0
in the so,2,2,1.0
the so the,2,2,1.0
so the rule,2,2,1.0
the rule could,2,2,1.0
rule could be,2,2,1.0
could be learned,2,2,1.0
learned from differences,1,1,1.0
from differences found,1,1,1.0
differences found between,2,2,1.0
found between historical,2,2,1.0
between historical instances,2,2,1.0
historical instances showing,2,2,1.0
instances showing that,2,2,1.0
showing that they,2,2,1.0
that they often,2,2,1.0
they often lead,2,2,1.0
often lead to,2,2,1.0
lead to a,3,3,1.0
to a uplift,2,2,1.0
a uplift in,2,2,1.0
uplift in price,2,2,1.0
in price other,2,2,1.0
price other features,2,2,1.0
other features being,2,2,1.0
features being equal,2,2,1.0
being equal so,2,2,1.0
equal so in,2,2,1.0
so in cbr,2,2,1.0
in cbr these,2,2,1.0
cbr these adaptation,2,2,1.0
these adaptation rules,2,2,1.0
adaptation rules help,2,2,1.0
rules help to,2,2,1.0
help to bridge,2,2,1.0
to bridge holes,2,2,1.0
bridge holes in,2,2,1.0
holes in the,2,2,1.0
in the by,3,3,1.0
the by providing,2,2,1.0
by providing plausible,2,2,1.0
providing plausible transformations,2,2,1.0
plausible transformations of,2,2,1.0
transformations of known,2,2,1.0
of known datapoints,2,2,1.0
known datapoints counterfactuals,2,2,1.0
datapoints counterfactuals are,2,2,1.0
counterfactuals are special,2,2,1.0
are special case,2,2,1.0
special case of,2,2,1.0
case of an,2,2,1.0
of an adaption,2,2,1.0
an adaption rule,2,2,1.0
adaption rule they,2,2,1.0
rule they capture,2,2,1.0
they capture the,2,2,1.0
capture the key,1,1,1.0
the key that,1,1,1.0
key that lead,1,1,1.0
that lead to,2,2,1.0
lead to class,2,2,1.0
to class changes,2,2,1.0
class changes across,2,2,1.0
changes across the,2,2,1.0
across the boundary,2,2,1.0
the boundary between,2,2,1.0
boundary between majority,2,2,1.0
minority instances so,2,2,1.0
instances so when,2,2,1.0
so when we,2,2,1.0
when we apply,2,2,1.0
we apply them,2,2,1.0
apply them to,2,2,1.0
them to majority,2,2,1.0
to majority instances,2,2,1.0
instances to create,2,2,1.0
to create synthetic,2,2,1.0
create synthetic minority,2,2,1.0
synthetic minority instances,4,2,2.0
minority instances they,1,1,1.0
instances they stand,1,1,1.0
they stand a,2,2,1.0
stand a good,2,2,1.0
a good chance,2,2,1.0
good chance of,2,2,1.0
chance of being,2,2,1.0
of being plausible,2,2,1.0
being plausible as,2,2,1.0
plausible as they,2,2,1.0
they are based,3,3,1.0
are based on,9,5,1.8
based on prior,2,2,1.0
on prior local,2,2,1.0
prior local transformations,2,2,1.0
local transformations though,2,2,1.0
transformations though they,1,1,1.0
though they lack,1,1,1.0
they lack generality,1,1,1.0
lack generality they,2,2,1.0
generality they are,2,2,1.0
are not created,2,2,1.0
not created from,2,2,1.0
created from multiple,2,2,1.0
from multiple pairings,2,2,1.0
multiple pairings of,2,2,1.0
pairings of the,2,2,1.0
the same as,3,2,1.5
same as is,1,1,1.0
as is the,2,2,1.0
is the case,2,2,1.0
the case for,3,3,1.0
case for learned,2,2,1.0
for learned adaption,2,2,1.0
learned adaption rules,2,2,1.0
adaption rules perhaps,2,2,1.0
rules perhaps they,2,2,1.0
perhaps they may,2,2,1.0
they may work,2,2,1.0
may work because,4,2,2.0
work because they,2,2,1.0
they are so,2,2,1.0
are so constrained,2,2,1.0
so constrained and,2,2,1.0
constrained and local,2,2,1.0
and local cfa,1,1,1.0
local cfa only,1,1,1.0
cfa only considers,2,2,1.0
only considers native,2,2,1.0
considers native counterfactual,2,2,1.0
native counterfactual with,2,2,1.0
counterfactual with feature,2,2,1.0
with feature differences,4,2,2.0
feature differences so,2,2,1.0
differences so the,2,2,1.0
so the is,1,1,1.0
the is highly,1,1,1.0
is highly constrained,2,2,1.0
highly constrained and,2,2,1.0
constrained and specific,2,2,1.0
and specific to,2,2,1.0
specific to the,2,2,1.0
to the instances,2,2,1.0
the instances that,2,2,1.0
that are already,2,2,1.0
are already very,1,1,1.0
already very similar,1,1,1.0
very similar all,1,1,1.0
similar all other,2,2,1.0
all other features,2,2,1.0
other features are,2,2,1.0
features are essentially,2,2,1.0
are essentially identical,2,2,1.0
essentially identical to,2,2,1.0
identical to put,2,2,1.0
to put it,2,2,1.0
put it simply,2,2,1.0
it simply cfa,2,2,1.0
simply cfa delivers,2,2,1.0
cfa delivers good,2,2,1.0
delivers good counterfactual,2,2,1.0
good counterfactual rules,2,2,1.0
counterfactual rules that,2,2,1.0
rules that work,2,2,1.0
that work locally,2,2,1.0
work locally to,2,2,1.0
locally to generate,2,2,1.0
generate plausible datapoints,2,2,1.0
plausible datapoints that,2,2,1.0
datapoints that are,2,2,1.0
that are predictively,2,2,1.0
are predictively useful,2,2,1.0
predictively useful so,2,2,1.0
useful so it,2,2,1.0
so it looks,2,2,1.0
it looks like,2,2,1.0
looks like the,2,2,1.0
like the nature,2,2,1.0
the nature of,5,5,1.0
nature of the,8,6,1.3333333333333333
of the is,3,3,1.0
the is important,3,3,1.0
is important to,3,3,1.0
important to the,2,2,1.0
to the success,4,2,2.0
of the method,3,3,1.0
the method what,2,2,1.0
method what are,2,2,1.0
what are cfa,2,2,1.0
are cfa s,2,2,1.0
cfa s limitations,2,2,1.0
s limitations the,2,2,1.0
limitations the to,2,2,1.0
the to the,2,2,1.0
the success question,2,2,1.0
success question is,2,2,1.0
question is the,2,2,1.0
is the failure,2,2,1.0
the failure one,2,2,1.0
failure one namely,2,2,1.0
one namely when,2,2,1.0
namely when do,2,2,1.0
when do we,2,2,1.0
do we think,2,2,1.0
we think cfa,2,2,1.0
think cfa will,2,2,1.0
cfa will fail,2,2,1.0
will fail and,2,2,1.0
fail and what,2,2,1.0
and what limitations,2,2,1.0
what limitations might,2,2,1.0
limitations might it,2,2,1.0
might it encounter,2,2,1.0
it encounter the,2,2,1.0
encounter the current,2,2,1.0
the current experiments,2,2,1.0
current experiments a,2,2,1.0
experiments a version,2,2,1.0
a version of,2,2,1.0
version of cfa,2,2,1.0
of cfa that,2,2,1.0
cfa that performs,2,2,1.0
that performs well,3,3,1.0
performs well so,2,2,1.0
well so it,2,2,1.0
so it is,4,3,1.3333333333333333
it is not,3,3,1.0
is not immediately,2,2,1.0
not immediately clear,2,2,1.0
immediately clear what,2,2,1.0
clear what would,2,2,1.0
what would lead,2,2,1.0
would lead cfa,2,2,1.0
lead cfa to,2,2,1.0
cfa to fail,4,2,2.0
to fail however,1,1,1.0
fail however there,1,1,1.0
however there are,2,2,1.0
there are several,5,4,1.25
are several conditions,2,2,1.0
several conditions under,2,2,1.0
under which cfa,2,2,1.0
which cfa is,2,2,1.0
cfa is likely,4,2,2.0
to be less,3,3,1.0
be less good,2,2,1.0
less good with,2,2,1.0
good with respect,2,2,1.0
respect to i,2,2,1.0
to i the,1,1,1.0
i the quality,1,1,1.0
the quality of,5,2,2.5
quality of dataset,1,1,1.0
of dataset differences,1,1,1.0
dataset differences ii,1,1,1.0
differences ii the,2,2,1.0
ii the use,2,2,1.0
use of the,7,4,1.75
of the constraint,2,2,1.0
the constraint and,2,2,1.0
constraint and iii,2,2,1.0
and iii the,2,2,1.0
iii the use,2,2,1.0
use of different,2,2,1.0
of different tolerances,2,2,1.0
different tolerances temraz,2,2,1.0
tolerances temraz keane,2,2,1.0
data augmentation quality,2,2,1.0
augmentation quality of,2,2,1.0
quality of the,5,3,1.6666666666666667
of the dataset,11,5,2.2
the dataset fundamentally,2,2,1.0
dataset fundamentally cfa,2,2,1.0
fundamentally cfa depends,2,2,1.0
cfa depends on,2,2,1.0
depends on the,4,3,1.3333333333333333
on the set,2,2,1.0
set of native,2,2,1.0
of native counterfactuals,2,2,1.0
dataset for its,2,2,1.0
for its success,2,2,1.0
its success to,2,2,1.0
success to put,2,2,1.0
to put this,2,2,1.0
put this another,2,2,1.0
this another way,2,2,1.0
another way there,2,2,1.0
way there needs,2,2,1.0
there needs to,2,2,1.0
to be a,4,4,1.0
be a rich,2,2,1.0
a rich and,2,2,1.0
rich and diverse,2,2,1.0
and diverse set,2,2,1.0
diverse set of,2,2,1.0
set of good,2,2,1.0
of good counterfactual,2,2,1.0
good counterfactual pairings,2,2,1.0
counterfactual pairings between,2,2,1.0
pairings between majority,2,2,1.0
minority instances either,2,2,1.0
of a clear,2,2,1.0
a clear decision,2,2,1.0
clear decision boundary,2,2,1.0
decision boundary without,2,2,1.0
boundary without these,2,2,1.0
without these counterfactuals,2,2,1.0
these counterfactuals the,2,2,1.0
counterfactuals the ability,2,2,1.0
the ability to,3,3,1.0
ability to generate,2,2,1.0
generate synthetic datapoints,1,1,1.0
synthetic datapoints will,1,1,1.0
datapoints will be,1,1,1.0
will be hampered,1,1,1.0
be hampered current,1,1,1.0
hampered current indications,2,2,1.0
current indications are,2,2,1.0
indications are that,2,2,1.0
are that at,2,2,1.0
that at least,2,2,1.0
at least of,2,2,1.0
least of the,2,2,1.0
majority class need,2,2,1.0
class need to,2,2,1.0
to be involved,2,2,1.0
be involved in,2,2,1.0
involved in these,2,2,1.0
in these native,2,2,1.0
counterfactuals in order,2,2,1.0
in order to,22,6,3.6666666666666665
order to provide,3,3,1.0
to provide a,5,3,1.6666666666666667
provide a basis,2,2,1.0
a basis for,2,2,1.0
basis for generating,2,2,1.0
for generating minority,2,2,1.0
generating minority instances,2,2,1.0
minority instances from,2,2,1.0
from the of,2,2,1.0
class however we,2,2,1.0
however we have,2,2,1.0
we have not,2,2,1.0
have not systematically,2,2,1.0
not systematically tested,1,1,1.0
systematically tested how,1,1,1.0
tested how changes,1,1,1.0
how changes in,2,2,1.0
changes in this,2,2,1.0
in this percentage,2,2,1.0
this percentage affect,2,2,1.0
percentage affect performance,2,2,1.0
affect performance what,2,2,1.0
performance what we,2,2,1.0
what we do,2,2,1.0
we do know,2,2,1.0
do know is,2,2,1.0
know is that,2,2,1.0
is that for,3,3,1.0
that for many,2,2,1.0
for many datasets,2,2,1.0
many datasets the,2,2,1.0
datasets the current,2,2,1.0
the current parameters,2,2,1.0
current parameters for,2,2,1.0
parameters for cfa,2,2,1.0
for cfa deliver,2,2,1.0
cfa deliver good,2,2,1.0
deliver good performance,2,2,1.0
good performance so,2,2,1.0
performance so this,2,2,1.0
so this factor,2,2,1.0
this factor might,2,2,1.0
factor might be,2,2,1.0
might be quite,2,2,1.0
be quite robust,2,2,1.0
quite robust to,2,2,1.0
robust to disruption,2,2,1.0
to disruption the,2,2,1.0
disruption the constraint,2,2,1.0
the constraint a,2,2,1.0
constraint a key,2,2,1.0
a key hyperparameter,2,2,1.0
key hyperparameter for,2,2,1.0
hyperparameter for cfa,2,2,1.0
cfa is the,2,2,1.0
is the constraint,2,2,1.0
that the native,2,2,1.0
the native counterfactuals,4,2,2.0
native counterfactuals build,2,2,1.0
counterfactuals build from,2,2,1.0
build from the,2,2,1.0
the dataset involve,2,2,1.0
dataset involve no,2,2,1.0
involve no more,2,2,1.0
than two this,2,2,1.0
two this is,2,2,1.0
this is a,2,2,1.0
is a strong,2,2,1.0
a strong constraint,2,2,1.0
strong constraint that,2,2,1.0
constraint that was,2,2,1.0
that was made,2,2,1.0
was made originally,2,2,1.0
made originally on,2,2,1.0
originally on psychological,2,2,1.0
psychological grounds in,2,2,1.0
grounds in xai,2,2,1.0
in xai that,2,2,1.0
xai that is,2,2,1.0
that is it,2,2,1.0
is it has,2,2,1.0
it has been,5,3,1.6666666666666667
been shown that,3,3,1.0
shown that people,2,2,1.0
that people prefer,2,2,1.0
people prefer sparse,2,2,1.0
prefer sparse counterfactuals,2,2,1.0
counterfactuals with feature,2,2,1.0
feature differences as,2,2,1.0
differences as they,2,2,1.0
they are easier,2,2,1.0
easier to comprehend,2,2,1.0
to comprehend however,2,2,1.0
comprehend however this,2,2,1.0
however this rationale,2,2,1.0
this rationale from,2,2,1.0
rationale from xai,2,2,1.0
from xai does,2,2,1.0
xai does not,2,2,1.0
does not apply,2,2,1.0
not apply to,2,2,1.0
apply to data,2,2,1.0
augmentation in data,1,1,1.0
augmentation the may,2,2,1.0
the may work,2,2,1.0
work because it,2,2,1.0
because it produces,2,2,1.0
it produces very,2,2,1.0
produces very counterfactual,1,1,1.0
very counterfactual pairs,1,1,1.0
counterfactual pairs so,2,2,1.0
pairs so they,2,2,1.0
so they produce,2,2,1.0
they produce very,2,2,1.0
produce very simple,2,2,1.0
very simple adaptation,2,2,1.0
simple adaptation rules,2,2,1.0
adaptation rules in,2,2,1.0
rules in which,2,2,1.0
in which most,3,3,1.0
which most features,2,2,1.0
most features remain,2,2,1.0
features remain the,2,2,1.0
remain the same,2,2,1.0
the same between,2,2,1.0
same between instances,2,2,1.0
between instances and,2,2,1.0
instances and a,2,2,1.0
and a small,2,2,1.0
a small number,3,3,1.0
small number of,3,3,1.0
number of features,5,5,1.0
of features differ,2,2,1.0
features differ these,2,2,1.0
differ these difference,2,2,1.0
these difference patterns,2,2,1.0
difference patterns may,2,2,1.0
patterns may be,2,2,1.0
be more representative,2,2,1.0
more representative of,2,2,1.0
representative of valid,2,2,1.0
of valid in,2,2,1.0
valid in the,2,2,1.0
the dataset and,6,4,1.5
dataset and hence,2,2,1.0
and hence be,2,2,1.0
hence be more,2,2,1.0
likely to produce,2,2,1.0
to produce good,2,2,1.0
produce good synthetic,2,2,1.0
good synthetic datapoints,2,2,1.0
synthetic datapoints these,2,2,1.0
datapoints these is,2,2,1.0
these is some,2,2,1.0
is some evidence,3,3,1.0
some evidence to,2,2,1.0
evidence to support,2,2,1.0
to support this,2,2,1.0
support this proposition,2,2,1.0
this proposition in,2,2,1.0
proposition in prior,2,2,1.0
in prior work,2,2,1.0
prior work temraz,2,2,1.0
work temraz et,2,2,1.0
et al report,2,2,1.0
al report that,2,2,1.0
report that in,2,2,1.0
that in pilot,2,2,1.0
in pilot runs,2,2,1.0
pilot runs of,2,2,1.0
runs of their,2,2,1.0
of their experiments,2,2,1.0
their experiments they,2,2,1.0
experiments they explored,2,2,1.0
they explored using,1,1,1.0
explored using and,1,1,1.0
using and counterfactuals,1,1,1.0
and counterfactuals but,2,2,1.0
counterfactuals but found,2,2,1.0
but found they,2,2,1.0
found they did,2,2,1.0
they did not,2,2,1.0
did not significantly,2,2,1.0
not significantly improve,2,2,1.0
significantly improve importance,1,1,1.0
improve importance that,1,1,1.0
importance that is,2,2,1.0
that is they,2,2,1.0
is they were,2,2,1.0
they were less,2,2,1.0
were less likely,2,2,1.0
to generate useful,2,2,1.0
generate useful minority,1,1,1.0
useful minority instances,1,1,1.0
minority instances we,1,1,1.0
instances we do,2,2,1.0
we do not,2,2,1.0
do not know,2,2,1.0
not know whether,2,2,1.0
know whether similar,2,2,1.0
whether similar results,2,2,1.0
similar results would,2,2,1.0
results would be,2,2,1.0
be found for,2,2,1.0
found for other,2,2,1.0
for other datasets,2,2,1.0
other datasets though,2,2,1.0
datasets though the,2,2,1.0
though the fact,2,2,1.0
the fact that,6,5,1.2
fact that the,4,4,1.0
that the keane,2,2,1.0
the keane counterfactual,2,2,1.0
data augmentation difference,2,2,1.0
augmentation difference constraint,2,2,1.0
difference constraint works,2,2,1.0
constraint works here,2,2,1.0
works here for,2,2,1.0
here for datasets,2,2,1.0
for datasets with,2,2,1.0
datasets with features,2,2,1.0
with features suggests,2,2,1.0
features suggests that,2,2,1.0
suggests that this,2,2,1.0
that this constraint,2,2,1.0
this constraint works,2,2,1.0
constraint works quite,2,2,1.0
works quite generally,2,2,1.0
quite generally so,2,2,1.0
generally so again,2,2,1.0
so again we,2,2,1.0
again we would,2,2,1.0
we would expect,2,2,1.0
would expect cfa,2,2,1.0
expect cfa to,2,2,1.0
to fail if,2,2,1.0
fail if higher,2,2,1.0
if higher numbers,2,2,1.0
higher numbers of,2,2,1.0
numbers of were,1,1,1.0
of were used,1,1,1.0
were used in,2,2,1.0
used in computing,2,2,1.0
in computing the,2,2,1.0
computing the native,2,2,1.0
native counterfactuals when,2,2,1.0
counterfactuals when applying,2,2,1.0
when applying the,2,2,1.0
applying the method,2,2,1.0
the method the,3,3,1.0
method the importance,2,2,1.0
importance of tolerance,2,2,1.0
of tolerance a,2,2,1.0
tolerance a final,2,2,1.0
a final key,2,2,1.0
final key parameter,2,2,1.0
key parameter in,2,2,1.0
in cfa is,2,2,1.0
cfa is that,2,2,1.0
is that of,2,2,1.0
that of tolerance,2,2,1.0
of tolerance in,2,2,1.0
tolerance in finding,2,2,1.0
native counterfactual we,2,2,1.0
counterfactual we apply,2,2,1.0
we apply a,2,2,1.0
apply a tolerance,2,2,1.0
a tolerance to,2,2,1.0
tolerance to the,2,2,1.0
to the feature,5,4,1.25
the feature values,3,3,1.0
feature values specifically,2,2,1.0
values specifically we,2,2,1.0
specifically we allow,2,2,1.0
we allow features,2,2,1.0
allow features to,2,2,1.0
that feature this,1,1,1.0
feature this was,1,1,1.0
this was applied,1,1,1.0
was applied uniformly,2,2,1.0
applied uniformly across,2,2,1.0
uniformly across all,2,2,1.0
across all of,2,2,1.0
all of our,2,2,1.0
of our datasets,2,2,1.0
our datasets keane,2,2,1.0
datasets keane smyth,2,2,1.0
keane smyth used,2,2,1.0
smyth used a,2,2,1.0
used a more,2,2,1.0
a more tolerance,1,1,1.0
more tolerance scheme,1,1,1.0
tolerance scheme that,2,2,1.0
scheme that tailored,2,2,1.0
that tailored the,2,2,1.0
tailored the tolerance,2,2,1.0
the tolerance to,2,2,1.0
tolerance to each,2,2,1.0
to each dataset,2,2,1.0
each dataset they,2,2,1.0
dataset they varied,2,2,1.0
they varied the,2,2,1.0
varied the tolerances,2,2,1.0
the tolerances for,2,2,1.0
tolerances for each,2,2,1.0
each feature until,2,2,1.0
feature until changes,2,2,1.0
until changes in,2,2,1.0
changes in classification,2,2,1.0
in classification of,2,2,1.0
classification of the,2,2,1.0
original dataset arose,2,2,1.0
dataset arose and,2,2,1.0
arose and then,2,2,1.0
and then chose,2,2,1.0
then chose a,2,2,1.0
chose a relative,2,2,1.0
a relative tolerance,2,2,1.0
relative tolerance that,2,2,1.0
tolerance that produced,2,2,1.0
that produced no,2,2,1.0
produced no classification,2,2,1.0
no classification change,2,2,1.0
classification change obviously,2,2,1.0
change obviously without,2,2,1.0
obviously without tolerance,2,2,1.0
tolerance fewer would,1,1,1.0
fewer would be,1,1,1.0
likely diminish the,2,2,1.0
diminish the importance,2,2,1.0
of the decision,3,3,1.0
decision boundary fundamentally,2,2,1.0
boundary fundamentally like,2,2,1.0
fundamentally like cfa,2,2,1.0
like cfa works,2,2,1.0
cfa works with,2,2,1.0
works with instances,2,2,1.0
with instances that,2,2,1.0
boundary so clearly,1,1,1.0
so clearly the,1,1,1.0
clearly the definition,1,1,1.0
the definition and,2,2,1.0
definition and nature,2,2,1.0
and nature of,2,2,1.0
nature of that,2,2,1.0
of that decision,2,2,1.0
that decision boundary,2,2,1.0
decision boundary is,4,4,1.0
boundary is critical,2,2,1.0
is critical to,2,2,1.0
critical to its,2,2,1.0
to its successful,2,2,1.0
its successful performance,2,2,1.0
successful performance if,2,2,1.0
performance if the,2,2,1.0
if the instances,2,2,1.0
the instances around,2,2,1.0
instances around the,2,2,1.0
around the boundary,2,2,1.0
the boundary are,2,2,1.0
boundary are noisy,2,2,1.0
are noisy and,2,2,1.0
noisy and the,2,2,1.0
and the boundary,2,2,1.0
the boundary is,2,2,1.0
boundary is less,2,2,1.0
is less then,2,2,1.0
less then cfa,2,2,1.0
then cfa is,2,2,1.0
likely to disimprove,1,1,1.0
to disimprove in,1,1,1.0
disimprove in this,2,2,1.0
in this respect,2,2,1.0
this respect it,2,2,1.0
respect it is,2,2,1.0
it is interesting,2,2,1.0
is interesting note,2,2,1.0
interesting note that,2,2,1.0
note that cfa,2,2,1.0
cfa does best,2,2,1.0
does best using,2,2,1.0
best using the,2,2,1.0
using the and,2,2,1.0
the and random,2,2,1.0
and random forests,2,2,1.0
random forests models,2,2,1.0
forests models relative,2,2,1.0
models relative to,2,2,1.0
relative to the,4,3,1.3333333333333333
to the mlp,2,2,1.0
the mlp and,2,2,1.0
mlp and linear,2,2,1.0
and linear regression,2,2,1.0
linear regression models,2,2,1.0
regression models with,2,2,1.0
models with the,2,2,1.0
with the latter,2,2,1.0
the latter doing,2,2,1.0
latter doing the,2,2,1.0
doing the worst,2,2,1.0
the worst on,2,2,1.0
worst on auc,2,2,1.0
on auc conclusion,2,2,1.0
auc conclusion in,2,2,1.0
conclusion in this,2,2,1.0
this paper a,2,2,1.0
paper a novel,2,2,1.0
a novel oversampling,2,2,1.0
novel oversampling method,2,2,1.0
oversampling method counterfactual,2,2,1.0
augmentation cfa was,2,2,1.0
cfa was proposed,2,2,1.0
was proposed to,3,3,1.0
proposed to handle,3,3,1.0
to handle the,2,2,1.0
handle the class,2,2,1.0
imbalanced problem for,2,2,1.0
problem for binary,2,2,1.0
for binary classification,2,2,1.0
classification tasks cfa,2,2,1.0
tasks cfa uses,2,2,1.0
uses a approach,1,1,1.0
a approach to,2,2,1.0
approach to generating,2,2,1.0
class the essence,2,2,1.0
the essence of,2,2,1.0
essence of this,2,2,1.0
of this temraz,2,2,1.0
this temraz keane,2,2,1.0
augmentation method is,2,2,1.0
method is that,2,2,1.0
that it oversamples,2,2,1.0
it oversamples by,2,2,1.0
oversamples by adaptively,2,2,1.0
by adaptively combining,2,2,1.0
adaptively combining actual,2,2,1.0
combining actual from,2,2,1.0
actual from dataset,2,2,1.0
from dataset instances,2,2,1.0
dataset instances rather,2,2,1.0
rather than values,2,2,1.0
than values between,2,2,1.0
between instances the,2,2,1.0
instances the key,2,2,1.0
the key discoveries,2,2,1.0
key discoveries made,2,2,1.0
discoveries made are,2,2,1.0
made are i,2,2,1.0
are i counterfactual,2,2,1.0
i counterfactual methods,2,2,1.0
counterfactual methods developed,2,2,1.0
methods developed for,2,2,1.0
developed for xai,2,2,1.0
for xai can,2,2,1.0
xai can be,2,2,1.0
can be usefully,2,2,1.0
be usefully deployed,2,2,1.0
usefully deployed to,2,2,1.0
deployed to augment,2,2,1.0
to augment datasets,2,2,1.0
augment datasets with,2,2,1.0
datasets with synthetic,2,2,1.0
with synthetic cases,2,2,1.0
synthetic cases in,2,2,1.0
cases in the,5,3,1.6666666666666667
minority class that,2,2,1.0
class that improve,2,2,1.0
of the ml,1,1,1.0
the ml models,1,1,1.0
ml models ii,1,1,1.0
models ii this,2,2,1.0
ii this method,2,2,1.0
method can successfully,2,2,1.0
can successfully introduce,2,2,1.0
successfully introduce new,2,2,1.0
introduce new synthetic,2,2,1.0
synthetic minority examples,3,3,1.0
minority examples by,2,2,1.0
examples by leveraging,2,2,1.0
by leveraging known,1,1,1.0
leveraging known in,1,1,1.0
known in the,1,1,1.0
dataset and iii,2,2,1.0
and iii this,2,2,1.0
iii this method,2,2,1.0
method can outperform,2,2,1.0
can outperform many,2,2,1.0
outperform many key,2,2,1.0
many key benchmark,2,2,1.0
key benchmark smote,2,2,1.0
benchmark smote on,1,1,1.0
smote on a,1,1,1.0
on a wide,2,2,1.0
a wide range,2,2,1.0
datasets with differing,2,2,1.0
with differing imbalance,2,2,1.0
differing imbalance ratios,2,2,1.0
imbalance ratios using,2,2,1.0
ratios using representative,2,2,1.0
using representative ml,2,2,1.0
representative ml models,2,2,1.0
ml models acknowledgements,2,2,1.0
models acknowledgements this,2,2,1.0
acknowledgements this publication,2,2,1.0
this publication has,2,2,1.0
publication has emanated,2,2,1.0
has emanated from,2,2,1.0
emanated from research,2,2,1.0
from research conducted,2,2,1.0
research conducted with,2,2,1.0
conducted with the,2,2,1.0
with the financial,2,2,1.0
the financial support,2,2,1.0
financial support of,2,2,1.0
support of i,2,2,1.0
of i science,2,2,1.0
i science foundation,2,2,1.0
science foundation ireland,2,2,1.0
foundation ireland sfi,2,2,1.0
ireland sfi to,2,2,1.0
sfi to the,2,2,1.0
to the insight,2,2,1.0
the insight centre,2,2,1.0
data analytics under,2,2,1.0
analytics under grant,2,2,1.0
under grant number,4,2,2.0
grant number and,2,2,1.0
number and ii,2,2,1.0
and ii sfi,2,2,1.0
ii sfi and,2,2,1.0
sfi and the,2,2,1.0
and the department,2,2,1.0
the department of,6,4,1.5
department of agriculture,2,2,1.0
of agriculture food,2,2,1.0
agriculture food and,2,2,1.0
food and marine,2,2,1.0
and marine on,2,2,1.0
marine on behalf,2,2,1.0
on behalf of,2,2,1.0
behalf of the,2,2,1.0
of the government,2,2,1.0
the government of,2,2,1.0
government of ireland,2,2,1.0
of ireland to,2,2,1.0
ireland to the,2,2,1.0
to the vistamilk,2,2,1.0
the vistamilk sfi,2,2,1.0
research centre under,2,2,1.0
centre under grant,2,2,1.0
grant number references,2,2,1.0
number references aggarwal,2,2,1.0
references aggarwal chen,2,2,1.0
aggarwal chen han,2,2,1.0
chen han j,2,2,1.0
han j the,2,2,1.0
j the inverse,2,2,1.0
the inverse classification,2,2,1.0
inverse classification problem,2,2,1.0
classification problem journal,2,2,1.0
problem journal of,2,2,1.0
journal of computer,6,3,2.0
computer science and,8,4,2.0
science and technology,6,4,1.5
and technology fernandez,2,2,1.0
technology fernandez luengo,2,2,1.0
fernandez luengo derrac,2,2,1.0
luengo derrac garcía,2,2,1.0
derrac garcía sánchez,2,2,1.0
garcía sánchez herrera,2,2,1.0
sánchez herrera keel,2,2,1.0
herrera keel software,2,2,1.0
keel software tool,2,2,1.0
software tool data,2,2,1.0
tool data set,2,2,1.0
data set repository,2,2,1.0
set repository integration,2,2,1.0
repository integration of,2,2,1.0
integration of and,1,1,1.0
of and experimental,1,1,1.0
and experimental analysis,2,2,1.0
experimental analysis framework,2,2,1.0
analysis framework journal,2,2,1.0
framework journal of,2,2,1.0
journal of logic,2,2,1.0
of logic and,2,2,1.0
logic and soft,2,2,1.0
and soft computing,2,2,1.0
soft computing asuncion,2,2,1.0
computing asuncion newman,2,2,1.0
asuncion newman uci,2,2,1.0
newman uci machine,3,3,1.0
uci machine learning,7,4,1.75
machine learning repository,7,4,1.75
learning repository https,2,2,1.0
repository https bache,2,2,1.0
https bache lichman,2,2,1.0
bache lichman uci,2,2,1.0
lichman uci machine,3,3,1.0
learning repository batista,2,2,1.0
repository batista prati,2,2,1.0
batista prati monard,3,3,1.0
prati monard a,3,3,1.0
monard a study,4,4,1.0
a study of,4,4,1.0
study of the,4,4,1.0
of the behavior,3,3,1.0
the behavior of,4,4,1.0
behavior of several,4,4,1.0
of several methods,5,5,1.0
several methods for,5,5,1.0
methods for balancing,4,4,1.0
for balancing machine,4,4,1.0
balancing machine learning,4,4,1.0
machine learning training,5,5,1.0
learning training data,5,5,1.0
training data sigkdd,4,4,1.0
data sigkdd explor,3,3,1.0
sigkdd explor temraz,2,2,1.0
explor temraz keane,2,2,1.0
data augmentation bellinger,2,2,1.0
augmentation bellinger sharma,2,2,1.0
bellinger sharma japkowicz,2,2,1.0
sharma japkowicz zaïane,2,2,1.0
japkowicz zaïane framework,2,2,1.0
zaïane framework for,2,2,1.0
framework for extreme,2,2,1.0
for extreme imbalance,2,2,1.0
extreme imbalance classification,2,2,1.0
imbalance classification with,2,2,1.0
classification with the,2,2,1.0
majority class knowledge,2,2,1.0
class knowledge and,2,2,1.0
knowledge and systems,1,1,1.0
and systems bishop,1,1,1.0
systems bishop pattern,2,2,1.0
bishop pattern recognition,2,2,1.0
pattern recognition and,3,3,1.0
recognition and machine,3,3,1.0
and machine learning,3,3,1.0
machine learning springer,2,2,1.0
learning springer blake,2,2,1.0
springer blake merz,2,2,1.0
blake merz uci,2,2,1.0
merz uci repository,2,2,1.0
uci repository of,2,2,1.0
repository of machine,2,2,1.0
of machine learning,11,5,2.2
machine learning databases,2,2,1.0
learning databases department,2,2,1.0
databases department of,2,2,1.0
department of computer,4,4,1.0
science university of,2,2,1.0
university of california,2,2,1.0
of california bunkhumpornpat,2,2,1.0
california bunkhumpornpat sinapiromsaran,2,2,1.0
bunkhumpornpat sinapiromsaran lursinsap,4,2,2.0
sinapiromsaran lursinsap minority,1,1,1.0
lursinsap minority technique,2,2,1.0
minority technique for,3,3,1.0
technique for handling,3,3,1.0
for handling the,3,3,1.0
the class problem,1,1,1.0
class problem in,1,1,1.0
problem in proceedings,4,4,1.0
in proceedings of,60,5,12.0
proceedings of the,63,5,12.6
of the conference,6,2,3.0
the conference on,6,2,3.0
conference on advances,4,3,1.3333333333333333
on advances in,6,3,2.0
advances in knowledge,4,3,1.3333333333333333
in knowledge discovery,4,3,1.3333333333333333
knowledge discovery and,9,5,1.8
discovery and data,9,5,1.8
and data mining,12,5,2.4
data mining bunkhumpornpat,2,2,1.0
mining bunkhumpornpat sinapiromsaran,2,2,1.0
sinapiromsaran lursinsap dbsmote,2,2,1.0
lursinsap dbsmote synthetic,1,1,1.0
dbsmote synthetic minority,1,1,1.0
minority technique applied,2,2,1.0
technique applied intelligence,2,2,1.0
applied intelligence chawla,2,2,1.0
intelligence chawla bowyer,2,2,1.0
chawla bowyer hall,4,4,1.0
bowyer hall kegelmeyer,2,2,1.0
hall kegelmeyer smote,2,2,1.0
kegelmeyer smote synthetic,5,5,1.0
minority technique journal,3,3,1.0
technique journal of,4,4,1.0
journal of artificial,6,3,2.0
of artificial intelligence,6,3,2.0
artificial intelligence research,6,3,2.0
intelligence research chawla,2,2,1.0
research chawla lazarevic,2,2,1.0
chawla lazarevic hall,4,4,1.0
lazarevic hall bowyer,2,2,1.0
hall bowyer smoteboost,2,2,1.0
bowyer smoteboost improving,3,3,1.0
smoteboost improving prediction,3,3,1.0
improving prediction of,4,4,1.0
prediction of the,4,4,1.0
minority class in,10,5,2.0
class in boosting,4,4,1.0
in boosting knowledge,2,2,1.0
boosting knowledge discovery,2,2,1.0
knowledge discovery in,3,3,1.0
discovery in databases,3,3,1.0
in databases pkdd,2,2,1.0
databases pkdd cristianini,2,2,1.0
pkdd cristianini j,2,2,1.0
cristianini j an,2,2,1.0
j an introduction,2,2,1.0
an introduction to,2,2,1.0
introduction to support,2,2,1.0
to support vector,2,2,1.0
support vector machines,9,5,1.8
vector machines and,2,2,1.0
machines and other,2,2,1.0
and other learning,2,2,1.0
other learning methods,2,2,1.0
learning methods cambridge,2,2,1.0
methods cambridge uk,2,2,1.0
cambridge uk cambridge,2,2,1.0
uk cambridge university,2,2,1.0
cambridge university press,2,2,1.0
university press dandl,2,2,1.0
press dandl molnar,2,2,1.0
dandl molnar binder,2,2,1.0
molnar binder bischl,2,2,1.0
binder bischl b,2,2,1.0
bischl b counterfactual,1,1,1.0
b counterfactual explanations,1,1,1.0
counterfactual explanations in,5,2,2.5
explanations in international,2,2,1.0
in international conference,14,2,7.0
international conference on,51,5,10.2
conference on parallel,2,2,1.0
on parallel problem,2,2,1.0
parallel problem solving,2,2,1.0
problem solving from,2,2,1.0
solving from nature,2,2,1.0
from nature d,2,2,1.0
nature d aquin,2,2,1.0
d aquin badra,2,2,1.0
aquin badra lafrogne,2,2,1.0
badra lafrogne lieber,2,2,1.0
lafrogne lieber napoli,2,2,1.0
lieber napoli szathmary,2,2,1.0
napoli szathmary case,2,2,1.0
szathmary case base,2,2,1.0
case base mining,2,2,1.0
base mining for,2,2,1.0
mining for adaptation,2,2,1.0
for adaptation knowledge,2,2,1.0
adaptation knowledge acquisition,2,2,1.0
knowledge acquisition in,2,2,1.0
acquisition in proceedings,2,2,1.0
of the twentieth,2,2,1.0
the twentieth joint,1,1,1.0
twentieth joint conference,1,1,1.0
joint conference on,15,4,3.75
conference on artificial,11,3,3.6666666666666665
on artificial intelligence,11,3,3.6666666666666665
artificial intelligence dasarathy,2,2,1.0
intelligence dasarathy b,2,2,1.0
dasarathy b minimal,2,2,1.0
b minimal consistent,2,2,1.0
minimal consistent set,2,2,1.0
consistent set mcs,2,2,1.0
set mcs identification,2,2,1.0
mcs identification for,2,2,1.0
identification for optimal,2,2,1.0
for optimal nearest,1,1,1.0
optimal nearest neighbor,1,1,1.0
nearest neighbor decision,1,1,1.0
neighbor decision systems,2,2,1.0
decision systems design,2,2,1.0
systems design ieee,2,2,1.0
design ieee transactions,2,2,1.0
ieee transactions on,36,5,7.2
transactions on systems,7,4,1.75
on systems man,8,4,2.0
systems man and,8,4,2.0
man and cybernetics,7,4,1.75
and cybernetics delaney,2,2,1.0
cybernetics delaney greene,2,2,1.0
delaney greene keane,2,2,1.0
greene keane counterfactual,2,2,1.0
keane counterfactual for,1,1,1.0
counterfactual for time,1,1,1.0
for time series,2,2,1.0
time series classification,2,2,1.0
series classification in,2,2,1.0
classification in international,2,2,1.0
conference on reasoning,7,2,3.5
on reasoning springer,7,2,3.5
reasoning springer germany,8,2,4.0
springer germany temraz,2,2,1.0
germany temraz keane,2,2,1.0
data augmentation douzas,2,2,1.0
augmentation douzas bacao,2,2,1.0
douzas bacao map,2,2,1.0
bacao map oversampling,2,2,1.0
map oversampling somo,2,2,1.0
oversampling somo for,2,2,1.0
somo for data,1,1,1.0
for data set,1,1,1.0
data set learning,4,3,1.3333333333333333
set learning expert,2,2,1.0
learning expert systems,2,2,1.0
expert systems with,2,2,1.0
systems with applications,2,2,1.0
with applications douzas,2,2,1.0
applications douzas bacao,2,2,1.0
douzas bacao last,2,2,1.0
bacao last improving,2,2,1.0
last improving imbalanced,2,2,1.0
improving imbalanced learning,2,2,1.0
imbalanced learning through,2,2,1.0
learning through a,2,2,1.0
through a oversampling,1,1,1.0
a oversampling method,1,1,1.0
oversampling method based,2,2,1.0
method based on,2,2,1.0
based on and,3,3,1.0
on and smote,2,2,1.0
and smote information,2,2,1.0
smote information sciences,2,2,1.0
information sciences elkan,2,2,1.0
sciences elkan the,2,2,1.0
elkan the foundations,2,2,1.0
the foundations of,2,2,1.0
foundations of learning,2,2,1.0
of learning in,2,2,1.0
learning in seventeenth,2,2,1.0
in seventeenth international,2,2,1.0
seventeenth international joint,2,2,1.0
international joint conference,12,4,3.0
artificial intelligence fernandez,2,2,1.0
intelligence fernandez garcia,2,2,1.0
fernandez garcia herrera,4,2,2.0
garcia herrera chawla,2,2,1.0
herrera chawla smote,2,2,1.0
chawla smote for,2,2,1.0
smote for learning,2,2,1.0
for learning from,5,4,1.25
learning from imbalanced,31,6,5.166666666666667
from imbalanced data,31,6,5.166666666666667
imbalanced data progress,2,2,1.0
data progress and,2,2,1.0
progress and challenges,2,2,1.0
and challenges marking,2,2,1.0
challenges marking the,2,2,1.0
marking the anniversary,2,2,1.0
the anniversary the,2,2,1.0
anniversary the journal,2,2,1.0
the journal of,2,2,1.0
intelligence research jair,2,2,1.0
research jair förster,2,2,1.0
jair förster klier,2,2,1.0
förster klier kluge,4,2,2.0
klier kluge sigler,4,2,2.0
kluge sigler i,4,2,2.0
sigler i evaluating,2,2,1.0
i evaluating explainable,2,2,1.0
evaluating explainable artifical,2,2,1.0
explainable artifical intelligence,2,2,1.0
artifical intelligence what,2,2,1.0
intelligence what users,2,2,1.0
what users really,2,2,1.0
users really appreciate,2,2,1.0
really appreciate in,2,2,1.0
appreciate in european,2,2,1.0
in european conference,2,2,1.0
european conference on,5,4,1.25
conference on information,2,2,1.0
on information systems,2,2,1.0
information systems förster,2,2,1.0
systems förster klier,2,2,1.0
sigler i fostering,2,2,1.0
i fostering human,2,2,1.0
fostering human agency,2,2,1.0
human agency a,2,2,1.0
agency a process,2,2,1.0
a process for,2,2,1.0
process for the,2,2,1.0
for the design,2,2,1.0
the design of,2,2,1.0
design of xai,2,2,1.0
of xai systems,2,2,1.0
xai systems in,2,2,1.0
systems in icis,2,2,1.0
in icis proceedings,2,2,1.0
icis proceedings freund,2,2,1.0
proceedings freund schapire,2,2,1.0
freund schapire a,2,2,1.0
schapire a generalization,4,4,1.0
a generalization of,4,4,1.0
generalization of learning,4,4,1.0
of learning and,4,4,1.0
learning and an,4,4,1.0
and an application,4,4,1.0
an application to,4,4,1.0
application to boosting,4,4,1.0
to boosting journal,3,3,1.0
boosting journal of,3,3,1.0
of computer and,3,3,1.0
computer and system,3,3,1.0
and system sciences,3,3,1.0
system sciences galar,2,2,1.0
sciences galar fernandez,2,2,1.0
galar fernandez barrenechea,2,2,1.0
fernandez barrenechea bustince,2,2,1.0
barrenechea bustince herrera,2,2,1.0
bustince herrera a,2,2,1.0
herrera a review,3,3,1.0
a review on,3,3,1.0
review on ensembles,3,3,1.0
on ensembles for,3,3,1.0
ensembles for the,3,3,1.0
problem and approaches,3,3,1.0
and approaches ieee,3,3,1.0
approaches ieee transactions,3,3,1.0
and cybernetics han,2,2,1.0
cybernetics han kamber,2,2,1.0
han kamber data,2,2,1.0
kamber data mining,2,2,1.0
data mining concepts,2,2,1.0
mining concepts and,2,2,1.0
concepts and technique,2,2,1.0
and technique han,2,2,1.0
technique han wang,2,2,1.0
han wang mao,2,2,1.0
wang mao a,2,2,1.0
mao a new,3,3,1.0
a new method,4,4,1.0
new method in,3,3,1.0
method in imbalanced,5,5,1.0
in imbalanced data,7,5,1.4
data sets learning,6,5,1.2
sets learning in,4,4,1.0
learning in international,2,2,1.0
conference on intelligent,3,3,1.0
on intelligent computing,3,3,1.0
intelligent computing hanney,2,2,1.0
computing hanney keane,2,2,1.0
hanney keane learning,2,2,1.0
keane learning adaptation,2,2,1.0
learning adaptation rules,2,2,1.0
adaptation rules from,2,2,1.0
rules from a,2,2,1.0
from a in,2,2,1.0
a in of,1,1,1.0
in of the,2,1,2.0
of the third,2,2,1.0
the third european,2,2,1.0
third european workshop,2,2,1.0
european workshop on,4,2,2.0
workshop on reasoning,2,2,1.0
on reasoning berlin,2,2,1.0
reasoning berlin springer,2,2,1.0
berlin springer hasan,2,2,1.0
springer hasan use,2,2,1.0
hasan use case,2,2,1.0
use case of,2,2,1.0
case of counterfactual,2,2,1.0
of counterfactual examples,2,2,1.0
counterfactual examples data,2,2,1.0
examples data augmentation,2,2,1.0
data augmentation proceedings,2,2,1.0
augmentation proceedings of,2,2,1.0
proceedings of student,2,2,1.0
of student research,2,2,1.0
student research and,2,2,1.0
research and creative,2,2,1.0
and creative inquiry,2,2,1.0
creative inquiry day,2,2,1.0
inquiry day he,2,2,1.0
day he bai,2,2,1.0
he bai garcia,2,2,1.0
bai garcia li,2,2,1.0
garcia li adasyn,2,2,1.0
li adasyn adaptive,4,4,1.0
adasyn adaptive synthetic,4,4,1.0
adaptive synthetic sampling,3,3,1.0
synthetic sampling approach,3,3,1.0
sampling approach for,3,3,1.0
approach for imbalanced,5,4,1.25
for imbalanced learning,9,4,2.25
imbalanced learning in,6,4,1.5
learning in ieee,2,2,1.0
in ieee international,3,3,1.0
ieee international joint,2,2,1.0
conference on neural,9,3,3.0
on neural networks,24,4,6.0
neural networks temraz,2,2,1.0
networks temraz keane,2,2,1.0
data augmentation he,2,2,1.0
augmentation he garcia,2,2,1.0
he garcia learning,2,2,1.0
garcia learning from,4,4,1.0
imbalanced data ieee,7,5,1.4
data ieee transactions,3,3,1.0
transactions on knowledge,5,3,1.6666666666666667
on knowledge and,5,3,1.6666666666666667
knowledge and data,5,3,1.6666666666666667
and data engineering,5,3,1.6666666666666667
data engineering holte,2,2,1.0
engineering holte acker,2,2,1.0
holte acker porter,2,2,1.0
acker porter b,2,2,1.0
porter b concept,2,2,1.0
b concept learning,2,2,1.0
concept learning and,2,2,1.0
learning and accuracy,2,2,1.0
and accuracy of,2,2,1.0
accuracy of small,2,2,1.0
of small disjuncts,4,3,1.3333333333333333
small disjuncts in,2,2,1.0
disjuncts in proceedings,2,2,1.0
of the llth,2,2,1.0
the llth international,2,2,1.0
llth international joint,2,2,1.0
artificial intelligence hsu,2,2,1.0
intelligence hsu lin,2,2,1.0
hsu lin a,2,2,1.0
lin a comparison,2,2,1.0
a comparison of,4,4,1.0
comparison of methods,2,2,1.0
of methods for,2,2,1.0
methods for multiclass,2,2,1.0
for multiclass support,2,2,1.0
multiclass support vector,2,2,1.0
support vector ieee,1,1,1.0
vector ieee transactions,1,1,1.0
transactions on neural,19,4,4.75
neural networks hu,2,2,1.0
networks hu liang,2,2,1.0
hu liang ma,2,2,1.0
liang ma he,2,2,1.0
ma he y,2,2,1.0
he y msmote,2,2,1.0
y msmote improving,2,2,1.0
msmote improving classification,2,2,1.0
improving classification when,1,1,1.0
classification when training,1,1,1.0
when training data,4,4,1.0
training data is,2,2,1.0
data is imbalanced,2,2,1.0
is imbalanced in,2,2,1.0
imbalanced in proceedings,2,2,1.0
of the second,2,2,1.0
the second international,2,2,1.0
second international workshop,2,2,1.0
international workshop on,3,3,1.0
workshop on computer,2,2,1.0
on computer science,3,3,1.0
science and engineering,6,4,1.5
and engineering jeni,2,2,1.0
engineering jeni cohn,2,2,1.0
jeni cohn torre,2,2,1.0
cohn torre facing,2,2,1.0
torre facing imbalanced,2,2,1.0
facing imbalanced data,2,2,1.0
imbalanced data recommendations,2,2,1.0
data recommendations for,2,2,1.0
recommendations for the,2,2,1.0
for the use,2,2,1.0
use of performance,2,2,1.0
of performance metrics,2,2,1.0
performance metrics in,2,2,1.0
metrics in proceedings,2,2,1.0
of the humaine,2,2,1.0
the humaine association,2,2,1.0
humaine association conference,2,2,1.0
association conference on,2,2,1.0
conference on affective,2,2,1.0
on affective computing,2,2,1.0
affective computing and,2,2,1.0
computing and intelligent,2,2,1.0
and intelligent interaction,2,2,1.0
intelligent interaction jiang,2,2,1.0
interaction jiang lu,2,2,1.0
jiang lu xia,2,2,1.0
lu xia a,2,2,1.0
xia a novel,2,2,1.0
a novel algorithm,2,2,1.0
novel algorithm for,2,2,1.0
algorithm for imbalance,2,2,1.0
for imbalance data,2,2,1.0
imbalance data classification,2,2,1.0
data classification based,1,1,1.0
classification based on,1,1,1.0
based on genetic,3,3,1.0
on genetic algorithm,3,3,1.0
genetic algorithm improved,2,2,1.0
algorithm improved smote,2,2,1.0
improved smote arabian,2,2,1.0
smote arabian journal,2,2,1.0
arabian journal for,2,2,1.0
journal for science,2,2,1.0
for science and,2,2,1.0
and engineering karimi,2,2,1.0
engineering karimi barthe,2,2,1.0
karimi barthe schölkopf,2,2,1.0
barthe schölkopf valera,2,2,1.0
schölkopf valera i,2,2,1.0
valera i a,2,2,1.0
i a survey,2,2,1.0
a survey of,2,2,1.0
survey of algorithmic,2,2,1.0
of algorithmic recourse,2,2,1.0
algorithmic recourse definitions,2,2,1.0
recourse definitions formulations,2,2,1.0
definitions formulations solutions,2,2,1.0
formulations solutions and,2,2,1.0
solutions and prospects,2,2,1.0
and prospects keane,2,2,1.0
prospects keane kenny,2,2,1.0
keane kenny delaney,2,2,1.0
kenny delaney smyth,2,2,1.0
delaney smyth b,2,2,1.0
smyth b if,2,2,1.0
b if only,2,2,1.0
if only we,2,2,1.0
only we had,2,2,1.0
we had better,2,2,1.0
had better counterfactual,2,2,1.0
better counterfactual explanations,2,2,1.0
explanations in proceedings,2,2,1.0
of the international,22,5,4.4
the international joint,6,4,1.5
artificial intelligence montreal,2,2,1.0
intelligence montreal canada,2,2,1.0
montreal canada keane,2,2,1.0
canada keane smyth,2,2,1.0
keane smyth b,2,2,1.0
smyth b good,2,2,1.0
b good counterfactuals,2,2,1.0
good counterfactuals and,2,2,1.0
counterfactuals and how,2,2,1.0
and how to,2,2,1.0
how to find,2,2,1.0
to find them,2,2,1.0
find them in,2,2,1.0
them in international,2,2,1.0
springer germany kotsiantis,2,2,1.0
germany kotsiantis supervised,2,2,1.0
kotsiantis supervised machine,2,2,1.0
supervised machine learning,2,2,1.0
machine learning a,3,3,1.0
learning a review,2,2,1.0
a review of,2,2,1.0
review of classification,2,2,1.0
of classification techniques,2,2,1.0
classification techniques informatica,2,2,1.0
techniques informatica krawczyk,2,2,1.0
informatica krawczyk koziarski,2,2,1.0
krawczyk koziarski woźniak,2,2,1.0
koziarski woźniak oversampling,2,2,1.0
woźniak oversampling for,2,2,1.0
oversampling for multiclass,2,2,1.0
for multiclass imbalanced,2,2,1.0
multiclass imbalanced data,2,2,1.0
imbalanced data classification,3,3,1.0
data classification ieee,2,2,1.0
classification ieee transactions,2,2,1.0
neural networks and,4,4,1.0
networks and learning,3,3,1.0
and learning systems,3,3,1.0
learning systems kubat,2,2,1.0
systems kubat holte,2,2,1.0
kubat holte matwin,2,2,1.0
holte matwin machine,2,2,1.0
matwin machine learning,3,3,1.0
machine learning for,3,3,1.0
learning for the,3,3,1.0
for the detection,3,3,1.0
the detection of,3,3,1.0
radar images machine,2,2,1.0
images machine learning,2,2,1.0
machine learning lash,2,2,1.0
learning lash lin,2,2,1.0
lash lin street,2,2,1.0
lin street robinson,2,2,1.0
street robinson ohlmann,2,2,1.0
robinson ohlmann j,2,2,1.0
ohlmann j generalized,2,2,1.0
j generalized inverse,2,2,1.0
generalized inverse classification,2,2,1.0
inverse classification in,2,2,1.0
classification in proceedings,2,2,1.0
of the siam,2,2,1.0
the siam international,2,2,1.0
siam international conference,2,2,1.0
conference on data,5,3,1.6666666666666667
on data mining,5,3,1.6666666666666667
data mining temraz,2,2,1.0
mining temraz keane,2,2,1.0
data augmentation laugel,2,2,1.0
augmentation laugel lesot,2,2,1.0
laugel lesot marsala,2,2,1.0
lesot marsala renard,2,2,1.0
marsala renard detyniecki,2,2,1.0
renard detyniecki the,2,2,1.0
detyniecki the dangers,2,2,1.0
the dangers of,2,2,1.0
dangers of interpretability,2,2,1.0
of interpretability unjustified,2,2,1.0
interpretability unjustified counterfactual,2,2,1.0
unjustified counterfactual explanations,2,2,1.0
counterfactual explanations proceedings,2,2,1.0
explanations proceedings of,2,2,1.0
artificial intelligence lewis,2,2,1.0
intelligence lewis counterfactuals,2,2,1.0
lewis counterfactuals john,2,2,1.0
counterfactuals john wiley,2,2,1.0
john wiley sons,3,3,1.0
wiley sons ling,2,2,1.0
sons ling li,2,2,1.0
ling li data,2,2,1.0
li data mining,2,2,1.0
data mining for,3,3,1.0
mining for direct,2,2,1.0
for direct marketing,2,2,1.0
direct marketing problems,2,2,1.0
marketing problems and,2,2,1.0
problems and solutions,2,2,1.0
and solutions in,2,2,1.0
solutions in proceedings,2,2,1.0
of the fourth,3,3,1.0
the fourth international,3,3,1.0
fourth international conference,6,3,2.0
conference on knowledge,5,4,1.25
on knowledge discovery,5,4,1.25
data mining kdd,2,2,1.0
mining kdd luengo,2,2,1.0
kdd luengo fernandez,2,2,1.0
luengo fernandez garcia,2,2,1.0
garcia herrera addressing,2,2,1.0
herrera addressing data,2,2,1.0
addressing data complexity,2,2,1.0
data complexity for,2,2,1.0
complexity for imbalanced,2,2,1.0
data sets analysis,2,2,1.0
sets analysis of,2,2,1.0
analysis of oversampling,2,2,1.0
of oversampling and,3,3,1.0
oversampling and evolutionally,1,1,1.0
and evolutionally underdamping,1,1,1.0
evolutionally underdamping soft,1,1,1.0
underdamping soft computing,2,2,1.0
soft computing luo,2,2,1.0
computing luo liu,2,2,1.0
luo liu minority,2,2,1.0
liu minority oversampling,2,2,1.0
minority oversampling for,2,2,1.0
oversampling for imbalanced,2,2,1.0
for imbalanced classification,2,2,1.0
imbalanced classification ma,2,2,1.0
classification ma fan,2,2,1.0
ma fan algorithm,2,2,1.0
fan algorithm and,2,2,1.0
algorithm and hybrid,2,2,1.0
and hybrid algorithm,2,2,1.0
hybrid algorithm for,2,2,1.0
algorithm for feature,2,2,1.0
for feature and,1,1,1.0
feature and parameter,1,1,1.0
and parameter optimization,2,2,1.0
parameter optimization based,2,2,1.0
optimization based on,2,2,1.0
based on random,6,3,2.0
on random forests,2,2,1.0
random forests bmc,2,2,1.0
forests bmc bioinformatics,2,2,1.0
bmc bioinformatics maciejewski,2,2,1.0
bioinformatics maciejewski stefanowski,2,2,1.0
maciejewski stefanowski j,2,2,1.0
stefanowski j local,2,2,1.0
j local neighbourhood,2,2,1.0
local neighbourhood extension,2,2,1.0
neighbourhood extension of,2,2,1.0
extension of smote,2,2,1.0
of smote for,3,3,1.0
smote for mining,2,2,1.0
for mining imbalanced,2,2,1.0
mining imbalanced data,4,3,1.3333333333333333
data ieee symposium,2,2,1.0
ieee symposium on,2,2,1.0
symposium on computational,3,3,1.0
on computational intelligence,2,2,1.0
computational intelligence and,2,2,1.0
intelligence and data,2,2,1.0
data mining cidm,2,2,1.0
mining cidm mckenna,2,2,1.0
cidm mckenna smyth,2,2,1.0
mckenna smyth b,2,2,1.0
smyth b editing,2,2,1.0
b editing techniques,2,2,1.0
editing techniques in,2,2,1.0
techniques in european,2,2,1.0
in european workshop,2,2,1.0
workshop on advances,2,2,1.0
advances in reasoning,2,2,1.0
in reasoning mothilal,2,2,1.0
reasoning mothilal sharma,2,2,1.0
mothilal sharma tan,2,2,1.0
sharma tan explaining,2,2,1.0
tan explaining machine,2,2,1.0
explaining machine learning,2,2,1.0
machine learning classifiers,2,2,1.0
learning classifiers through,2,2,1.0
classifiers through diverse,2,2,1.0
through diverse counterfactual,2,2,1.0
counterfactual explanations conference,2,2,1.0
explanations conference on,2,2,1.0
conference on fairness,2,2,1.0
on fairness accountability,2,2,1.0
fairness accountability and,2,2,1.0
accountability and transparency,2,2,1.0
and transparency fat,2,2,1.0
transparency fat ng,2,2,1.0
fat ng hu,2,2,1.0
ng hu yeung,2,2,1.0
hu yeung yin,2,2,1.0
yeung yin roli,2,2,1.0
yin roli diversified,2,2,1.0
roli diversified undersampling,2,2,1.0
diversified undersampling for,2,2,1.0
undersampling for imbalance,2,2,1.0
for imbalance classification,2,2,1.0
imbalance classification problems,2,2,1.0
classification problems ieee,2,2,1.0
problems ieee transactions,2,2,1.0
transactions on cybernetics,2,2,1.0
on cybernetics nguyen,2,2,1.0
cybernetics nguyen cooper,2,2,1.0
nguyen cooper kamei,2,2,1.0
cooper kamei borderline,2,2,1.0
kamei borderline for,3,3,1.0
borderline for data,1,1,1.0
for data classification,1,1,1.0
data classification international,2,2,1.0
classification international journal,2,2,1.0
international journal of,11,4,2.75
journal of knowledge,3,3,1.0
of knowledge engineering,3,3,1.0
knowledge engineering and,3,3,1.0
engineering and soft,3,3,1.0
and soft data,3,3,1.0
soft data paradigms,2,2,1.0
data paradigms nugent,2,2,1.0
paradigms nugent doyle,2,2,1.0
nugent doyle cunningham,2,2,1.0
doyle cunningham gaining,2,2,1.0
cunningham gaining insight,2,2,1.0
gaining insight through,2,2,1.0
insight through explanation,2,2,1.0
through explanation journal,2,2,1.0
explanation journal of,2,2,1.0
journal of intelligent,2,2,1.0
of intelligent information,2,2,1.0
intelligent information systems,2,2,1.0
information systems pitis,2,2,1.0
systems pitis creager,2,2,1.0
pitis creager garg,2,2,1.0
creager garg a,2,2,1.0
garg a counterfactual,2,2,1.0
a counterfactual data,2,2,1.0
augmentation using locally,2,2,1.0
using locally factored,2,2,1.0
locally factored dynamics,2,2,1.0
factored dynamics advances,2,2,1.0
dynamics advances in,2,2,1.0
advances in neural,3,3,1.0
in neural information,3,3,1.0
neural information processing,8,4,2.0
information processing systems,4,3,1.3333333333333333
processing systems temraz,2,2,1.0
systems temraz keane,2,2,1.0
data augmentation provost,2,2,1.0
augmentation provost machine,2,2,1.0
provost machine learning,2,2,1.0
machine learning from,2,2,1.0
data sets aaai,2,2,1.0
sets aaai workshop,2,2,1.0
aaai workshop on,3,3,1.0
workshop on imbalanced,2,2,1.0
on imbalanced data,7,3,2.3333333333333335
data sets prusty,2,2,1.0
sets prusty jayanthi,2,2,1.0
prusty jayanthi velusamy,2,2,1.0
jayanthi velusamy a,2,2,1.0
velusamy a modification,2,2,1.0
a modification to,2,2,1.0
modification to smote,2,2,1.0
to smote for,2,2,1.0
smote for event,2,2,1.0
for event classification,2,2,1.0
event classification in,2,2,1.0
classification in sodium,2,2,1.0
in sodium cooled,2,2,1.0
sodium cooled fast,2,2,1.0
cooled fast reactors,2,2,1.0
fast reactors progress,2,2,1.0
reactors progress in,2,2,1.0
progress in nuclear,2,2,1.0
in nuclear energy,2,2,1.0
nuclear energy ramentol,2,2,1.0
energy ramentol caballero,2,2,1.0
ramentol caballero bello,2,2,1.0
caballero bello herrera,2,2,1.0
bello herrera a,2,2,1.0
herrera a hybrid,2,2,1.0
a hybrid preprocessing,2,2,1.0
hybrid preprocessing approach,2,2,1.0
preprocessing approach based,2,2,1.0
on oversampling and,2,2,1.0
oversampling and undersampling,2,2,1.0
and undersampling for,2,2,1.0
undersampling for high,2,2,1.0
for high imbalanced,2,2,1.0
high imbalanced using,2,2,1.0
imbalanced using smote,2,2,1.0
using smote and,2,2,1.0
smote and rough,2,2,1.0
and rough sets,2,2,1.0
rough sets theory,2,2,1.0
sets theory knowledge,2,2,1.0
theory knowledge and,2,2,1.0
knowledge and information,2,2,1.0
and information systems,2,2,1.0
information systems ryan,2,2,1.0
systems ryan gúeret,2,2,1.0
ryan gúeret berry,2,2,1.0
gúeret berry corcoran,2,2,1.0
berry corcoran keane,2,2,1.0
corcoran keane mac,2,2,1.0
keane mac namee,2,2,1.0
mac namee b,2,2,1.0
namee b predicting,2,2,1.0
b predicting illness,2,2,1.0
predicting illness for,2,2,1.0
illness for a,2,2,1.0
for a sustainable,2,2,1.0
a sustainable dairy,2,2,1.0
sustainable dairy agriculture,2,2,1.0
dairy agriculture predicting,2,2,1.0
agriculture predicting and,2,2,1.0
predicting and explaining,2,2,1.0
and explaining the,2,2,1.0
explaining the onset,2,2,1.0
the onset of,2,2,1.0
onset of mastitis,2,2,1.0
of mastitis in,2,2,1.0
mastitis in dairy,2,2,1.0
in dairy cows,2,2,1.0
dairy cows in,2,2,1.0
cows in workshop,2,2,1.0
in workshop on,2,2,1.0
workshop on explainable,2,2,1.0
on explainable agency,2,2,1.0
explainable agency in,2,2,1.0
agency in ai,2,2,1.0
in ai xai,2,2,1.0
ai xai sandhan,2,2,1.0
xai sandhan choi,2,2,1.0
sandhan choi y,2,2,1.0
choi y handling,2,2,1.0
y handling imbalanced,2,2,1.0
handling imbalanced datasets,3,3,1.0
imbalanced datasets by,2,2,1.0
datasets by partially,2,2,1.0
by partially guided,2,2,1.0
partially guided sampling,1,1,1.0
guided sampling for,1,1,1.0
sampling for pattern,2,2,1.0
for pattern recognition,2,2,1.0
pattern recognition international,2,2,1.0
recognition international conference,2,2,1.0
conference on pattern,2,2,1.0
on pattern recognition,2,2,1.0
pattern recognition schleich,2,2,1.0
recognition schleich geng,2,2,1.0
schleich geng zhang,2,2,1.0
geng zhang suciu,2,2,1.0
zhang suciu geco,2,2,1.0
suciu geco quality,2,2,1.0
geco quality counterfactual,2,2,1.0
quality counterfactual explanations,2,2,1.0
explanations in real,2,2,1.0
in real time,2,2,1.0
real time arxiv,2,2,1.0
time arxiv preprint,2,2,1.0
arxiv preprint shorten,2,2,1.0
preprint shorten khoshgoftaar,2,2,1.0
shorten khoshgoftaar a,2,2,1.0
khoshgoftaar a survey,2,2,1.0
a survey on,2,2,1.0
survey on image,2,2,1.0
on image data,2,2,1.0
image data augmentation,2,2,1.0
deep learning journal,2,2,1.0
learning journal of,3,3,1.0
journal of big,2,2,1.0
of big data,2,2,1.0
big data smyth,2,2,1.0
data smyth keane,2,2,1.0
smyth keane a,2,2,1.0
keane a few,2,2,1.0
a few good,2,2,1.0
few good counterfactuals,2,2,1.0
good counterfactuals generating,2,2,1.0
counterfactuals generating interpretable,2,2,1.0
generating interpretable plausible,2,2,1.0
interpretable plausible and,2,2,1.0
plausible and diverse,2,2,1.0
and diverse counterfactual,2,2,1.0
counterfactual explanations arxiv,2,2,1.0
explanations arxiv preprint,2,2,1.0
arxiv preprint subbaswamy,2,2,1.0
preprint subbaswamy saria,2,2,1.0
subbaswamy saria counterfactual,2,2,1.0
saria counterfactual normalization,2,2,1.0
counterfactual normalization proactively,2,2,1.0
normalization proactively addressing,2,2,1.0
proactively addressing dataset,2,2,1.0
addressing dataset shift,2,2,1.0
dataset shift using,2,2,1.0
shift using causal,2,2,1.0
using causal mechanisms,2,2,1.0
causal mechanisms in,2,2,1.0
mechanisms in proceedings,2,2,1.0
conference on uncertainty,3,3,1.0
on uncertainty in,3,3,1.0
uncertainty in artificial,3,3,1.0
in artificial intelligence,3,3,1.0
artificial intelligence uai,2,2,1.0
intelligence uai tek,2,2,1.0
uai tek dempster,2,2,1.0
tek dempster kale,2,2,1.0
dempster kale parasite,2,2,1.0
kale parasite detection,2,2,1.0
parasite detection and,2,2,1.0
detection and identification,2,2,1.0
and identification for,2,2,1.0
identification for automated,2,2,1.0
for automated thin,2,2,1.0
automated thin blood,2,2,1.0
thin blood film,2,2,1.0
blood film malaria,2,2,1.0
film malaria diagnosis,2,2,1.0
malaria diagnosis computer,2,2,1.0
diagnosis computer vision,2,2,1.0
computer vision and,3,3,1.0
vision and image,2,2,1.0
and image understanding,2,2,1.0
image understanding temraz,2,2,1.0
understanding temraz kenny,2,2,1.0
temraz kenny ruelle,2,2,1.0
kenny ruelle shalloo,2,2,1.0
ruelle shalloo smyth,2,2,1.0
shalloo smyth keane,2,2,1.0
smyth keane handling,2,2,1.0
keane handling climate,2,2,1.0
handling climate change,2,2,1.0
climate change using,2,2,1.0
change using counterfactuals,2,2,1.0
using counterfactuals using,2,2,1.0
counterfactuals using counterfactuals,2,2,1.0
using counterfactuals in,2,2,1.0
counterfactuals in data,2,2,1.0
augmentation to predict,1,1,1.0
to predict crop,2,2,1.0
predict crop growth,2,2,1.0
crop growth in,2,2,1.0
growth in an,2,2,1.0
in an uncertain,2,2,1.0
an uncertain climate,2,2,1.0
uncertain climate future,2,2,1.0
climate future in,2,2,1.0
future in international,2,2,1.0
springer germany torres,2,2,1.0
germany torres a,2,2,1.0
torres a deterministic,2,2,1.0
a deterministic version,2,2,1.0
deterministic version of,2,2,1.0
version of smote,9,3,3.0
of smote pattern,2,2,1.0
smote pattern recognition,2,2,1.0
pattern recognition temraz,2,2,1.0
recognition temraz keane,2,2,1.0
data augmentation vapnik,2,2,1.0
augmentation vapnik statistical,2,2,1.0
vapnik statistical learning,2,2,1.0
statistical learning theory,2,2,1.0
learning theory wiley,2,2,1.0
theory wiley wachter,2,2,1.0
wiley wachter mittelstadt,2,2,1.0
wachter mittelstadt russell,2,2,1.0
mittelstadt russell counterfactual,2,2,1.0
russell counterfactual explanations,2,2,1.0
counterfactual explanations without,2,2,1.0
explanations without opening,2,2,1.0
without opening the,2,2,1.0
opening the black,2,2,1.0
the black box,2,2,1.0
black box automated,2,2,1.0
box automated decisions,2,2,1.0
automated decisions and,2,2,1.0
decisions and the,2,2,1.0
and the gdpr,2,2,1.0
the gdpr harvard,2,2,1.0
gdpr harvard journal,2,2,1.0
harvard journal of,2,2,1.0
journal of law,2,2,1.0
of law technology,2,2,1.0
law technology wang,2,2,1.0
technology wang xu,2,2,1.0
wang xu wang,2,2,1.0
xu wang zhang,2,2,1.0
wang zhang j,2,2,1.0
zhang j classification,2,2,1.0
j classification of,2,2,1.0
classification of imbalanced,3,3,1.0
of imbalanced data,5,5,1.0
data by using,2,2,1.0
by using the,4,4,1.0
using the smote,3,3,1.0
the smote algorithm,10,6,1.6666666666666667
smote algorithm and,2,2,1.0
algorithm and locally,2,2,1.0
and locally linear,2,2,1.0
locally linear embedding,2,2,1.0
linear embedding international,2,2,1.0
embedding international conference,2,2,1.0
conference on signal,2,2,1.0
on signal processing,2,2,1.0
signal processing weiss,2,2,1.0
processing weiss mccarthy,2,2,1.0
weiss mccarthy zabar,2,2,1.0
mccarthy zabar b,2,2,1.0
zabar b learning,2,2,1.0
b learning sampling,2,2,1.0
learning sampling which,2,2,1.0
sampling which is,2,2,1.0
which is best,2,2,1.0
is best for,2,2,1.0
best for handling,2,2,1.0
for handling unbalanced,2,2,1.0
handling unbalanced classes,2,2,1.0
unbalanced classes with,2,2,1.0
classes with unequal,2,2,1.0
with unequal error,2,2,1.0
unequal error costs,2,2,1.0
error costs in,2,2,1.0
costs in of,1,1,1.0
the international conference,15,5,3.0
data mining dmin,2,2,1.0
mining dmin wen,2,2,1.0
dmin wen sun,2,2,1.0
wen sun yang,2,2,1.0
sun yang song,2,2,1.0
yang song gao,2,2,1.0
song gao xue,2,2,1.0
gao xue huan,2,2,1.0
xue huan x,2,2,1.0
huan x time,2,2,1.0
x time series,2,2,1.0
time series data,2,2,1.0
series data augmentation,2,2,1.0
deep learning a,2,2,1.0
learning a survey,2,2,1.0
a survey arxiv,2,2,1.0
survey arxiv preprint,2,2,1.0
arxiv preprint wong,2,2,1.0
preprint wong gatt,2,2,1.0
wong gatt stamatescu,2,2,1.0
gatt stamatescu mcdonnell,2,2,1.0
stamatescu mcdonnell understanding,2,2,1.0
mcdonnell understanding data,2,2,1.0
understanding data augmentation,2,2,1.0
augmentation for classification,2,2,1.0
for classification when,2,2,1.0
classification when to,2,2,1.0
when to warp,2,2,1.0
to warp in,2,2,1.0
warp in international,2,2,1.0
conference on digital,2,2,1.0
on digital image,2,2,1.0
digital image computing,2,2,1.0
image computing techniques,2,2,1.0
computing techniques and,2,2,1.0
techniques and applications,2,2,1.0
and applications dicta,2,2,1.0
applications dicta ye,2,2,1.0
dicta ye leake,2,2,1.0
ye leake jalali,2,2,1.0
leake jalali crandall,2,2,1.0
jalali crandall learning,2,2,1.0
crandall learning adaptations,2,2,1.0
learning adaptations for,2,2,1.0
adaptations for classification,2,2,1.0
for classification a,2,2,1.0
classification a neural,2,2,1.0
a neural network,3,3,1.0
neural network approach,2,2,1.0
network approach in,2,2,1.0
approach in international,2,2,1.0
springer germany yun,2,2,1.0
germany yun ha,2,2,1.0
yun ha lee,2,2,1.0
ha lee j,2,2,1.0
lee j automatic,2,2,1.0
j automatic determination,2,2,1.0
automatic determination of,2,2,1.0
determination of neighborhood,2,2,1.0
of neighborhood size,2,2,1.0
neighborhood size in,2,2,1.0
size in smote,2,2,1.0
in smote in,2,2,1.0
smote in proceedings,2,2,1.0
conference on ubiquitous,2,2,1.0
on ubiquitous information,2,2,1.0
ubiquitous information management,2,2,1.0
information management and,2,2,1.0
management and communication,2,2,1.0
and communication zeng,2,2,1.0
communication zeng li,2,2,1.0
zeng li zhai,2,2,1.0
li zhai zhang,2,2,1.0
zhai zhang y,2,2,1.0
zhang y counterfactual,2,2,1.0
y counterfactual generator,2,2,1.0
counterfactual generator in,2,2,1.0
generator in proceedings,2,2,1.0
conference on empirical,2,2,1.0
on empirical methods,2,2,1.0
empirical methods in,2,2,1.0
methods in natural,2,2,1.0
in natural language,2,2,1.0
natural language processing,2,2,1.0
language processing zheng,2,2,1.0
processing zheng wu,2,2,1.0
zheng wu srihari,2,2,1.0
wu srihari feature,2,2,1.0
srihari feature selection,3,3,1.0
feature selection for,5,3,1.6666666666666667
selection for text,3,3,1.0
for text categorization,3,3,1.0
text categorization on,3,3,1.0
categorization on imbalanced,3,3,1.0
imbalanced data acm,2,2,1.0
data acm sigkdd,3,3,1.0
acm sigkdd explorations,7,3,2.3333333333333335
journal of computational,8,1,8.0
of computational intelligence,8,1,8.0
computational intelligence systems,8,1,8.0
intelligence systems vol,1,1,1.0
systems vol pp,2,2,1.0
vol pp doi,1,1,1.0
pp doi https,1,1,1.0
doi https issn,1,1,1.0
https issn eissn,1,1,1.0
issn eissn https,1,1,1.0
eissn https distributed,1,1,1.0
https distributed synthetic,1,1,1.0
distributed synthetic minority,1,1,1.0
oversampling technique sakshi,1,1,1.0
technique sakshi suman,1,1,1.0
sakshi suman research,1,1,1.0
suman research scholar,1,1,1.0
research scholar ipu,1,1,1.0
scholar ipu new,1,1,1.0
ipu new delhi,1,1,1.0
new delhi india,2,1,2.0
delhi india associate,1,1,1.0
india associate professor,1,1,1.0
associate professor msit,1,1,1.0
professor msit new,1,1,1.0
msit new delhi,1,1,1.0
delhi india a,1,1,1.0
india a rt,1,1,1.0
a rt i,1,1,1.0
rt i c,1,1,1.0
i c l,1,1,1.0
c l e,1,1,1.0
l e i,1,1,1.0
e i n,1,1,1.0
i n f,1,1,1.0
n f o,1,1,1.0
f o article,1,1,1.0
o article history,1,1,1.0
article history received,1,1,1.0
history received may,1,1,1.0
received may accepted,1,1,1.0
may accepted jul,1,1,1.0
accepted jul keywords,1,1,1.0
jul keywords smote,1,1,1.0
keywords smote apache,1,1,1.0
smote apache spark,1,1,1.0
apache spark prediction,1,1,1.0
spark prediction machine,1,1,1.0
prediction machine learning,1,1,1.0
machine learning imbalanced,1,1,1.0
learning imbalanced classification,1,1,1.0
imbalanced classification a,1,1,1.0
classification a b,1,1,1.0
a b s,1,1,1.0
b s t,1,1,1.0
s t r,1,1,1.0
t r ac,1,1,1.0
r ac t,1,1,1.0
ac t real,1,1,1.0
t real world,1,1,1.0
real world problems,1,1,1.0
world problems for,1,1,1.0
problems for prediction,1,1,1.0
for prediction usually,1,1,1.0
prediction usually try,1,1,1.0
usually try to,1,1,1.0
try to predict,1,1,1.0
to predict rare,1,1,1.0
predict rare occurrences,1,1,1.0
rare occurrences application,1,1,1.0
occurrences application of,1,1,1.0
application of standard,1,1,1.0
of standard classification,1,1,1.0
standard classification algorithm,1,1,1.0
classification algorithm is,1,1,1.0
algorithm is biased,1,1,1.0
is biased toward,1,1,1.0
biased toward against,1,1,1.0
toward against these,1,1,1.0
against these rare,1,1,1.0
these rare events,2,1,2.0
rare events due,1,1,1.0
events due to,1,1,1.0
due to this,1,1,1.0
to this data,1,1,1.0
this data imbalance,2,1,2.0
data imbalance typicalapproaches,1,1,1.0
imbalance typicalapproaches to,1,1,1.0
typicalapproaches to solve,1,1,1.0
to solve this,2,1,2.0
solve this data,1,1,1.0
data imbalance involve,1,1,1.0
imbalance involve oversampling,1,1,1.0
involve oversampling these,1,1,1.0
oversampling these rare,1,1,1.0
rare events or,1,1,1.0
events or under,1,1,1.0
or under sampling,1,1,1.0
under sampling the,1,1,1.0
sampling the majority,1,1,1.0
the majority occurring,1,1,1.0
majority occurring events,1,1,1.0
occurring events synthetic,1,1,1.0
events synthetic minority,1,1,1.0
oversampling technique is,1,1,1.0
technique is one,1,1,1.0
is one technique,1,1,1.0
one technique that,1,1,1.0
technique that addresses,1,1,1.0
that addresses this,1,1,1.0
addresses this class,1,1,1.0
this class imbalance,1,1,1.0
class imbalance effectively,1,1,1.0
imbalance effectively however,1,1,1.0
effectively however the,1,1,1.0
however the existing,1,1,1.0
the existing implementations,1,1,1.0
existing implementations of,1,1,1.0
implementations of smote,1,1,1.0
of smote fail,1,1,1.0
smote fail when,1,1,1.0
fail when data,1,1,1.0
when data grows,1,1,1.0
data grows and,1,1,1.0
grows and can,1,1,1.0
and can t,1,1,1.0
can t be,2,1,2.0
t be stored,1,1,1.0
be stored on,1,1,1.0
stored on a,1,1,1.0
on a single,3,1,3.0
a single machine,6,1,6.0
single machine in,1,1,1.0
machine in this,1,1,1.0
this paper present,1,1,1.0
paper present our,1,1,1.0
present our solution,1,1,1.0
our solution to,1,1,1.0
solution to address,1,1,1.0
to address the,6,4,1.5
address the big,1,1,1.0
the big data,1,1,1.0
big data challenge,1,1,1.0
data challenge we,1,1,1.0
challenge we provide,1,1,1.0
we provide a,2,2,1.0
provide a distributed,1,1,1.0
a distributed version,1,1,1.0
distributed version of,2,1,2.0
of smote by,1,1,1.0
smote by using,1,1,1.0
by using scalable,1,1,1.0
using scalable and,1,1,1.0
scalable and with,1,1,1.0
and with this,1,1,1.0
with this implementation,2,1,2.0
this implementation of,3,2,1.5
implementation of smote,4,1,4.0
of smote we,3,1,3.0
smote we were,1,1,1.0
we were able,1,1,1.0
were able to,1,1,1.0
able to oversample,1,1,1.0
to oversample the,1,1,1.0
oversample the rare,1,1,1.0
the rare events,1,1,1.0
rare events and,1,1,1.0
events and achieve,1,1,1.0
and achieve results,1,1,1.0
achieve results which,1,1,1.0
results which are,1,1,1.0
which are better,1,1,1.0
are better than,1,1,1.0
better than the,2,2,1.0
than the existing,1,1,1.0
the existing python,1,1,1.0
existing python version,1,1,1.0
python version of,5,1,5.0
smote the authors,1,1,1.0
the authors published,1,1,1.0
authors published by,1,1,1.0
published by atlantis,1,1,1.0
by atlantis press,1,1,1.0
atlantis press sarl,1,1,1.0
press sarl this,1,1,1.0
sarl this is,1,1,1.0
this is an,1,1,1.0
is an open,1,1,1.0
an open access,1,1,1.0
open access article,1,1,1.0
access article distributed,1,1,1.0
article distributed under,1,1,1.0
distributed under the,1,1,1.0
under the cc,1,1,1.0
the cc license,1,1,1.0
cc license http,1,1,1.0
license http introduction,1,1,1.0
http introduction in,1,1,1.0
introduction in the,1,1,1.0
in the aspect,1,1,1.0
the aspect of,1,1,1.0
aspect of machine,1,1,1.0
machine learning one,1,1,1.0
learning one of,1,1,1.0
most popular issues,1,1,1.0
popular issues involves,1,1,1.0
issues involves classification,1,1,1.0
involves classification in,1,1,1.0
classification in particular,1,1,1.0
in particular it,1,1,1.0
particular it is,1,1,1.0
it is expected,1,1,1.0
is expected that,1,1,1.0
expected that the,1,1,1.0
that the data,1,1,1.0
the data sets,6,2,3.0
data sets containing,1,1,1.0
sets containing information,1,1,1.0
containing information for,1,1,1.0
information for extraction,1,1,1.0
for extraction constitute,1,1,1.0
extraction constitute all,1,1,1.0
constitute all the,1,1,1.0
all the required,1,1,1.0
the required content,1,1,1.0
required content through,1,1,1.0
content through which,1,1,1.0
through which relevant,1,1,1.0
which relevant concepts,1,1,1.0
relevant concepts can,1,1,1.0
concepts can be,1,1,1.0
can be learned,1,1,1.0
be learned especially,1,1,1.0
learned especially in,1,1,1.0
especially in relation,1,1,1.0
in relation to,5,2,2.5
relation to certain,1,1,1.0
to certain underlying,1,1,1.0
certain underlying generating,1,1,1.0
underlying generating functions,1,1,1.0
generating functions given,1,1,1.0
functions given a,1,1,1.0
given a sification,1,1,1.0
a sification problem,1,1,1.0
sification problem what,1,1,1.0
problem what remains,1,1,1.0
what remains notable,1,1,1.0
remains notable is,1,1,1.0
notable is that,1,1,1.0
that it is,4,3,1.3333333333333333
it is challenging,1,1,1.0
is challenging to,1,1,1.0
challenging to predict,1,1,1.0
predict the outcomes,1,1,1.0
the outcomes some,1,1,1.0
outcomes some of,1,1,1.0
of these classification,1,1,1.0
these classification problems,1,1,1.0
classification problems include,1,1,1.0
problems include oil,1,1,1.0
include oil spills,1,1,1.0
oil spills fraud,1,1,1.0
spills fraud detection,1,1,1.0
fraud detection and,1,1,1.0
detection and intrusion,1,1,1.0
and intrusion detection,2,2,1.0
intrusion detection a,1,1,1.0
detection a dilemma,1,1,1.0
a dilemma that,1,1,1.0
dilemma that translates,1,1,1.0
that translates into,1,1,1.0
translates into a,1,1,1.0
into a state,1,1,1.0
a state of,1,1,1.0
state of class,1,1,1.0
imbalance in classes,1,1,1.0
in classes it,1,1,1.0
classes it is,1,1,1.0
is also worth,1,1,1.0
also worth indicating,1,1,1.0
worth indicating that,1,1,1.0
indicating that the,1,1,1.0
that the resultant,1,1,1.0
the resultant state,1,1,1.0
resultant state of,1,1,1.0
state of asymmetry,1,1,1.0
of asymmetry could,1,1,1.0
asymmetry could be,1,1,1.0
could be puzzled,1,1,1.0
be puzzled due,1,1,1.0
puzzled due to,1,1,1.0
due to the,12,4,3.0
to the infrequent,1,1,1.0
the infrequent nature,1,1,1.0
infrequent nature of,1,1,1.0
nature of such,1,1,1.0
of such rare,1,1,1.0
such rare events,1,1,1.0
rare events instead,1,1,1.0
events instead the,1,1,1.0
instead the events,1,1,1.0
the events emerge,1,1,1.0
events emerge as,1,1,1.0
emerge as exceptional,1,1,1.0
as exceptional situations,1,1,1.0
exceptional situations that,1,1,1.0
situations that are,1,1,1.0
that are mostly,1,1,1.0
are mostly neglected,1,1,1.0
mostly neglected or,1,1,1.0
neglected or unexplored,1,1,1.0
or unexplored in,1,1,1.0
unexplored in some,1,1,1.0
in some cases,8,2,4.0
some cases these,1,1,1.0
cases these events,1,1,1.0
these events have,1,1,1.0
events have been,1,1,1.0
have been treated,1,1,1.0
been treated as,2,1,2.0
treated as extreme,1,1,1.0
as extreme values,1,1,1.0
extreme values including,1,1,1.0
values including outliers,1,1,1.0
including outliers or,1,1,1.0
outliers or noisy,1,1,1.0
or noisy disturbance,1,1,1.0
noisy disturbance it,1,1,1.0
disturbance it is,1,1,1.0
is also ironical,1,1,1.0
also ironical that,1,1,1.0
ironical that most,1,1,1.0
that most of,1,1,1.0
the classes attract,1,1,1.0
classes attract greater,1,1,1.0
attract greater concern,1,1,1.0
greater concern and,1,1,1.0
concern and much,1,1,1.0
and much significance,1,1,1.0
much significance and,1,1,1.0
significance and acknowledgment,1,1,1.0
and acknowledgment are,1,1,1.0
acknowledgment are extended,1,1,1.0
are extended to,1,1,1.0
extended to them,1,1,1.0
to them including,1,1,1.0
them including cases,1,1,1.0
including cases such,1,1,1.0
cases such as,1,1,1.0
such as those,1,1,1.0
as those involving,1,1,1.0
those involving the,1,1,1.0
involving the identification,1,1,1.0
the identification of,1,1,1.0
identification of credit,1,1,1.0
of credit card,1,1,1.0
credit card data,1,1,1.0
card data breaches,1,1,1.0
data breaches in,1,1,1.0
breaches in online,1,1,1.0
in online transactions,1,1,1.0
online transactions as,1,1,1.0
transactions as such,1,1,1.0
as such most,1,1,1.0
such most of,1,1,1.0
the data experiences,1,1,1.0
data experiences irregular,1,1,1.0
experiences irregular or,1,1,1.0
irregular or disproportionate,1,1,1.0
or disproportionate observation,1,1,1.0
disproportionate observation ratios,1,1,1.0
observation ratios in,1,1,1.0
ratios in ferent,1,1,1.0
in ferent classes,1,1,1.0
ferent classes and,1,1,1.0
classes and end,1,1,1.0
and end up,1,1,1.0
end up yielding,1,1,1.0
up yielding unacceptable,1,1,1.0
yielding unacceptable classification,1,1,1.0
unacceptable classification to,1,1,1.0
classification to fraud,1,1,1.0
to fraud case,1,1,1.0
fraud case identification,1,1,1.0
case identification this,1,1,1.0
identification this trend,1,1,1.0
this trend points,1,1,1.0
trend points to,1,1,1.0
points to the,1,1,1.0
to the criticality,1,1,1.0
the criticality of,1,1,1.0
criticality of a,1,1,1.0
of a relevant,1,1,1.0
a relevant model,1,1,1.0
relevant model capable,1,1,1.0
model capable of,1,1,1.0
capable of identifying,1,1,1.0
of identifying the,1,1,1.0
identifying the minority,1,1,1.0
minority classes or,1,1,1.0
classes or rare,1,1,1.0
or rare occurrences,1,1,1.0
rare occurrences in,1,1,1.0
occurrences in various,1,1,1.0
in various sets,1,1,1.0
various sets of,1,1,1.0
sets of data,6,1,6.0
of data associated,1,1,1.0
data associated with,1,1,1.0
associated with higher,1,1,1.0
with higher accuracy,1,1,1.0
higher accuracy as,1,1,1.0
accuracy as mentioned,1,1,1.0
as mentioned earlier,1,1,1.0
mentioned earlier most,1,1,1.0
earlier most of,1,1,1.0
the minority ples,2,1,2.0
minority ples have,1,1,1.0
ples have continually,1,1,1.0
have continually been,1,1,1.0
continually been treated,1,1,1.0
treated as noise,1,1,1.0
noise and ended,1,1,1.0
and ended up,1,1,1.0
ended up being,1,1,1.0
up being ignored,1,1,1.0
being ignored the,1,1,1.0
ignored the eventuality,1,1,1.0
the eventuality is,2,1,2.0
eventuality is that,2,1,2.0
is that valuable,1,1,1.0
that valuable data,1,1,1.0
valuable data has,1,1,1.0
data has been,1,1,1.0
has been lost,1,1,1.0
been lost relative,1,1,1.0
lost relative corresponding,1,1,1.0
relative corresponding author,1,1,1.0
corresponding author email,1,1,1.0
author email sakshihoodars,1,1,1.0
email sakshihoodars to,1,1,1.0
sakshihoodars to the,1,1,1.0
to the affected,1,1,1.0
the affected samples,1,1,1.0
affected samples an,1,1,1.0
samples an example,1,1,1.0
an example is,2,2,1.0
example is a,2,2,1.0
is a case,1,1,1.0
a case in,2,1,2.0
case in which,2,1,2.0
which a given,1,1,1.0
a given set,2,1,2.0
given set of,2,1,2.0
set of data,4,1,4.0
of data exhibits,1,1,1.0
data exhibits an,1,1,1.0
exhibits an imbalanced,1,1,1.0
an imbalanced ratio,1,1,1.0
imbalanced ratio in,2,2,1.0
ratio in the,1,1,1.0
in the form,2,1,2.0
the form this,1,1,1.0
form this ple,1,1,1.0
this ple ratio,1,1,1.0
ple ratio implies,1,1,1.0
ratio implies that,1,1,1.0
implies that training,1,1,1.0
that training examples,1,1,1.0
training examples of,1,1,1.0
examples of a,1,1,1.0
of a negative,1,1,1.0
a negative class,1,1,1.0
negative class cide,1,1,1.0
class cide with,1,1,1.0
cide with one,1,1,1.0
with one positive,1,1,1.0
one positive class,1,1,1.0
positive class training,1,1,1.0
class training sample,1,1,1.0
training sample when,1,1,1.0
sample when a,1,1,1.0
when a classifier,1,1,1.0
a classifier is,1,1,1.0
classifier is employed,1,1,1.0
is employed it,1,1,1.0
employed it could,1,1,1.0
it could generate,1,1,1.0
could generate the,1,1,1.0
generate the instances,1,1,1.0
the instances into,1,1,1.0
instances into negative,1,1,1.0
into negative forms,1,1,1.0
negative forms to,1,1,1.0
forms to ensure,1,1,1.0
to ensure that,7,1,7.0
ensure that the,3,1,3.0
that the classification,1,1,1.0
the classification rule,1,1,1.0
classification rule accuracy,1,1,1.0
rule accuracy is,1,1,1.0
accuracy is maximized,1,1,1.0
is maximized yielding,1,1,1.0
maximized yielding accuracy,1,1,1.0
yielding accuracy notably,1,1,1.0
accuracy notably there,1,1,1.0
notably there exist,1,1,1.0
there exist unique,1,1,1.0
exist unique categorizations,1,1,1.0
unique categorizations of,1,1,1.0
categorizations of solutions,1,1,1.0
of solutions through,1,1,1.0
solutions through which,1,1,1.0
through which problems,1,1,1.0
which problems linked,1,1,1.0
problems linked to,1,1,1.0
linked to imbalanced,1,1,1.0
to imbalanced classification,2,1,2.0
imbalanced classification could,1,1,1.0
classification could be,1,1,1.0
could be solved,1,1,1.0
be solved one,1,1,1.0
solved one of,1,1,1.0
one of such,1,1,1.0
of such approaches,1,1,1.0
such approaches entails,1,1,1.0
approaches entails the,1,1,1.0
entails the learning,1,1,1.0
the learning process,5,3,1.6666666666666667
learning process ification,1,1,1.0
process ification with,1,1,1.0
ification with class,1,1,1.0
with class dispersion,1,1,1.0
class dispersion on,1,1,1.0
dispersion on focus,1,1,1.0
on focus this,1,1,1.0
focus this process,1,1,1.0
this process is,1,1,1.0
process is achieved,1,1,1.0
is achieved by,2,2,1.0
achieved by focusing,1,1,1.0
by focusing on,1,1,1.0
on the undersampling,1,1,1.0
the undersampling majority,1,1,1.0
undersampling majority class,1,1,1.0
majority class oversampling,1,1,1.0
class oversampling minority,1,1,1.0
oversampling minority class,1,1,1.0
class or both,1,1,1.0
or both in,1,1,1.0
both in other,1,1,1.0
in other situations,1,1,1.0
other situations approaches,1,1,1.0
situations approaches can,1,1,1.0
approaches can be,1,1,1.0
can be employed,1,1,1.0
be employed to,1,1,1.0
employed to address,1,1,1.0
address the respective,1,1,1.0
the respective classes,3,2,1.5
respective classes states,1,1,1.0
classes states of,1,1,1.0
states of misclassification,1,1,1.0
of misclassification with,1,1,1.0
misclassification with growth,1,1,1.0
with growth in,1,1,1.0
growth in the,1,1,1.0
in the volume,2,2,1.0
the volume of,1,1,1.0
volume of data,2,1,2.0
of data it,1,1,1.0
data it is,1,1,1.0
is also important,2,1,2.0
also important to,2,1,2.0
important to modify,1,1,1.0
modify the traditional,1,1,1.0
the traditional methods,1,1,1.0
traditional methods of,1,1,1.0
methods of data,1,1,1.0
of data preprocessing,1,1,1.0
data preprocessing indeed,1,1,1.0
preprocessing indeed this,1,1,1.0
indeed this approach,2,1,2.0
this approach also,1,1,1.0
approach also applies,1,1,1.0
also applies to,1,1,1.0
applies to situations,1,1,1.0
to situations where,2,1,2.0
situations where a,1,1,1.0
where a single,1,1,1.0
single machine might,1,1,1.0
machine might not,1,1,1.0
might not be,2,1,2.0
not be better,1,1,1.0
be better placed,1,1,1.0
better placed to,1,1,1.0
placed to process,1,1,1.0
to process the,1,1,1.0
process the data,1,1,1.0
the data for,2,1,2.0
data for big,2,1,2.0
for big data,3,1,3.0
big data case,1,1,1.0
data case studies,1,1,1.0
case studies the,1,1,1.0
studies the scalability,1,1,1.0
the scalability issues,1,1,1.0
scalability issues related,1,1,1.0
issues related with,1,1,1.0
related with data,1,1,1.0
with data preprocessing,1,1,1.0
data preprocessing need,1,1,1.0
preprocessing need to,1,1,1.0
to be tackled,1,1,1.0
be tackled appropriately,1,1,1.0
tackled appropriately in,1,1,1.0
appropriately in order,1,1,1.0
order to formulate,1,1,1.0
to formulate novel,1,1,1.0
formulate novel solutions,1,1,1.0
novel solutions or,1,1,1.0
solutions or accommodate,1,1,1.0
or accommodate existing,1,1,1.0
accommodate existing ones,1,1,1.0
existing ones data,1,1,1.0
ones data for,1,1,1.0
data for smaller,1,1,1.0
for smaller class,1,1,1.0
smaller class minority,1,1,1.0
class minority that,1,1,1.0
minority that can,1,1,1.0
that can be,3,2,1.5
can be produced,1,1,1.0
be produced artificially,1,1,1.0
produced artificially with,1,1,1.0
artificially with the,1,1,1.0
with the assistance,1,1,1.0
the assistance of,1,1,1.0
assistance of the,1,1,1.0
of the thetic,1,1,1.0
the thetic minority,1,1,1.0
thetic minority oversampling,1,1,1.0
technique smote indeed,1,1,1.0
smote indeed this,1,1,1.0
this approach refers,1,1,1.0
approach refers plays,1,1,1.0
refers plays a,1,1,1.0
plays a role,1,1,1.0
a role of,1,1,1.0
role of minimizing,1,1,1.0
of minimizing class,1,1,1.0
minimizing class imbalance,1,1,1.0
imbalance in a,1,1,1.0
in a given,1,1,1.0
of data or,1,1,1.0
data or data,1,1,1.0
or data sample,1,1,1.0
data sample at,1,1,1.0
sample at present,1,1,1.0
at present nondistributed,1,1,1.0
present nondistributed ronment,1,1,1.0
nondistributed ronment is,1,1,1.0
ronment is making,1,1,1.0
is making use,1,1,1.0
making use of,1,1,1.0
use of availability,1,1,1.0
of availability of,1,1,1.0
availability of smote,1,1,1.0
of smote although,1,1,1.0
smote although the,1,1,1.0
although the main,1,1,1.0
the main confront,1,1,1.0
main confront arises,1,1,1.0
confront arises when,1,1,1.0
arises when we,1,1,1.0
when we have,1,1,1.0
we have necessity,1,1,1.0
have necessity to,1,1,1.0
necessity to yield,1,1,1.0
to yield the,2,2,1.0
yield the data,1,1,1.0
data for large,1,1,1.0
for large minority,1,1,1.0
large minority class,1,1,1.0
minority class artificially,1,1,1.0
class artificially using,1,1,1.0
artificially using smote,1,1,1.0
using smote data,1,1,1.0
smote data for,1,1,1.0
data for minority,1,1,1.0
for minority class,2,1,2.0
class is generated,1,1,1.0
is generated through,1,1,1.0
generated through smote,1,1,1.0
through smote with,1,1,1.0
smote with the,1,1,1.0
with the help,4,1,4.0
the help of,4,1,4.0
help of interpolation,1,1,1.0
of interpolation of,1,1,1.0
interpolation of data,1,1,1.0
of data hooda,1,1,1.0
data hooda and,1,1,1.0
hooda and mann,7,1,7.0
and mann international,7,1,7.0
mann international journal,7,1,7.0
intelligence systems instances,1,1,1.0
systems instances that,1,1,1.0
that are related,1,1,1.0
are related to,1,1,1.0
related to minority,1,1,1.0
to minority as,1,1,1.0
minority as well,1,1,1.0
as well as,7,2,3.5
well as which,1,1,1.0
as which are,1,1,1.0
which are closure,1,1,1.0
are closure with,1,1,1.0
closure with each,1,1,1.0
with each other,1,1,1.0
each other there,1,1,1.0
other there are,1,1,1.0
there are issues,1,1,1.0
are issues related,1,1,1.0
issues related to,3,2,1.5
related to distributed,1,1,1.0
to distributed smote,1,1,1.0
distributed smote as,1,1,1.0
smote as well,1,1,1.0
as well one,1,1,1.0
well one of,1,1,1.0
one of these,2,2,1.0
of these issues,1,1,1.0
these issues is,1,1,1.0
issues is the,1,1,1.0
is the appropriate,1,1,1.0
the appropriate management,1,1,1.0
appropriate management of,1,1,1.0
management of data,2,1,2.0
of data that,1,1,1.0
data that is,2,2,1.0
that is to,1,1,1.0
is to be,1,1,1.0
to be distributed,1,1,1.0
be distributed among,1,1,1.0
distributed among a,1,1,1.0
among a cluster,1,1,1.0
a cluster of,3,1,3.0
cluster of machines,2,1,2.0
of machines for,1,1,1.0
machines for ing,1,1,1.0
for ing to,1,1,1.0
ing to ensure,1,1,1.0
to ensure effective,1,1,1.0
ensure effective data,1,1,1.0
effective data generation,1,1,1.0
data generation via,1,1,1.0
generation via the,1,1,1.0
via the smote,1,1,1.0
the smote technique,1,1,1.0
smote technique it,1,1,1.0
technique it becomes,1,1,1.0
it becomes imperative,1,1,1.0
becomes imperative to,1,1,1.0
imperative to preserve,1,1,1.0
to preserve sample,1,1,1.0
preserve sample spatial,1,1,1.0
sample spatial arrangement,1,1,1.0
spatial arrangement propriate,1,1,1.0
arrangement propriate management,1,1,1.0
propriate management may,1,1,1.0
management may result,1,1,1.0
may result in,1,1,1.0
result in inconsistent,1,1,1.0
in inconsistent dissemination,1,1,1.0
inconsistent dissemination of,1,1,1.0
dissemination of data,1,1,1.0
of data among,1,1,1.0
data among the,1,1,1.0
among the nodes,1,1,1.0
the nodes cluster,1,1,1.0
nodes cluster therefore,1,1,1.0
cluster therefore may,1,1,1.0
therefore may affect,1,1,1.0
may affect the,1,1,1.0
affect the sampling,1,1,1.0
the sampling and,1,1,1.0
sampling and consequently,1,1,1.0
and consequently affecting,1,1,1.0
consequently affecting the,1,1,1.0
affecting the efficiency,1,1,1.0
the efficiency of,1,1,1.0
efficiency of the,1,1,1.0
the algorithm to,2,2,1.0
algorithm to tackle,1,1,1.0
to tackle the,1,1,1.0
tackle the problem,1,1,1.0
the problem the,4,3,1.3333333333333333
problem the solution,1,1,1.0
the solution should,1,1,1.0
solution should be,1,1,1.0
should be efficient,1,1,1.0
be efficient scalable,1,1,1.0
efficient scalable and,1,1,1.0
scalable and parallel,1,1,1.0
and parallel we,1,1,1.0
parallel we introduce,1,1,1.0
we introduce an,1,1,1.0
introduce an algorithm,1,1,1.0
an algorithm that,1,1,1.0
algorithm that utilizes,1,1,1.0
that utilizes scalable,1,1,1.0
utilizes scalable and,1,1,1.0
scalable and to,1,1,1.0
and to synthetically,1,1,1.0
to synthetically generate,1,1,1.0
synthetically generate the,1,1,1.0
generate the minority,2,1,2.0
the minority data,5,2,2.5
minority data in,1,1,1.0
data in this,3,1,3.0
paper we use,1,1,1.0
we use apache,1,1,1.0
use apache spark,1,1,1.0
apache spark for,1,1,1.0
spark for implementing,1,1,1.0
for implementing the,1,1,1.0
implementing the distributed,1,1,1.0
the distributed version,1,1,1.0
smote we believe,1,1,1.0
we believe that,2,2,1.0
believe that distributed,1,1,1.0
that distributed smote,1,1,1.0
distributed smote is,1,1,1.0
smote is an,1,1,1.0
is an active,1,1,1.0
an active area,1,1,1.0
active area of,1,1,1.0
area of research,1,1,1.0
of research our,1,1,1.0
research our research,1,1,1.0
our research can,1,1,1.0
research can be,1,1,1.0
can be extended,1,1,1.0
be extended further,1,1,1.0
extended further toward,1,1,1.0
further toward developing,1,1,1.0
toward developing an,1,1,1.0
developing an effective,1,1,1.0
an effective and,1,1,1.0
effective and efficient,1,1,1.0
and efficient algorithm,1,1,1.0
efficient algorithm background,1,1,1.0
algorithm background the,1,1,1.0
background the problem,1,1,1.0
problem of classification,1,1,1.0
of classification is,1,1,1.0
classification is termed,1,1,1.0
is termed imbalanced,1,1,1.0
termed imbalanced classification,1,1,1.0
imbalanced classification when,1,1,1.0
classification when the,1,1,1.0
when the positive,1,1,1.0
the positive class,6,4,1.5
positive class samples,1,1,1.0
class samples are,1,1,1.0
samples are far,1,1,1.0
are far too,1,1,1.0
far too few,1,1,1.0
too few compared,1,1,1.0
few compared to,1,1,1.0
compared to the,2,2,1.0
to the negative,1,1,1.0
the negative samples,1,1,1.0
negative samples given,1,1,1.0
samples given negative,1,1,1.0
given negative and,1,1,1.0
negative and positive,3,2,1.5
and positive samples,1,1,1.0
positive samples the,1,1,1.0
samples the mance,1,1,1.0
the mance on,1,1,1.0
mance on the,1,1,1.0
on the latter,1,1,1.0
the latter overwhelms,1,1,1.0
latter overwhelms that,1,1,1.0
overwhelms that which,1,1,1.0
that which is,1,1,1.0
which is felt,1,1,1.0
is felt on,1,1,1.0
felt on the,1,1,1.0
on the former,1,1,1.0
the former type,1,1,1.0
former type of,1,1,1.0
type of sample,1,1,1.0
of sample in,1,1,1.0
sample in imbalance,1,1,1.0
in imbalance data,1,1,1.0
data classification issues,1,1,1.0
classification issues we,1,1,1.0
issues we may,1,1,1.0
we may neglect,1,1,1.0
may neglect the,1,1,1.0
neglect the asymmetry,1,1,1.0
the asymmetry of,1,1,1.0
asymmetry of data,1,1,1.0
of data across,1,1,1.0
data across a,1,1,1.0
across a single,1,1,1.0
a single class,1,1,1.0
single class occasionally,1,1,1.0
class occasionally and,1,1,1.0
occasionally and this,1,1,1.0
and this asymmetry,1,1,1.0
this asymmetry within,1,1,1.0
asymmetry within a,1,1,1.0
within a class,3,3,1.0
a class is,2,2,1.0
class is cited,1,1,1.0
is cited for,1,1,1.0
cited for small,1,1,1.0
for small disjunct,1,1,1.0
small disjunct these,1,1,1.0
disjunct these small,1,1,1.0
these small disjuncts,1,1,1.0
small disjuncts can,1,1,1.0
disjuncts can be,1,1,1.0
can be influenced,1,1,1.0
be influenced through,1,1,1.0
influenced through the,1,1,1.0
through the undersampling,1,1,1.0
the undersampling of,1,1,1.0
undersampling of the,1,1,1.0
majority class we,1,1,1.0
class we may,1,1,1.0
we may have,2,1,2.0
may have to,2,1,2.0
have to confront,1,1,1.0
to confront the,1,1,1.0
confront the possible,1,1,1.0
the possible downfall,1,1,1.0
possible downfall of,1,1,1.0
downfall of essential,1,1,1.0
of essential information,1,1,1.0
essential information through,1,1,1.0
information through this,1,1,1.0
through this undersampling,1,1,1.0
this undersampling of,1,1,1.0
undersampling of majority,1,1,1.0
of majority class,2,2,1.0
majority class analogously,1,1,1.0
class analogously we,1,1,1.0
analogously we may,1,1,1.0
have to face,1,1,1.0
to face the,1,1,1.0
face the issues,1,1,1.0
the issues of,2,1,2.0
issues of data,1,1,1.0
of data tion,1,1,1.0
data tion in,1,1,1.0
tion in oversampling,1,1,1.0
in oversampling which,1,1,1.0
oversampling which will,1,1,1.0
which will enhance,1,1,1.0
will enhance the,1,1,1.0
enhance the count,1,1,1.0
the count of,1,1,1.0
count of illustrations,1,1,1.0
of illustrations excluding,1,1,1.0
illustrations excluding the,1,1,1.0
excluding the addition,1,1,1.0
the addition of,1,1,1.0
addition of any,1,1,1.0
of any value,1,1,1.0
any value to,1,1,1.0
value to data,1,1,1.0
to data as,1,1,1.0
data as well,1,1,1.0
well as any,1,1,1.0
as any tion,1,1,1.0
any tion related,1,1,1.0
tion related to,1,1,1.0
related to class,1,1,1.0
to class the,2,2,1.0
class the chances,1,1,1.0
the chances of,1,1,1.0
chances of over,1,1,1.0
of over fitting,1,1,1.0
over fitting can,1,1,1.0
fitting can additionally,1,1,1.0
can additionally be,1,1,1.0
additionally be enhanced,1,1,1.0
be enhanced by,1,1,1.0
enhanced by oversampling,1,1,1.0
by oversampling which,1,1,1.0
oversampling which can,1,1,1.0
which can reinforce,1,1,1.0
can reinforce all,1,1,1.0
reinforce all minority,1,1,1.0
all minority ters,1,1,1.0
minority ters without,1,1,1.0
ters without taking,1,1,1.0
without taking into,1,1,1.0
taking into account,1,1,1.0
into account their,1,1,1.0
account their tangible,1,1,1.0
their tangible participation,1,1,1.0
tangible participation to,1,1,1.0
participation to the,1,1,1.0
the problem lot,1,1,1.0
problem lot of,1,1,1.0
lot of exploration,1,1,1.0
of exploration has,1,1,1.0
exploration has been,1,1,1.0
has been going,1,1,1.0
been going to,1,1,1.0
going to find,1,1,1.0
find the solution,2,1,2.0
the solution for,2,1,2.0
solution for the,1,1,1.0
for the issues,2,1,2.0
issues of classification,1,1,1.0
of classification in,1,1,1.0
classification in a,1,1,1.0
in a disseminated,1,1,1.0
a disseminated distributed,1,1,1.0
disseminated distributed environment,1,1,1.0
distributed environment the,1,1,1.0
environment the difficulties,1,1,1.0
the difficulties faced,1,1,1.0
difficulties faced in,1,1,1.0
faced in classifying,1,1,1.0
in classifying imbalanced,1,1,1.0
classifying imbalanced data,1,1,1.0
imbalanced data and,3,3,1.0
data and the,3,3,1.0
and the concerns,1,1,1.0
the concerns related,1,1,1.0
concerns related to,1,1,1.0
to the generation,1,1,1.0
generation of imitated,1,1,1.0
of imitated or,1,1,1.0
imitated or we,1,1,1.0
or we can,2,1,2.0
we can say,2,1,2.0
can say synthetic,1,1,1.0
say synthetic data,1,1,1.0
big data classification,2,1,2.0
data classification problems,1,1,1.0
classification problems which,1,1,1.0
problems which are,2,1,2.0
which are extremely,1,1,1.0
are extremely imbalanced,1,1,1.0
extremely imbalanced is,1,1,1.0
imbalanced is examined,1,1,1.0
is examined by,1,1,1.0
examined by chawla,1,1,1.0
by chawla applying,1,1,1.0
chawla applying smote,1,1,1.0
applying smote requires,1,1,1.0
smote requires us,1,1,1.0
requires us to,1,1,1.0
us to find,1,1,1.0
to find neighbors,2,1,2.0
find neighbors when,1,1,1.0
neighbors when the,1,1,1.0
when the dataset,2,1,2.0
the dataset is,4,2,2.0
dataset is big,1,1,1.0
is big the,1,1,1.0
big the cost,1,1,1.0
the cost of,5,2,2.5
cost of computing,1,1,1.0
of computing the,2,2,1.0
computing the neighbors,1,1,1.0
the neighbors increases,1,1,1.0
neighbors increases for,1,1,1.0
increases for problems,1,1,1.0
for problems where,2,1,2.0
problems where the,3,2,1.5
where the data,1,1,1.0
the data set,3,1,3.0
set is huge,1,1,1.0
is huge and,1,1,1.0
huge and distributed,1,1,1.0
and distributed one,1,1,1.0
distributed one approach,1,1,1.0
one approach would,1,1,1.0
approach would be,1,1,1.0
would be to,1,1,1.0
be to group,1,1,1.0
to group the,1,1,1.0
group the samples,1,1,1.0
the samples into,1,1,1.0
samples into groups,1,1,1.0
into groups and,1,1,1.0
groups and move,1,1,1.0
and move these,1,1,1.0
move these groups,1,1,1.0
these groups on,1,1,1.0
groups on different,1,1,1.0
on different machines,2,1,2.0
different machines where,1,1,1.0
machines where can,1,1,1.0
where can be,1,1,1.0
can be achieved,1,1,1.0
be achieved lsh,1,1,1.0
achieved lsh is,1,1,1.0
lsh is another,1,1,1.0
is another technique,1,1,1.0
another technique for,1,1,1.0
technique for distributed,1,1,1.0
for distributed knn,1,1,1.0
distributed knn search,2,1,2.0
knn search in,2,1,2.0
search in high,2,1,2.0
in high dimensions,2,1,2.0
high dimensions clustering,1,1,1.0
dimensions clustering problems,1,1,1.0
clustering problems have,1,1,1.0
problems have been,1,1,1.0
have been one,1,1,1.0
been one of,1,1,1.0
the most commonly,1,1,1.0
most commonly researched,1,1,1.0
commonly researched problems,1,1,1.0
researched problems more,1,1,1.0
problems more details,1,1,1.0
more details on,1,1,1.0
details on various,1,1,1.0
on various clustering,1,1,1.0
various clustering algorithms,1,1,1.0
clustering algorithms can,1,1,1.0
algorithms can be,1,1,1.0
can be found,4,3,1.3333333333333333
be found in,4,3,1.3333333333333333
found in one,1,1,1.0
in one of,2,1,2.0
of the common,2,2,1.0
the common clustering,1,1,1.0
common clustering algorithms,1,1,1.0
clustering algorithms involves,1,1,1.0
algorithms involves the,1,1,1.0
involves the simple,1,1,1.0
the simple nature,1,1,1.0
simple nature of,1,1,1.0
the algorithm accounts,1,1,1.0
algorithm accounts for,1,1,1.0
accounts for its,1,1,1.0
for its popularity,1,1,1.0
its popularity in,1,1,1.0
popularity in recent,1,1,1.0
in recent scholarly,1,1,1.0
recent scholarly investigations,1,1,1.0
scholarly investigations much,1,1,1.0
investigations much tion,1,1,1.0
much tion has,1,1,1.0
tion has been,1,1,1.0
has been extended,1,1,1.0
been extended to,1,1,1.0
extended to initialization,1,1,1.0
to initialization improvement,1,1,1.0
initialization improvement the,1,1,1.0
improvement the need,1,1,1.0
the need for,1,1,1.0
need for better,1,1,1.0
for better convergence,1,1,1.0
better convergence has,1,1,1.0
convergence has been,1,1,1.0
has been attributed,1,1,1.0
been attributed to,1,1,1.0
attributed to this,1,1,1.0
to this trend,1,1,1.0
this trend given,1,1,1.0
trend given better,1,1,1.0
given better initialization,1,1,1.0
better initialization a,1,1,1.0
initialization a resultant,1,1,1.0
a resultant merit,1,1,1.0
resultant merit is,1,1,1.0
merit is that,1,1,1.0
that it yields,1,1,1.0
it yields cant,1,1,1.0
yields cant improvements,1,1,1.0
cant improvements in,1,1,1.0
improvements in an,1,1,1.0
in an algorithm,2,2,1.0
an algorithm s,1,1,1.0
algorithm s running,1,1,1.0
s running time,1,1,1.0
running time in,1,1,1.0
time in a,1,1,1.0
in a distributed,3,1,3.0
a distributed setup,1,1,1.0
distributed setup it,1,1,1.0
setup it remains,1,1,1.0
it remains notable,2,1,2.0
remains notable that,2,1,2.0
notable that there,1,1,1.0
that there is,3,2,1.5
there is also,1,1,1.0
is also increasing,1,1,1.0
also increasing attention,1,1,1.0
increasing attention and,1,1,1.0
attention and effort,1,1,1.0
and effort toward,1,1,1.0
effort toward understanding,1,1,1.0
toward understanding the,1,1,1.0
understanding the running,1,1,1.0
the running of,1,1,1.0
running of the,1,1,1.0
the algorithm the,1,1,1.0
algorithm the role,1,1,1.0
the role of,2,1,2.0
role of the,1,1,1.0
of the scalable,1,1,1.0
the scalable lies,1,1,1.0
scalable lies in,1,1,1.0
lies in center,1,1,1.0
in center ization,1,1,1.0
center ization having,1,1,1.0
ization having evolved,1,1,1.0
having evolved in,1,1,1.0
evolved in a,1,1,1.0
in a form,1,1,1.0
a form that,1,1,1.0
form that supports,1,1,1.0
that supports parallel,1,1,1.0
supports parallel mentation,1,1,1.0
parallel mentation given,1,1,1.0
mentation given which,1,1,1.0
given which is,1,1,1.0
which is inherently,1,1,1.0
is inherently sequential,1,1,1.0
inherently sequential its,1,1,1.0
sequential its efficient,1,1,1.0
its efficient and,1,1,1.0
efficient and parallel,1,1,1.0
and parallel version,1,1,1.0
parallel version involves,1,1,1.0
version involves the,1,1,1.0
involves the scalable,1,1,1.0
the scalable able,1,1,1.0
scalable able sampleso,1,1,1.0
able sampleso m,1,1,1.0
sampleso m points,1,1,1.0
m points in,1,1,1.0
points in each,1,1,1.0
in each round,1,1,1.0
each round and,1,1,1.0
round and repeats,1,1,1.0
and repeats the,1,1,1.0
repeats the process,1,1,1.0
the process for,1,1,1.0
process for approximatelyo,1,1,1.0
for approximatelyo logn,1,1,1.0
approximatelyo logn rounds,1,1,1.0
logn rounds instead,1,1,1.0
rounds instead of,1,1,1.0
instead of sampling,1,1,1.0
of sampling a,2,2,1.0
sampling a single,1,1,1.0
a single point,1,1,1.0
single point in,1,1,1.0
point in each,1,1,1.0
in each pass,1,1,1.0
each pass as,1,1,1.0
pass as used,1,1,1.0
as used to,1,1,1.0
used to be,1,1,1.0
be in algorithm,1,1,1.0
in algorithm so,1,1,1.0
algorithm so o,1,1,1.0
so o mlogn,1,1,1.0
o mlogn are,1,1,1.0
mlogn are sampled,1,1,1.0
are sampled which,1,1,1.0
sampled which is,1,1,1.0
which is typically,1,1,1.0
is typically more,1,1,1.0
typically more than,1,1,1.0
more than the,1,1,1.0
than the m,1,1,1.0
the m clusters,1,1,1.0
m clusters which,1,1,1.0
clusters which are,1,1,1.0
which are then,1,1,1.0
are then clustered,1,1,1.0
then clustered into,1,1,1.0
clustered into m,1,1,1.0
into m initial,1,1,1.0
m initial centers,1,1,1.0
initial centers for,1,1,1.0
centers for llyod,1,1,1.0
for llyod s,1,1,1.0
llyod s iteration,1,1,1.0
s iteration since,1,1,1.0
iteration since the,1,1,1.0
since the size,1,1,1.0
size of sample,1,1,1.0
of sample is,1,1,1.0
sample is smaller,1,1,1.0
is smaller than,1,1,1.0
smaller than the,1,1,1.0
than the input,4,2,2.0
the input size,1,1,1.0
input size the,1,1,1.0
size the can,1,1,1.0
the can be,1,1,1.0
can be done,1,1,1.0
be done quickly,1,1,1.0
done quickly once,1,1,1.0
quickly once the,1,1,1.0
once the samples,1,1,1.0
the samples are,1,1,1.0
samples are tered,1,1,1.0
are tered into,1,1,1.0
tered into m,1,1,1.0
into m clusters,1,1,1.0
m clusters each,1,1,1.0
clusters each of,1,1,1.0
of these clusters,1,1,1.0
these clusters can,1,1,1.0
clusters can use,1,1,1.0
can use any,1,1,1.0
use any of,1,1,1.0
of the widely,1,1,1.0
the widely available,1,1,1.0
widely available algorithms,1,1,1.0
available algorithms to,1,1,1.0
algorithms to find,1,1,1.0
find neighbors we,1,1,1.0
neighbors we have,1,1,1.0
we have used,1,1,1.0
have used space,1,1,1.0
used space indexing,1,1,1.0
space indexing algorithms,1,1,1.0
indexing algorithms to,1,1,1.0
algorithms to index,1,1,1.0
to index the,2,1,2.0
index the samples,2,1,2.0
the samples and,1,1,1.0
samples and then,1,1,1.0
and then search,1,1,1.0
then search for,1,1,1.0
search for neighbors,1,1,1.0
for neighbors whenever,1,1,1.0
neighbors whenever the,1,1,1.0
whenever the dimensionality,1,1,1.0
the dimensionality is,1,1,1.0
dimensionality is small,1,1,1.0
is small we,1,1,1.0
small we take,1,1,1.0
we take advantage,1,1,1.0
take advantage of,3,3,1.0
advantage of space,1,1,1.0
of space partitioning,1,1,1.0
space partitioning approaches,1,1,1.0
partitioning approaches that,1,1,1.0
approaches that includes,1,1,1.0
that includes methods,1,1,1.0
includes methods alike,1,1,1.0
methods alike trees,1,1,1.0
alike trees which,1,1,1.0
trees which is,1,1,1.0
which is capable,1,1,1.0
is capable of,1,1,1.0
capable of performing,1,1,1.0
of performing extremely,1,1,1.0
performing extremely better,1,1,1.0
extremely better in,1,1,1.0
better in that,1,1,1.0
in that case,1,1,1.0
that case the,1,1,1.0
case the apportioning,1,1,1.0
the apportioning of,1,1,1.0
apportioning of the,1,1,1.0
of the points,1,1,1.0
the points results,1,1,1.0
points results in,1,1,1.0
results in disjoint,1,1,1.0
in disjoint sets,1,1,1.0
disjoint sets a,1,1,1.0
sets a process,1,1,1.0
a process achieved,1,1,1.0
process achieved through,1,1,1.0
achieved through titioning,1,1,1.0
through titioning in,1,1,1.0
titioning in the,1,1,1.0
in the target,1,1,1.0
the target cell,1,1,1.0
target cell all,1,1,1.0
cell all points,1,1,1.0
all points are,1,1,1.0
points are considered,1,1,1.0
are considered nearest,1,1,1.0
considered nearest to,1,1,1.0
nearest to its,1,1,1.0
to its to,1,1,1.0
its to other,1,1,1.0
to other pivots,1,1,1.0
other pivots regarding,1,1,1.0
pivots regarding the,1,1,1.0
regarding the indexing,1,1,1.0
the indexing and,1,1,1.0
indexing and searching,1,1,1.0
and searching of,1,1,1.0
searching of metric,1,1,1.0
of metric spaces,1,1,1.0
metric spaces to,1,1,1.0
spaces to discern,1,1,1.0
to discern similarity,1,1,1.0
discern similarity another,1,1,1.0
similarity another efficient,1,1,1.0
another efficient method,1,1,1.0
efficient method entails,1,1,1.0
method entails this,1,1,1.0
entails this technique,1,1,1.0
this technique has,1,1,1.0
technique has been,1,1,1.0
has been mented,1,1,1.0
been mented to,1,1,1.0
mented to be,1,1,1.0
to be the,6,3,2.0
be the most,1,1,1.0
the most efficient,2,1,2.0
most efficient if,1,1,1.0
efficient if the,1,1,1.0
if the situation,1,1,1.0
the situation on,1,1,1.0
situation on focus,1,1,1.0
on focus entails,1,1,1.0
focus entails a,1,1,1.0
entails a large,1,1,1.0
a large number,3,2,1.5
large number of,4,2,2.0
number of records,1,1,1.0
of records or,1,1,1.0
records or numbers,1,1,1.0
or numbers of,1,1,1.0
numbers of dimensions,1,1,1.0
of dimensions indeed,1,1,1.0
dimensions indeed as,1,1,1.0
indeed as a,1,1,1.0
as a technique,1,1,1.0
a technique of,1,1,1.0
technique of object,1,1,1.0
of object indexing,1,1,1.0
object indexing seeks,1,1,1.0
indexing seeks to,1,1,1.0
seeks to support,1,1,1.0
to support similarity,1,1,1.0
support similarity queries,1,1,1.0
similarity queries by,1,1,1.0
queries by comparing,1,1,1.0
by comparing features,1,1,1.0
comparing features based,1,1,1.0
features based on,1,1,1.0
on the parameter,1,1,1.0
the parameter of,1,1,1.0
parameter of distance,1,1,1.0
of distance given,1,1,1.0
distance given m,1,1,1.0
given m the,1,1,1.0
m the metric,1,1,1.0
the metric space,3,1,3.0
metric space is,1,1,1.0
space is established,2,2,1.0
is established in,1,1,1.0
established in the,1,1,1.0
the form m,1,1,1.0
form m o,1,1,1.0
m o d,1,1,1.0
o d in,1,1,1.0
d in this,1,1,1.0
in this case,22,4,5.5
this case d,1,1,1.0
case d represents,1,1,1.0
d represents the,1,1,1.0
represents the distance,2,2,1.0
the distance function,1,1,1.0
distance function while,1,1,1.0
function while o,1,1,1.0
while o refers,1,1,1.0
o refers to,1,1,1.0
refers to the,2,1,2.0
feature values domain,1,1,1.0
values domain d,1,1,1.0
domain d ox,1,1,1.0
d ox oy,3,1,3.0
ox oy d,3,1,3.0
oy d oy,1,1,1.0
d oy ox,1,1,1.0
oy ox d,1,1,1.0
ox d ox,2,1,2.0
ox oy ox,1,1,1.0
oy ox oy,1,1,1.0
oy d ox,2,1,2.0
d ox ox,1,1,1.0
ox ox d,1,1,1.0
d ox oz,1,1,1.0
ox oz d,1,1,1.0
oz d oz,1,1,1.0
d oz oy,1,1,1.0
oz oy in,1,1,1.0
oy in o,1,1,1.0
in o the,1,1,1.0
o the objects,1,1,1.0
the objects are,2,1,2.0
objects are represented,1,1,1.0
are represented by,1,1,1.0
represented by oz,1,1,1.0
by oz oy,1,1,1.0
oz oy and,1,1,1.0
oy and ox,1,1,1.0
and ox to,1,1,1.0
ox to determine,1,1,1.0
to determine any,1,1,1.0
determine any similarity,1,1,1.0
any similarity or,1,1,1.0
similarity or dissimilarity,1,1,1.0
or dissimilarity between,1,1,1.0
dissimilarity between objects,1,1,1.0
between objects d,1,1,1.0
objects d is,1,1,1.0
d is considered,1,1,1.0
considered as the,1,1,1.0
as the informative,1,1,1.0
the informative parameter,1,1,1.0
informative parameter for,1,1,1.0
parameter for the,1,1,1.0
for the entries,1,1,1.0
the entries that,1,1,1.0
entries that can,1,1,1.0
can be stored,1,1,1.0
be stored can,1,1,1.0
stored can go,1,1,1.0
can go up,1,1,1.0
go up to,1,1,1.0
up to l,1,1,1.0
to l hence,1,1,1.0
l hence l,1,1,1.0
hence l represents,1,1,1.0
l represents the,1,1,1.0
represents the node,1,1,1.0
the node capacity,1,1,1.0
node capacity it,1,1,1.0
capacity it is,1,1,1.0
important to highlight,1,1,1.0
to highlight that,1,1,1.0
highlight that exhibits,1,1,1.0
that exhibits two,1,1,1.0
exhibits two internal,1,1,1.0
two internal node,1,1,1.0
internal node types,1,1,1.0
node types these,1,1,1.0
types these nodes,1,1,1.0
these nodes include,1,1,1.0
nodes include the,1,1,1.0
include the leaf,1,1,1.0
the leaf nodes,2,1,2.0
leaf nodes and,1,1,1.0
nodes and the,1,1,1.0
and the routing,1,1,1.0
the routing nodes,1,1,1.0
routing nodes poised,1,1,1.0
nodes poised to,1,1,1.0
poised to represent,1,1,1.0
to represent data,1,1,1.0
represent data or,1,1,1.0
data or samples,1,1,1.0
or samples indeed,1,1,1.0
samples indeed tions,1,1,1.0
indeed tions through,1,1,1.0
tions through sample,1,1,1.0
through sample organization,1,1,1.0
sample organization to,1,1,1.0
organization to ensure,1,1,1.0
ensure that nodes,1,1,1.0
that nodes are,1,1,1.0
nodes are achieved,1,1,1.0
are achieved notably,1,1,1.0
achieved notably the,1,1,1.0
notably the resultant,1,1,1.0
the resultant nodes,1,1,1.0
resultant nodes correspond,1,1,1.0
nodes correspond to,1,1,1.0
correspond to the,2,2,1.0
to the metric,2,1,2.0
metric space region,1,1,1.0
space region grows,1,1,1.0
region grows in,1,1,1.0
grows in a,1,1,1.0
in a bottom,1,1,1.0
a bottom up,1,1,1.0
bottom up fashion,1,1,1.0
up fashion when,1,1,1.0
fashion when the,1,1,1.0
when the number,2,2,1.0
number of leaf,1,1,1.0
of leaf nodes,2,1,2.0
leaf nodes grows,1,1,1.0
nodes grows beyond,1,1,1.0
grows beyond the,1,1,1.0
beyond the capacity,1,1,1.0
the capacity of,3,1,3.0
capacity of the,3,1,3.0
of the routing,1,1,1.0
the routing node,2,1,2.0
routing node a,2,1,2.0
node a new,1,1,1.0
a new node,1,1,1.0
new node from,1,1,1.0
node from the,1,1,1.0
from the pool,1,1,1.0
the pool is,1,1,1.0
pool is promoted,1,1,1.0
is promoted as,1,1,1.0
promoted as a,1,1,1.0
as a routing,1,1,1.0
a routing node,1,1,1.0
routing node and,1,1,1.0
node and the,1,1,1.0
and the tree,1,1,1.0
the tree is,1,1,1.0
tree is split,1,1,1.0
is split and,1,1,1.0
split and nodes,1,1,1.0
and nodes partitioned,1,1,1.0
nodes partitioned between,1,1,1.0
partitioned between the,1,1,1.0
between the two,3,2,1.5
the two routing,1,1,1.0
two routing nodes,1,1,1.0
routing nodes based,1,1,1.0
nodes based on,1,1,1.0
on the partitioning,1,1,1.0
the partitioning policy,1,1,1.0
partitioning policy hooda,1,1,1.0
policy hooda and,1,1,1.0
intelligence systems provides,1,1,1.0
systems provides a,1,1,1.0
provides a very,1,1,1.0
a very fast,1,1,1.0
very fast and,1,1,1.0
fast and efficient,1,1,1.0
and efficient nearest,1,1,1.0
efficient nearest neighbor,1,1,1.0
nearest neighbor search,4,2,2.0
neighbor search search,1,1,1.0
search search is,1,1,1.0
search is based,1,1,1.0
on the concept,1,1,1.0
the concept that,1,1,1.0
concept that if,1,1,1.0
that if a,1,1,1.0
if a sample,1,1,1.0
a sample q,1,1,1.0
sample q is,1,1,1.0
q is at,1,1,1.0
is at a,2,1,2.0
at a distance,2,1,2.0
a distance of,2,1,2.0
distance of d,2,1,2.0
of d or,2,1,2.0
d or q,3,1,3.0
or q and,1,1,1.0
q and another,1,1,1.0
and another sample,1,1,1.0
another sample p,1,1,1.0
sample p is,1,1,1.0
p is at,1,1,1.0
d or p,3,1,3.0
or p then,1,1,1.0
p then the,1,1,1.0
then the distance,1,1,1.0
the distance between,6,4,1.5
distance between d,1,1,1.0
between d q,1,1,1.0
d q p,2,1,2.0
q p will,1,1,1.0
p will always,1,1,1.0
will always be,1,1,1.0
always be greater,1,1,1.0
be greater than,1,1,1.0
greater than d,1,1,1.0
than d or,1,1,1.0
or q d,2,1,2.0
q d or,1,1,1.0
or p this,2,1,2.0
p this follows,1,1,1.0
this follows from,1,1,1.0
follows from the,1,1,1.0
from the triangle,1,1,1.0
the triangle inequality,1,1,1.0
triangle inequality which,1,1,1.0
inequality which states,1,1,1.0
which states that,1,1,1.0
states that d,1,1,1.0
that d or,1,1,1.0
q d q,1,1,1.0
q p d,1,1,1.0
p d or,1,1,1.0
p this simple,1,1,1.0
this simple inequality,1,1,1.0
simple inequality optimizes,1,1,1.0
inequality optimizes the,1,1,1.0
optimizes the search,1,1,1.0
the search for,1,1,1.0
search for the,2,2,1.0
for the nearest,2,2,1.0
the nearest bors,1,1,1.0
nearest bors of,1,1,1.0
bors of each,1,1,1.0
of each sample,1,1,1.0
each sample as,1,1,1.0
sample as all,1,1,1.0
as all these,1,1,1.0
all these points,1,1,1.0
these points are,1,1,1.0
points are organized,1,1,1.0
are organized in,1,1,1.0
organized in a,1,1,1.0
in a metric,1,1,1.0
a metric space,1,1,1.0
metric space imperatively,1,1,1.0
space imperatively these,1,1,1.0
imperatively these methods,1,1,1.0
these methods are,1,1,1.0
methods are meant,1,1,1.0
are meant to,1,1,1.0
meant to operate,1,1,1.0
to operate on,1,1,1.0
operate on single,1,1,1.0
on single machines,1,1,1.0
single machines as,1,1,1.0
machines as such,1,1,1.0
as such the,1,1,1.0
such the techniques,1,1,1.0
the techniques become,1,1,1.0
techniques become impractical,1,1,1.0
become impractical and,1,1,1.0
impractical and cient,1,1,1.0
and cient when,1,1,1.0
cient when a,1,1,1.0
a given sample,1,1,1.0
given sample involves,1,1,1.0
sample involves big,1,1,1.0
involves big data,1,1,1.0
big data in,1,1,1.0
data in one,1,1,1.0
of the ous,1,1,1.0
the ous investigations,1,1,1.0
ous investigations it,1,1,1.0
investigations it was,1,1,1.0
it was documented,1,1,1.0
was documented that,1,1,1.0
documented that a,1,1,1.0
that a combination,1,1,1.0
a combination of,3,2,1.5
combination of spill,1,1,1.0
of spill trees,1,1,1.0
spill trees and,1,1,1.0
trees and scales,1,1,1.0
and scales in,1,1,1.0
scales in a,1,1,1.0
in a case,1,1,1.0
a case involving,1,1,1.0
case involving distributed,1,1,1.0
involving distributed sample,1,1,1.0
distributed sample space,1,1,1.0
sample space ensuring,1,1,1.0
space ensuring further,1,1,1.0
ensuring further that,1,1,1.0
further that similar,1,1,1.0
that similar samples,1,1,1.0
similar samples are,2,1,2.0
samples are searched,1,1,1.0
are searched efficiently,1,1,1.0
searched efficiently in,1,1,1.0
efficiently in the,1,1,1.0
the current investigation,1,1,1.0
current investigation the,1,1,1.0
investigation the central,1,1,1.0
the central purpose,1,1,1.0
central purpose is,1,1,1.0
purpose is to,1,1,1.0
is to present,1,1,1.0
to present a,1,1,1.0
present a hybrid,1,1,1.0
a hybrid technique,1,1,1.0
hybrid technique through,1,1,1.0
technique through which,1,1,1.0
through which artificial,1,1,1.0
which artificial minority,1,1,1.0
artificial minority data,1,1,1.0
minority data could,1,1,1.0
data could be,2,1,2.0
could be generated,1,1,1.0
be generated using,2,2,1.0
generated using scalable,1,1,1.0
using scalable similar,1,1,1.0
scalable similar samples,1,1,1.0
samples are clustered,2,1,2.0
are clustered together,1,1,1.0
clustered together also,1,1,1.0
together also are,1,1,1.0
also are used,1,1,1.0
used to index,1,1,1.0
the samples before,1,1,1.0
samples before ing,1,1,1.0
before ing for,1,1,1.0
ing for the,1,1,1.0
for the perceived,1,1,1.0
the perceived samples,1,1,1.0
perceived samples performance,1,1,1.0
samples performance measures,1,1,1.0
performance measures as,1,1,1.0
measures as the,1,1,1.0
the minority smaller,1,1,1.0
minority smaller data,1,1,1.0
smaller data is,1,1,1.0
data is devastated,1,1,1.0
is devastated by,1,1,1.0
devastated by the,1,1,1.0
by the majority,4,2,2.0
the majority data,2,2,1.0
majority data present,1,1,1.0
data present for,1,1,1.0
present for this,1,1,1.0
for this reason,2,2,1.0
this reason the,1,1,1.0
reason the problem,1,1,1.0
problem of imbalanced,1,1,1.0
of imbalanced classification,1,1,1.0
imbalanced classification which,1,1,1.0
classification which was,1,1,1.0
which was described,1,1,1.0
was described previously,1,1,1.0
described previously can,1,1,1.0
previously can t,1,1,1.0
t be contrasted,1,1,1.0
be contrasted through,1,1,1.0
contrasted through the,1,1,1.0
through the use,1,1,1.0
use of accuracy,1,1,1.0
of accuracy metrics,1,1,1.0
accuracy metrics for,1,1,1.0
metrics for illustration,1,1,1.0
for illustration the,1,1,1.0
illustration the accuracy,1,1,1.0
the accuracy of,3,2,1.5
accuracy of the,5,3,1.6666666666666667
of the prediction,1,1,1.0
the prediction can,1,1,1.0
prediction can simply,1,1,1.0
can simply be,1,1,1.0
simply be taken,1,1,1.0
be taken as,1,1,1.0
taken as accurate,1,1,1.0
as accurate in,1,1,1.0
accurate in case,1,1,1.0
in case a,1,1,1.0
case a provided,1,1,1.0
a provided dataset,1,1,1.0
provided dataset has,1,1,1.0
dataset has a,1,1,1.0
has a distribution,1,1,1.0
a distribution by,1,1,1.0
distribution by means,1,1,1.0
by means of,3,3,1.0
means of merely,1,1,1.0
of merely predicting,1,1,1.0
merely predicting entire,1,1,1.0
predicting entire data,1,1,1.0
entire data as,1,1,1.0
data as the,1,1,1.0
as the majority,1,1,1.0
majority class table,2,2,1.0
class table is,1,1,1.0
table is being,1,1,1.0
is being implemented,1,1,1.0
being implemented based,1,1,1.0
implemented based upon,1,1,1.0
based upon plete,1,1,1.0
upon plete confusion,1,1,1.0
plete confusion matrix,1,1,1.0
confusion matrix and,2,2,1.0
matrix and that,1,1,1.0
and that confusion,1,1,1.0
that confusion matrix,1,1,1.0
confusion matrix is,1,1,1.0
matrix is essential,1,1,1.0
is essential for,1,1,1.0
essential for the,1,1,1.0
the issues related,1,1,1.0
related to imbalanced,1,1,1.0
imbalanced classification relative,1,1,1.0
classification relative to,1,1,1.0
to the case,1,1,1.0
the case of,10,2,5.0
case of the,8,2,4.0
of the confusion,1,1,1.0
the confusion matrix,5,3,1.6666666666666667
confusion matrix it,1,1,1.0
matrix it is,2,2,1.0
it is evident,1,1,1.0
is evident that,1,1,1.0
evident that the,1,1,1.0
that the performance,3,3,1.0
the performance categorization,1,1,1.0
performance categorization for,1,1,1.0
categorization for negative,1,1,1.0
for negative and,2,2,1.0
and positive classes,2,2,1.0
positive classes can,1,1,1.0
classes can be,2,2,1.0
can be discerned,1,1,1.0
be discerned the,1,1,1.0
discerned the tpr,1,1,1.0
the tpr recall,1,1,1.0
tpr recall or,1,1,1.0
recall or true,1,1,1.0
or true positive,1,1,1.0
true positive becomes,1,1,1.0
positive becomes tpr,1,1,1.0
becomes tpr tp,1,1,1.0
tpr tp fn,1,1,1.0
tp fn the,1,1,1.0
fn the resultant,1,1,1.0
the resultant value,1,1,1.0
resultant value obtained,1,1,1.0
value obtained from,2,2,1.0
obtained from the,4,2,2.0
from the equation,1,1,1.0
the equation above,1,1,1.0
equation above represents,1,1,1.0
above represents the,1,1,1.0
represents the percentage,2,1,2.0
the percentage of,5,2,2.5
percentage of positive,2,1,2.0
of positive situations,2,1,2.0
positive situations that,1,1,1.0
situations that have,2,1,2.0
that have been,1,1,1.0
have been categorized,1,1,1.0
been categorized accurately,2,1,2.0
categorized accurately regarding,1,1,1.0
accurately regarding fpr,1,1,1.0
regarding fpr the,1,1,1.0
fpr the rate,1,1,1.0
the rate it,2,1,2.0
rate it represents,1,1,1.0
it represents the,1,1,1.0
percentage of negative,1,1,1.0
of negative situations,1,1,1.0
negative situations that,1,1,1.0
that have not,1,1,1.0
have not been,1,1,1.0
not been categorized,1,1,1.0
categorized accurately it,1,1,1.0
accurately it is,1,1,1.0
it is expressed,1,1,1.0
is expressed as,3,1,3.0
expressed as fnr,2,1,2.0
as fnr tp,2,1,2.0
fnr tp fn,2,1,2.0
tp fn table,1,1,1.0
fn table confusion,1,1,1.0
matrix for a,1,1,1.0
for a sample,2,1,2.0
a sample problem,1,1,1.0
sample problem actual,1,1,1.0
problem actual class,1,1,1.0
actual class predicted,1,1,1.0
class predicted class,1,1,1.0
predicted class positive,1,1,1.0
class positive negative,1,1,1.0
positive negative negative,1,1,1.0
negative negative class,1,1,1.0
negative class fp,1,1,1.0
class fp false,1,1,1.0
fp false positive,2,2,1.0
false positive tn,2,2,1.0
positive tn true,2,2,1.0
tn true negative,2,2,1.0
true negative positive,1,1,1.0
negative positive class,1,1,1.0
positive class tp,1,1,1.0
class tp true,1,1,1.0
tp true positive,2,2,1.0
true positive fn,2,2,1.0
positive fn false,2,2,1.0
fn false negative,2,2,1.0
false negative lastly,1,1,1.0
negative lastly the,1,1,1.0
lastly the fnr,1,1,1.0
the fnr which,1,1,1.0
fnr which indicates,1,1,1.0
which indicates the,1,1,1.0
indicates the rate,1,1,1.0
rate it refers,1,1,1.0
it refers to,1,1,1.0
to the percentage,1,1,1.0
positive situations where,1,1,1.0
situations where there,1,1,1.0
where there has,1,1,1.0
there has been,1,1,1.0
has been priate,1,1,1.0
been priate categorization,1,1,1.0
priate categorization the,1,1,1.0
categorization the parameter,1,1,1.0
the parameter is,1,1,1.0
parameter is expressed,1,1,1.0
tp fn combining,1,1,1.0
fn combining the,1,1,1.0
combining the fpr,1,1,1.0
the fpr and,1,1,1.0
fpr and the,1,1,1.0
and the tpr,1,1,1.0
the tpr in,1,1,1.0
tpr in one,1,1,1.0
in one metric,1,1,1.0
one metric calls,1,1,1.0
metric calls for,1,1,1.0
calls for the,1,1,1.0
for the ting,1,1,1.0
the ting of,1,1,1.0
ting of tpr,1,1,1.0
of tpr against,1,1,1.0
tpr against fpr,1,1,1.0
against fpr the,1,1,1.0
fpr the resultant,1,1,1.0
the resultant curve,1,1,1.0
resultant curve becomes,1,1,1.0
curve becomes the,2,1,2.0
becomes the roc,1,1,1.0
the roc receiver,1,1,1.0
roc receiver operating,1,1,1.0
operating characteristic the,1,1,1.0
characteristic the area,1,1,1.0
the area under,4,3,1.3333333333333333
area under the,5,3,1.6666666666666667
under the curve,1,1,1.0
the curve the,1,1,1.0
curve the roc,1,1,1.0
roc curve becomes,1,1,1.0
becomes the auc,1,1,1.0
the auc other,1,1,1.0
auc other common,1,1,1.0
other common metrics,1,1,1.0
common metrics through,1,1,1.0
metrics through which,1,1,1.0
through which the,2,1,2.0
which the classes,1,1,1.0
the classes joint,1,1,1.0
classes joint mance,1,1,1.0
joint mance has,1,1,1.0
mance has been,1,1,1.0
has been maximized,1,1,1.0
been maximized include,1,1,1.0
maximized include the,1,1,1.0
include the true,1,1,1.0
the true rates,1,1,1.0
true rates geometric,1,1,1.0
rates geometric mean,1,1,1.0
geometric mean gm,1,1,1.0
mean gm and,1,1,1.0
gm and the,1,1,1.0
and the auc,2,2,1.0
the auc whereas,1,1,1.0
auc whereas the,1,1,1.0
whereas the aim,1,1,1.0
the aim of,2,2,1.0
aim of the,1,1,1.0
of the gm,1,1,1.0
the gm lies,1,1,1.0
gm lies in,1,1,1.0
lies in racy,1,1,1.0
in racy maximization,1,1,1.0
racy maximization relative,1,1,1.0
maximization relative to,1,1,1.0
to the respective,2,2,1.0
respective classes the,1,1,1.0
classes the auc,1,1,1.0
the auc ric,1,1,1.0
auc ric seeks,1,1,1.0
ric seeks to,1,1,1.0
seeks to manifest,1,1,1.0
to manifest between,1,1,1.0
manifest between costs,1,1,1.0
between costs and,1,1,1.0
costs and benefits,1,1,1.0
and benefits gm,1,1,1.0
benefits gm is,1,1,1.0
gm is expressed,1,1,1.0
expressed as gm,1,1,1.0
as gm tprate,1,1,1.0
gm tprate tnrate,1,1,1.0
tprate tnrate algorithm,1,1,1.0
tnrate algorithm in,1,1,1.0
algorithm in situations,1,1,1.0
in situations where,1,1,1.0
situations where distributed,1,1,1.0
where distributed environments,1,1,1.0
distributed environments have,1,1,1.0
environments have seen,1,1,1.0
have seen based,1,1,1.0
seen based oversampling,1,1,1.0
based oversampling procedures,1,1,1.0
oversampling procedures applied,1,1,1.0
procedures applied most,1,1,1.0
applied most of,1,1,1.0
of the results,1,1,1.0
the results have,2,2,1.0
results have proved,1,1,1.0
have proved less,1,1,1.0
proved less successful,1,1,1.0
less successful this,1,1,1.0
successful this failure,1,1,1.0
this failure continues,1,1,1.0
failure continues to,1,1,1.0
continues to be,1,1,1.0
to be attributed,1,1,1.0
be attributed to,1,1,1.0
attributed to the,1,1,1.0
the data s,1,1,1.0
data s random,1,1,1.0
s random partitioning,1,1,1.0
random partitioning as,1,1,1.0
partitioning as well,1,1,1.0
well as artificial,1,1,1.0
as artificial sample,1,1,1.0
artificial sample eration,1,1,1.0
sample eration with,1,1,1.0
eration with these,1,1,1.0
with these parameters,1,1,1.0
these parameters lacking,1,1,1.0
parameters lacking spatial,1,1,1.0
lacking spatial relationships,1,1,1.0
spatial relationships in,1,1,1.0
relationships in this,1,1,1.0
in this study,3,1,3.0
this study we,1,1,1.0
study we try,1,1,1.0
we try to,1,1,1.0
try to resolve,1,1,1.0
to resolve that,1,1,1.0
resolve that issue,1,1,1.0
that issue with,1,1,1.0
issue with the,1,1,1.0
help of effectually,1,1,1.0
of effectually segmenting,1,1,1.0
effectually segmenting and,1,1,1.0
segmenting and dispensing,1,1,1.0
and dispensing the,1,1,1.0
dispensing the dataset,1,1,1.0
dataset in space,1,1,1.0
in space apache,1,1,1.0
space apache based,1,1,1.0
apache based smote,1,1,1.0
based smote implementation,1,1,1.0
smote implementation is,2,1,2.0
implementation is being,1,1,1.0
is being represented,1,1,1.0
being represented in,1,1,1.0
represented in our,1,1,1.0
in our work,1,1,1.0
our work for,1,1,1.0
work for big,1,1,1.0
big data smaller,1,1,1.0
data smaller minority,1,1,1.0
smaller minority dataset,1,1,1.0
minority dataset we,1,1,1.0
dataset we need,1,1,1.0
we need to,2,1,2.0
need to determine,1,1,1.0
to determine est,1,1,1.0
determine est neighbors,1,1,1.0
est neighbors for,1,1,1.0
neighbors for a,1,1,1.0
for a point,1,1,1.0
a point in,1,1,1.0
point in d,1,1,1.0
in d dimensional,1,1,1.0
d dimensional space,1,1,1.0
dimensional space in,1,1,1.0
space in order,2,2,1.0
order to inate,1,1,1.0
to inate samples,1,1,1.0
inate samples for,1,1,1.0
samples for minority,1,1,1.0
minority class synthetically,1,1,1.0
class synthetically for,1,1,1.0
synthetically for the,1,1,1.0
for the context,1,1,1.0
the context of,5,3,1.6666666666666667
context of issue,1,1,1.0
of issue or,1,1,1.0
issue or problem,1,1,1.0
or problem which,1,1,1.0
problem which we,1,1,1.0
which we are,1,1,1.0
we are tackling,1,1,1.0
are tackling the,1,1,1.0
tackling the data,1,1,1.0
the data distribution,5,2,2.5
data distribution to,1,1,1.0
distribution to several,1,1,1.0
to several different,1,1,1.0
several different nodes,1,1,1.0
different nodes which,1,1,1.0
nodes which is,1,1,1.0
which is done,1,1,1.0
is done randomly,1,1,1.0
done randomly in,1,1,1.0
randomly in a,1,1,1.0
a distributed cluster,1,1,1.0
distributed cluster might,1,1,1.0
cluster might result,1,1,1.0
might result in,1,1,1.0
result in distribution,1,1,1.0
in distribution of,1,1,1.0
distribution of points,1,1,1.0
of points to,1,1,1.0
points to different,1,1,1.0
to different nodes,1,1,1.0
different nodes rather,1,1,1.0
nodes rather than,1,1,1.0
rather than their,1,1,1.0
than their nearest,1,1,1.0
their nearest nodes,1,1,1.0
nearest nodes hence,1,1,1.0
nodes hence it,1,1,1.0
hence it will,1,1,1.0
it will become,1,1,1.0
will become unfeasible,1,1,1.0
become unfeasible for,1,1,1.0
unfeasible for different,1,1,1.0
for different individual,1,1,1.0
different individual nodes,1,1,1.0
individual nodes to,1,1,1.0
nodes to identify,1,1,1.0
to identify their,1,1,1.0
identify their closest,1,1,1.0
their closest neighbors,1,1,1.0
closest neighbors therefore,1,1,1.0
neighbors therefore for,1,1,1.0
therefore for problems,1,1,1.0
where the dataset,1,1,1.0
dataset is very,2,2,1.0
is very big,1,1,1.0
very big and,1,1,1.0
big and most,1,1,1.0
and most probably,1,1,1.0
most probably distributed,1,1,1.0
probably distributed across,1,1,1.0
distributed across a,1,1,1.0
across a cluster,1,1,1.0
of machines it,1,1,1.0
machines it is,1,1,1.0
it is significant,1,1,1.0
is significant that,1,1,1.0
significant that the,1,1,1.0
that the identical,1,1,1.0
the identical samples,1,1,1.0
identical samples are,1,1,1.0
are clustered and,1,1,1.0
clustered and samples,1,1,1.0
and samples belonging,1,1,1.0
samples belonging to,2,1,2.0
belonging to a,2,1,2.0
to a cluster,4,1,4.0
a cluster brought,1,1,1.0
cluster brought to,1,1,1.0
brought to a,1,1,1.0
to a machine,1,1,1.0
a machine so,1,1,1.0
machine so that,1,1,1.0
so that all,1,1,1.0
all the samples,3,1,3.0
the samples ing,1,1,1.0
samples ing to,1,1,1.0
ing to a,1,1,1.0
a cluster now,1,1,1.0
cluster now are,1,1,1.0
now are present,1,1,1.0
are present on,2,1,2.0
present on a,2,1,2.0
single machine that,1,1,1.0
machine that is,1,1,1.0
is we the,1,1,1.0
we the data,1,1,1.0
the data in,1,1,1.0
data in such,1,1,1.0
in such a,3,1,3.0
such a way,3,1,3.0
a way that,3,1,3.0
way that all,1,1,1.0
the samples which,2,1,2.0
samples which belong,1,1,1.0
which belong to,1,1,1.0
belong to a,1,1,1.0
a cluster are,1,1,1.0
cluster are present,1,1,1.0
on a machine,1,1,1.0
a machine these,1,1,1.0
machine these different,1,1,1.0
these different ters,1,1,1.0
different ters on,1,1,1.0
ters on different,1,1,1.0
different machines can,1,1,1.0
machines can then,1,1,1.0
can then be,1,1,1.0
then be processed,1,1,1.0
be processed in,1,1,1.0
processed in parallel,1,1,1.0
in parallel to,1,1,1.0
parallel to erate,1,1,1.0
to erate synthetic,1,1,1.0
erate synthetic data,1,1,1.0
synthetic data using,1,1,1.0
data using smote,1,1,1.0
using smote though,1,1,1.0
smote though the,1,1,1.0
though the dataset,1,1,1.0
dataset is huge,1,1,1.0
is huge the,1,1,1.0
huge the individual,1,1,1.0
the individual groups,1,1,1.0
individual groups that,1,1,1.0
groups that we,1,1,1.0
that we create,1,1,1.0
we create can,1,1,1.0
create can fit,1,1,1.0
can fit on,1,1,1.0
fit on a,1,1,1.0
single machine and,1,1,1.0
machine and hence,1,1,1.0
and hence can,1,1,1.0
hence can be,1,1,1.0
can be processed,1,1,1.0
be processed independently,1,1,1.0
processed independently and,1,1,1.0
independently and in,1,1,1.0
and in parallel,1,1,1.0
in parallel thus,1,1,1.0
parallel thus ing,1,1,1.0
thus ing motivation,1,1,1.0
ing motivation to,1,1,1.0
motivation to solve,1,1,1.0
solve this complex,1,1,1.0
this complex problem,1,1,1.0
complex problem during,1,1,1.0
problem during the,1,1,1.0
during the application,1,1,1.0
the application of,5,2,2.5
application of smote,2,1,2.0
smote the main,1,1,1.0
the main objective,1,1,1.0
main objective is,1,1,1.0
objective is to,3,2,1.5
is to mine,1,1,1.0
to mine the,1,1,1.0
mine the most,1,1,1.0
most efficient way,1,1,1.0
efficient way through,1,1,1.0
way through which,1,1,1.0
which the data,1,1,1.0
the data could,1,1,1.0
could be tioned,1,1,1.0
be tioned one,1,1,1.0
tioned one of,1,1,1.0
of the perceived,1,1,1.0
the perceived convenient,1,1,1.0
perceived convenient ways,1,1,1.0
convenient ways involves,1,1,1.0
ways involves the,1,1,1.0
involves the random,1,1,1.0
the random grouping,1,1,1.0
random grouping of,1,1,1.0
grouping of data,1,1,1.0
of data to,1,1,1.0
data to obtain,1,1,1.0
to obtain m,1,1,1.0
obtain m clusters,1,1,1.0
m clusters despite,1,1,1.0
clusters despite the,1,1,1.0
despite the contributory,1,1,1.0
the contributory role,1,1,1.0
contributory role of,1,1,1.0
role of this,1,1,1.0
of this technique,1,1,1.0
this technique it,1,1,1.0
technique it remains,1,1,1.0
notable that it,1,1,1.0
that it requires,1,1,1.0
it requires that,1,1,1.0
requires that each,1,1,1.0
that each query,1,1,1.0
each query is,1,1,1.0
query is run,1,1,1.0
is run through,1,1,1.0
run through the,1,1,1.0
through the clusters,1,1,1.0
the clusters in,1,1,1.0
clusters in the,1,1,1.0
in the entirety,1,1,1.0
the entirety algorithms,1,1,1.0
entirety algorithms detail,1,1,1.0
algorithms detail the,1,1,1.0
detail the techniques,1,1,1.0
the techniques for,1,1,1.0
for the searching,1,1,1.0
the searching indexing,1,1,1.0
searching indexing and,1,1,1.0
indexing and partitioning,1,1,1.0
and partitioning hooda,1,1,1.0
partitioning hooda and,1,1,1.0
intelligence systems given,1,1,1.0
systems given larger,1,1,1.0
given larger sets,1,1,1.0
larger sets of,1,1,1.0
of data they,1,1,1.0
data they could,1,1,1.0
they could be,1,1,1.0
could be clustered,1,1,1.0
be clustered or,1,1,1.0
clustered or segmented,1,1,1.0
or segmented using,1,1,1.0
segmented using different,1,1,1.0
using different techniques,1,1,1.0
different techniques an,1,1,1.0
techniques an example,1,1,1.0
an example of,2,2,1.0
example of a,2,2,1.0
of a fast,1,1,1.0
a fast algorithm,1,1,1.0
fast algorithm that,1,1,1.0
algorithm that could,1,1,1.0
that could achieve,1,1,1.0
could achieve this,1,1,1.0
achieve this role,1,1,1.0
this role is,1,1,1.0
role is scalable,1,1,1.0
is scalable in,1,1,1.0
scalable in this,1,1,1.0
this study the,1,1,1.0
study the spark,1,1,1.0
the spark implementation,2,1,2.0
spark implementation of,1,1,1.0
implementation of scalable,1,1,1.0
of scalable k,1,1,1.0
scalable k is,1,1,1.0
k is employed,1,1,1.0
is employed the,1,1,1.0
employed the use,1,1,1.0
use of this,2,2,1.0
this approach is,1,1,1.0
approach is to,2,2,1.0
is to ensure,1,1,1.0
ensure that large,1,1,1.0
that large sets,1,1,1.0
large sets of,2,1,2.0
of data are,2,1,2.0
data are segmented,1,1,1.0
are segmented to,1,1,1.0
segmented to yield,1,1,1.0
to yield m,1,1,1.0
yield m spaces,1,1,1.0
m spaces upon,1,1,1.0
spaces upon data,1,1,1.0
upon data clustering,1,1,1.0
data clustering are,1,1,1.0
clustering are established,1,1,1.0
are established for,1,1,1.0
established for sample,1,1,1.0
for sample indexing,1,1,1.0
sample indexing before,1,1,1.0
indexing before establishing,1,1,1.0
before establishing the,1,1,1.0
establishing the nearest,1,1,1.0
the nearest for,1,1,1.0
nearest for the,1,1,1.0
for the respective,2,1,2.0
the respective samples,3,1,3.0
respective samples indeed,1,1,1.0
samples indeed serves,1,1,1.0
indeed serves the,1,1,1.0
serves the role,1,1,1.0
role of partitioning,1,1,1.0
of partitioning objects,1,1,1.0
partitioning objects via,1,1,1.0
objects via relative,1,1,1.0
via relative distances,1,1,1.0
relative distances in,1,1,1.0
distances in this,1,1,1.0
in this investigation,2,1,2.0
this investigation the,1,1,1.0
investigation the euclidean,1,1,1.0
the euclidean approach,1,1,1.0
euclidean approach is,1,1,1.0
approach is employed,1,1,1.0
is employed and,1,1,1.0
employed and seeks,1,1,1.0
and seeks to,1,1,1.0
seeks to ensure,1,1,1.0
that the objects,1,1,1.0
objects are formed,1,1,1.0
are formed into,1,1,1.0
formed into nodes,1,1,1.0
into nodes aligning,1,1,1.0
nodes aligning them,1,1,1.0
aligning them to,1,1,1.0
them to the,2,2,1.0
metric space constrained,1,1,1.0
space constrained regions,1,1,1.0
constrained regions upon,1,1,1.0
regions upon point,1,1,1.0
upon point clustering,1,1,1.0
point clustering each,1,1,1.0
clustering each cluster,1,1,1.0
each cluster of,1,1,1.0
cluster of points,1,1,1.0
of points is,1,1,1.0
points is pushed,1,1,1.0
is pushed to,1,1,1.0
pushed to the,1,1,1.0
to the tive,1,1,1.0
the tive nodes,1,1,1.0
tive nodes before,1,1,1.0
nodes before being,1,1,1.0
before being arranged,1,1,1.0
being arranged relative,1,1,1.0
arranged relative to,1,1,1.0
relative to distances,1,1,1.0
to distances given,1,1,1.0
distances given the,1,1,1.0
given the respective,2,1,2.0
respective samples the,1,1,1.0
samples the nearest,1,1,1.0
nearest neighbors are,2,2,1.0
neighbors are searched,1,1,1.0
are searched before,1,1,1.0
searched before generating,1,1,1.0
before generating synthetic,1,1,1.0
generating synthetic samples,1,1,1.0
synthetic samples dataset,1,1,1.0
samples dataset the,1,1,1.0
dataset the represents,1,1,1.0
the represents the,1,1,1.0
represents the data,1,1,1.0
data set employed,1,1,1.0
set employed in,1,1,1.0
employed in this,2,1,2.0
in this tion,1,1,1.0
this tion in,1,1,1.0
tion in the,1,1,1.0
in the context,4,3,1.3333333333333333
context of vancouver,1,1,1.0
of vancouver canada,1,1,1.0
vancouver canada this,1,1,1.0
canada this set,1,1,1.0
this set was,1,1,1.0
set was used,1,1,1.0
was used at,1,1,1.0
used at one,1,1,1.0
at one of,1,1,1.0
the data mining,1,1,1.0
data mining competitions,1,1,1.0
mining competitions the,1,1,1.0
competitions the set,1,1,1.0
the set involves,1,1,1.0
set involves a,1,1,1.0
involves a big,1,1,1.0
a big data,1,1,1.0
big data problem,1,1,1.0
data problem with,1,1,1.0
problem with imbalance,1,1,1.0
with imbalance bioinformatics,1,1,1.0
imbalance bioinformatics whereas,1,1,1.0
bioinformatics whereas there,1,1,1.0
whereas there were,1,1,1.0
there were attributes,1,1,1.0
were attributes and,1,1,1.0
attributes and million,1,1,1.0
and million instances,1,1,1.0
million instances positive,1,1,1.0
instances positive instances,1,1,1.0
positive instances were,1,1,1.0
instances were found,1,1,1.0
were found to,2,2,1.0
found to be,2,2,1.0
be this study,1,1,1.0
this study relies,1,1,1.0
study relies on,1,1,1.0
relies on s,1,1,1.0
on s two,1,1,1.0
s two imbalanced,1,1,1.0
two imbalanced sets,1,1,1.0
imbalanced sets of,1,1,1.0
of data the,3,1,3.0
data the sets,1,1,1.0
the sets are,1,1,1.0
sets are generated,1,1,1.0
are generated in,2,2,1.0
generated in such,1,1,1.0
way that the,2,1,2.0
that the same,1,1,1.0
the same algorithm,1,1,1.0
same algorithm cluster,1,1,1.0
algorithm cluster the,1,1,1.0
cluster the dataset,1,1,1.0
the dataset cluster,1,1,1.0
dataset cluster the,1,1,1.0
cluster the samples,1,1,1.0
the samples using,1,1,1.0
samples using the,1,1,1.0
using the scalable,1,1,1.0
the scalable api,1,1,1.0
scalable api provided,1,1,1.0
api provided by,1,1,1.0
provided by spark,1,1,1.0
by spark ml,1,1,1.0
spark ml library,1,1,1.0
ml library number,1,1,1.0
library number of,1,1,1.0
number of groups,1,1,1.0
of groups given,1,1,1.0
groups given as,1,1,1.0
given as input,1,1,1.0
as input for,1,1,1.0
input for clustering,1,1,1.0
for clustering should,1,1,1.0
clustering should be,1,1,1.0
should be equal,2,2,1.0
be equal to,1,1,1.0
equal to the,2,2,1.0
number of for,1,1,1.0
of for each,1,1,1.0
for each sample,4,1,4.0
each sample do,2,1,2.0
sample do id,1,1,1.0
do id closest,1,1,1.0
id closest pivot,1,1,1.0
closest pivot for,1,1,1.0
pivot for each,1,1,1.0
for each point,1,1,1.0
each point end,1,1,1.0
point end for,1,1,1.0
end for each,1,1,1.0
sample do calculate,1,1,1.0
do calculate tance,1,1,1.0
calculate tance with,1,1,1.0
tance with pivots,1,1,1.0
with pivots parent,1,1,1.0
pivots parent pivot,1,1,1.0
parent pivot pivot,1,1,1.0
pivot pivot with,1,1,1.0
pivot with shortest,1,1,1.0
with shortest distance,1,1,1.0
shortest distance done,1,1,1.0
distance done for,1,1,1.0
done for each,1,1,1.0
for each cluster,1,1,1.0
each cluster do,1,1,1.0
cluster do push,1,1,1.0
do push each,1,1,1.0
push each cluster,1,1,1.0
each cluster to,1,1,1.0
cluster to a,1,1,1.0
to a different,1,1,1.0
a different done,1,1,1.0
different done up,1,1,1.0
done up sampling,1,1,1.0
up sampling ratio,1,1,1.0
sampling ratio and,1,1,1.0
ratio and nearest,1,1,1.0
and nearest neighbors,1,1,1.0
nearest neighbors k,1,1,1.0
neighbors k provided,1,1,1.0
k provided as,1,1,1.0
provided as input,1,1,1.0
as input number,1,1,1.0
input number of,1,1,1.0
number of synthetic,13,3,4.333333333333333
of synthetic data,5,2,2.5
data to be,1,1,1.0
be generated per,1,1,1.0
generated per sample,1,1,1.0
per sample m,1,1,1.0
sample m is,1,1,1.0
m is given,1,1,1.0
is given by,2,2,1.0
given by this,1,1,1.0
by this part,1,1,1.0
this part runs,1,1,1.0
part runs on,1,1,1.0
runs on individual,1,1,1.0
on individual given,1,1,1.0
individual given the,1,1,1.0
the respective partition,1,1,1.0
respective partition perform,1,1,1.0
partition perform the,1,1,1.0
perform the procedure,1,1,1.0
procedure for ual,1,1,1.0
for ual samples,1,1,1.0
ual samples also,1,1,1.0
samples also establish,1,1,1.0
also establish an,1,1,1.0
establish an the,1,1,1.0
an the second,1,1,1.0
the second algorithm,1,1,1.0
second algorithm end,1,1,1.0
algorithm end for,1,1,1.0
end for the,1,1,1.0
respective samples in,1,1,1.0
samples in the,1,1,1.0
in the selected,1,1,1.0
the selected or,1,1,1.0
selected or target,1,1,1.0
or target partitions,1,1,1.0
target partitions before,1,1,1.0
partitions before searching,1,1,1.0
before searching k,1,1,1.0
searching k nearest,1,1,1.0
k nearest neighbors,8,2,4.0
from the establish,1,1,1.0
the establish the,1,1,1.0
establish the third,1,1,1.0
the third algorithm,1,1,1.0
third algorithm given,1,1,1.0
algorithm given the,1,1,1.0
given the individual,1,1,1.0
the individual k,1,1,1.0
individual k neighbors,1,1,1.0
k neighbors establish,1,1,1.0
neighbors establish m,1,1,1.0
establish m thetic,1,1,1.0
m thetic samples,1,1,1.0
thetic samples to,1,1,1.0
samples to achieve,1,1,1.0
to achieve the,1,1,1.0
achieve the fourth,1,1,1.0
the fourth algorithm,1,1,1.0
fourth algorithm algorithm,1,1,1.0
algorithm algorithm build,1,1,1.0
algorithm build m,1,1,1.0
build m tree,1,1,1.0
m tree this,1,1,1.0
tree this algorithm,1,1,1.0
this algorithm takes,1,1,1.0
algorithm takes the,1,1,1.0
takes the capacity,1,1,1.0
of the c,1,1,1.0
the c as,1,1,1.0
c as the,1,1,1.0
as the input,1,1,1.0
the input init,1,1,1.0
input init add,1,1,1.0
init add the,1,1,1.0
add the first,1,1,1.0
the first point,1,1,1.0
first point as,1,1,1.0
point as a,1,1,1.0
as a router,1,1,1.0
a router for,1,1,1.0
router for p,1,1,1.0
for p each,1,1,1.0
p each sample,1,1,1.0
each sample in,1,1,1.0
sample in the,2,1,2.0
in the partition,2,1,2.0
the partition do,2,1,2.0
partition do calculate,1,1,1.0
do calculate the,1,1,1.0
calculate the distances,1,1,1.0
the distances from,1,1,1.0
distances from each,1,1,1.0
from each router,1,1,1.0
each router choose,1,1,1.0
router choose the,1,1,1.0
choose the router,1,1,1.0
the router with,1,1,1.0
router with minimum,1,1,1.0
with minimum distance,1,1,1.0
minimum distance if,1,1,1.0
distance if distance,1,1,1.0
if distance router,1,1,1.0
distance router covering,1,1,1.0
router covering radius,1,1,1.0
covering radius update,1,1,1.0
radius update covering,1,1,1.0
update covering radius,1,1,1.0
covering radius add,1,1,1.0
radius add the,1,1,1.0
add the sample,1,1,1.0
the sample as,1,1,1.0
sample as leaf,1,1,1.0
as leaf node,1,1,1.0
leaf node to,2,1,2.0
node to the,1,1,1.0
to the router,2,1,2.0
the router increment,1,1,1.0
router increment leaf,1,1,1.0
increment leaf count,1,1,1.0
leaf count if,1,1,1.0
count if leaf,1,1,1.0
if leaf count,1,1,1.0
leaf count maxleafsize,1,1,1.0
count maxleafsize promote,1,1,1.0
maxleafsize promote another,1,1,1.0
promote another leaf,1,1,1.0
another leaf node,1,1,1.0
node to router,1,1,1.0
to router split,1,1,1.0
router split the,1,1,1.0
split the leaf,1,1,1.0
leaf nodes between,1,1,1.0
nodes between the,1,1,1.0
the two routers,1,1,1.0
two routers done,1,1,1.0
routers done algorithm,1,1,1.0
done algorithm finding,1,1,1.0
algorithm finding nearest,1,1,1.0
finding nearest neighbors,1,1,1.0
nearest neighbors for,1,1,1.0
neighbors for each,1,1,1.0
each sample p,2,1,2.0
sample p in,2,1,2.0
p in the,1,1,1.0
partition do store,1,1,1.0
do store the,1,1,1.0
store the first,1,1,1.0
the first k,1,1,1.0
first k points,1,1,1.0
k points in,1,1,1.0
points in a,2,2,1.0
in a priority,1,1,1.0
a priority queue,1,1,1.0
priority queue of,1,1,1.0
queue of size,1,1,1.0
of size k,1,1,1.0
size k priority,1,1,1.0
k priority distance,1,1,1.0
priority distance from,1,1,1.0
distance from point,1,1,1.0
from point p,1,1,1.0
point p smaller,1,1,1.0
p smaller distance,1,1,1.0
smaller distance means,1,1,1.0
distance means higher,1,1,1.0
means higher priority,1,1,1.0
higher priority if,1,1,1.0
priority if distance,1,1,1.0
if distance of,1,1,1.0
distance of sample,1,1,1.0
of sample p,1,1,1.0
sample p from,1,1,1.0
p from router,1,1,1.0
from router distance,1,1,1.0
router distance of,1,1,1.0
distance of another,1,1,1.0
of another sample,1,1,1.0
another sample belonging,1,1,1.0
sample belonging to,1,1,1.0
belonging to the,9,3,3.0
the router the,1,1,1.0
router the max,1,1,1.0
the max distance,1,1,1.0
max distance of,1,1,1.0
distance of the,1,1,1.0
of the samples,3,1,3.0
the samples stored,1,1,1.0
samples stored in,1,1,1.0
stored in the,1,1,1.0
in the priority,2,1,2.0
the priority q,2,1,2.0
priority q ignore,1,1,1.0
q ignore the,1,1,1.0
ignore the sample,1,1,1.0
the sample else,1,1,1.0
sample else calculate,1,1,1.0
else calculate the,1,1,1.0
calculate the distance,1,1,1.0
the distance from,1,1,1.0
distance from sample,1,1,1.0
from sample p,1,1,1.0
sample p and,1,1,1.0
p and store,1,1,1.0
and store the,1,1,1.0
store the sample,1,1,1.0
the sample in,1,1,1.0
priority q end,1,1,1.0
q end if,1,1,1.0
end if done,1,1,1.0
if done class,1,1,1.0
done class distribution,1,1,1.0
distribution is considered,1,1,1.0
is considered for,1,1,1.0
considered for training,1,1,1.0
for training purposes,1,1,1.0
training purposes the,1,1,1.0
purposes the tigation,1,1,1.0
the tigation also,1,1,1.0
tigation also considers,1,1,1.0
also considers a,1,1,1.0
considers a larger,1,1,1.0
a larger dataset,1,1,1.0
larger dataset however,1,1,1.0
dataset however when,1,1,1.0
however when it,1,1,1.0
when it comes,1,1,1.0
it comes to,1,1,1.0
comes to testing,1,1,1.0
to testing procedure,1,1,1.0
testing procedure a,1,1,1.0
procedure a smaller,1,1,1.0
a smaller set,1,1,1.0
smaller set is,1,1,1.0
set is used,1,1,1.0
is used with,1,1,1.0
used with senting,1,1,1.0
with senting a,1,1,1.0
senting a large,1,1,1.0
of features the,2,2,1.0
features the feature,1,1,1.0
the feature reduction,2,1,2.0
feature reduction procedure,1,1,1.0
reduction procedure hooda,1,1,1.0
procedure hooda and,1,1,1.0
intelligence systems algorithm,1,1,1.0
systems algorithm smote,1,1,1.0
algorithm smote for,1,1,1.0
smote for all,1,1,1.0
for all the,4,2,2.0
all the of,1,1,1.0
the of samples,1,1,1.0
of samples for,1,1,1.0
samples for each,1,1,1.0
p in partition,1,1,1.0
in partition for,1,1,1.0
partition for each,1,1,1.0
of the k,2,2,1.0
the k neighbors,1,1,1.0
k neighbors gap,1,1,1.0
neighbors gap random,1,1,1.0
gap random number,1,1,1.0
between and synthetic,1,1,1.0
and synthetic attribute,1,1,1.0
synthetic attribute p,1,1,1.0
attribute p attribute,1,1,1.0
p attribute gap,1,1,1.0
attribute gap difference,1,1,1.0
gap difference in,1,1,1.0
difference in values,1,1,1.0
in values of,1,1,1.0
values of this,1,1,1.0
of this attribute,1,1,1.0
this attribute between,1,1,1.0
attribute between p,1,1,1.0
between p and,1,1,1.0
p and neighbor,1,1,1.0
and neighbor done,1,1,1.0
neighbor done the,1,1,1.0
done the choice,1,1,1.0
the choice for,1,1,1.0
choice for n,1,1,1.0
for n took,1,1,1.0
n took some,1,1,1.0
took some experimentation,1,1,1.0
some experimentation we,1,1,1.0
experimentation we tried,1,1,1.0
we tried for,1,1,1.0
tried for various,1,1,1.0
for various distinct,1,1,1.0
various distinct values,1,1,1.0
distinct values of,1,1,1.0
values of n,1,1,1.0
of n and,1,1,1.0
n and found,1,1,1.0
found that the,1,1,1.0
that the optimum,1,1,1.0
the optimum results,1,1,1.0
optimum results were,1,1,1.0
results were obtained,1,1,1.0
were obtained for,1,1,1.0
obtained for n,1,1,1.0
for n has,1,1,1.0
n has been,1,1,1.0
has been employed,1,1,1.0
been employed to,1,1,1.0
employed to ensure,1,1,1.0
ensure that relevant,1,1,1.0
that relevant and,1,1,1.0
relevant and utmost,1,1,1.0
and utmost ate,1,1,1.0
utmost ate features,1,1,1.0
ate features are,1,1,1.0
features are acquired,1,1,1.0
are acquired in,1,1,1.0
acquired in particular,1,1,1.0
in particular the,2,2,1.0
particular the feature,1,1,1.0
feature reduction lates,1,1,1.0
reduction lates into,1,1,1.0
lates into concentration,1,1,1.0
into concentration on,1,1,1.0
concentration on out,1,1,1.0
on out of,1,1,1.0
out of the,1,1,1.0
the original features,1,1,1.0
original features using,1,1,1.0
features using the,1,1,1.0
using the hadoop,1,1,1.0
the hadoop map,1,1,1.0
hadoop map reduce,1,1,1.0
map reduce approach,1,1,1.0
reduce approach the,1,1,1.0
approach the smote,1,1,1.0
the smote implementation,1,1,1.0
smote implementation which,1,1,1.0
implementation which is,1,1,1.0
which is distributed,1,1,1.0
is distributed is,1,1,1.0
distributed is benchmarked,1,1,1.0
is benchmarked using,1,1,1.0
benchmarked using the,1,1,1.0
using the dataset,1,1,1.0
dataset to compare,1,1,1.0
to compare the,2,2,1.0
compare the performance,1,1,1.0
the algorithm with,1,1,1.0
algorithm with the,1,1,1.0
with the results,1,1,1.0
the results obtained,8,2,4.0
results obtained from,2,2,1.0
obtained from previous,1,1,1.0
from previous approaches,1,1,1.0
previous approaches such,1,1,1.0
approaches such as,2,2,1.0
such as python,1,1,1.0
as python implementation,1,1,1.0
python implementation of,2,1,2.0
of smote two,1,1,1.0
smote two additional,1,1,1.0
two additional sets,1,1,1.0
additional sets of,1,1,1.0
data are used,1,1,1.0
are used they,1,1,1.0
used they include,1,1,1.0
they include sat,1,1,1.0
include sat image,1,1,1.0
sat image and,1,1,1.0
image and yeast,1,1,1.0
and yeast which,1,1,1.0
yeast which originate,1,1,1.0
which originate form,1,1,1.0
originate form uci,1,1,1.0
form uci viz,1,1,1.0
uci viz abalone,1,1,1.0
viz abalone the,1,1,1.0
abalone the framework,1,1,1.0
the framework or,1,1,1.0
framework or we,1,1,1.0
can say infrastructure,1,1,1.0
say infrastructure which,1,1,1.0
infrastructure which was,1,1,1.0
which was utilized,1,1,1.0
was utilized for,1,1,1.0
utilized for these,1,1,1.0
for these experiments,1,1,1.0
these experiments consist,1,1,1.0
experiments consist of,1,1,1.0
consist of a,1,1,1.0
of a cluster,1,1,1.0
cluster of four,1,1,1.0
of four centos,1,1,1.0
four centos linux,1,1,1.0
centos linux machines,1,1,1.0
linux machines each,1,1,1.0
machines each having,1,1,1.0
each having eight,1,1,1.0
having eight cores,1,1,1.0
eight cores and,1,1,1.0
cores and gb,1,1,1.0
and gb ram,1,1,1.0
gb ram spark,1,1,1.0
ram spark was,1,1,1.0
spark was used,1,1,1.0
was used to,1,1,1.0
used to configure,1,1,1.0
to configure the,1,1,1.0
configure the cluster,1,1,1.0
the cluster and,1,1,1.0
cluster and input,1,1,1.0
and input as,1,1,1.0
input as well,1,1,1.0
well as output,1,1,1.0
as output data,1,1,1.0
output data was,1,1,1.0
data was stored,1,1,1.0
was stored with,1,1,1.0
stored with the,1,1,1.0
help of hadoop,1,1,1.0
of hadoop hdfs,1,1,1.0
hadoop hdfs analysis,1,1,1.0
hdfs analysis the,1,1,1.0
analysis the input,1,1,1.0
the input parameters,1,1,1.0
input parameters to,1,1,1.0
parameters to the,1,1,1.0
to the algorithm,2,2,1.0
the algorithm were,1,1,1.0
algorithm were as,1,1,1.0
were as follows,1,1,1.0
as follows the,2,2,1.0
follows the number,2,2,1.0
number of clusters,4,1,4.0
of clusters to,1,1,1.0
clusters to be,1,1,1.0
to be created,1,1,1.0
be created by,1,1,1.0
created by scalable,1,1,1.0
by scalable was,1,1,1.0
scalable was taken,1,1,1.0
was taken as,1,1,1.0
taken as eight,1,1,1.0
as eight the,1,1,1.0
eight the of,1,1,1.0
the of clusters,1,1,1.0
of clusters must,1,1,1.0
clusters must be,1,1,1.0
must be tuned,2,1,2.0
be tuned so,1,1,1.0
tuned so that,1,1,1.0
so that it,1,1,1.0
it is neither,1,1,1.0
is neither too,1,1,1.0
neither too high,1,1,1.0
too high nor,1,1,1.0
high nor too,1,1,1.0
nor too low,1,1,1.0
too low providing,1,1,1.0
low providing a,1,1,1.0
providing a small,1,1,1.0
of clusters may,1,1,1.0
clusters may again,1,1,1.0
may again lead,1,1,1.0
again lead to,1,1,1.0
to a problem,1,1,1.0
a problem where,1,1,1.0
problem where all,1,1,1.0
where all the,1,1,1.0
the samples belonging,1,1,1.0
a cluster may,1,1,1.0
cluster may not,1,1,1.0
may not fit,2,1,2.0
not fit into,1,1,1.0
fit into a,1,1,1.0
into a single,1,1,1.0
single machine disk,1,1,1.0
machine disk memory,1,1,1.0
disk memory setting,1,1,1.0
memory setting the,1,1,1.0
setting the number,1,1,1.0
of clusters too,1,1,1.0
clusters too high,1,1,1.0
too high though,1,1,1.0
high though may,1,1,1.0
though may increase,1,1,1.0
may increase the,2,1,2.0
increase the parallelism,1,1,1.0
the parallelism and,1,1,1.0
parallelism and throughput,1,1,1.0
and throughput but,1,1,1.0
throughput but may,1,1,1.0
but may cause,1,1,1.0
may cause the,1,1,1.0
cause the samples,1,1,1.0
samples which are,2,1,2.0
which are similar,1,1,1.0
are similar and,1,1,1.0
similar and near,1,1,1.0
and near to,1,1,1.0
near to one,1,1,1.0
one another to,1,1,1.0
another to fall,1,1,1.0
to fall into,1,1,1.0
fall into different,1,1,1.0
into different clusters,1,1,1.0
different clusters thereby,1,1,1.0
clusters thereby decreasing,1,1,1.0
thereby decreasing the,1,1,1.0
decreasing the quality,1,1,1.0
of the synthetic,6,3,2.0
the synthetic data,4,2,2.0
synthetic data produced,1,1,1.0
data produced as,1,1,1.0
produced as the,1,1,1.0
as the smote,1,1,1.0
smote algorithm relies,1,1,1.0
algorithm relies heavily,1,1,1.0
heavily on finding,1,1,1.0
on finding the,1,1,1.0
finding the nearest,1,1,1.0
nearest neighbors this,1,1,1.0
neighbors this number,1,1,1.0
this number must,1,1,1.0
number must be,1,1,1.0
be tuned based,1,1,1.0
tuned based on,1,1,1.0
on the dimension,1,1,1.0
the dimension of,1,1,1.0
dimension of the,2,2,1.0
dataset and the,3,3,1.0
and the segment,1,1,1.0
the segment cluster,1,1,1.0
segment cluster specifications,1,1,1.0
cluster specifications with,1,1,1.0
specifications with the,1,1,1.0
with the emphasis,1,1,1.0
the emphasis on,1,1,1.0
emphasis on using,1,1,1.0
on using the,1,1,1.0
using the least,1,1,1.0
the least ble,1,1,1.0
least ble for,1,1,1.0
ble for the,1,1,1.0
for the cluster,1,1,1.0
the cluster configuration,1,1,1.0
cluster configuration another,1,1,1.0
configuration another crucial,1,1,1.0
another crucial input,1,1,1.0
crucial input parameter,1,1,1.0
input parameter that,1,1,1.0
parameter that is,1,1,1.0
that is worth,1,1,1.0
is worth considering,1,1,1.0
worth considering involves,1,1,1.0
considering involves the,1,1,1.0
involves the number,1,1,1.0
number of nearest,5,2,2.5
of nearest numbers,1,1,1.0
nearest numbers that,1,1,1.0
numbers that need,1,1,1.0
that need to,3,2,1.5
to be used,6,3,2.0
be used to,8,4,2.0
used to ensure,1,1,1.0
that the synthetic,1,1,1.0
synthetic data is,2,1,2.0
data is generated,1,1,1.0
is generated particularly,1,1,1.0
generated particularly the,1,1,1.0
particularly the stage,1,1,1.0
the stage focuses,1,1,1.0
stage focuses on,1,1,1.0
focuses on the,2,2,1.0
on the number,2,2,1.0
of nearest neighbors,5,2,2.5
nearest neighbors to,2,2,1.0
neighbors to be,1,1,1.0
to be sought,1,1,1.0
be sought from,1,1,1.0
sought from the,1,1,1.0
from the respective,1,1,1.0
the respective ples,1,1,1.0
respective ples upon,1,1,1.0
ples upon which,1,1,1.0
upon which the,1,1,1.0
which the smote,1,1,1.0
smote algorithm could,1,1,1.0
algorithm could be,1,1,1.0
could be used,2,2,1.0
used to establish,1,1,1.0
to establish synthetic,1,1,1.0
establish synthetic data,1,1,1.0
synthetic data in,2,1,2.0
this investigation five,1,1,1.0
investigation five is,1,1,1.0
five is the,1,1,1.0
is the number,6,4,1.5
the number at,1,1,1.0
number at which,1,1,1.0
at which the,1,1,1.0
which the number,1,1,1.0
nearest neighbors used,2,2,1.0
neighbors used to,2,2,1.0
data is kept,1,1,1.0
is kept in,1,1,1.0
kept in the,1,1,1.0
in the study,1,1,1.0
the study by,1,1,1.0
study by chawla,1,1,1.0
by chawla this,1,1,1.0
chawla this approach,1,1,1.0
this approach was,1,1,1.0
approach was employed,1,1,1.0
was employed ratio,1,1,1.0
employed ratio determines,1,1,1.0
ratio determines the,1,1,1.0
determines the fraction,1,1,1.0
the fraction by,1,1,1.0
fraction by which,1,1,1.0
by which we,1,1,1.0
which we need,1,1,1.0
need to the,1,1,1.0
the minority dataset,2,2,1.0
minority dataset a,1,1,1.0
dataset a ratio,1,1,1.0
a ratio of,1,1,1.0
ratio of one,1,1,1.0
of one would,1,1,1.0
one would generate,1,1,1.0
would generate the,1,1,1.0
the minority samples,1,1,1.0
minority samples so,1,1,1.0
samples so that,1,1,1.0
so that they,2,2,1.0
they are equal,1,1,1.0
are equal in,1,1,1.0
equal in count,1,1,1.0
in count with,1,1,1.0
count with the,1,1,1.0
the majority samples,1,1,1.0
majority samples providing,1,1,1.0
samples providing an,1,1,1.0
providing an ratio,1,1,1.0
an ratio of,1,1,1.0
ratio of will,1,1,1.0
of will create,1,1,1.0
will create a,1,1,1.0
create a final,1,1,1.0
a final distribution,1,1,1.0
final distribution of,1,1,1.0
distribution of majority,1,1,1.0
of majority samples,1,1,1.0
majority samples vs,1,1,1.0
samples vs minority,1,1,1.0
vs minority samples,1,1,1.0
minority samples to,1,1,1.0
samples to be,2,2,1.0
in the ratio,1,1,1.0
the ratio of,2,2,1.0
ratio of are,1,1,1.0
of are balanced,1,1,1.0
are balanced trees,1,1,1.0
balanced trees and,1,1,1.0
trees and each,1,1,1.0
and each routing,1,1,1.0
each routing node,1,1,1.0
routing node of,1,1,1.0
node of the,1,1,1.0
of the tree,1,1,1.0
the tree has,1,1,1.0
tree has a,1,1,1.0
has a maximum,1,1,1.0
a maximum size,1,1,1.0
maximum size capacity,1,1,1.0
size capacity of,1,1,1.0
capacity of leaf,1,1,1.0
leaf nodes that,1,1,1.0
nodes that it,1,1,1.0
that it can,2,2,1.0
it can store,1,1,1.0
can store the,1,1,1.0
store the knn,1,1,1.0
the knn search,1,1,1.0
knn search for,1,1,1.0
search for a,1,1,1.0
a sample is,1,1,1.0
sample is performed,1,1,1.0
is performed in,2,2,1.0
performed in each,1,1,1.0
in each of,2,2,1.0
of these leaf,1,1,1.0
these leaf nodes,1,1,1.0
leaf nodes ing,1,1,1.0
nodes ing to,1,1,1.0
ing to the,2,2,1.0
to the routing,1,1,1.0
node a large,1,1,1.0
a large capacity,1,1,1.0
large capacity may,1,1,1.0
capacity may improve,1,1,1.0
may improve the,1,1,1.0
improve the quality,1,1,1.0
quality of nearest,1,1,1.0
of nearest neighbor,2,2,1.0
neighbor search but,1,1,1.0
search but may,1,1,1.0
but may impact,1,1,1.0
may impact the,1,1,1.0
impact the throughput,1,1,1.0
the throughput as,1,1,1.0
throughput as larger,1,1,1.0
as larger number,1,1,1.0
larger number of,1,1,1.0
number of samples,1,1,1.0
of samples need,1,1,1.0
samples need to,1,1,1.0
to be searched,1,1,1.0
be searched for,1,1,1.0
searched for identifying,1,1,1.0
for identifying the,1,1,1.0
identifying the nearest,1,1,1.0
nearest neighbors while,1,1,1.0
neighbors while a,1,1,1.0
while a smaller,1,1,1.0
a smaller leaf,1,1,1.0
smaller leaf capacity,1,1,1.0
leaf capacity may,1,1,1.0
capacity may split,1,1,1.0
may split the,1,1,1.0
split the similar,1,1,1.0
the similar ples,1,1,1.0
similar ples into,1,1,1.0
ples into two,1,1,1.0
into two different,1,1,1.0
two different groups,1,1,1.0
different groups thereby,1,1,1.0
groups thereby reducing,1,1,1.0
thereby reducing the,1,1,1.0
reducing the quality,1,1,1.0
the samples generated,2,1,2.0
samples generated tuning,1,1,1.0
generated tuning the,1,1,1.0
tuning the capacity,1,1,1.0
is important and,1,1,1.0
important and for,1,1,1.0
and for this,1,1,1.0
for this experiment,2,2,1.0
this experiment we,2,2,1.0
experiment we arrived,1,1,1.0
we arrived at,1,1,1.0
arrived at a,1,1,1.0
at a size,1,1,1.0
a size of,1,1,1.0
size of after,1,1,1.0
of after few,1,1,1.0
after few iments,1,1,1.0
few iments minority,1,1,1.0
iments minority class,1,1,1.0
minority class data,1,1,1.0
class data was,1,1,1.0
data was originated,1,1,1.0
was originated synthetically,1,1,1.0
originated synthetically with,1,1,1.0
synthetically with the,1,1,1.0
help of our,1,1,1.0
of our algorithms,1,1,1.0
our algorithms then,1,1,1.0
algorithms then we,1,1,1.0
then we merged,1,1,1.0
we merged that,1,1,1.0
merged that data,1,1,1.0
that data with,1,1,1.0
data with the,1,1,1.0
with the data,1,1,1.0
data set which,1,1,1.0
set which we,1,1,1.0
which we have,1,1,1.0
we have set,1,1,1.0
have set alongside,1,1,1.0
set alongside for,1,1,1.0
alongside for the,1,1,1.0
for the purpose,3,2,1.5
the purpose of,7,2,3.5
purpose of training,1,1,1.0
of training given,1,1,1.0
training given the,1,1,1.0
given the ecbdl,1,1,1.0
the ecbdl set,2,1,2.0
ecbdl set of,2,1,2.0
data the accuracy,2,2,1.0
the model was,1,1,1.0
model was assessed,1,1,1.0
was assessed using,1,1,1.0
assessed using classification,1,1,1.0
using classification algorithms,1,1,1.0
classification algorithms linked,1,1,1.0
algorithms linked to,1,1,1.0
linked to the,1,1,1.0
to the distributed,1,1,1.0
the distributed random,1,1,1.0
distributed random forest,2,1,2.0
random forest drf,1,1,1.0
forest drf this,1,1,1.0
drf this process,1,1,1.0
this process targeted,1,1,1.0
process targeted the,1,1,1.0
targeted the test,1,1,1.0
the test data,3,2,1.5
test data that,2,2,1.0
data that was,1,1,1.0
that was achieved,1,1,1.0
was achieved after,1,1,1.0
achieved after feature,1,1,1.0
after feature reduction,1,1,1.0
feature reduction the,1,1,1.0
reduction the model,1,1,1.0
the model parameters,1,1,1.0
model parameters employed,1,1,1.0
parameters employed in,1,1,1.0
this study included,1,1,1.0
study included sat,1,1,1.0
included sat image,1,1,1.0
sat image datasets,1,1,1.0
image datasets and,1,1,1.0
datasets and the,1,1,1.0
and the scikit,1,1,1.0
the scikit python,1,1,1.0
scikit python implementation,1,1,1.0
implementation of random,1,1,1.0
of random forest,1,1,1.0
random forest for,1,1,1.0
forest for the,1,1,1.0
for the abalone,1,1,1.0
the abalone tables,1,1,1.0
abalone tables and,1,1,1.0
tables and from,1,1,1.0
and from the,1,1,1.0
from the spark,1,1,1.0
spark implementation the,1,1,1.0
implementation the synthetic,1,1,1.0
synthetic data s,1,1,1.0
data s accuracy,1,1,1.0
s accuracy was,1,1,1.0
accuracy was benchmarked,1,1,1.0
was benchmarked via,1,1,1.0
benchmarked via minority,1,1,1.0
via minority data,1,1,1.0
minority data generation,1,1,1.0
data generation facilitated,1,1,1.0
generation facilitated through,1,1,1.0
facilitated through smote,1,1,1.0
through smote s,1,1,1.0
smote s python,1,1,1.0
s python implementation,1,1,1.0
python implementation also,1,1,1.0
implementation also various,1,1,1.0
also various input,1,1,1.0
various input parameters,1,1,1.0
input parameters aided,1,1,1.0
parameters aided in,1,1,1.0
aided in the,1,1,1.0
in the tuning,1,1,1.0
the tuning of,1,1,1.0
tuning of the,1,1,1.0
of the performance,2,2,1.0
the algorithm indeed,1,1,1.0
algorithm indeed optimal,1,1,1.0
indeed optimal outcomes,1,1,1.0
optimal outcomes were,2,1,2.0
outcomes were realized,1,1,1.0
were realized at,1,1,1.0
realized at a,1,1,1.0
at a point,1,1,1.0
a point when,1,1,1.0
point when four,1,1,1.0
when four was,1,1,1.0
four was set,1,1,1.0
was set as,1,1,1.0
set as the,1,1,1.0
as the number,1,1,1.0
of clusters with,1,1,1.0
with the set,1,1,1.0
the set s,2,2,1.0
set s leaf,1,1,1.0
s leaf size,1,1,1.0
leaf size set,1,1,1.0
size set at,1,1,1.0
set at for,1,1,1.0
at for the,1,1,1.0
for the case,4,2,2.0
of the smaller,1,1,1.0
the smaller set,1,1,1.0
smaller set of,1,1,1.0
set of was,1,1,1.0
of was less,1,1,1.0
was less than,1,1,1.0
less than for,2,2,1.0
than for the,2,2,1.0
for the larger,1,1,1.0
the larger data,1,1,1.0
larger data set,1,1,1.0
data set optimal,1,1,1.0
set optimal outcomes,1,1,1.0
outcomes were also,1,1,1.0
were also table,1,1,1.0
also table distribution,1,1,1.0
table distribution for,1,1,1.0
distribution for the,1,1,1.0
the dataset being,1,1,1.0
dataset being used,1,1,1.0
being used dataset,1,1,1.0
used dataset attr,1,1,1.0
dataset attr class,1,1,1.0
attr class max,1,1,1.0
class max min,2,1,2.0
max min class,2,1,2.0
min class max,1,1,1.0
min class ecbdl,1,1,1.0
class ecbdl million,1,1,1.0
ecbdl million samples,1,1,1.0
million samples keel,1,1,1.0
samples keel samples,2,1,2.0
keel samples keel,1,1,1.0
keel samples uci,1,1,1.0
samples uci satimage,1,1,1.0
uci satimage samples,1,1,1.0
satimage samples table,1,1,1.0
samples table input,1,1,1.0
table input parameters,1,1,1.0
input parameters parameter,1,1,1.0
parameters parameter value,1,1,1.0
parameter value of,1,1,1.0
value of clusters,1,1,1.0
of clusters nearest,1,1,1.0
clusters nearest neighbor,1,1,1.0
nearest neighbor upsampling,1,1,1.0
neighbor upsampling ratio,1,1,1.0
upsampling ratio capacity,1,1,1.0
ratio capacity hooda,1,1,1.0
capacity hooda and,1,1,1.0
intelligence systems achieved,1,1,1.0
systems achieved when,1,1,1.0
achieved when the,1,1,1.0
when the study,1,1,1.0
the study s,1,1,1.0
study s conditions,1,1,1.0
s conditions were,1,1,1.0
conditions were set,1,1,1.0
were set in,1,1,1.0
set in such,1,1,1.0
that the leaf,1,1,1.0
the leaf size,1,1,1.0
leaf size was,1,1,1.0
size was set,1,1,1.0
was set at,1,1,1.0
set at and,1,1,1.0
at and the,1,1,1.0
and the cluster,1,1,1.0
the cluster number,1,1,1.0
cluster number established,1,1,1.0
number established at,1,1,1.0
established at or,1,1,1.0
at or tables,1,1,1.0
or tables and,1,1,1.0
tables and represent,1,1,1.0
and represent the,1,1,1.0
represent the confusion,1,1,1.0
confusion matrix gm,1,1,1.0
matrix gm recall,1,1,1.0
gm recall and,1,1,1.0
recall and auc,1,1,1.0
and auc for,1,1,1.0
auc for the,1,1,1.0
for the implemented,1,1,1.0
the implemented drf,1,1,1.0
implemented drf and,1,1,1.0
drf and random,1,1,1.0
and random forest,1,1,1.0
random forest indeed,1,1,1.0
forest indeed the,1,1,1.0
indeed the outcomes,1,1,1.0
the outcomes demonstrate,1,1,1.0
outcomes demonstrate that,1,1,1.0
demonstrate that there,1,1,1.0
there is data,1,1,1.0
is data sampling,1,1,1.0
data sampling in,1,1,1.0
sampling in relation,1,1,1.0
relation to the,3,2,1.5
minority data findings,1,1,1.0
data findings suggest,1,1,1.0
findings suggest further,1,1,1.0
suggest further that,1,1,1.0
further that the,1,1,1.0
that the spark,1,1,1.0
the spark tation,1,1,1.0
spark tation tends,1,1,1.0
tation tends to,1,1,1.0
tends to exceed,1,1,1.0
to exceed or,1,1,1.0
exceed or match,1,1,1.0
or match compared,1,1,1.0
match compared to,1,1,1.0
compared to a,2,2,1.0
to a case,1,1,1.0
which the python,1,1,1.0
the python smote,1,1,1.0
python smote implementation,1,1,1.0
implementation is applied,1,1,1.0
is applied these,1,1,1.0
applied these observations,1,1,1.0
these observations are,1,1,1.0
observations are evidenced,1,1,1.0
are evidenced by,1,1,1.0
evidenced by the,1,1,1.0
by the information,1,1,1.0
the information in,1,1,1.0
information in tables,1,1,1.0
in tables therefore,1,1,1.0
tables therefore the,1,1,1.0
therefore the tributed,1,1,1.0
the tributed spark,1,1,1.0
tributed spark smote,1,1,1.0
spark smote exhibits,1,1,1.0
smote exhibits superior,2,1,2.0
exhibits superior performance,3,1,3.0
superior performance with,2,1,2.0
performance with a,1,1,1.0
with a lar,1,1,1.0
a lar trend,1,1,1.0
lar trend observed,1,1,1.0
trend observed even,1,1,1.0
observed even in,1,1,1.0
even in situations,1,1,1.0
in situations involving,3,1,3.0
situations involving an,1,1,1.0
involving an increased,1,1,1.0
an increased amount,1,1,1.0
increased amount of,1,1,1.0
amount of dataset,1,1,1.0
of dataset given,1,1,1.0
dataset given a,1,1,1.0
given a large,1,1,1.0
a large dataset,1,1,1.0
large dataset an,1,1,1.0
dataset an application,1,1,1.0
an application of,1,1,1.0
of smote on,1,1,1.0
on the ecbdl,1,1,1.0
of data in,1,1,1.0
this case is,1,1,1.0
case is seen,1,1,1.0
is seen to,3,2,1.5
seen to fail,1,1,1.0
to fail to,1,1,1.0
fail to steer,1,1,1.0
to steer improvements,1,1,1.0
steer improvements in,1,1,1.0
improvements in minority,1,1,1.0
in minority class,2,1,2.0
class to situations,1,1,1.0
situations where smote,1,1,1.0
where smote is,1,1,1.0
smote is not,1,1,1.0
is not employed,1,1,1.0
not employed the,1,1,1.0
employed the implication,1,1,1.0
the implication is,1,1,1.0
implication is that,1,1,1.0
is that smote,1,1,1.0
that smote is,1,1,1.0
smote is unlikely,1,1,1.0
is unlikely to,1,1,1.0
unlikely to prove,1,1,1.0
to prove beneficial,1,1,1.0
prove beneficial to,1,1,1.0
beneficial to the,1,1,1.0
to the given,1,1,1.0
the given dataset,1,1,1.0
given dataset because,1,1,1.0
dataset because does,1,1,1.0
because does not,1,1,1.0
does not yield,1,1,1.0
not yield a,1,1,1.0
yield a significant,1,1,1.0
a significant improvement,1,1,1.0
significant improvement in,1,1,1.0
improvement in minority,1,1,1.0
minority class prediction,1,1,1.0
class prediction on,1,1,1.0
prediction on the,1,1,1.0
other hand distributed,1,1,1.0
hand distributed smote,1,1,1.0
distributed smote exhibits,1,1,1.0
superior performance when,1,1,1.0
performance when employed,1,1,1.0
when employed in,1,1,1.0
employed in situations,1,1,1.0
situations involving large,2,1,2.0
involving large sets,1,1,1.0
of data outperforming,1,1,1.0
data outperforming sklearn,1,1,1.0
outperforming sklearn python,1,1,1.0
sklearn python implementation,1,1,1.0
python implementation hence,1,1,1.0
implementation hence the,1,1,1.0
hence the results,1,1,1.0
the results validate,1,1,1.0
results validate our,1,1,1.0
validate our implementation,1,1,1.0
our implementation indeed,1,1,1.0
implementation indeed the,1,1,1.0
indeed the findings,1,1,1.0
the findings demonstrate,1,1,1.0
findings demonstrate that,1,1,1.0
demonstrate that the,1,1,1.0
that the use,1,1,1.0
use of for,1,1,1.0
of for lishing,1,1,1.0
for lishing the,1,1,1.0
lishing the nearest,1,1,1.0
the nearest neighbor,2,1,2.0
nearest neighbor and,1,1,1.0
neighbor and also,1,1,1.0
and also the,1,1,1.0
also the implementation,1,1,1.0
the implementation of,2,1,2.0
implementation of the,2,2,1.0
of the distributing,1,1,1.0
the distributing data,1,1,1.0
distributing data aid,1,1,1.0
data aid in,1,1,1.0
aid in preserving,1,1,1.0
in preserving spatial,1,1,1.0
preserving spatial sample,1,1,1.0
spatial sample arrangements,1,1,1.0
sample arrangements in,1,1,1.0
arrangements in turn,1,1,1.0
in turn artificial,1,1,1.0
turn artificial samples,1,1,1.0
artificial samples are,1,1,1.0
samples are generated,1,1,1.0
are generated ensuring,1,1,1.0
generated ensuring that,1,1,1.0
ensuring that the,1,1,1.0
that the desired,1,1,1.0
the desired scale,1,1,1.0
desired scale and,1,1,1.0
scale and degree,1,1,1.0
and degree of,1,1,1.0
degree of accuracy,1,1,1.0
of accuracy are,1,1,1.0
accuracy are realized,1,1,1.0
are realized with,1,1,1.0
realized with increasing,1,1,1.0
with increasing volume,2,1,2.0
increasing volume of,2,1,2.0
volume of datasets,1,1,1.0
of datasets table,1,1,1.0
datasets table random,1,1,1.0
table random forest,1,1,1.0
random forest parameters,1,1,1.0
forest parameters algorithm,1,1,1.0
parameters algorithm parameters,1,1,1.0
algorithm parameters distributed,1,1,1.0
parameters distributed random,1,1,1.0
random forest number,1,1,1.0
forest number of,1,1,1.0
number of tress,1,1,1.0
of tress sample,1,1,1.0
tress sample rat,1,1,1.0
sample rat maximum,1,1,1.0
rat maximum tree,1,1,1.0
maximum tree depth,1,1,1.0
tree depth nbins,1,1,1.0
depth nbins table,1,1,1.0
nbins table performance,1,1,1.0
table performance comparison,4,1,4.0
performance comparison abalone,1,1,1.0
comparison abalone dataset,1,1,1.0
abalone dataset technique,1,1,1.0
dataset technique auc,4,1,4.0
technique auc recall,4,1,4.0
auc recall gm,4,1,4.0
recall gm confusion,4,1,4.0
gm confusion matrix,4,1,4.0
confusion matrix no,4,1,4.0
matrix no sampling,4,1,4.0
no sampling python,4,1,4.0
sampling python version,4,1,4.0
of smote spark,4,1,4.0
smote spark distributed,4,1,4.0
spark distributed smote,4,1,4.0
distributed smote gm,4,1,4.0
smote gm geometric,4,1,4.0
gm geometric mean,4,1,4.0
geometric mean smote,4,1,4.0
mean smote synthetic,4,1,4.0
oversampling technique table,2,1,2.0
technique table performance,2,1,2.0
performance comparison y,1,1,1.0
comparison y dataset,1,1,1.0
y dataset technique,1,1,1.0
oversampling technique observations,1,1,1.0
technique observations given,1,1,1.0
observations given the,1,1,1.0
given the proposed,1,1,1.0
the proposed algorithm,1,1,1.0
proposed algorithm the,1,1,1.0
algorithm the findings,1,1,1.0
the findings suggest,1,1,1.0
findings suggest the,1,1,1.0
suggest the capability,1,1,1.0
the capability of,1,1,1.0
capability of realizing,1,1,1.0
of realizing quality,1,1,1.0
realizing quality samples,1,1,1.0
quality samples the,1,1,1.0
samples the eventuality,1,1,1.0
is that the,5,3,1.6666666666666667
minority ples prediction,1,1,1.0
ples prediction is,1,1,1.0
prediction is likely,1,1,1.0
to be improved,1,1,1.0
be improved with,1,1,1.0
improved with increasing,1,1,1.0
data the results,1,1,1.0
the results indicate,1,1,1.0
results indicate that,2,2,1.0
indicate that the,1,1,1.0
that the traditional,1,1,1.0
the traditional smote,1,1,1.0
traditional smote tends,1,1,1.0
smote tends to,1,1,1.0
tends to be,1,1,1.0
be less efficient,1,1,1.0
less efficient making,1,1,1.0
efficient making it,1,1,1.0
making it less,1,1,1.0
it less applicable,1,1,1.0
less applicable in,1,1,1.0
applicable in situations,1,1,1.0
involving large datasets,1,1,1.0
large datasets previous,1,1,1.0
datasets previous research,1,1,1.0
previous research work,1,1,1.0
research work guided,1,1,1.0
work guided this,1,1,1.0
guided this investigation,1,1,1.0
this investigation paving,1,1,1.0
investigation paving the,1,1,1.0
paving the way,1,1,1.0
the way for,1,1,1.0
way for the,1,1,1.0
for the expansion,1,1,1.0
the expansion of,1,1,1.0
expansion of smote,1,1,1.0
smote for a,1,1,1.0
for a distributed,1,1,1.0
a distributed implementation,1,1,1.0
distributed implementation from,1,1,1.0
implementation from the,1,1,1.0
from the outcomes,1,1,1.0
the outcomes especially,1,1,1.0
outcomes especially when,1,1,1.0
especially when various,1,1,1.0
when various parameters,1,1,1.0
various parameters are,1,1,1.0
parameters are benchmarked,1,1,1.0
are benchmarked the,1,1,1.0
benchmarked the tation,1,1,1.0
the tation exhibits,1,1,1.0
tation exhibits superior,1,1,1.0
performance with our,1,1,1.0
with our implementation,1,1,1.0
our implementation of,2,2,1.0
smote we are,1,1,1.0
we are able,1,1,1.0
are able to,1,1,1.0
able to generate,1,1,1.0
data in a,1,1,1.0
a distributed ronment,1,1,1.0
distributed ronment when,1,1,1.0
ronment when the,1,1,1.0
the dataset may,1,1,1.0
dataset may not,1,1,1.0
not fit in,1,1,1.0
fit in a,1,1,1.0
in a single,1,1,1.0
single machine an,1,1,1.0
machine an extension,1,1,1.0
extension to the,1,1,1.0
to the existing,1,1,1.0
the existing implementation,1,1,1.0
existing implementation can,1,1,1.0
implementation can include,1,1,1.0
can include an,1,1,1.0
include an lap,1,1,1.0
an lap when,1,1,1.0
lap when indexing,1,1,1.0
when indexing samples,1,1,1.0
indexing samples in,1,1,1.0
samples in so,1,1,1.0
in so that,1,1,1.0
so that samples,1,1,1.0
that samples which,1,1,1.0
which are at,1,1,1.0
are at the,1,1,1.0
at the partition,1,1,1.0
the partition boundary,1,1,1.0
partition boundary may,1,1,1.0
boundary may be,1,1,1.0
may be included,1,1,1.0
be included in,1,1,1.0
included in both,1,1,1.0
in both the,1,1,1.0
both the partitions,1,1,1.0
the partitions this,1,1,1.0
partitions this may,1,1,1.0
this may increase,1,1,1.0
increase the quality,1,1,1.0
samples generated however,1,1,1.0
generated however it,1,1,1.0
however it may,2,2,1.0
it may be,1,1,1.0
may be possible,1,1,1.0
be possible that,1,1,1.0
possible that smote,1,1,1.0
that smote may,1,1,1.0
smote may not,1,1,1.0
may not be,4,3,1.3333333333333333
not be the,3,2,1.5
be the answer,1,1,1.0
the answer of,1,1,1.0
answer of all,1,1,1.0
of all the,1,1,1.0
all the data,1,1,1.0
the data imbalance,1,1,1.0
data imbalance issues,1,1,1.0
imbalance issues within,1,1,1.0
issues within class,1,1,1.0
within class imbalance,8,2,4.0
class imbalance is,5,3,1.6666666666666667
imbalance is not,3,2,1.5
is not handled,1,1,1.0
not handled properly,1,1,1.0
handled properly with,1,1,1.0
properly with this,1,1,1.0
implementation of distributed,1,1,1.0
of distributed smote,1,1,1.0
distributed smote there,1,1,1.0
smote there is,1,1,1.0
is a need,2,1,2.0
a need to,1,1,1.0
need to merge,1,1,1.0
to merge the,1,1,1.0
merge the current,1,1,1.0
the current solution,1,1,1.0
current solution with,1,1,1.0
solution with the,1,1,1.0
with the techniques,1,1,1.0
the techniques to,1,1,1.0
techniques to tify,1,1,1.0
to tify within,1,1,1.0
tify within class,1,1,1.0
imbalance and handle,1,1,1.0
and handle the,1,1,1.0
handle the within,1,1,1.0
the within class,5,3,1.6666666666666667
class imbalance accordingly,1,1,1.0
imbalance accordingly also,1,1,1.0
accordingly also standard,1,1,1.0
also standard might,1,1,1.0
standard might not,1,1,1.0
not be optimum,1,1,1.0
be optimum class,1,1,1.0
optimum class semination,1,1,1.0
class semination in,1,1,1.0
semination in order,1,1,1.0
order to find,1,1,1.0
solution for data,1,1,1.0
for data problems,1,1,1.0
data problems which,1,1,1.0
which are imbalanced,1,1,1.0
are imbalanced so,1,1,1.0
imbalanced so the,1,1,1.0
so the ratio,1,1,1.0
the ratio may,1,1,1.0
ratio may be,1,1,1.0
may be varied,1,1,1.0
be varied to,1,1,1.0
varied to achieve,1,1,1.0
to achieve a,1,1,1.0
achieve a better,1,1,1.0
a better accuracy,1,1,1.0
better accuracy in,1,1,1.0
accuracy in different,1,1,1.0
in different scenarios,1,1,1.0
different scenarios in,1,1,1.0
scenarios in future,1,1,1.0
in future there,1,1,1.0
future there is,1,1,1.0
a need for,1,1,1.0
need for the,1,1,1.0
for the further,1,1,1.0
the further investigation,1,1,1.0
further investigation of,1,1,1.0
investigation of smote,1,1,1.0
of smote in,1,1,1.0
smote in its,1,1,1.0
in its entirety,1,1,1.0
its entirety summary,1,1,1.0
entirety summary this,1,1,1.0
summary this study,1,1,1.0
this study involved,1,1,1.0
study involved the,1,1,1.0
involved the implementation,1,1,1.0
of smote via,1,1,1.0
smote via the,1,1,1.0
via the apache,1,1,1.0
the apache spark,1,1,1.0
apache spark framework,1,1,1.0
spark framework the,1,1,1.0
framework the investigation,1,1,1.0
the investigation was,1,1,1.0
investigation was conducted,1,1,1.0
was conducted based,1,1,1.0
conducted based on,1,1,1.0
on and algorithms,1,1,1.0
and algorithms in,1,1,1.0
algorithms in future,1,1,1.0
in future the,1,1,1.0
future the scope,1,1,1.0
the scope of,1,1,1.0
scope of such,1,1,1.0
of such table,1,1,1.0
such table performance,1,1,1.0
performance comparison ucisat,1,1,1.0
comparison ucisat dataset,1,1,1.0
ucisat dataset technique,1,1,1.0
performance comparison ecbdl,1,1,1.0
comparison ecbdl dataset,1,1,1.0
ecbdl dataset technique,1,1,1.0
oversampling technique hooda,1,1,1.0
technique hooda and,1,1,1.0
intelligence systems a,1,1,1.0
systems a study,1,1,1.0
a study could,1,1,1.0
study could be,1,1,1.0
could be expanded,1,1,1.0
be expanded by,1,1,1.0
expanded by considering,1,1,1.0
by considering alternative,1,1,1.0
considering alternative based,1,1,1.0
alternative based algorithms,1,1,1.0
based algorithms including,1,1,1.0
algorithms including and,1,1,1.0
including and borderline,1,1,1.0
and borderline smote,1,1,1.0
borderline smote in,1,1,1.0
smote in big,1,1,1.0
in big data,2,1,2.0
big data the,1,1,1.0
data the dependency,1,1,1.0
the dependency reflects,1,1,1.0
dependency reflects the,1,1,1.0
reflects the clustering,1,1,1.0
the clustering algorithm,2,1,2.0
clustering algorithm in,1,1,1.0
algorithm in use,1,1,1.0
in use proving,1,1,1.0
use proving more,1,1,1.0
proving more effective,1,1,1.0
more effective the,1,1,1.0
effective the clustering,1,1,1.0
clustering algorithm is,1,1,1.0
algorithm is seen,1,1,1.0
seen to exhibit,1,1,1.0
to exhibit superior,1,1,1.0
exhibit superior performance,1,1,1.0
superior performance due,1,1,1.0
performance due to,2,2,1.0
to the provision,1,1,1.0
the provision of,1,1,1.0
provision of more,1,1,1.0
of more accurate,1,1,1.0
more accurate results,1,1,1.0
accurate results notably,1,1,1.0
results notably the,1,1,1.0
notably the smote,1,1,1.0
the smote mentation,1,1,1.0
smote mentation could,1,1,1.0
mentation could be,1,1,1.0
could be extended,1,1,1.0
be extended to,1,1,1.0
extended to ensure,1,1,1.0
ensure that it,1,1,1.0
that it incorporates,1,1,1.0
it incorporates class,1,1,1.0
incorporates class juncts,1,1,1.0
class juncts other,1,1,1.0
juncts other areas,1,1,1.0
other areas that,1,1,1.0
areas that are,1,1,1.0
that are worth,1,1,1.0
are worth utilizing,1,1,1.0
worth utilizing include,1,1,1.0
utilizing include hybrid,1,1,1.0
include hybrid trees,1,1,1.0
hybrid trees and,1,1,1.0
trees and spill,1,1,1.0
and spill trees,1,1,1.0
spill trees through,1,1,1.0
trees through their,1,1,1.0
through their implementation,1,1,1.0
their implementation it,1,1,1.0
implementation it is,1,1,1.0
it is predicted,1,1,1.0
is predicted that,1,1,1.0
predicted that there,1,1,1.0
that there might,1,1,1.0
there might be,2,2,1.0
might be improvements,1,1,1.0
be improvements in,1,1,1.0
improvements in the,1,1,1.0
in the performance,1,1,1.0
of the nearest,1,1,1.0
nearest neighbor algorithm,1,1,1.0
neighbor algorithm as,1,1,1.0
algorithm as well,1,1,1.0
well as the,2,2,1.0
as the clustering,1,1,1.0
the clustering process,1,1,1.0
clustering process references,1,1,1.0
process references yu,1,1,1.0
references yu hong,1,1,1.0
yu hong yang,1,1,1.0
hong yang ni,1,1,1.0
yang ni y,1,1,1.0
ni y dan,1,1,1.0
y dan qin,1,1,1.0
dan qin recognition,1,1,1.0
qin recognition of,1,1,1.0
recognition of multiple,1,1,1.0
of multiple imbalanced,1,1,1.0
multiple imbalanced cancer,1,1,1.0
imbalanced cancer types,1,1,1.0
cancer types based,1,1,1.0
types based on,1,1,1.0
based on dna,1,1,1.0
on dna microarray,1,1,1.0
dna microarray data,1,1,1.0
microarray data using,1,1,1.0
data using ensemble,1,1,1.0
using ensemble classifiers,1,1,1.0
ensemble classifiers biomed,1,1,1.0
classifiers biomed res,1,1,1.0
biomed res int,1,1,1.0
res int y,1,1,1.0
int y chen,1,1,1.0
y chen an,1,1,1.0
chen an empirical,1,1,1.0
an empirical study,2,2,1.0
empirical study of,2,2,1.0
study of a,1,1,1.0
of a hybrid,1,1,1.0
a hybrid rst,1,1,1.0
hybrid rst classification,1,1,1.0
rst classification procedure,1,1,1.0
classification procedure to,1,1,1.0
procedure to elucidate,1,1,1.0
to elucidate therapeutic,1,1,1.0
elucidate therapeutic effects,1,1,1.0
therapeutic effects in,1,1,1.0
effects in uremia,1,1,1.0
in uremia patients,1,1,1.0
uremia patients med,1,1,1.0
patients med biol,1,1,1.0
med biol eng,1,1,1.0
biol eng comput,1,1,1.0
eng comput haixiang,1,1,1.0
comput haixiang yijing,1,1,1.0
haixiang yijing yanan,1,1,1.0
yijing yanan xiao,1,1,1.0
yanan xiao jinling,1,1,1.0
xiao jinling knn,1,1,1.0
jinling knn ensemble,1,1,1.0
knn ensemble learning,1,1,1.0
ensemble learning algorithm,1,1,1.0
learning algorithm imbalanced,1,1,1.0
algorithm imbalanced data,1,1,1.0
data classification appl,1,1,1.0
classification appl artif,1,1,1.0
appl artif intell,1,1,1.0
artif intell he,1,1,1.0
intell he garcía,1,1,1.0
he garcía learning,1,1,1.0
garcía learning from,1,1,1.0
data ieee trans,3,3,1.0
ieee trans knowl,6,2,3.0
trans knowl data,6,2,3.0
knowl data eng,5,2,2.5
data eng y,1,1,1.0
eng y sun,1,1,1.0
y sun wong,1,1,1.0
sun wong mohamed,1,1,1.0
wong mohamed classification,1,1,1.0
mohamed classification of,1,1,1.0
imbalanced data a,1,1,1.0
data a review,1,1,1.0
a review int,1,1,1.0
review int pattern,1,1,1.0
int pattern recognit,1,1,1.0
pattern recognit artif,1,1,1.0
recognit artif intell,1,1,1.0
artif intell fernández,2,1,2.0
intell fernández chawla,1,1,1.0
fernández chawla garcía,1,1,1.0
chawla garcía v,1,1,1.0
garcía v palade,1,1,1.0
v palade herrera,1,1,1.0
palade herrera an,1,1,1.0
herrera an insight,1,1,1.0
an insight into,2,1,2.0
insight into classification,1,1,1.0
into classification with,1,1,1.0
classification with imbalanced,1,1,1.0
with imbalanced data,5,4,1.25
imbalanced data empirical,1,1,1.0
data empirical results,1,1,1.0
empirical results and,1,1,1.0
results and current,1,1,1.0
and current trends,1,1,1.0
current trends on,1,1,1.0
trends on using,1,1,1.0
on using data,1,1,1.0
using data intrinsic,1,1,1.0
data intrinsic characteristics,1,1,1.0
intrinsic characteristics plex,1,1,1.0
characteristics plex intell,1,1,1.0
plex intell syst,1,1,1.0
intell syst krawczyk,1,1,1.0
syst krawczyk learning,1,1,1.0
krawczyk learning from,1,1,1.0
imbalanced data open,1,1,1.0
data open challenges,1,1,1.0
open challenges and,1,1,1.0
challenges and future,1,1,1.0
and future directions,1,1,1.0
future directions prog,1,1,1.0
directions prog artif,1,1,1.0
prog artif intell,1,1,1.0
artif intell batista,1,1,1.0
intell batista prati,1,1,1.0
of the ior,1,1,1.0
the ior of,1,1,1.0
ior of several,1,1,1.0
sigkdd explor ramentol,1,1,1.0
explor ramentol vluymans,1,1,1.0
ramentol vluymans verbiest,1,1,1.0
vluymans verbiest y,1,1,1.0
verbiest y caballero,1,1,1.0
y caballero bello,1,1,1.0
caballero bello cornelis,1,1,1.0
bello cornelis herrera,1,1,1.0
cornelis herrera ifrow,1,1,1.0
herrera ifrow ann,1,1,1.0
ifrow ann imbalanced,1,1,1.0
ann imbalanced ordered,1,1,1.0
imbalanced ordered weighted,1,1,1.0
ordered weighted average,1,1,1.0
weighted average nearest,1,1,1.0
average nearest neighbor,1,1,1.0
nearest neighbor classification,1,1,1.0
neighbor classification ieee,1,1,1.0
classification ieee trans,1,1,1.0
ieee trans fuzzy,1,1,1.0
trans fuzzy syst,1,1,1.0
fuzzy syst p,1,1,1.0
syst p domingos,1,1,1.0
p domingos metacost,1,1,1.0
domingos metacost a,3,3,1.0
metacost a general,2,2,1.0
a general method,2,2,1.0
general method for,2,2,1.0
method for making,3,3,1.0
for making classifiers,2,2,1.0
making classifiers in,2,2,1.0
classifiers in proceedings,2,2,1.0
data mining vol,1,1,1.0
mining vol pp,1,1,1.0
vol pp v,1,1,1.0
pp v lópez,1,1,1.0
v lópez fernández,1,1,1.0
lópez fernández herrera,1,1,1.0
fernández herrera analysis,1,1,1.0
herrera analysis of,1,1,1.0
analysis of preprocessing,1,1,1.0
of preprocessing learning,1,1,1.0
preprocessing learning for,1,1,1.0
learning for imbalanced,1,1,1.0
for imbalanced fication,1,1,1.0
imbalanced fication open,1,1,1.0
fication open problems,1,1,1.0
open problems on,1,1,1.0
problems on intrinsic,1,1,1.0
on intrinsic data,1,1,1.0
intrinsic data characteristics,1,1,1.0
data characteristics expert,1,1,1.0
characteristics expert syst,1,1,1.0
expert syst appl,1,1,1.0
syst appl laney,1,1,1.0
appl laney data,1,1,1.0
laney data management,1,1,1.0
data management controlling,1,1,1.0
management controlling data,1,1,1.0
controlling data volume,1,1,1.0
data volume velocity,1,1,1.0
volume velocity and,1,1,1.0
velocity and variety,1,1,1.0
and variety meta,1,1,1.0
variety meta group,1,1,1.0
meta group kambatla,1,1,1.0
group kambatla kollias,1,1,1.0
kambatla kollias v,1,1,1.0
kollias v kumar,1,1,1.0
v kumar grama,1,1,1.0
kumar grama trends,1,1,1.0
grama trends in,1,1,1.0
trends in big,1,1,1.0
big data analytics,1,1,1.0
data analytics parallel,1,1,1.0
analytics parallel distrib,1,1,1.0
parallel distrib comput,1,1,1.0
distrib comput p,1,1,1.0
comput p zikopoulos,1,1,1.0
p zikopoulos eaton,1,1,1.0
zikopoulos eaton deroos,1,1,1.0
eaton deroos deutsch,1,1,1.0
deroos deutsch lapis,1,1,1.0
deutsch lapis understanding,1,1,1.0
lapis understanding big,1,1,1.0
understanding big for,1,1,1.0
big for enterprise,1,1,1.0
for enterprise class,1,1,1.0
enterprise class hadoop,1,1,1.0
class hadoop and,1,1,1.0
hadoop and streaming,1,1,1.0
and streaming data,1,1,1.0
streaming data osborne,1,1,1.0
data osborne media,1,1,1.0
osborne media new,1,1,1.0
media new y,1,1,1.0
new y ork,5,2,2.5
y ork wu,1,1,1.0
ork wu zhu,1,1,1.0
wu zhu wu,1,1,1.0
zhu wu w,1,1,1.0
wu w ding,1,1,1.0
w ding data,1,1,1.0
ding data mining,1,1,1.0
data mining with,1,1,1.0
mining with bigdata,1,1,1.0
with bigdata ieee,1,1,1.0
bigdata ieee trans,1,1,1.0
data eng chawla,1,1,1.0
eng chawla bowyer,1,1,1.0
bowyer hall w,1,1,1.0
hall w kegelmeyer,1,1,1.0
w kegelmeyer synthetic,1,1,1.0
kegelmeyer synthetic minority,1,1,1.0
minority technique artif,1,1,1.0
technique artif intell,1,1,1.0
artif intell res,1,1,1.0
intell res apache,1,1,1.0
res apache spark,1,1,1.0
apache spark https,1,1,1.0
spark https japkowicz,1,1,1.0
https japkowicz the,1,1,1.0
japkowicz the class,1,1,1.0
imbalance problem significance,1,1,1.0
problem significance and,1,1,1.0
significance and strategies,1,1,1.0
and strategies in,1,1,1.0
strategies in proceedings,1,1,1.0
artificial intelligence pp,3,2,1.5
intelligence pp prati,2,2,1.0
pp prati batista,3,3,1.0
prati batista monard,1,1,1.0
batista monard learning,1,1,1.0
monard learning with,1,1,1.0
learning with class,2,2,1.0
with class skews,1,1,1.0
class skews and,1,1,1.0
skews and small,1,1,1.0
and small disjuncts,1,1,1.0
small disjuncts adv,1,1,1.0
disjuncts adv artif,1,1,1.0
adv artif intell,1,1,1.0
intell fernández sara,1,1,1.0
fernández sara del,1,1,1.0
sara del chawla,1,1,1.0
del chawla herreral,1,1,1.0
chawla herreral an,1,1,1.0
herreral an insight,1,1,1.0
insight into imbalanced,1,1,1.0
into imbalanced big,1,1,1.0
imbalanced big data,1,1,1.0
data classification outcomes,1,1,1.0
classification outcomes and,1,1,1.0
outcomes and challenges,1,1,1.0
and challenges complex,1,1,1.0
challenges complex intell,1,1,1.0
complex intell syst,1,1,1.0
intell syst garcía,1,1,1.0
syst garcía herrera,1,1,1.0
garcía herrera evolutionary,1,1,1.0
herrera evolutionary undersampling,1,1,1.0
evolutionary undersampling for,1,1,1.0
undersampling for cation,1,1,1.0
for cation with,1,1,1.0
cation with imbalanced,1,1,1.0
with imbalanced datasets,3,2,1.5
imbalanced datasets proposals,1,1,1.0
datasets proposals and,1,1,1.0
proposals and taxonomy,1,1,1.0
and taxonomy evol,1,1,1.0
taxonomy evol comput,1,1,1.0
evol comput p,1,1,1.0
comput p haghani,1,1,1.0
p haghani michel,1,1,1.0
haghani michel p,1,1,1.0
michel p aberer,1,1,1.0
p aberer lsh,1,1,1.0
aberer lsh at,1,1,1.0
lsh at large,1,1,1.0
at large distributed,1,1,1.0
large distributed knn,1,1,1.0
high dimensions in,1,1,1.0
dimensions in international,1,1,1.0
in international workshop,1,1,1.0
workshop on the,1,1,1.0
on the web,1,1,1.0
the web and,1,1,1.0
web and databases,1,1,1.0
and databases webdb,1,1,1.0
databases webdb vancouver,1,1,1.0
webdb vancouver kanungo,1,1,1.0
vancouver kanungo mount,1,1,1.0
kanungo mount netanyahu,1,1,1.0
mount netanyahu piatko,1,1,1.0
netanyahu piatko man,1,1,1.0
piatko man wu,1,1,1.0
man wu an,1,1,1.0
wu an efficient,1,1,1.0
an efficient clustering,1,1,1.0
efficient clustering algorithm,1,1,1.0
clustering algorithm analysis,1,1,1.0
algorithm analysis and,1,1,1.0
analysis and implementation,1,1,1.0
and implementation ieee,1,1,1.0
implementation ieee trans,1,1,1.0
ieee trans pattern,1,1,1.0
trans pattern anal,1,1,1.0
pattern anal mach,1,1,1.0
anal mach intell,1,1,1.0
mach intell kaufman,1,1,1.0
intell kaufman p,1,1,1.0
kaufman p rousseeuw,1,1,1.0
p rousseeuw finding,1,1,1.0
rousseeuw finding groups,1,1,1.0
finding groups in,1,1,1.0
groups in data,1,1,1.0
in data an,1,1,1.0
data an duction,1,1,1.0
an duction to,1,1,1.0
duction to cluster,1,1,1.0
to cluster analysis,1,1,1.0
cluster analysis john,1,1,1.0
analysis john wiley,1,1,1.0
wiley sons new,1,1,1.0
sons new y,1,1,1.0
y ork v,1,1,1.0
ork v capoyleas,1,1,1.0
v capoyleas woeginger,1,1,1.0
capoyleas woeginger ªgeometric,1,1,1.0
woeginger ªgeometric clusterings,1,1,1.0
ªgeometric clusterings algorithms,1,1,1.0
clusterings algorithms jain,1,1,1.0
algorithms jain dubes,1,1,1.0
jain dubes algorithms,1,1,1.0
dubes algorithms for,1,1,1.0
algorithms for clustering,1,1,1.0
for clustering data,1,1,1.0
clustering data prentice,1,1,1.0
data prentice hall,1,1,1.0
prentice hall englewood,1,1,1.0
hall englewood cliffs,1,1,1.0
englewood cliffs jain,1,1,1.0
cliffs jain murty,1,1,1.0
jain murty p,1,1,1.0
murty p ªdata,1,1,1.0
p ªdata clustering,1,1,1.0
ªdata clustering a,1,1,1.0
clustering a review,1,1,1.0
a review acm,1,1,1.0
review acm comput,1,1,1.0
acm comput surv,1,1,1.0
comput surv bahmani,1,1,1.0
surv bahmani moseley,1,1,1.0
bahmani moseley vattani,1,1,1.0
moseley vattani kumar,1,1,1.0
vattani kumar scalable,1,1,1.0
kumar scalable proc,1,1,1.0
scalable proc vldb,1,1,1.0
proc vldb endowment,1,1,1.0
vldb endowment choi,1,1,1.0
endowment choi lee,1,1,1.0
choi lee distributed,1,1,1.0
lee distributed high,1,1,1.0
distributed high dimensional,1,1,1.0
high dimensional indexing,1,1,1.0
dimensional indexing for,1,1,1.0
indexing for search,1,1,1.0
for search supercomput,1,1,1.0
search supercomput jiang,1,1,1.0
supercomput jiang tseng,1,1,1.0
jiang tseng su,1,1,1.0
tseng su clustering,1,1,1.0
su clustering process,1,1,1.0
clustering process for,1,1,1.0
process for outliers,1,1,1.0
for outliers detection,1,1,1.0
outliers detection pattern,1,1,1.0
detection pattern recognit,1,1,1.0
pattern recognit lett,1,1,1.0
recognit lett guttman,1,1,1.0
lett guttman a,1,1,1.0
guttman a dynamic,1,1,1.0
a dynamic index,2,1,2.0
dynamic index structure,1,1,1.0
index structure for,1,1,1.0
structure for spatial,1,1,1.0
for spatial ing,1,1,1.0
spatial ing in,1,1,1.0
ing in proceedings,1,1,1.0
of the acm,1,1,1.0
the acm sigmod,1,1,1.0
acm sigmod international,1,1,1.0
sigmod international conference,1,1,1.0
conference on management,1,1,1.0
on management of,1,1,1.0
of data sigmod,1,1,1.0
data sigmod acm,1,1,1.0
sigmod acm new,1,1,1.0
acm new y,2,1,2.0
y ork pp,2,1,2.0
ork pp bentley,1,1,1.0
pp bentley trees,1,1,1.0
bentley trees for,1,1,1.0
trees for semidynamic,1,1,1.0
for semidynamic point,1,1,1.0
semidynamic point sets,1,1,1.0
point sets in,1,1,1.0
sets in ings,1,1,1.0
in ings of,3,3,1.0
ings of the,3,3,1.0
of the sixth,1,1,1.0
the sixth annual,1,1,1.0
sixth annual symposium,1,1,1.0
annual symposium on,1,1,1.0
on computational try,1,1,1.0
computational try scg,1,1,1.0
try scg acm,1,1,1.0
scg acm new,1,1,1.0
ork pp p,1,1,1.0
pp p ciaccia,2,1,2.0
p ciaccia patella,2,1,2.0
ciaccia patella p,1,1,1.0
patella p zezula,1,1,1.0
p zezula an,1,1,1.0
zezula an efficient,1,1,1.0
an efficient access,1,1,1.0
efficient access method,1,1,1.0
access method for,1,1,1.0
method for similarity,1,1,1.0
for similarity search,1,1,1.0
similarity search in,1,1,1.0
search in metric,1,1,1.0
in metric spaces,1,1,1.0
metric spaces in,1,1,1.0
spaces in vldb,1,1,1.0
in vldb in,1,1,1.0
vldb in proceedings,1,1,1.0
conference on very,1,1,1.0
on very large,1,1,1.0
very large data,1,1,1.0
large data bases,1,1,1.0
data bases pp,1,1,1.0
bases pp p,1,1,1.0
ciaccia patella rabitti,1,1,1.0
patella rabitti p,1,1,1.0
rabitti p zezula,1,1,1.0
p zezula indexing,1,1,1.0
zezula indexing metric,1,1,1.0
indexing metric spaces,1,1,1.0
metric spaces with,1,1,1.0
spaces with in,1,1,1.0
with in proceedings,1,1,1.0
in proceedings qunito,1,1,1.0
proceedings qunito convegno,1,1,1.0
qunito convegno nazionale,1,1,1.0
convegno nazionale sebd,1,1,1.0
nazionale sebd pp,1,1,1.0
sebd pp moore,1,1,1.0
pp moore the,1,1,1.0
moore the anchors,1,1,1.0
the anchors hierarchy,1,1,1.0
anchors hierarchy using,1,1,1.0
hierarchy using the,1,1,1.0
using the triangle,1,1,1.0
the triangle ity,1,1,1.0
triangle ity to,1,1,1.0
ity to survive,1,1,1.0
to survive high,1,1,1.0
survive high dimensional,1,1,1.0
high dimensional data,1,1,1.0
dimensional data in,1,1,1.0
data in proceedings,2,2,1.0
of the teenth,1,1,1.0
the teenth conference,1,1,1.0
teenth conference on,1,1,1.0
intelligence pp p,1,1,1.0
pp p zezula,1,1,1.0
p zezula p,1,1,1.0
zezula p ciaccia,1,1,1.0
p ciaccia rabitti,1,1,1.0
ciaccia rabitti a,1,1,1.0
rabitti a dynamic,1,1,1.0
dynamic index for,1,1,1.0
index for ilarity,1,1,1.0
for ilarity queries,1,1,1.0
ilarity queries in,1,1,1.0
queries in multimedia,1,1,1.0
in multimedia databases,1,1,1.0
multimedia databases in,1,1,1.0
databases in technical,1,1,1.0
in technical report,1,1,1.0
technical report hermes,1,1,1.0
report hermes esprit,1,1,1.0
hermes esprit ltr,1,1,1.0
esprit ltr projects,1,1,1.0
ltr projects liu,1,1,1.0
projects liu rosenberg,1,1,1.0
liu rosenberg rowley,1,1,1.0
rosenberg rowley clustering,1,1,1.0
rowley clustering billions,1,1,1.0
clustering billions of,1,1,1.0
billions of images,1,1,1.0
of images with,1,1,1.0
images with large,1,1,1.0
with large scale,1,1,1.0
large scale nearest,1,1,1.0
scale nearest neighbor,1,1,1.0
neighbor search in,1,1,1.0
search in ieee,1,1,1.0
in ieee workshop,2,1,2.0
ieee workshop on,2,1,2.0
workshop on applications,2,1,2.0
on applications of,2,1,2.0
applications of computer,2,1,2.0
of computer vision,2,1,2.0
computer vision published,1,1,1.0
vision published in,1,1,1.0
published in ieee,1,1,1.0
computer vision hooda,1,1,1.0
vision hooda and,1,1,1.0
intelligence systems scikitlearn,1,1,1.0
systems scikitlearn http,1,1,1.0
scikitlearn http uci,1,1,1.0
http uci satellite,1,1,1.0
uci satellite image,1,1,1.0
satellite image dataset,1,1,1.0
image dataset https,1,1,1.0
dataset https y,1,1,1.0
https y east,1,1,1.0
y east dataset,1,1,1.0
east dataset https,1,1,1.0
dataset https abalone,1,1,1.0
https abalone dataset,1,1,1.0
abalone dataset https,1,1,1.0
dataset https https,1,1,1.0
https https han,1,1,1.0
https han w,1,1,1.0
han w wang,1,1,1.0
w wang mao,1,1,1.0
wang mao borderline,1,1,1.0
mao borderline smote,1,1,1.0
borderline smote a,1,1,1.0
smote a new,2,2,1.0
a new over,1,1,1.0
new over sampling,1,1,1.0
over sampling method,1,1,1.0
sampling method in,2,2,1.0
sets learning int,1,1,1.0
learning int conf,1,1,1.0
int conf intell,2,2,1.0
conf intell comput,1,1,1.0
see discussions stats,1,1,1.0
discussions stats and,1,1,1.0
stats and author,1,1,1.0
and author profiles,1,1,1.0
author profiles for,1,1,1.0
profiles for this,1,1,1.0
for this publication,1,1,1.0
this publication at,1,1,1.0
publication at https,1,1,1.0
at https on,1,1,1.0
https on the,1,1,1.0
on the class,5,2,2.5
imbalance problem article,1,1,1.0
problem article october,1,1,1.0
article october doi,1,1,1.0
october doi citations,1,1,1.0
doi citations reads,1,1,1.0
citations reads authors,1,1,1.0
reads authors including,1,1,1.0
authors including gongping,1,1,1.0
including gongping yang,1,1,1.0
gongping yang shandong,1,1,1.0
yang shandong university,1,1,1.0
shandong university publications,1,1,1.0
university publications citations,1,1,1.0
publications citations see,1,1,1.0
citations see profile,1,1,1.0
see profile all,1,1,1.0
profile all content,1,1,1.0
all content following,1,1,1.0
content following this,1,1,1.0
following this page,1,1,1.0
this page was,1,1,1.0
page was uploaded,1,1,1.0
was uploaded by,1,1,1.0
uploaded by gongping,1,1,1.0
by gongping yang,1,1,1.0
gongping yang on,1,1,1.0
yang on october,1,1,1.0
on october the,1,1,1.0
october the user,1,1,1.0
the user has,1,1,1.0
user has requested,1,1,1.0
has requested enhancement,1,1,1.0
requested enhancement of,1,1,1.0
enhancement of the,1,1,1.0
of the downloaded,1,1,1.0
the downloaded the,1,1,1.0
downloaded the class,1,1,1.0
imbalance problem xinjian,1,1,1.0
problem xinjian guo,1,1,1.0
xinjian guo yilong,1,1,1.0
guo yilong cailing,1,1,1.0
yilong cailing dong,1,1,1.0
cailing dong gongping,1,1,1.0
dong gongping yang,1,1,1.0
gongping yang guangtong,1,1,1.0
yang guangtong zhou,1,1,1.0
guangtong zhou school,1,1,1.0
zhou school of,1,1,1.0
and technology shandong,2,1,2.0
technology shandong university,2,1,2.0
shandong university jinan,1,1,1.0
university jinan china,1,1,1.0
jinan china xinjianguo,1,1,1.0
china xinjianguo ylyin,1,1,1.0
xinjianguo ylyin this,1,1,1.0
ylyin this paper,1,1,1.0
this paper concentrates,1,1,1.0
paper concentrates on,1,1,1.0
concentrates on binary,1,1,1.0
binary classification problem,1,1,1.0
classification problem and,1,1,1.0
problem and by,1,1,1.0
and by convention,1,1,1.0
by convention the,1,1,1.0
convention the class,1,1,1.0
the class label,10,2,5.0
class label of,4,2,2.0
label of the,3,1,3.0
class is positive,2,1,2.0
is positive and,2,1,2.0
positive and that,1,1,1.0
and that of,1,1,1.0
class is negative,2,1,2.0
is negative corresponding,1,1,1.0
negative corresponding author,1,1,1.0
corresponding author yilong,1,1,1.0
author yilong yin,1,1,1.0
yilong yin is,1,1,1.0
yin is with,1,1,1.0
is with school,1,1,1.0
with school of,1,1,1.0
shandong university ylyin,1,1,1.0
university ylyin abstract,1,1,1.0
ylyin abstract the,1,1,1.0
abstract the class,1,1,1.0
imbalance problem has,3,1,3.0
in many practical,1,1,1.0
many practical domains,1,1,1.0
practical domains and,1,1,1.0
domains and a,1,1,1.0
and a hot,1,1,1.0
hot topic of,1,1,1.0
topic of machine,1,1,1.0
machine learning in,3,1,3.0
learning in recent,3,1,3.0
in recent years,6,2,3.0
recent years in,1,1,1.0
years in su,1,1,1.0
in su ch,1,1,1.0
su ch a,1,1,1.0
ch a problem,1,1,1.0
a problem almost,1,1,1.0
problem almost all,1,1,1.0
almost all the,2,1,2.0
all the examples,4,1,4.0
the examples are,2,1,2.0
examples are labeled,4,1,4.0
are labeled as,4,1,4.0
labeled as one,2,1,2.0
as one class,2,1,2.0
one class while,2,1,2.0
class while far,2,1,2.0
while far fewer,2,1,2.0
far fewer examples,2,1,2.0
fewer examples are,2,1,2.0
labeled as the,2,1,2.0
as the other,2,1,2.0
other class usually,2,1,2.0
class usually the,2,1,2.0
usually the more,2,1,2.0
the more important,2,1,2.0
more important class,2,1,2.0
important class in,1,1,1.0
class in this,2,2,1.0
this case standard,2,1,2.0
case standard machine,1,1,1.0
standard machine learning,3,1,3.0
learning algorithms tend,1,1,1.0
algorithms tend to,1,1,1.0
to be overwhelmed,2,1,2.0
be overwhelmed by,2,1,2.0
overwhelmed by the,2,1,2.0
class and ignore,2,1,2.0
and ignore the,2,1,2.0
ignore the minority,2,1,2.0
minority class since,2,2,1.0
class since traditional,1,1,1.0
since traditional classifiers,1,1,1.0
traditional classifiers seeking,1,1,1.0
classifiers seeking an,1,1,1.0
seeking an accurate,1,1,1.0
an accurate performance,1,1,1.0
accurate performance over,1,1,1.0
performance over a,1,1,1.0
over a full,1,1,1.0
a full range,1,1,1.0
full range of,1,1,1.0
range of instances,1,1,1.0
of instances this,1,1,1.0
instances this paper,1,1,1.0
this paper reviewed,1,1,1.0
paper reviewed academic,1,1,1.0
reviewed academic activities,2,1,2.0
academic activities special,1,1,1.0
activities special for,1,1,1.0
special for the,2,1,2.0
imbalance problem firstly,1,1,1.0
problem firstly then,1,1,1.0
firstly then investigated,1,1,1.0
then investigated various,1,1,1.0
investigated various remedies,1,1,1.0
various remedies in,1,1,1.0
remedies in four,1,1,1.0
in four different,1,1,1.0
four different levels,3,1,3.0
different levels acco,1,1,1.0
levels acco rding,1,1,1.0
acco rding to,1,1,1.0
rding to learning,1,1,1.0
to learning phases,1,1,1.0
learning phases following,1,1,1.0
phases following surveying,1,1,1.0
following surveying evaluation,1,1,1.0
surveying evaluation metrics,1,1,1.0
evaluation metrics and,1,1,1.0
metrics and some,1,1,1.0
and some other,3,1,3.0
some other related,2,1,2.0
other related factors,1,1,1.0
related factors this,1,1,1.0
factors this paper,1,1,1.0
this paper showed,1,1,1.0
paper showed some,1,1,1.0
showed some future,2,1,2.0
some future directions,2,1,2.0
future directions at,1,1,1.0
directions at last,1,1,1.0
at last introduction,1,1,1.0
last introduction many,1,1,1.0
introduction many traditional,1,1,1.0
many traditional algorithms,1,1,1.0
traditional algorithms to,1,1,1.0
algorithms to machine,1,1,1.0
to machine learning,1,1,1.0
machine learning and,2,2,1.0
learning and data,1,1,1.0
data mining problems,1,1,1.0
mining problems assume,1,1,1.0
problems assume that,1,1,1.0
that the target,1,1,1.0
the target classes,1,1,1.0
target classes share,1,1,1.0
classes share similar,1,1,1.0
share similar prior,1,1,1.0
similar prior probabilities,1,1,1.0
prior probabilities however,1,1,1.0
probabilities however in,1,1,1.0
however in many,2,2,1.0
in many real,1,1,1.0
many real world,1,1,1.0
real world applications,3,1,3.0
world applications such,1,1,1.0
applications such as,3,2,1.5
such as detection,1,1,1.0
as detection network,1,1,1.0
detection network intrusion,1,1,1.0
network intrusion detection,2,2,1.0
intrusion detection fraud,2,1,2.0
detection fraud detection,2,1,2.0
fraud detection this,1,1,1.0
detection this assumption,1,1,1.0
this assumption is,1,1,1.0
assumption is grossly,1,1,1.0
is grossly violated,1,1,1.0
grossly violated in,1,1,1.0
violated in such,1,1,1.0
in such problems,1,1,1.0
such problems almost,1,1,1.0
problems almost all,1,1,1.0
important class this,1,1,1.0
class this situation,1,1,1.0
this situation is,1,1,1.0
situation is known,1,1,1.0
is known as,3,2,1.5
known as the,4,3,1.3333333333333333
as the problem,1,1,1.0
problem of class,3,1,3.0
case standard classifiers,1,1,1.0
standard classifiers tend,1,1,1.0
classifiers tend to,1,1,1.0
minority class its,3,2,1.5
class its importance,1,1,1.0
its importance grew,1,1,1.0
importance grew as,1,1,1.0
grew as more,1,1,1.0
as more and,1,1,1.0
more and more,1,1,1.0
and more researchers,1,1,1.0
more researchers realized,1,1,1.0
researchers realized that,1,1,1.0
realized that this,1,1,1.0
that this imbalance,1,1,1.0
this imbalance causes,1,1,1.0
imbalance causes suboptimal,1,1,1.0
causes suboptimal classification,1,1,1.0
suboptimal classification performance,1,1,1.0
classification performance and,1,1,1.0
performance and that,1,1,1.0
and that most,1,1,1.0
that most algorithms,1,1,1.0
most algorithms behave,1,1,1.0
algorithms behave badly,1,1,1.0
behave badly when,1,1,1.0
badly when the,1,1,1.0
when the data,3,1,3.0
data sets are,6,2,3.0
sets are highly,2,1,2.0
are highly imbalanced,2,1,2.0
highly imbalanced the,1,1,1.0
imbalanced the class,1,1,1.0
has been a,1,1,1.0
been a hot,1,1,1.0
topic in machine,1,1,1.0
in machine learning,4,2,2.0
recent years class,1,1,1.0
years class imbalance,1,1,1.0
been recognized to,1,1,1.0
recognized to be,1,1,1.0
to be existing,1,1,1.0
be existing in,1,1,1.0
existing in lots,1,1,1.0
in lots of,1,1,1.0
lots of application,1,1,1.0
of application domains,1,1,1.0
such as spotting,1,1,1.0
as spotting unreliable,1,1,1.0
spotting unreliable telecommunication,1,1,1.0
unreliable telecommunication customers,1,1,1.0
telecommunication customers detection,1,1,1.0
customers detection of,1,1,1.0
radar images learning,1,1,1.0
images learning word,1,1,1.0
learning word pronunciations,1,1,1.0
word pronunciations text,1,1,1.0
pronunciations text classification,1,1,1.0
text classification risk,1,1,1.0
classification risk management,1,1,1.0
risk management information,1,1,1.0
management information retrieval,1,1,1.0
information retrieval and,1,1,1.0
retrieval and filtering,1,1,1.0
and filtering tasks,1,1,1.0
filtering tasks medical,1,1,1.0
tasks medical diagnosis,1,1,1.0
medical diagnosis rare,1,1,1.0
diagnosis rare disease,1,1,1.0
rare disease and,1,1,1.0
disease and rare,1,1,1.0
and rare genes,1,1,1.0
rare genes mutations,1,1,1.0
genes mutations network,1,1,1.0
mutations network monitoring,1,1,1.0
network monitoring and,1,1,1.0
monitoring and intrusion,1,1,1.0
fraud detection shuttle,1,1,1.0
detection shuttle system,1,1,1.0
shuttle system failure,1,1,1.0
system failure earthquakes,1,1,1.0
failure earthquakes and,1,1,1.0
earthquakes and nuclear,1,1,1.0
and nuclear explosions,1,1,1.0
nuclear explosions and,1,1,1.0
explosions and helicopter,1,1,1.0
and helicopter fault,1,1,1.0
helicopter fault monitoring,1,1,1.0
fault monitoring from,1,1,1.0
monitoring from the,1,1,1.0
from the view,1,1,1.0
the view of,1,1,1.0
view of applications,1,1,1.0
of applications the,1,1,1.0
applications the nature,1,1,1.0
of the imbalance,1,1,1.0
the imbalance falls,1,1,1.0
imbalance falls in,1,1,1.0
falls in two,1,1,1.0
in two cases,1,1,1.0
two cases the,1,1,1.0
cases the data,1,1,1.0
the data are,3,2,1.5
data are naturally,1,1,1.0
are naturally imbalanced,1,1,1.0
naturally imbalanced credit,1,1,1.0
imbalanced credit card,1,1,1.0
credit card frauds,1,1,1.0
card frauds and,1,1,1.0
frauds and rare,1,1,1.0
and rare disease,1,1,1.0
rare disease or,1,1,1.0
disease or the,1,1,1.0
or the data,1,1,1.0
data are not,1,1,1.0
are not naturally,1,1,1.0
not naturally imbalanced,1,1,1.0
naturally imbalanced but,1,1,1.0
imbalanced but it,1,1,1.0
but it is,2,1,2.0
it is too,1,1,1.0
is too expensive,1,1,1.0
too expensive to,2,1,2.0
expensive to obtain,1,1,1.0
to obtain data,1,1,1.0
obtain data of,1,1,1.0
data of the,1,1,1.0
minority class shuttle,1,1,1.0
class shuttle failure,1,1,1.0
shuttle failure for,1,1,1.0
failure for learning,1,1,1.0
for learning there,1,1,1.0
learning there have,1,1,1.0
there have been,2,1,2.0
have been lots,1,1,1.0
been lots of,1,1,1.0
lots of researches,1,1,1.0
of researches on,1,1,1.0
researches on class,1,1,1.0
on class imbalance,1,1,1.0
imbalance problem paper,1,1,1.0
problem paper reviewed,1,1,1.0
paper reviewed various,1,1,1.0
reviewed various techniques,1,1,1.0
various techniques for,1,1,1.0
techniques for handling,1,1,1.0
for handling imbalance,1,1,1.0
handling imbalance dataset,1,1,1.0
imbalance dataset problems,1,1,1.0
dataset problems paper,1,1,1.0
problems paper traced,1,1,1.0
paper traced some,1,1,1.0
traced some of,1,1,1.0
of the recent,1,1,1.0
the recent progress,1,1,1.0
recent progress in,1,1,1.0
progress in the,1,1,1.0
in the field,1,1,1.0
the field of,2,1,2.0
field of learning,2,1,2.0
of learning from,2,1,2.0
data sets in,3,2,1.5
sets in which,1,1,1.0
in which sofia,1,1,1.0
which sofia visa,1,1,1.0
sofia visa et,1,1,1.0
visa et al,1,1,1.0
et al argued,1,1,1.0
al argued that,1,1,1.0
argued that the,4,1,4.0
that the poor,1,1,1.0
the poor performance,1,1,1.0
poor performance of,1,1,1.0
the classifiers produced,1,1,1.0
classifiers produced by,1,1,1.0
produced by the,1,1,1.0
by the standard,1,1,1.0
the standard machine,1,1,1.0
learning algorithms on,1,1,1.0
algorithms on imbalanced,1,1,1.0
data sets is,2,1,2.0
sets is mainly,1,1,1.0
is mainly due,1,1,1.0
mainly due to,1,1,1.0
to the following,1,1,1.0
the following three,1,1,1.0
following three factors,1,1,1.0
three factors accuracy,1,1,1.0
factors accuracy class,1,1,1.0
accuracy class distribution,1,1,1.0
class distribution and,2,1,2.0
distribution and error,1,1,1.0
and error costs,1,1,1.0
error costs since,1,1,1.0
costs since they,1,1,1.0
since they are,2,2,1.0
they are rarely,1,1,1.0
are rarely well,1,1,1.0
rarely well satisfied,1,1,1.0
well satisfied in,1,1,1.0
satisfied in real,1,1,1.0
in real world,1,1,1.0
world applications paper,1,1,1.0
applications paper discussed,1,1,1.0
paper discussed several,1,1,1.0
discussed several issues,1,1,1.0
several issues related,1,1,1.0
related to learning,1,1,1.0
to learning with,1,1,1.0
learning with skewed,3,1,3.0
with skewed class,3,1,3.0
skewed class distributions,3,1,3.0
class distributions such,1,1,1.0
distributions such as,1,1,1.0
such as the,6,3,2.0
as the relationship,1,1,1.0
relationship between learning,1,1,1.0
between learning and,1,1,1.0
learning and class,1,1,1.0
and class distributions,3,2,1.5
class distributions and,3,1,3.0
distributions and the,1,1,1.0
and the limitations,1,1,1.0
the limitations of,1,1,1.0
limitations of accuracy,1,1,1.0
of accuracy and,1,1,1.0
accuracy and error,1,1,1.0
and error rate,1,1,1.0
error rate to,1,1,1.0
rate to measure,1,1,1.0
measure the performance,1,1,1.0
of classifiers weiss,1,1,1.0
classifiers weiss presented,1,1,1.0
weiss presented an,1,1,1.0
presented an overview,1,1,1.0
an overview of,1,1,1.0
overview of the,1,1,1.0
of the field,1,1,1.0
imbalanced data he,1,1,1.0
data he pays,1,1,1.0
he pays particular,1,1,1.0
pays particular attention,1,1,1.0
particular attention to,1,1,1.0
attention to differences,1,1,1.0
to differences and,1,1,1.0
differences and similarities,1,1,1.0
and similarities between,1,1,1.0
similarities between the,1,1,1.0
between the problems,1,1,1.0
the problems of,3,1,3.0
problems of rare,1,1,1.0
of rare classes,1,1,1.0
rare classes and,1,1,1.0
classes and rare,1,1,1.0
and rare cases,1,1,1.0
rare cases he,1,1,1.0
cases he then,1,1,1.0
he then discussed,1,1,1.0
then discussed some,1,1,1.0
discussed some of,1,1,1.0
the common issues,1,1,1.0
common issues and,1,1,1.0
issues and their,1,1,1.0
and their range,1,1,1.0
their range of,1,1,1.0
range of solutions,1,1,1.0
of solutions in,1,1,1.0
solutions in mining,1,1,1.0
in mining imbalanced,2,1,2.0
sets the reminder,1,1,1.0
the reminder of,1,1,1.0
reminder of the,1,1,1.0
of the paper,2,1,2.0
the paper is,2,2,1.0
paper is organized,2,2,1.0
is organized as,2,2,1.0
organized as follows,2,2,1.0
as follows section,2,2,1.0
follows section reviewed,1,1,1.0
section reviewed academic,1,1,1.0
academic activities including,1,1,1.0
activities including two,1,1,1.0
including two fourth,1,1,1.0
two fourth international,1,1,1.0
conference on natural,3,1,3.0
on natural computation,3,1,3.0
natural computation ieee,3,1,3.0
computation ieee doi,3,1,3.0
ieee doi fourth,2,1,2.0
doi fourth international,2,1,2.0
ieee doi workshops,1,1,1.0
doi workshops and,1,1,1.0
workshops and one,1,1,1.0
and one special,2,1,2.0
one special issue,2,1,2.0
special issue on,3,2,1.5
issue on the,1,1,1.0
on the problem,2,1,2.0
class imbalance sections,1,1,1.0
imbalance sections surveyed,1,1,1.0
sections surveyed the,1,1,1.0
surveyed the remedies,1,1,1.0
the remedies to,2,1,2.0
remedies to the,1,1,1.0
imbalance problem from,1,1,1.0
problem from four,1,1,1.0
from four different,1,1,1.0
different levels popular,1,1,1.0
levels popular evaluation,1,1,1.0
popular evaluation metrics,1,1,1.0
evaluation metrics for,1,1,1.0
metrics for imbalanced,1,1,1.0
data sets were,1,1,1.0
sets were summarized,1,1,1.0
were summarized in,1,1,1.0
summarized in section,1,1,1.0
in section section,1,1,1.0
section section briefly,1,1,1.0
section briefly analyzed,1,1,1.0
briefly analyzed some,1,1,1.0
analyzed some other,1,1,1.0
some other factors,1,1,1.0
other factors related,1,1,1.0
factors related to,1,1,1.0
problem and section,1,1,1.0
and section concluded,1,1,1.0
section concluded the,1,1,1.0
concluded the paper,1,1,1.0
the paper and,2,2,1.0
paper and showed,1,1,1.0
and showed some,1,1,1.0
future directions academic,1,1,1.0
directions academic activiti,1,1,1.0
academic activiti es,1,1,1.0
activiti es on,1,1,1.0
es on the,1,1,1.0
imbalance problem as,1,1,1.0
problem as described,1,1,1.0
as described above,1,1,1.0
described above recognizing,1,1,1.0
above recognizing class,1,1,1.0
recognizing class imbalance,1,1,1.0
imbalance problem exists,2,1,2.0
problem exists in,1,1,1.0
exists in extensive,1,1,1.0
in extensive application,1,1,1.0
extensive application domains,1,1,1.0
application domains gave,1,1,1.0
domains gave rise,1,1,1.0
gave rise to,1,1,1.0
rise to two,1,1,1.0
to two workshops,1,1,1.0
two workshops held,1,1,1.0
workshops held at,1,1,1.0
held at the,1,1,1.0
at the top,1,1,1.0
the top conferences,1,1,1.0
top conferences in,1,1,1.0
conferences in ai,1,1,1.0
in ai and,1,1,1.0
ai and one,1,1,1.0
issue on dealing,1,1,1.0
on dealing with,1,1,1.0
with the class,2,1,2.0
imbalance problem the,3,2,1.5
problem the first,1,1,1.0
the first workshop,2,1,2.0
first workshop dedicated,1,1,1.0
workshop dedicated to,1,1,1.0
dedicated to the,1,1,1.0
imbalance problem was,2,1,2.0
problem was held,1,1,1.0
was held in,1,1,1.0
held in conjunction,1,1,1.0
in conjunction with,1,1,1.0
conjunction with the,1,1,1.0
with the american,1,1,1.0
the american association,1,1,1.0
american association for,1,1,1.0
association for artificial,1,1,1.0
for artificial intelligence,1,1,1.0
artificial intelligence conference,1,1,1.0
intelligence conference aaai,1,1,1.0
conference aaai its,1,1,1.0
aaai its main,1,1,1.0
its main contribution,1,1,1.0
main contribution includes,1,1,1.0
contribution includes observation,1,1,1.0
includes observation of,1,1,1.0
observation of many,1,1,1.0
of many application,1,1,1.0
application domains dealing,1,1,1.0
domains dealing with,1,1,1.0
dealing with imbalanced,2,1,2.0
data sets and,2,1,2.0
sets and several,1,1,1.0
and several important,1,1,1.0
several important issues,1,1,1.0
important issues such,1,1,1.0
issues such as,1,1,1.0
such as how,1,1,1.0
as how to,1,1,1.0
how to evaluate,1,1,1.0
to evaluate learning,1,1,1.0
evaluate learning algorithms,1,1,1.0
learning algorithms what,1,1,1.0
algorithms what evaluation,1,1,1.0
what evaluation measures,1,1,1.0
evaluation measures should,1,1,1.0
measures should be,1,1,1.0
should be used,1,1,1.0
be used one,1,1,1.0
used one class,1,1,1.0
one class learning,1,1,1.0
class learning versus,1,1,1.0
learning versus discriminating,1,1,1.0
versus discriminating methods,1,1,1.0
discriminating methods discussions,1,1,1.0
methods discussions over,1,1,1.0
discussions over various,1,1,1.0
over various methods,1,1,1.0
various methods discussion,1,1,1.0
methods discussion of,1,1,1.0
discussion of the,1,1,1.0
of the relation,2,2,1.0
the relation between,2,1,2.0
relation between class,2,1,2.0
between class imbalance,3,1,3.0
problem and learning,1,1,1.0
and learning the,1,1,1.0
learning the goal,1,1,1.0
the goal of,1,1,1.0
goal of creating,1,1,1.0
of creating classifiers,1,1,1.0
creating classifiers that,1,1,1.0
classifiers that performs,1,1,1.0
performs well across,1,1,1.0
well across a,1,1,1.0
across a range,1,1,1.0
a range of,1,1,1.0
range of costs,1,1,1.0
of costs and,1,1,1.0
costs and so,1,1,1.0
and so on,2,2,1.0
so on the,1,1,1.0
on the second,1,1,1.0
the second workshop,1,1,1.0
second workshop special,1,1,1.0
workshop special for,1,1,1.0
imbalance problem is,4,1,4.0
problem is part,1,1,1.0
part of the,2,2,1.0
conference on machine,8,2,4.0
on machine learning,9,2,4.5
machine learning icml,1,1,1.0
learning icml in,1,1,1.0
icml in which,1,1,1.0
which most research,1,1,1.0
most research on,1,1,1.0
research on the,1,1,1.0
the problem was,1,1,1.0
problem was guided,1,1,1.0
was guided by,1,1,1.0
guided by the,1,1,1.0
by the first,1,1,1.0
first workshop for,1,1,1.0
workshop for example,1,1,1.0
for example roc,1,1,1.0
example roc or,1,1,1.0
roc or cost,1,1,1.0
or cost curves,1,1,1.0
cost curves were,1,1,1.0
curves were used,1,1,1.0
were used as,2,2,1.0
used as evaluation,1,1,1.0
as evaluation metrics,1,1,1.0
evaluation metrics rather,1,1,1.0
metrics rather than,1,1,1.0
rather than accuracy,1,1,1.0
than accuracy the,1,1,1.0
accuracy the workshop,1,1,1.0
the workshop was,1,1,1.0
workshop was followed,1,1,1.0
was followed by,1,1,1.0
followed by an,1,1,1.0
by an interesting,1,1,1.0
an interesting and,1,1,1.0
interesting and vivid,1,1,1.0
and vivid panel,1,1,1.0
vivid panel discussion,1,1,1.0
panel discussion two,1,1,1.0
discussion two major,1,1,1.0
two major directions,1,1,1.0
major directions presented,1,1,1.0
directions presented in,1,1,1.0
presented in the,1,1,1.0
in the research,1,1,1.0
the research papers,1,1,1.0
research papers of,1,1,1.0
papers of the,1,1,1.0
of the workshop,2,1,2.0
the workshop many,1,1,1.0
workshop many papers,1,1,1.0
many papers still,1,1,1.0
papers still reported,1,1,1.0
still reported various,1,1,1.0
reported various tuning,1,1,1.0
various tuning methods,1,1,1.0
tuning methods applied,1,1,1.0
methods applied to,1,1,1.0
applied to decision,1,1,1.0
to decision trees,1,1,1.0
decision trees in,1,1,1.0
trees in order,1,1,1.0
order to perform,1,1,1.0
perform better on,1,1,1.0
better on imbalanced,1,1,1.0
data sets even,1,1,1.0
sets even though,1,1,1.0
even though presentations,1,1,1.0
though presentations in,1,1,1.0
presentations in the,1,1,1.0
in the previous,2,2,1.0
the previous workshop,1,1,1.0
previous workshop showed,1,1,1.0
workshop showed their,1,1,1.0
showed their shortcomings,1,1,1.0
their shortcomings and,1,1,1.0
shortcomings and it,1,1,1.0
and it was,1,1,1.0
it was commonly,1,1,1.0
was commonly agreed,1,1,1.0
commonly agreed that,1,1,1.0
agreed that new,1,1,1.0
that new classifiers,1,1,1.0
new classifiers are,1,1,1.0
classifiers are needed,1,1,1.0
are needed for,1,1,1.0
needed for imbalanced,1,1,1.0
data sets besides,1,1,1.0
sets besides under,1,1,1.0
besides under various,1,1,1.0
under various aspects,1,1,1.0
various aspects was,1,1,1.0
aspects was present,1,1,1.0
was present in,1,1,1.0
present in half,1,1,1.0
in half of,1,1,1.0
half of the,5,2,2.5
of the papers,1,1,1.0
the papers and,1,1,1.0
papers and was,1,1,1.0
and was the,1,1,1.0
was the most,1,1,1.0
the most debated,1,1,1.0
most debated issue,1,1,1.0
debated issue even,1,1,1.0
issue even though,1,1,1.0
even though shows,1,1,1.0
though shows that,1,1,1.0
shows that sampling,1,1,1.0
that sampling has,1,1,1.0
sampling has the,1,1,1.0
has the same,1,1,1.0
the same result,1,1,1.0
same result as,1,1,1.0
result as moving,1,1,1.0
as moving the,1,1,1.0
moving the decision,1,1,1.0
the decision threshold,2,2,1.0
decision threshold or,1,1,1.0
threshold or adjusting,1,1,1.0
or adjusting the,1,1,1.0
adjusting the cost,1,1,1.0
the cost matrix,1,1,1.0
cost matrix a,1,1,1.0
matrix a result,1,1,1.0
a result known,1,1,1.0
result known since,1,1,1.0
known since in,1,1,1.0
since in addition,1,1,1.0
in addition japkowicz,1,1,1.0
addition japkowicz questioned,1,1,1.0
japkowicz questioned the,1,1,1.0
questioned the fact,1,1,1.0
that the within,1,1,1.0
imbalance is responsible,1,1,1.0
is responsible for,1,1,1.0
responsible for the,2,1,2.0
for the problem,2,1,2.0
problem the idea,1,1,1.0
the idea is,2,2,1.0
idea is that,1,1,1.0
is that within,1,1,1.0
that within class,1,1,1.0
class imbalance leads,1,1,1.0
imbalance leads to,1,1,1.0
leads to a,1,1,1.0
to a severe,1,1,1.0
a severe lack,1,1,1.0
severe lack of,1,1,1.0
lack of representation,1,1,1.0
of representation of,1,1,1.0
representation of some,1,1,1.0
of some important,1,1,1.0
some important aspects,1,1,1.0
important aspects of,1,1,1.0
aspects of the,1,1,1.0
class the sixth,1,1,1.0
the sixth issue,1,1,1.0
sixth issue of,1,1,1.0
issue of sigkdd,1,1,1.0
of sigkdd exploration,1,1,1.0
sigkdd exploration was,1,1,1.0
exploration was dedicated,1,1,1.0
was dedicated entirely,1,1,1.0
dedicated entirely to,1,1,1.0
entirely to the,1,1,1.0
imbalance problem in,4,2,2.0
problem in which,1,1,1.0
in which weiss,1,1,1.0
which weiss presented,1,1,1.0
weiss presented a,1,1,1.0
presented a very,1,1,1.0
a very good,1,1,1.0
very good review,1,1,1.0
good review of,1,1,1.0
review of the,4,2,2.0
of the current,3,3,1.0
the current research,1,1,1.0
current research on,1,1,1.0
research on learning,1,1,1.0
on learning from,7,2,3.5
sets and the,1,1,1.0
and the other,1,1,1.0
the other papers,1,1,1.0
papers in the,1,1,1.0
the volume address,1,1,1.0
volume address mainly,1,1,1.0
address mainly issues,1,1,1.0
mainly issues of,1,1,1.0
issues of sampling,1,1,1.0
of sampling feature,1,1,1.0
sampling feature selection,1,1,1.0
feature selection and,1,1,1.0
selection and learning,1,1,1.0
and learning for,1,1,1.0
learning for example,1,1,1.0
for example investigated,1,1,1.0
example investigated a,1,1,1.0
investigated a boosting,1,1,1.0
a boosting method,1,1,1.0
boosting method combined,1,1,1.0
method combined with,1,1,1.0
combined with various,1,1,1.0
with various techniques,1,1,1.0
various techniques of,1,1,1.0
techniques of the,1,1,1.0
of the hard,1,1,1.0
the hard to,1,1,1.0
hard to classify,1,1,1.0
to classify examples,1,1,1.0
classify examples the,1,1,1.0
examples the method,1,1,1.0
the method improves,1,1,1.0
method improves the,1,1,1.0
improves the prediction,1,1,1.0
the prediction accuracy,1,1,1.0
prediction accuracy for,1,1,1.0
accuracy for both,1,1,1.0
for both the,1,1,1.0
both the classes,1,1,1.0
the classes and,1,1,1.0
classes and does,1,1,1.0
does not sacrifice,1,1,1.0
not sacrifice one,1,1,1.0
sacrifice one class,1,1,1.0
one class for,1,1,1.0
class for the,2,1,2.0
for the other,1,1,1.0
the other with,1,1,1.0
other with experiments,1,1,1.0
with experiments on,1,1,1.0
experiments on data,1,1,1.0
data sets remedies,1,1,1.0
sets remedies for,1,1,1.0
remedies for the,1,1,1.0
problem the remedies,1,1,1.0
remedies to deal,1,1,1.0
deal with the,3,2,1.5
with the problem,3,1,3.0
class imbalance are,2,1,2.0
imbalance are of,1,1,1.0
are of four,1,1,1.0
of four different,1,1,1.0
different levels according,1,1,1.0
levels according to,1,1,1.0
according to the,12,3,4.0
to the phases,1,1,1.0
the phases in,1,1,1.0
phases in learning,1,1,1.0
in learning changing,1,1,1.0
learning changing class,1,1,1.0
changing class distributions,4,1,4.0
class distributions mainly,1,1,1.0
distributions mainly by,1,1,1.0
mainly by techniques,1,1,1.0
by techniques features,1,1,1.0
techniques features selection,1,1,1.0
features selection in,1,1,1.0
selection in the,1,1,1.0
the feature level,1,1,1.0
feature level classifiers,1,1,1.0
level classifiers level,1,1,1.0
classifiers level by,1,1,1.0
level by manipulating,1,1,1.0
by manipulating classifiers,1,1,1.0
manipulating classifiers internally,2,1,2.0
classifiers internally and,1,1,1.0
internally and ensemble,1,1,1.0
and ensemble learning,2,2,1.0
ensemble learning for,1,1,1.0
learning for final,1,1,1.0
for final classification,1,1,1.0
final classification changing,1,1,1.0
classification changing class,1,1,1.0
class distributions since,1,1,1.0
distributions since examples,1,1,1.0
since examples belong,1,1,1.0
examples belong to,1,1,1.0
class are far,1,1,1.0
are far fewer,1,1,1.0
far fewer than,1,1,1.0
fewer than those,1,1,1.0
than those belong,1,1,1.0
those belong to,1,1,1.0
class in situations,1,1,1.0
in situations of,2,1,2.0
situations of the,1,1,1.0
of the class,3,2,1.5
imbalance problem one,1,1,1.0
problem one direct,1,1,1.0
one direct way,1,1,1.0
direct way to,1,1,1.0
way to counter,1,1,1.0
to counter the,1,1,1.0
counter the problem,1,1,1.0
is to change,1,1,1.0
to change class,1,1,1.0
change class distributions,1,1,1.0
class distributions balanced,1,1,1.0
distributions balanced distributions,1,1,1.0
balanced distributions can,1,1,1.0
distributions can be,1,1,1.0
can be obtained,1,1,1.0
be obtained by,1,1,1.0
obtained by the,1,1,1.0
majority class minority,1,1,1.0
class minority class,1,1,1.0
minority class combining,1,1,1.0
class combining the,1,1,1.0
combining the both,1,1,1.0
the both and,1,1,1.0
both and some,1,1,1.0
some other advanced,1,1,1.0
other advanced sampling,1,1,1.0
advanced sampling ways,1,1,1.0
sampling ways there,1,1,1.0
ways there are,1,1,1.0
there are numerous,1,1,1.0
are numerous researches,1,1,1.0
numerous researches on,1,1,1.0
researches on changing,1,1,1.0
on changing class,1,1,1.0
class distributions weiss,1,1,1.0
distributions weiss investigated,1,1,1.0
weiss investigated the,2,1,2.0
investigated the effect,1,1,1.0
the effect of,8,3,2.6666666666666665
effect of class,3,2,1.5
of class distributions,1,1,1.0
class distributions on,1,1,1.0
distributions on decision,1,1,1.0
on decision tree,2,1,2.0
decision tree by,1,1,1.0
tree by altering,1,1,1.0
by altering class,1,1,1.0
altering class distributions,1,1,1.0
class distributions in,1,1,1.0
distributions in several,1,1,1.0
in several ratios,1,1,1.0
several ratios with,1,1,1.0
ratios with accuracy,1,1,1.0
with accuracy and,1,1,1.0
accuracy and auc,1,1,1.0
and auc as,1,1,1.0
auc as metrics,1,1,1.0
as metrics in,1,1,1.0
metrics in his,1,1,1.0
in his doctoral,1,1,1.0
his doctoral dissertation,1,1,1.0
doctoral dissertation all,1,1,1.0
dissertation all the,1,1,1.0
all the methods,3,2,1.5
the methods falls,1,1,1.0
methods falls into,1,1,1.0
falls into three,1,1,1.0
into three basic,1,1,1.0
three basic techniques,1,1,1.0
basic techniques heuristic,1,1,1.0
techniques heuristic and,1,1,1.0
heuristic and heuristic,1,1,1.0
and heuristic and,1,1,1.0
heuristic and sampling,1,1,1.0
and sampling and,1,1,1.0
sampling and advanced,1,1,1.0
and advanced sampling,1,1,1.0
advanced sampling compared,1,1,1.0
sampling compared the,1,1,1.0
compared the most,1,1,1.0
the most naivest,1,1,1.0
most naivest sampling,1,1,1.0
naivest sampling method,1,1,1.0
sampling method is,1,1,1.0
method is random,1,1,1.0
is random a,1,1,1.0
random a method,1,1,1.0
a method trying,1,1,1.0
method trying to,1,1,1.0
trying to balance,1,1,1.0
to balance class,2,1,2.0
balance class distributions,2,1,2.0
class distributions through,2,1,2.0
distributions through the,2,1,2.0
through the random,2,1,2.0
the random elimination,1,1,1.0
random elimination of,1,1,1.0
elimination of majority,1,1,1.0
majority class examples,4,2,2.0
class examples this,1,1,1.0
examples this leads,1,1,1.0
this leads to,1,1,1.0
leads to discarding,1,1,1.0
to discarding useful,1,1,1.0
discarding useful data,1,1,1.0
useful data that,1,1,1.0
data that could,1,1,1.0
that could be,1,1,1.0
could be important,1,1,1.0
be important for,1,1,1.0
important for classifiers,1,1,1.0
for classifiers there,1,1,1.0
classifiers there have,1,1,1.0
have been several,1,1,1.0
been several heuristic,1,1,1.0
several heuristic methods,2,1,2.0
heuristic methods proposed,1,1,1.0
methods proposed or,1,1,1.0
proposed or introduced,2,1,2.0
or introduced from,2,1,2.0
introduced from data,1,1,1.0
from data cleaning,1,1,1.0
data cleaning in,1,1,1.0
cleaning in recent,1,1,1.0
recent years they,1,1,1.0
years they are,1,1,1.0
based on either,1,1,1.0
on either of,1,1,1.0
either of two,1,1,1.0
of two different,2,2,1.0
two different noise,1,1,1.0
different noise model,1,1,1.0
noise model hypotheses,1,1,1.0
model hypotheses one,1,1,1.0
hypotheses one thinks,1,1,1.0
one thinks examples,1,1,1.0
thinks examples that,1,1,1.0
examples that are,5,2,2.5
that are near,1,1,1.0
are near to,1,1,1.0
near to the,1,1,1.0
to the classification,1,1,1.0
the classification boundary,1,1,1.0
classification boundary of,1,1,1.0
of the two,5,2,2.5
the two classes,2,2,1.0
two classes are,1,1,1.0
classes are noise,1,1,1.0
are noise while,1,1,1.0
noise while the,1,1,1.0
while the other,1,1,1.0
the other considers,1,1,1.0
other considers examples,1,1,1.0
considers examples with,1,1,1.0
examples with more,1,1,1.0
with more neighbors,1,1,1.0
more neighbors of,1,1,1.0
neighbors of different,1,1,1.0
of different labels,1,1,1.0
different labels are,1,1,1.0
labels are noise,1,1,1.0
are noise condensed,1,1,1.0
noise condensed nearest,1,1,1.0
condensed nearest neighbor,2,1,2.0
nearest neighbor rule,3,1,3.0
neighbor rule cnn,1,1,1.0
rule cnn bases,1,1,1.0
cnn bases on,1,1,1.0
bases on the,1,1,1.0
on the notion,1,1,1.0
notion of a,1,1,1.0
of a consistent,2,1,2.0
a consistent subset,2,1,2.0
consistent subset of,1,1,1.0
subset of a,1,1,1.0
of a sample,1,1,1.0
a sample set,1,1,1.0
sample set which,1,1,1.0
set which is,1,1,1.0
which is a,3,2,1.5
is a subset,1,1,1.0
a subset who,1,1,1.0
subset who can,1,1,1.0
who can correctly,1,1,1.0
can correctly classifies,1,1,1.0
correctly classifies all,1,1,1.0
classifies all of,1,1,1.0
of the remaining,1,1,1.0
the remaining examples,1,1,1.0
remaining examples in,1,1,1.0
training set when,1,1,1.0
set when used,1,1,1.0
when used as,1,1,1.0
as a stored,1,1,1.0
a stored reference,1,1,1.0
stored reference set,1,1,1.0
reference set for,1,1,1.0
set for the,1,1,1.0
for the nn,1,1,1.0
the nn rule,1,1,1.0
nn rule if,1,1,1.0
rule if the,1,1,1.0
if the bayesian,2,1,2.0
the bayesian risk,2,1,2.0
bayesian risk is,2,1,2.0
risk is small,1,1,1.0
is small if,1,1,1.0
small if the,1,1,1.0
if the underlying,1,1,1.0
the underlying densities,1,1,1.0
underlying densities of,1,1,1.0
densities of the,1,1,1.0
of the various,1,1,1.0
the various classes,1,1,1.0
various classes have,1,1,1.0
classes have small,1,1,1.0
have small overlapping,1,1,1.0
small overlapping then,1,1,1.0
overlapping then the,1,1,1.0
then the algorithm,1,1,1.0
the algorithm will,1,1,1.0
algorithm will tend,1,1,1.0
tend to pick,1,1,1.0
to pick out,1,1,1.0
pick out examples,1,1,1.0
out examples near,1,1,1.0
examples near the,2,1,2.0
near the perhaps,1,1,1.0
the perhaps fuzzy,1,1,1.0
perhaps fuzzy boundary,1,1,1.0
fuzzy boundary between,1,1,1.0
boundary between the,1,1,1.0
between the classes,1,1,1.0
the classes typically,1,1,1.0
classes typically points,1,1,1.0
typically points deeply,1,1,1.0
points deeply imbedded,1,1,1.0
deeply imbedded within,1,1,1.0
imbedded within a,1,1,1.0
a class will,1,1,1.0
class will not,1,1,1.0
will not be,2,2,1.0
not be transferred,1,1,1.0
be transferred to,1,1,1.0
transferred to store,1,1,1.0
to store since,1,1,1.0
store since they,1,1,1.0
since they will,1,1,1.0
they will be,1,1,1.0
will be correctly,1,1,1.0
be correctly classified,1,1,1.0
correctly classified if,1,1,1.0
classified if the,1,1,1.0
risk is high,1,1,1.0
is high then,1,1,1.0
high then store,1,1,1.0
then store will,1,1,1.0
store will contain,1,1,1.0
will contain essentially,1,1,1.0
contain essentially all,1,1,1.0
essentially all the,1,1,1.0
the examples in,3,1,3.0
in the original,1,1,1.0
the original training,2,2,1.0
original training set,1,1,1.0
training set and,2,1,2.0
set and no,1,1,1.0
and no important,1,1,1.0
no important reduction,1,1,1.0
important reduction in,1,1,1.0
reduction in training,1,1,1.0
in training size,1,1,1.0
training size will,1,1,1.0
size will have,1,1,1.0
will have been,1,1,1.0
have been achieved,1,1,1.0
been achieved so,1,1,1.0
achieved so cnn,1,1,1.0
so cnn is,1,1,1.0
cnn is effective,1,1,1.0
is effective only,1,1,1.0
effective only binary,1,1,1.0
only binary classes,1,1,1.0
binary classes are,1,1,1.0
classes are of,1,1,1.0
are of small,1,1,1.0
of small overlapping,1,1,1.0
small overlapping oss,1,1,1.0
overlapping oss randomly,1,1,1.0
oss randomly draws,1,1,1.0
randomly draws one,1,1,1.0
draws one majority,1,1,1.0
one majority class,1,1,1.0
majority class example,1,1,1.0
class example and,1,1,1.0
example and all,1,1,1.0
and all examples,1,1,1.0
all examples from,1,1,1.0
class and then,1,1,1.0
and then puts,1,1,1.0
then puts these,1,1,1.0
puts these examples,1,1,1.0
these examples in,1,1,1.0
examples in afterwards,1,1,1.0
in afterwards use,1,1,1.0
afterwards use a,1,1,1.0
use a over,1,1,1.0
a over the,1,1,1.0
over the examples,1,1,1.0
examples in e,1,1,1.0
in e to,1,1,1.0
e to classify,1,1,1.0
to classify the,1,1,1.0
classify the examples,1,1,1.0
examples in every,1,1,1.0
in every misclassified,1,1,1.0
every misclassified example,1,1,1.0
misclassified example from,1,1,1.0
example from e,1,1,1.0
from e is,1,1,1.0
e is moved,1,1,1.0
is moved to,1,1,1.0
moved to e,1,1,1.0
to e the,1,1,1.0
e the idea,1,1,1.0
the idea behind,1,1,1.0
idea behind this,3,2,1.5
behind this implementation,1,1,1.0
implementation of a,1,1,1.0
consistent subset is,1,1,1.0
subset is to,1,1,1.0
is to eliminate,1,1,1.0
to eliminate the,1,1,1.0
eliminate the examples,1,1,1.0
the examples from,3,2,1.5
that are distant,1,1,1.0
are distant from,1,1,1.0
distant from the,1,1,1.0
from the decision,1,1,1.0
the decision border,1,1,1.0
decision border since,1,1,1.0
border since these,1,1,1.0
since these examples,1,1,1.0
these examples might,1,1,1.0
examples might be,1,1,1.0
might be considered,1,1,1.0
be considered less,1,1,1.0
considered less relevant,1,1,1.0
less relevant for,1,1,1.0
relevant for learning,1,1,1.0
for learning wilson,1,1,1.0
learning wilson s,1,1,1.0
wilson s edited,1,1,1.0
s edited nearest,1,1,1.0
edited nearest neighbor,2,2,1.0
neighbor rule enn,1,1,1.0
rule enn removes,1,1,1.0
enn removes any,1,1,1.0
removes any example,1,1,1.0
any example whose,1,1,1.0
example whose class,1,1,1.0
whose class label,1,1,1.0
class label differs,1,1,1.0
label differs from,1,1,1.0
differs from the,1,1,1.0
class of at,1,1,1.0
of at least,1,1,1.0
at least two,1,1,1.0
least two of,1,1,1.0
two of its,1,1,1.0
of its three,1,1,1.0
its three nearest,4,1,4.0
three nearest neighbors,4,1,4.0
nearest neighbors different,1,1,1.0
neighbors different from,1,1,1.0
different from enn,1,1,1.0
from enn neighborhood,1,1,1.0
enn neighborhood cleaning,1,1,1.0
neighborhood cleaning rule,1,1,1.0
cleaning rule ncl,1,1,1.0
rule ncl deals,1,1,1.0
ncl deals with,1,1,1.0
deals with majority,1,1,1.0
with majority and,1,1,1.0
and minority samples,1,1,1.0
minority samples separately,1,1,1.0
samples separately when,1,1,1.0
separately when cleaning,1,1,1.0
when cleaning the,1,1,1.0
cleaning the data,1,1,1.0
data sets ncl,1,1,1.0
sets ncl uses,1,1,1.0
ncl uses enn,1,1,1.0
uses enn to,1,1,1.0
enn to remove,1,1,1.0
to remove majority,1,1,1.0
remove majority examples,1,1,1.0
majority examples for,1,1,1.0
examples for each,1,1,1.0
for each example,3,2,1.5
each example e,1,1,1.0
example e i,1,1,1.0
e i in,1,1,1.0
i in the,2,2,1.0
training set its,1,1,1.0
set its three,1,1,1.0
neighbors are found,1,1,1.0
are found if,1,1,1.0
found if ei,1,1,1.0
if ei belongs,1,1,1.0
ei belongs to,1,1,1.0
and the classification,1,1,1.0
the classification given,1,1,1.0
classification given by,1,1,1.0
given by its,1,1,1.0
by its three,1,1,1.0
nearest neighbors contradicts,1,1,1.0
neighbors contradicts the,1,1,1.0
contradicts the original,1,1,1.0
the original class,1,1,1.0
original class of,1,1,1.0
class of e,1,1,1.0
of e i,1,1,1.0
e i then,2,1,2.0
i then ei,1,1,1.0
then ei is,1,1,1.0
ei is removed,1,1,1.0
is removed if,1,1,1.0
removed if e,1,1,1.0
if e i,1,1,1.0
e i belongs,1,1,1.0
i belongs to,1,1,1.0
class and its,1,1,1.0
and its three,1,1,1.0
nearest neighbors misclassify,1,1,1.0
neighbors misclassify e,1,1,1.0
misclassify e i,1,1,1.0
i then the,1,1,1.0
then the nearest,1,1,1.0
nearest neighbors that,1,1,1.0
class are removed,1,1,1.0
are removed compared,1,1,1.0
removed compared with,1,1,1.0
compared with above,1,1,1.0
with above four,1,1,1.0
above four methods,1,1,1.0
four methods tomek,1,1,1.0
methods tomek links,1,1,1.0
tomek links consider,1,1,1.0
links consider samples,1,1,1.0
consider samples near,1,1,1.0
samples near the,1,1,1.0
near the borderline,2,1,2.0
the borderline should,1,1,1.0
borderline should be,1,1,1.0
should be paid,1,1,1.0
be paid more,1,1,1.0
paid more attention,1,1,1.0
more attention given,1,1,1.0
attention given two,1,1,1.0
given two examples,1,1,1.0
two examples e,1,1,1.0
examples e i,1,1,1.0
e i and,1,1,1.0
i and e,1,1,1.0
and e j,1,1,1.0
e j belonging,1,1,1.0
j belonging to,1,1,1.0
belonging to different,1,1,1.0
to different classes,1,1,1.0
different classes and,2,2,1.0
classes and d,1,1,1.0
and d e,1,1,1.0
d e i,1,1,1.0
e i e,1,1,1.0
i e j,1,1,1.0
e j is,1,1,1.0
j is the,1,1,1.0
is the distance,1,1,1.0
distance between ei,1,1,1.0
between ei and,1,1,1.0
ei and ej,1,1,1.0
and ej a,1,1,1.0
ej a ei,1,1,1.0
a ei ej,1,1,1.0
ei ej pair,1,1,1.0
ej pair is,1,1,1.0
pair is called,1,1,1.0
is called a,1,1,1.0
called a tomek,1,1,1.0
tomek link if,1,1,1.0
link if there,1,1,1.0
if there is,1,1,1.0
there is not,1,1,1.0
is not an,1,1,1.0
not an example,1,1,1.0
an example such,1,1,1.0
example such that,1,1,1.0
such that d,1,1,1.0
that d ei,1,1,1.0
d ei d,1,1,1.0
ei d ei,1,1,1.0
d ei ej,2,1,2.0
ei ej or,1,1,1.0
ej or d,1,1,1.0
or d ej,1,1,1.0
d ej d,1,1,1.0
ej d ei,1,1,1.0
ei ej if,1,1,1.0
ej if two,1,1,1.0
if two examples,1,1,1.0
two examples form,1,1,1.0
examples form a,1,1,1.0
form a tomek,1,1,1.0
tomek link then,1,1,1.0
link then either,1,1,1.0
then either one,1,1,1.0
either one of,1,1,1.0
of these examples,1,1,1.0
these examples is,1,1,1.0
examples is noise,1,1,1.0
is noise or,1,1,1.0
noise or both,1,1,1.0
or both examples,1,1,1.0
both examples form,1,1,1.0
examples form borderline,1,1,1.0
form borderline so,1,1,1.0
borderline so tomek,1,1,1.0
so tomek link,1,1,1.0
tomek link can,1,1,1.0
link can be,1,1,1.0
can be viewed,2,1,2.0
be viewed as,2,1,2.0
viewed as an,1,1,1.0
as an method,1,1,1.0
an method when,1,1,1.0
method when examples,1,1,1.0
when examples of,1,1,1.0
examples of both,1,1,1.0
of both classes,1,1,1.0
both classes are,1,1,1.0
classes are removed,1,1,1.0
are removed it,1,1,1.0
removed it should,1,1,1.0
noted that tomek,1,1,1.0
that tomek link,1,1,1.0
tomek link enn,1,1,1.0
link enn and,1,1,1.0
enn and ncl,1,1,1.0
and ncl are,1,1,1.0
ncl are highly,1,1,1.0
are highly since,1,1,1.0
highly since for,1,1,1.0
since for any,1,1,1.0
for any example,1,1,1.0
any example in,1,1,1.0
in the data,1,1,1.0
data sets nearest,1,1,1.0
sets nearest neighbors,1,1,1.0
neighbors of the,3,2,1.5
of the sample,1,1,1.0
the sample must,1,1,1.0
sample must be,1,1,1.0
must be found,1,1,1.0
be found so,1,1,1.0
found so it,1,1,1.0
it is impossible,1,1,1.0
is impossible for,1,1,1.0
impossible for large,1,1,1.0
for large datasets,1,1,1.0
large datasets random,1,1,1.0
datasets random is,1,1,1.0
random is a,1,1,1.0
is a method,1,1,1.0
a method that,1,1,1.0
method that aims,1,1,1.0
that aims to,1,1,1.0
aims to balance,1,1,1.0
the random replication,1,1,1.0
random replication of,2,2,1.0
replication of minority,2,2,1.0
of minority class,5,3,1.6666666666666667
minority class examples,8,2,4.0
class examples random,1,1,1.0
examples random has,1,1,1.0
random has two,1,1,1.0
has two shortcomings,1,1,1.0
two shortcomings first,1,1,1.0
shortcomings first it,1,1,1.0
first it will,1,1,1.0
it will increase,1,1,1.0
will increase the,3,2,1.5
increase the likelihood,1,1,1.0
the likelihood of,1,1,1.0
likelihood of occurring,1,1,1.0
of occurring since,1,1,1.0
occurring since it,1,1,1.0
since it makes,1,1,1.0
it makes exact,1,1,1.0
makes exact copies,1,1,1.0
exact copies of,1,1,1.0
copies of the,1,1,1.0
class examples second,1,1,1.0
examples second sampling,1,1,1.0
second sampling makes,1,1,1.0
sampling makes learning,1,1,1.0
makes learning process,1,1,1.0
learning process more,1,1,1.0
process more consuming,1,1,1.0
more consuming if,1,1,1.0
consuming if the,1,1,1.0
if the original,1,1,1.0
original data set,1,1,1.0
set is already,1,1,1.0
is already fairly,1,1,1.0
already fairly large,1,1,1.0
fairly large but,1,1,1.0
large but imbalanced,1,1,1.0
but imbalanced there,1,1,1.0
imbalanced there are,1,1,1.0
are several heuristic,1,1,1.0
heuristic methods mainly,1,1,1.0
methods mainly based,1,1,1.0
mainly based on,1,1,1.0
based on smote,1,1,1.0
on smote smote,1,1,1.0
smote smote generates,1,1,1.0
smote generates synthetic,1,1,1.0
generates synthetic minority,2,2,1.0
minority examples to,1,1,1.0
examples to the,1,1,1.0
class its main,1,1,1.0
its main idea,1,1,1.0
main idea is,1,1,1.0
idea is to,5,2,2.5
is to form,1,1,1.0
to form new,1,1,1.0
form new minority,1,1,1.0
new minority class,1,1,1.0
class examples by,1,1,1.0
examples by interpolating,1,1,1.0
by interpolating between,1,1,1.0
interpolating between several,1,1,1.0
class examples that,2,2,1.0
examples that lie,1,1,1.0
that lie together,1,1,1.0
lie together by,1,1,1.0
together by interpolating,1,1,1.0
instead of replication,1,1,1.0
of replication smote,1,1,1.0
replication smote avoids,1,1,1.0
problem and causes,1,1,1.0
and causes the,1,1,1.0
causes the decision,1,1,1.0
the decision boundaries,1,1,1.0
boundaries for the,1,1,1.0
class to spread,1,1,1.0
to spread further,1,1,1.0
spread further into,1,1,1.0
further into the,1,1,1.0
into the majority,2,2,1.0
majority class space,1,1,1.0
class space recognizing,1,1,1.0
space recognizing examples,1,1,1.0
recognizing examples near,1,1,1.0
the borderline of,1,1,1.0
borderline of the,1,1,1.0
the classes are,3,2,1.5
classes are more,1,1,1.0
are more important,2,2,1.0
more important and,1,1,1.0
important and more,1,1,1.0
and more easily,1,1,1.0
more easily misclassified,1,1,1.0
easily misclassified than,1,1,1.0
misclassified than those,1,1,1.0
than those far,1,1,1.0
those far from,1,1,1.0
far from the,2,2,1.0
from the borderline,1,1,1.0
the borderline was,1,1,1.0
borderline was proposed,1,1,1.0
was proposed it,1,1,1.0
proposed it only,1,1,1.0
it only sample,1,1,1.0
only sample the,1,1,1.0
sample the borderline,1,1,1.0
the borderline examples,1,1,1.0
borderline examples of,1,1,1.0
minority class while,1,1,1.0
class while smote,1,1,1.0
while smote and,1,1,1.0
smote and random,2,1,2.0
and random augment,1,1,1.0
random augment the,1,1,1.0
augment the minority,1,1,1.0
minority class through,2,2,1.0
class through all,1,1,1.0
through all the,1,1,1.0
class or a,1,1,1.0
or a random,1,1,1.0
a random subset,1,1,1.0
random subset of,1,1,1.0
subset of the,2,1,2.0
minority class experiments,1,1,1.0
class experiments show,1,1,1.0
experiments show that,1,1,1.0
show that their,1,1,1.0
that their approaches,1,1,1.0
their approaches achieve,1,1,1.0
approaches achieve better,1,1,1.0
achieve better tp,1,1,1.0
better tp rate,1,1,1.0
tp rate and,3,1,3.0
rate and than,1,1,1.0
and than smote,1,1,1.0
than smote and,1,1,1.0
and random methods,1,1,1.0
random methods advanced,1,1,1.0
methods advanced sampling,1,1,1.0
advanced sampling different,1,1,1.0
sampling different from,1,1,1.0
different from various,1,1,1.0
from various and,1,1,1.0
various and methods,1,1,1.0
and methods above,1,1,1.0
methods above the,1,1,1.0
above the following,1,1,1.0
the following advanced,1,1,1.0
following advanced sampling,1,1,1.0
advanced sampling methods,2,2,1.0
sampling methods do,1,1,1.0
methods do based,1,1,1.0
do based on,1,1,1.0
on the results,3,2,1.5
the results of,6,3,2.0
results of preliminary,1,1,1.0
of preliminary classifications,1,1,1.0
preliminary classifications boosting,1,1,1.0
classifications boosting is,1,1,1.0
boosting is an,1,1,1.0
is an iterative,1,1,1.0
an iterative algorithm,1,1,1.0
iterative algorithm that,1,1,1.0
algorithm that place,1,1,1.0
that place different,1,1,1.0
place different weights,1,1,1.0
different weights on,1,1,1.0
weights on the,1,1,1.0
the training distributions,1,1,1.0
training distributions each,1,1,1.0
distributions each iteration,1,1,1.0
each iteration after,1,1,1.0
iteration after each,1,1,1.0
after each iteration,1,1,1.0
each iteration boosting,1,1,1.0
iteration boosting increases,1,1,1.0
boosting increases the,1,1,1.0
increases the weights,1,1,1.0
the weights associated,3,1,3.0
weights associated with,3,1,3.0
associated with the,5,2,2.5
with the incorrectly,1,1,1.0
the incorrectly classified,2,1,2.0
incorrectly classified examples,3,1,3.0
classified examples and,1,1,1.0
examples and decreases,1,1,1.0
and decreases the,1,1,1.0
decreases the weights,1,1,1.0
with the correctly,1,1,1.0
the correctly classified,2,1,2.0
correctly classified examples,1,1,1.0
classified examples separately,1,1,1.0
examples separately this,1,1,1.0
separately this forces,1,1,1.0
this forces the,1,1,1.0
forces the learner,1,1,1.0
the learner to,1,1,1.0
learner to focus,1,1,1.0
to focus more,2,2,1.0
focus more on,2,2,1.0
more on the,1,1,1.0
on the incorrectly,1,1,1.0
classified examples in,1,1,1.0
the next iteration,2,2,1.0
next iteration note,1,1,1.0
iteration note that,1,1,1.0
note that boosting,1,1,1.0
that boosting effectively,1,1,1.0
boosting effectively alters,1,1,1.0
effectively alters the,1,1,1.0
alters the distributions,2,1,2.0
the distributions of,2,1,2.0
distributions of the,1,1,1.0
of the training,13,2,6.5
training data can,1,1,1.0
can be considered,3,2,1.5
be considered to,3,3,1.0
considered to be,2,2,1.0
be a type,1,1,1.0
a type of,1,1,1.0
type of advanced,1,1,1.0
of advanced sampling,1,1,1.0
advanced sampling technique,1,1,1.0
sampling technique weiss,1,1,1.0
technique weiss proposed,1,1,1.0
weiss proposed a,1,1,1.0
proposed a heuristic,1,1,1.0
a heuristic algorithm,1,1,1.0
heuristic algorithm for,1,1,1.0
algorithm for selecting,1,1,1.0
for selecting training,1,1,1.0
selecting training data,1,1,1.0
data that approximates,1,1,1.0
that approximates optimum,1,1,1.0
approximates optimum the,1,1,1.0
optimum the sensitive,1,1,1.0
the sensitive sampling,1,1,1.0
sensitive sampling strategy,1,1,1.0
sampling strategy makes,1,1,1.0
strategy makes two,1,1,1.0
makes two additional,1,1,1.0
two additional assumptions,1,1,1.0
additional assumptions first,1,1,1.0
assumptions first it,1,1,1.0
first it assumes,1,1,1.0
it assumes that,2,2,1.0
assumes that the,1,1,1.0
number of potentially,1,1,1.0
of potentially available,1,1,1.0
potentially available training,1,1,1.0
available training examples,1,1,1.0
training examples from,1,1,1.0
examples from each,1,1,1.0
from each class,1,1,1.0
each class is,1,1,1.0
class is sufficiently,1,1,1.0
is sufficiently large,1,1,1.0
sufficiently large so,1,1,1.0
large so that,1,1,1.0
so that a,1,1,1.0
that a training,1,1,1.0
a training set,1,1,1.0
training set with,1,1,1.0
set with n,1,1,1.0
with n examples,1,1,1.0
n examples can,1,1,1.0
examples can be,1,1,1.0
can be formed,1,1,1.0
be formed with,1,1,1.0
formed with any,1,1,1.0
with any desired,1,1,1.0
any desired marginal,1,1,1.0
desired marginal class,1,1,1.0
marginal class distributions,1,1,1.0
class distributions the,2,1,2.0
distributions the second,1,1,1.0
the second assumption,1,1,1.0
second assumption is,1,1,1.0
assumption is that,1,1,1.0
that the cost,1,1,1.0
cost of executing,1,1,1.0
of executing the,1,1,1.0
executing the learning,1,1,1.0
the learning algorithm,6,3,2.0
learning algorithm is,3,2,1.5
algorithm is negligible,1,1,1.0
is negligible compared,1,1,1.0
negligible compared to,1,1,1.0
to the cost,1,1,1.0
cost of procuring,1,1,1.0
of procuring examples,1,1,1.0
procuring examples this,1,1,1.0
examples this assumption,1,1,1.0
this assumption permits,1,1,1.0
assumption permits the,1,1,1.0
permits the learning,1,1,1.0
learning algorithm to,3,3,1.0
algorithm to be,3,3,1.0
to be run,1,1,1.0
be run multiple,1,1,1.0
run multiple times,1,1,1.0
multiple times in,1,1,1.0
times in order,1,1,1.0
to provide guidance,1,1,1.0
provide guidance about,1,1,1.0
guidance about which,1,1,1.0
about which examples,1,1,1.0
which examples to,1,1,1.0
examples to select,1,1,1.0
to select he,1,1,1.0
select he argued,1,1,1.0
he argued that,1,1,1.0
argued that though,1,1,1.0
that though the,1,1,1.0
though the heuristically,1,1,1.0
the heuristically determined,1,1,1.0
heuristically determined class,1,1,1.0
determined class distributions,1,1,1.0
class distributions associated,1,1,1.0
distributions associated with,1,1,1.0
with the final,1,1,1.0
the final training,1,1,1.0
final training set,1,1,1.0
set is not,1,1,1.0
is not guaranteed,1,1,1.0
not guaranteed to,1,1,1.0
guaranteed to yield,1,1,1.0
yield the classifier,1,1,1.0
the classifier the,1,1,1.0
classifier the classifier,1,1,1.0
the classifier induced,1,1,1.0
classifier induced using,1,1,1.0
induced using this,1,1,1.0
using this class,1,1,1.0
this class distributions,1,1,1.0
class distributions performs,1,1,1.0
distributions performs well,1,1,1.0
performs well in,1,1,1.0
well in practice,1,1,1.0
in practice han,1,1,1.0
practice han et,1,1,1.0
al proposed an,1,1,1.0
proposed an algorithm,1,1,1.0
an algorithm based,1,1,1.0
algorithm based on,2,1,2.0
based on preliminary,2,1,2.0
on preliminary classification,2,1,2.0
preliminary classification ospc,1,1,1.0
classification ospc firstly,1,1,1.0
ospc firstly preliminary,1,1,1.0
firstly preliminary classification,1,1,1.0
preliminary classification was,1,1,1.0
classification was made,1,1,1.0
was made on,1,1,1.0
made on the,1,1,1.0
on the test,1,1,1.0
test data in,1,1,1.0
data in order,1,1,1.0
order to save,1,1,1.0
to save the,1,1,1.0
save the useful,1,1,1.0
the useful information,1,1,1.0
useful information of,1,1,1.0
information of the,4,2,2.0
class as much,1,1,1.0
as much as,1,1,1.0
much as possible,1,1,1.0
as possible then,1,1,1.0
possible then the,1,1,1.0
then the test,1,1,1.0
data that were,1,1,1.0
that were predicted,1,1,1.0
were predicted to,1,1,1.0
predicted to belong,1,1,1.0
to belong to,1,1,1.0
belong to minority,1,1,1.0
to minority class,2,1,2.0
minority class were,1,1,1.0
class were reclassified,1,1,1.0
were reclassified to,1,1,1.0
reclassified to improve,1,1,1.0
improve the classification,1,1,1.0
the classification performance,2,1,2.0
classification performance of,2,1,2.0
minority class ospc,1,1,1.0
class ospc was,1,1,1.0
ospc was argued,1,1,1.0
was argued to,1,1,1.0
argued to perform,1,1,1.0
perform better than,2,2,1.0
better than methods,1,1,1.0
than methods and,1,1,1.0
methods and smote,1,1,1.0
and smote in,1,1,1.0
smote in terms,1,1,1.0
in terms of,8,3,2.6666666666666665
terms of the,2,2,1.0
of the classification,1,1,1.0
class and majority,3,2,1.5
and majority class,5,2,2.5
class it should,1,1,1.0
the methods of,1,1,1.0
methods of changing,1,1,1.0
of changing class,1,1,1.0
class distributions above,1,1,1.0
distributions above are,1,1,1.0
above are trying,1,1,1.0
are trying to,2,2,1.0
trying to deal,1,1,1.0
problem of imbalance,1,1,1.0
of imbalance a,1,1,1.0
imbalance a proposed,1,1,1.0
a proposed to,1,1,1.0
proposed to improve,1,1,1.0
to improve accuracy,1,1,1.0
improve accuracy of,1,1,1.0
accuracy of minority,1,1,1.0
class by dealing,1,1,1.0
by dealing with,1,1,1.0
with the problems,2,1,2.0
problems of between,1,1,1.0
of between and,1,1,1.0
between and within,1,1,1.0
and within class,2,2,1.0
class imbalance simultaneously,1,1,1.0
imbalance simultaneously when,1,1,1.0
simultaneously when the,1,1,1.0
sets are severely,1,1,1.0
are severely skewed,1,1,1.0
severely skewed sampling,1,1,1.0
skewed sampling and,1,1,1.0
sampling and methods,1,1,1.0
and methods are,1,1,1.0
methods are often,1,1,1.0
are often combined,1,1,1.0
often combined to,1,1,1.0
combined to improve,1,1,1.0
to improve generalization,1,1,1.0
improve generalization of,1,1,1.0
generalization of the,1,1,1.0
of the learner,1,1,1.0
the learner batista,1,1,1.0
learner batista et,1,1,1.0
batista et al,1,1,1.0
et al presented,2,1,2.0
al presented a,1,1,1.0
presented a comparison,1,1,1.0
a comparison and,1,1,1.0
comparison and combination,1,1,1.0
and combination of,1,1,1.0
combination of various,1,1,1.0
of various sampling,1,1,1.0
various sampling strategies,1,1,1.0
sampling strategies they,1,1,1.0
strategies they noted,1,1,1.0
they noted that,3,1,3.0
noted that combining,1,1,1.0
that combining focused,1,1,1.0
combining focused and,1,1,1.0
focused and such,1,1,1.0
and such as,1,1,1.0
as smote combining,1,1,1.0
smote combining with,2,1,2.0
combining with tomek,1,1,1.0
with tomek link,1,1,1.0
tomek link or,1,1,1.0
link or smote,1,1,1.0
or smote combining,1,1,1.0
combining with enn,1,1,1.0
with enn is,1,1,1.0
enn is applicable,1,1,1.0
is applicable when,1,1,1.0
applicable when the,1,1,1.0
highly imbalanced or,1,1,1.0
imbalanced or there,1,1,1.0
or there are,1,1,1.0
there are very,1,1,1.0
are very few,1,1,1.0
very few examples,1,1,1.0
few examples of,1,1,1.0
minority class feature,1,1,1.0
class feature selection,1,1,1.0
feature selection the,1,1,1.0
selection the majority,1,1,1.0
the majority of,1,1,1.0
majority of work,1,1,1.0
of work in,1,1,1.0
work in feature,1,1,1.0
in feature selection,1,1,1.0
selection for imbalanced,1,1,1.0
data sets has,1,1,1.0
sets has focused,1,1,1.0
has focused on,1,1,1.0
focused on text,1,1,1.0
on text classification,2,1,2.0
text classification or,2,1,2.0
classification or web,2,1,2.0
or web categorization,2,1,2.0
web categorization domain,1,1,1.0
categorization domain a,1,1,1.0
domain a couple,1,1,1.0
a couple of,1,1,1.0
couple of papers,1,1,1.0
of papers in,1,1,1.0
papers in this,1,1,1.0
in this issue,1,1,1.0
this issue look,1,1,1.0
issue look at,1,1,1.0
look at feature,1,1,1.0
at feature selection,1,1,1.0
feature selection in,2,1,2.0
selection in situations,1,1,1.0
situations of imbalanced,1,1,1.0
data sets albeit,1,1,1.0
sets albeit in,1,1,1.0
albeit in text,1,1,1.0
in text classification,1,1,1.0
web categorization zheng,1,1,1.0
categorization zheng et,1,1,1.0
zheng et al,1,1,1.0
suggested that existing,1,1,1.0
that existing measures,1,1,1.0
existing measures used,1,1,1.0
measures used for,1,1,1.0
used for feature,1,1,1.0
for feature selection,1,1,1.0
feature selection are,1,1,1.0
selection are not,1,1,1.0
are not very,1,1,1.0
not very appropriate,1,1,1.0
very appropriate for,1,1,1.0
appropriate for imbalanced,1,1,1.0
data sets they,3,1,3.0
sets they proposed,1,1,1.0
they proposed a,1,1,1.0
proposed a feature,1,1,1.0
a feature selection,1,1,1.0
feature selection framework,1,1,1.0
selection framework which,1,1,1.0
framework which selects,1,1,1.0
which selects features,1,1,1.0
selects features for,1,1,1.0
features for positive,1,1,1.0
for positive and,1,1,1.0
positive and negative,6,2,3.0
and negative classes,1,1,1.0
negative classes separately,1,1,1.0
classes separately and,1,1,1.0
separately and then,1,1,1.0
and then explicitly,1,1,1.0
then explicitly combines,1,1,1.0
explicitly combines them,1,1,1.0
combines them the,1,1,1.0
them the authors,1,1,1.0
the authors showed,1,1,1.0
authors showed simple,1,1,1.0
showed simple ways,1,1,1.0
simple ways of,1,1,1.0
ways of converting,1,1,1.0
of converting existing,1,1,1.0
converting existing measures,1,1,1.0
existing measures so,1,1,1.0
measures so that,1,1,1.0
that they separately,1,1,1.0
they separately consider,1,1,1.0
separately consider features,1,1,1.0
consider features for,1,1,1.0
features for negative,1,1,1.0
positive classes castillo,1,1,1.0
classes castillo and,1,1,1.0
castillo and serrano,3,1,3.0
and serrano did,1,1,1.0
serrano did not,1,1,1.0
did not particularly,1,1,1.0
not particularly focus,2,1,2.0
particularly focus on,2,1,2.0
focus on feature,2,1,2.0
on feature selection,2,1,2.0
feature selection but,2,1,2.0
selection but made,1,1,1.0
but made it,1,1,1.0
made it a,1,1,1.0
it a part,2,1,2.0
a part of,2,1,2.0
part of their,2,1,2.0
of their complete,2,1,2.0
their complete framework,2,1,2.0
complete framework putten,1,1,1.0
framework putten and,1,1,1.0
putten and someren,1,1,1.0
and someren analyzed,1,1,1.0
someren analyzed the,1,1,1.0
analyzed the coil,1,1,1.0
the coil data,1,1,1.0
coil data sets,1,1,1.0
data sets using,3,3,1.0
sets using the,1,1,1.0
using the decomposition,1,1,1.0
the decomposition and,1,1,1.0
decomposition and they,1,1,1.0
and they reported,1,1,1.0
they reported that,1,1,1.0
reported that the,1,1,1.0
that the key,1,1,1.0
the key issue,1,1,1.0
key issue for,1,1,1.0
issue for this,2,2,1.0
for this particular,1,1,1.0
this particular data,1,1,1.0
particular data set,1,1,1.0
data set was,1,1,1.0
set was avoiding,1,1,1.0
was avoiding they,1,1,1.0
avoiding they concluded,1,1,1.0
they concluded that,1,1,1.0
concluded that feature,1,1,1.0
that feature selection,1,1,1.0
selection in such,1,1,1.0
in such domains,1,1,1.0
such domains is,1,1,1.0
domains is even,1,1,1.0
is even more,1,1,1.0
even more important,1,1,1.0
more important than,2,2,1.0
important than the,2,2,1.0
than the choice,1,1,1.0
the choice of,1,1,1.0
choice of the,1,1,1.0
of the learning,5,3,1.6666666666666667
the learning method,1,1,1.0
learning method classifiers,1,1,1.0
method classifiers level,1,1,1.0
classifiers level manipulating,1,1,1.0
level manipulating classifiers,1,1,1.0
classifiers internally drummond,1,1,1.0
internally drummond and,1,1,1.0
drummond and holte,2,1,2.0
and holte reported,1,1,1.0
holte reported that,1,1,1.0
reported that when,1,1,1.0
that when using,2,2,1.0
when using s,1,1,1.0
using s default,1,1,1.0
s default settings,1,1,1.0
default settings is,1,1,1.0
settings is surprisingly,1,1,1.0
is surprisingly ineffective,1,1,1.0
surprisingly ineffective often,1,1,1.0
ineffective often producing,1,1,1.0
often producing little,1,1,1.0
producing little or,1,1,1.0
little or no,1,1,1.0
or no change,1,1,1.0
no change in,1,1,1.0
change in performance,1,1,1.0
in performance in,1,1,1.0
performance in response,1,1,1.0
in response to,1,1,1.0
response to modifications,1,1,1.0
to modifications of,1,1,1.0
modifications of misclassification,1,1,1.0
of misclassification costs,1,1,1.0
misclassification costs and,1,1,1.0
costs and class,1,1,1.0
class distributions moreover,1,1,1.0
distributions moreover they,1,1,1.0
moreover they noted,1,1,1.0
noted that prunes,1,1,1.0
that prunes less,1,1,1.0
prunes less and,1,1,1.0
less and therefore,1,1,1.0
and therefore generalizes,1,1,1.0
therefore generalizes less,1,1,1.0
generalizes less than,1,1,1.0
less than and,1,1,1.0
than and that,1,1,1.0
and that a,1,1,1.0
that a modification,1,1,1.0
a modification of,1,1,1.0
modification of the,1,1,1.0
of the s,1,1,1.0
the s parameter,1,1,1.0
s parameter settings,1,1,1.0
parameter settings to,1,1,1.0
settings to increase,1,1,1.0
to increase the,1,1,1.0
increase the influence,1,1,1.0
the influence of,1,1,1.0
influence of pruning,1,1,1.0
of pruning and,1,1,1.0
pruning and other,1,1,1.0
and other avoidance,1,1,1.0
other avoidance factors,1,1,1.0
avoidance factors can,1,1,1.0
factors can reestablish,1,1,1.0
can reestablish the,1,1,1.0
reestablish the performance,1,1,1.0
performance of some,1,1,1.0
of some classifiers,1,1,1.0
some classifiers such,1,1,1.0
classifiers such as,2,1,2.0
as the naive,1,1,1.0
the naive bayes,1,1,1.0
naive bayes classifier,1,1,1.0
bayes classifier or,1,1,1.0
classifier or some,1,1,1.0
or some neural,1,1,1.0
some neural networks,1,1,1.0
neural networks yield,1,1,1.0
networks yield a,1,1,1.0
yield a score,1,1,1.0
a score that,1,1,1.0
score that represents,1,1,1.0
that represents the,1,1,1.0
represents the degree,1,1,1.0
the degree to,1,1,1.0
degree to which,1,1,1.0
to which an,1,1,1.0
which an example,1,1,1.0
is a member,1,1,1.0
a member of,1,1,1.0
member of a,1,1,1.0
of a class,1,1,1.0
a class such,1,1,1.0
class such ranking,1,1,1.0
such ranking can,1,1,1.0
ranking can be,1,1,1.0
can be used,5,2,2.5
used to produce,1,1,1.0
to produce several,1,1,1.0
produce several classifiers,1,1,1.0
several classifiers by,1,1,1.0
classifiers by varying,1,1,1.0
by varying the,1,1,1.0
varying the threshold,1,1,1.0
the threshold of,1,1,1.0
threshold of an,1,1,1.0
of an example,2,2,1.0
an example pertaining,1,1,1.0
example pertaining to,1,1,1.0
pertaining to a,1,1,1.0
to a class,1,1,1.0
a class for,1,1,1.0
class for internally,1,1,1.0
for internally biasing,1,1,1.0
internally biasing the,1,1,1.0
biasing the discrimination,1,1,1.0
the discrimination procedure,1,1,1.0
discrimination procedure a,1,1,1.0
procedure a weighted,1,1,1.0
a weighted distance,1,1,1.0
weighted distance function,1,1,1.0
distance function was,1,1,1.0
function was proposed,1,1,1.0
was proposed in,3,2,1.5
proposed in to,1,1,1.0
in to be,1,1,1.0
be used in,3,2,1.5
in the classification,1,1,1.0
the classification phase,1,1,1.0
classification phase of,1,1,1.0
phase of knn,1,1,1.0
of knn the,1,1,1.0
knn the basic,1,1,1.0
the basic idea,2,2,1.0
basic idea behind,2,2,1.0
behind this weighted,1,1,1.0
this weighted distance,1,1,1.0
weighted distance is,1,1,1.0
distance is to,1,1,1.0
is to compensate,1,1,1.0
to compensate for,3,2,1.5
compensate for the,2,1,2.0
for the imbalance,1,1,1.0
the imbalance in,1,1,1.0
imbalance in the,1,1,1.0
the training sample,1,1,1.0
training sample without,1,1,1.0
sample without actually,1,1,1.0
without actually altering,1,1,1.0
actually altering the,1,1,1.0
altering the class,1,1,1.0
the class distributions,4,1,4.0
class distributions thus,1,1,1.0
distributions thus weights,1,1,1.0
thus weights are,1,1,1.0
weights are assigned,1,1,1.0
are assigned unlike,1,1,1.0
assigned unlike in,1,1,1.0
unlike in the,1,1,1.0
in the usual,1,1,1.0
the usual weighted,1,1,1.0
usual weighted rule,1,1,1.0
weighted rule to,1,1,1.0
rule to the,1,1,1.0
respective classes and,1,1,1.0
classes and not,1,1,1.0
and not to,1,1,1.0
not to the,1,1,1.0
to the individual,1,1,1.0
the individual prototypes,1,1,1.0
individual prototypes in,1,1,1.0
prototypes in this,1,1,1.0
this way since,1,1,1.0
way since the,1,1,1.0
since the weighting,1,1,1.0
the weighting factor,1,1,1.0
weighting factor is,1,1,1.0
factor is greater,1,1,1.0
is greater for,1,1,1.0
greater for the,1,1,1.0
for the majority,2,2,1.0
majority class than,2,1,2.0
class than for,1,1,1.0
the minority one,2,2,1.0
minority one the,1,1,1.0
one the distance,1,1,1.0
the distance to,2,1,2.0
distance to positive,1,1,1.0
to positive minority,1,1,1.0
minority class prototypes,1,1,1.0
class prototypes becomes,1,1,1.0
prototypes becomes much,1,1,1.0
becomes much lower,1,1,1.0
much lower than,1,1,1.0
lower than the,1,1,1.0
than the distance,1,1,1.0
distance to prototypes,1,1,1.0
to prototypes of,1,1,1.0
prototypes of the,2,1,2.0
majority class this,1,1,1.0
class this produces,1,1,1.0
this produces a,1,1,1.0
produces a tendency,1,1,1.0
a tendency for,1,1,1.0
tendency for the,1,1,1.0
for the new,1,1,1.0
the new patterns,1,1,1.0
new patterns to,1,1,1.0
patterns to find,1,1,1.0
to find their,1,1,1.0
find their nearest,1,1,1.0
their nearest neighbor,1,1,1.0
nearest neighbor among,1,1,1.0
neighbor among the,1,1,1.0
among the prototypes,1,1,1.0
the prototypes of,1,1,1.0
minority class approach,1,1,1.0
class approach to,1,1,1.0
approach to dealing,1,1,1.0
to dealing with,1,1,1.0
imbalanced datasets using,1,1,1.0
datasets using svm,1,1,1.0
using svm biases,1,1,1.0
svm biases the,1,1,1.0
biases the algorithm,1,1,1.0
the algorithm so,1,1,1.0
algorithm so that,1,1,1.0
that the academic,1,1,1.0
the academic is,1,1,1.0
academic is further,1,1,1.0
is further away,1,1,1.0
further away from,1,1,1.0
away from the,2,2,1.0
from the positive,2,2,1.0
positive class this,2,1,2.0
class this is,1,1,1.0
this is done,3,2,1.5
is done in,2,2,1.0
done in order,1,1,1.0
order to compensate,2,2,1.0
for the skew,1,1,1.0
the skew associated,1,1,1.0
skew associated with,1,1,1.0
associated with imbalanced,1,1,1.0
imbalanced datasets which,1,1,1.0
datasets which pushes,1,1,1.0
which pushes the,1,1,1.0
pushes the closer,1,1,1.0
the closer to,1,1,1.0
to the positive,1,1,1.0
class this biasing,1,1,1.0
this biasing can,1,1,1.0
biasing can be,1,1,1.0
can be accomplished,1,1,1.0
be accomplished in,1,1,1.0
accomplished in various,1,1,1.0
in various ways,1,1,1.0
various ways in,1,1,1.0
ways in an,1,1,1.0
an algorithm is,1,1,1.0
algorithm is proposed,1,1,1.0
is proposed by,1,1,1.0
proposed by changing,1,1,1.0
by changing the,1,1,1.0
changing the kernel,1,1,1.0
the kernel function,10,2,5.0
kernel function to,2,2,1.0
function to develop,1,1,1.0
to develop this,1,1,1.0
develop this bias,1,1,1.0
this bias veropoulos,1,1,1.0
bias veropoulos et,1,1,1.0
veropoulos et al,1,1,1.0
al s uggested,1,1,1.0
s uggested using,1,1,1.0
uggested using different,1,1,1.0
using different penalty,1,1,1.0
different penalty constants,1,1,1.0
penalty constants for,1,1,1.0
constants for different,1,1,1.0
for different classes,1,1,1.0
different classes of,1,1,1.0
classes of data,1,1,1.0
of data making,1,1,1.0
data making errors,1,1,1.0
making errors on,1,1,1.0
errors on positive,1,1,1.0
on positive examples,1,1,1.0
positive examples costlier,1,1,1.0
examples costlier than,1,1,1.0
costlier than errors,1,1,1.0
than errors on,1,1,1.0
errors on negative,1,1,1.0
on negative examples,1,1,1.0
negative examples kaizhu,1,1,1.0
examples kaizhu huang,1,1,1.0
kaizhu huang et,1,1,1.0
huang et al,1,1,1.0
al presented biased,1,1,1.0
presented biased minimax,1,1,1.0
biased minimax probability,2,1,2.0
minimax probability machine,2,1,2.0
probability machine bmpm,1,1,1.0
machine bmpm to,1,1,1.0
bmpm to resolve,1,1,1.0
to resolve the,1,1,1.0
resolve the imbalance,1,1,1.0
the imbalance problem,4,2,2.0
imbalance problem given,1,1,1.0
problem given the,1,1,1.0
given the reliable,1,1,1.0
the reliable mean,1,1,1.0
reliable mean and,1,1,1.0
mean and covariance,1,1,1.0
and covariance matrices,1,1,1.0
covariance matrices of,1,1,1.0
matrices of the,1,1,1.0
minority classes bmpm,1,1,1.0
classes bmpm can,1,1,1.0
bmpm can derive,1,1,1.0
can derive the,1,1,1.0
derive the decision,1,1,1.0
the decision by,1,1,1.0
decision by adjusting,1,1,1.0
by adjusting the,2,2,1.0
adjusting the lower,1,1,1.0
the lower bound,1,1,1.0
lower bound of,1,1,1.0
bound of the,1,1,1.0
of the real,1,1,1.0
the real accuracy,1,1,1.0
real accuracy of,1,1,1.0
of the testing,1,1,1.0
the testing set,1,1,1.0
testing set learning,1,1,1.0
set learning besides,1,1,1.0
learning besides changing,1,1,1.0
besides changing the,1,1,1.0
changing the class,1,1,1.0
class distributions incorporating,1,1,1.0
distributions incorporating costs,1,1,1.0
incorporating costs in,1,1,1.0
costs in making,1,1,1.0
in making is,1,1,1.0
making is another,1,1,1.0
is another way,1,1,1.0
another way to,1,1,1.0
way to improve,1,1,1.0
to improve classifier,1,1,1.0
improve classifier s,1,1,1.0
s performance when,1,1,1.0
performance when learning,1,1,1.0
when learning from,1,1,1.0
from imbalanced datasets,1,1,1.0
imbalanced datasets cost,1,1,1.0
datasets cost model,1,1,1.0
cost model takes,1,1,1.0
model takes the,1,1,1.0
takes the form,1,1,1.0
the form of,1,1,1.0
form of a,1,1,1.0
of a cost,1,1,1.0
a cost matrix,1,1,1.0
cost matrix as,1,1,1.0
matrix as shown,1,1,1.0
as shown in,6,2,3.0
shown in where,2,1,2.0
in where the,1,1,1.0
where the cost,1,1,1.0
cost of classifying,1,1,1.0
of classifying a,1,1,1.0
classifying a sample,1,1,1.0
a sample from,1,1,1.0
sample from a,1,1,1.0
from a true,1,1,1.0
a true class,1,1,1.0
true class j,1,1,1.0
class j to,1,1,1.0
j to class,1,1,1.0
to class i,1,1,1.0
class i corresponds,1,1,1.0
i corresponds to,1,1,1.0
corresponds to the,5,2,2.5
to the matrix,2,2,1.0
the matrix entry,1,1,1.0
matrix entry λij,1,1,1.0
entry λij this,1,1,1.0
λij this matrix,1,1,1.0
this matrix is,1,1,1.0
matrix is usually,1,1,1.0
is usually expressed,1,1,1.0
usually expressed in,1,1,1.0
expressed in terms,1,1,1.0
terms of average,1,1,1.0
of average misclassification,1,1,1.0
average misclassification costs,1,1,1.0
misclassification costs for,1,1,1.0
costs for the,1,1,1.0
problem the diagonal,1,1,1.0
the diagonal elements,1,1,1.0
diagonal elements are,1,1,1.0
elements are usually,1,1,1.0
are usually set,1,1,1.0
usually set to,1,1,1.0
set to zero,1,1,1.0
to zero meaning,1,1,1.0
zero meaning correct,1,1,1.0
meaning correct classification,1,1,1.0
correct classification has,1,1,1.0
classification has no,1,1,1.0
has no cost,1,1,1.0
no cost the,1,1,1.0
cost the goal,1,1,1.0
the goal in,2,2,1.0
goal in sensitive,1,1,1.0
in sensitive classification,1,1,1.0
sensitive classification is,1,1,1.0
classification is to,1,1,1.0
is to minimize,1,1,1.0
to minimize the,1,1,1.0
minimize the cost,1,1,1.0
cost of misclassification,1,1,1.0
of misclassification which,1,1,1.0
misclassification which can,1,1,1.0
which can be,5,2,2.5
can be realized,1,1,1.0
be realized by,1,1,1.0
realized by choosing,1,1,1.0
by choosing the,1,1,1.0
choosing the class,1,1,1.0
the class with,1,1,1.0
class with the,1,1,1.0
with the minimum,1,1,1.0
the minimum conditional,1,1,1.0
minimum conditional risk,1,1,1.0
conditional risk prediction,1,1,1.0
risk prediction class,1,1,1.0
prediction class i,1,1,1.0
class i class,1,1,1.0
i class j,1,1,1.0
class j true,1,1,1.0
j true class,1,1,1.0
true class i,1,1,1.0
class i ijλ,1,1,1.0
i ijλ class,1,1,1.0
ijλ class j,1,1,1.0
class j jiλ,1,1,1.0
j jiλ fig,1,1,1.0
jiλ fig cost,1,1,1.0
fig cost matrix,1,1,1.0
cost matrix metacost,1,1,1.0
matrix metacost is,1,1,1.0
metacost is another,2,1,2.0
another method to,1,1,1.0
method to make,1,1,1.0
to make a,1,1,1.0
make a classifier,1,1,1.0
a classifier the,1,1,1.0
classifier the procedure,1,1,1.0
the procedure begins,1,1,1.0
procedure begins to,1,1,1.0
begins to learn,2,1,2.0
to learn an,2,1,2.0
learn an internal,2,1,2.0
an internal model,2,1,2.0
internal model by,1,1,1.0
model by applying,1,1,1.0
by applying a,1,1,1.0
applying a sensitive,1,1,1.0
a sensitive procedure,1,1,1.0
sensitive procedure which,1,1,1.0
procedure which employs,1,1,1.0
which employs a,1,1,1.0
employs a base,1,1,1.0
a base learning,1,1,1.0
base learning algorithm,1,1,1.0
learning algorithm then,1,1,1.0
algorithm then metacost,1,1,1.0
then metacost procedure,1,1,1.0
metacost procedure estimates,1,1,1.0
procedure estimates class,1,1,1.0
estimates class probabilities,2,1,2.0
class probabilities using,2,1,2.0
probabilities using bagging,2,1,2.0
using bagging and,2,1,2.0
bagging and then,2,1,2.0
and then the,2,2,1.0
then the training,1,1,1.0
the training examples,2,1,2.0
training examples with,2,1,2.0
examples with their,2,1,2.0
with their minimum,2,1,2.0
their minimum expected,2,1,2.0
minimum expected cost,2,1,2.0
expected cost classes,2,1,2.0
cost classes and,2,1,2.0
classes and finally,2,1,2.0
and finally relearns,2,1,2.0
finally relearns a,2,1,2.0
relearns a model,2,1,2.0
a model using,2,1,2.0
model using the,2,1,2.0
using the modified,2,1,2.0
the modified training,2,1,2.0
modified training set,2,1,2.0
training set adaboost,1,1,1.0
set adaboost s,1,1,1.0
adaboost s rule,2,1,2.0
s rule has,1,1,1.0
rule has been,1,1,1.0
has been made,1,1,1.0
been made sensitive,1,1,1.0
made sensitive so,1,1,1.0
sensitive so that,1,1,1.0
so that examples,1,1,1.0
that examples belonging,1,1,1.0
examples belonging to,2,1,2.0
belonging to rare,1,1,1.0
to rare class,1,1,1.0
rare class that,1,1,1.0
that are misclassified,1,1,1.0
are misclassified are,1,1,1.0
misclassified are assigned,1,1,1.0
are assigned higher,1,1,1.0
assigned higher weights,1,1,1.0
higher weights than,1,1,1.0
weights than those,1,1,1.0
than those belonging,1,1,1.0
those belonging to,1,1,1.0
belonging to common,1,1,1.0
to common class,1,1,1.0
common class the,1,1,1.0
class the resulting,1,1,1.0
the resulting system,2,1,2.0
resulting system adacost,2,1,2.0
system adacost has,2,1,2.0
adacost has been,2,1,2.0
has been empirically,3,1,3.0
been empirically shown,2,1,2.0
empirically shown to,2,1,2.0
shown to produce,2,1,2.0
to produce lower,2,1,2.0
produce lower cumulative,2,1,2.0
lower cumulative misclassification,2,1,2.0
cumulative misclassification costs,2,1,2.0
misclassification costs than,2,1,2.0
costs than adaboost,2,1,2.0
than adaboost learning,1,1,1.0
adaboost learning learning,1,1,1.0
learning learning is,1,1,1.0
learning is a,1,1,1.0
is a approach,1,1,1.0
a approach which,1,1,1.0
approach which provides,1,1,1.0
which provides an,1,1,1.0
provides an alternative,1,1,1.0
an alternative to,1,1,1.0
alternative to discrimination,1,1,1.0
to discrimination where,1,1,1.0
discrimination where the,1,1,1.0
where the model,1,1,1.0
the model can,1,1,1.0
model can be,1,1,1.0
can be created,1,1,1.0
be created based,1,1,1.0
created based on,1,1,1.0
on the examples,1,1,1.0
the examples of,2,2,1.0
of the target,2,2,1.0
the target class,2,1,2.0
target class alone,1,1,1.0
class alone here,1,1,1.0
alone here classification,1,1,1.0
here classification is,1,1,1.0
classification is accomplished,1,1,1.0
is accomplished by,3,2,1.5
accomplished by imposing,1,1,1.0
by imposing a,1,1,1.0
imposing a threshold,1,1,1.0
a threshold on,1,1,1.0
threshold on the,1,1,1.0
on the similarity,1,1,1.0
the similarity value,1,1,1.0
similarity value between,1,1,1.0
value between a,1,1,1.0
between a query,1,1,1.0
a query object,1,1,1.0
query object and,1,1,1.0
object and the,1,1,1.0
target class mainly,1,1,1.0
class mainly two,1,1,1.0
mainly two classes,1,1,1.0
two classes of,1,1,1.0
classes of learners,1,1,1.0
of learners were,1,1,1.0
learners were previously,1,1,1.0
were previously studied,1,1,1.0
previously studied in,1,1,1.0
studied in the,1,1,1.0
context of the,1,1,1.0
of the approach,1,1,1.0
the approach svms,1,1,1.0
approach svms and,1,1,1.0
svms and and,1,1,1.0
and and they,1,1,1.0
and they were,1,1,1.0
they were found,1,1,1.0
to be competitive,1,1,1.0
be competitive besides,1,1,1.0
competitive besides systems,1,1,1.0
besides systems that,1,1,1.0
systems that learn,1,1,1.0
that learn only,1,1,1.0
learn only the,1,1,1.0
only the minority,1,1,1.0
minority class may,2,2,1.0
class may still,1,1,1.0
may still train,1,1,1.0
still train using,1,1,1.0
train using examples,1,1,1.0
using examples belonging,1,1,1.0
belonging to all,1,1,1.0
to all classes,1,1,1.0
all classes brute,1,1,1.0
classes brute shrink,1,1,1.0
brute shrink and,1,1,1.0
shrink and ripper,1,1,1.0
and ripper are,1,1,1.0
ripper are three,1,1,1.0
are three such,1,1,1.0
three such data,1,1,1.0
such data mining,1,1,1.0
data mining systems,1,1,1.0
mining systems brute,1,1,1.0
systems brute has,1,1,1.0
brute has been,1,1,1.0
has been used,2,1,2.0
been used to,2,2,1.0
used to look,1,1,1.0
to look for,1,1,1.0
look for flaws,1,1,1.0
for flaws in,1,1,1.0
flaws in the,1,1,1.0
in the boeing,1,1,1.0
the boeing manufacturing,1,1,1.0
boeing manufacturing process,1,1,1.0
manufacturing process shrink,1,1,1.0
process shrink uses,1,1,1.0
shrink uses a,1,1,1.0
uses a similar,1,1,1.0
a similar approach,1,1,1.0
similar approach to,1,1,1.0
approach to detect,1,1,1.0
to detect rare,1,1,1.0
detect rare oil,1,1,1.0
rare oil spills,1,1,1.0
oil spills from,1,1,1.0
spills from satellite,1,1,1.0
from satellite radar,1,1,1.0
radar images based,1,1,1.0
images based on,1,1,1.0
on the assumption,1,1,1.0
the assumption that,1,1,1.0
assumption that there,1,1,1.0
that there will,1,1,1.0
there will be,1,1,1.0
will be many,1,1,1.0
be many more,1,1,1.0
many more negative,1,1,1.0
more negative examples,1,1,1.0
negative examples than,1,1,1.0
examples than positive,1,1,1.0
than positive examples,1,1,1.0
positive examples shrink,1,1,1.0
examples shrink labels,1,1,1.0
shrink labels mixed,1,1,1.0
labels mixed regions,1,1,1.0
mixed regions regions,1,1,1.0
regions regions with,1,1,1.0
regions with positive,1,1,1.0
with positive and,1,1,1.0
and negative examples,3,1,3.0
negative examples with,1,1,1.0
examples with the,2,2,1.0
with the positive,1,1,1.0
positive class ripper,1,1,1.0
class ripper is,1,1,1.0
ripper is a,1,1,1.0
is a rule,1,1,1.0
a rule induction,1,1,1.0
rule induction system,1,1,1.0
induction system that,1,1,1.0
system that utilizes,1,1,1.0
that utilizes a,1,1,1.0
utilizes a approach,1,1,1.0
approach to iteratively,1,1,1.0
to iteratively build,1,1,1.0
iteratively build rules,1,1,1.0
build rules to,1,1,1.0
rules to cover,1,1,1.0
to cover previously,1,1,1.0
cover previously uncovered,1,1,1.0
previously uncovered training,1,1,1.0
uncovered training examples,1,1,1.0
training examples each,1,1,1.0
examples each rule,1,1,1.0
each rule is,1,1,1.0
rule is grown,1,1,1.0
is grown by,1,1,1.0
grown by adding,1,1,1.0
by adding conditions,1,1,1.0
adding conditions until,1,1,1.0
conditions until no,1,1,1.0
until no negative,1,1,1.0
no negative examples,1,1,1.0
negative examples are,1,1,1.0
examples are covered,1,1,1.0
are covered it,1,1,1.0
covered it normally,1,1,1.0
it normally generates,1,1,1.0
normally generates rules,1,1,1.0
generates rules for,1,1,1.0
rules for each,1,1,1.0
for each class,2,2,1.0
each class from,1,1,1.0
class from the,2,2,1.0
from the most,2,2,1.0
the most rare,1,1,1.0
most rare class,1,1,1.0
rare class to,1,1,1.0
class to the,1,1,1.0
to the most,1,1,1.0
the most common,4,2,2.0
most common class,1,1,1.0
common class so,1,1,1.0
class so in,1,1,1.0
so in this,1,1,1.0
in this view,1,1,1.0
this view ripper,1,1,1.0
view ripper can,1,1,1.0
ripper can be,1,1,1.0
can be view,1,1,1.0
be view as,1,1,1.0
view as a,1,1,1.0
as a learner,1,1,1.0
a learner an,1,1,1.0
learner an interesting,1,1,1.0
an interesting aspect,1,1,1.0
interesting aspect of,1,1,1.0
aspect of based,1,1,1.0
of based learning,1,1,1.0
based learning is,1,1,1.0
learning is that,1,1,1.0
is that under,1,1,1.0
that under certain,1,1,1.0
under certain conditions,1,1,1.0
certain conditions such,1,1,1.0
conditions such as,1,1,1.0
such as of,1,1,1.0
as of the,1,1,1.0
of the domain,1,1,1.0
the domain space,1,1,1.0
domain space one,1,1,1.0
space one class,1,1,1.0
one class approaches,1,1,1.0
class approaches to,1,1,1.0
approaches to solving,1,1,1.0
to solving the,1,1,1.0
solving the classification,1,1,1.0
the classification problem,1,1,1.0
classification problem may,1,1,1.0
problem may in,1,1,1.0
may in fact,1,1,1.0
in fact be,1,1,1.0
fact be superior,1,1,1.0
be superior to,1,1,1.0
superior to discriminative,1,1,1.0
to discriminative approaches,1,1,1.0
discriminative approaches such,1,1,1.0
such as decision,1,1,1.0
as decision trees,1,1,1.0
decision trees or,1,1,1.0
trees or neural,1,1,1.0
or neural networks,1,1,1.0
neural networks raskutti,1,1,1.0
networks raskutti and,1,1,1.0
raskutti and kowalczyk,2,1,2.0
and kowalczyk demonstrated,1,1,1.0
kowalczyk demonstrated the,1,1,1.0
demonstrated the optimality,1,1,1.0
the optimality of,1,1,1.0
optimality of svms,1,1,1.0
of svms over,1,1,1.0
svms over ones,1,1,1.0
over ones in,1,1,1.0
ones in certain,1,1,1.0
in certain important,1,1,1.0
certain important domains,1,1,1.0
important domains including,1,1,1.0
domains including genomic,1,1,1.0
including genomic data,1,1,1.0
genomic data in,1,1,1.0
data in particular,1,1,1.0
in particular they,1,1,1.0
particular they showed,1,1,1.0
they showed that,1,1,1.0
showed that class,1,1,1.0
that class learning,1,1,1.0
class learning is,1,1,1.0
learning is particularly,1,1,1.0
is particularly useful,1,1,1.0
particularly useful when,1,1,1.0
useful when used,1,1,1.0
when used on,1,1,1.0
used on extremely,1,1,1.0
on extremely unbalanced,1,1,1.0
extremely unbalanced data,1,1,1.0
unbalanced data sets,2,1,2.0
data sets composed,1,1,1.0
sets composed of,1,1,1.0
composed of a,1,1,1.0
of a high,1,1,1.0
a high dimensional,1,1,1.0
high dimensional noisy,1,1,1.0
dimensional noisy feature,1,1,1.0
noisy feature space,1,1,1.0
feature space they,1,1,1.0
space they argued,1,1,1.0
they argued that,1,1,1.0
that the approach,2,2,1.0
the approach is,1,1,1.0
approach is related,1,1,1.0
related to aggressive,1,1,1.0
to aggressive feature,1,1,1.0
aggressive feature selection,1,1,1.0
feature selection methods,1,1,1.0
selection methods but,1,1,1.0
methods but is,1,1,1.0
but is more,1,1,1.0
is more practical,1,1,1.0
more practical since,1,1,1.0
practical since feature,1,1,1.0
since feature selection,1,1,1.0
feature selection can,1,1,1.0
selection can often,1,1,1.0
can often be,1,1,1.0
often be too,1,1,1.0
be too expensive,1,1,1.0
expensive to apply,1,1,1.0
to apply ensemble,1,1,1.0
apply ensemble learning,1,1,1.0
ensemble learning methods,2,1,2.0
learning methods ensemble,1,1,1.0
methods ensemble learning,1,1,1.0
ensemble learning has,1,1,1.0
learning has established,1,1,1.0
has established its,1,1,1.0
established its superiority,1,1,1.0
its superiority in,1,1,1.0
superiority in machine,1,1,1.0
recent years of,1,1,1.0
years of which,1,1,1.0
of which boosting,1,1,1.0
which boosting bagging,1,1,1.0
boosting bagging are,1,1,1.0
bagging are the,1,1,1.0
are the most,1,1,1.0
the most successful,1,1,1.0
most successful approaches,1,1,1.0
successful approaches ensemble,1,1,1.0
approaches ensemble learning,1,1,1.0
learning methods have,2,2,1.0
have been extensively,1,1,1.0
been extensively used,1,1,1.0
extensively used to,1,1,1.0
used to handle,1,1,1.0
to handle class,1,1,1.0
handle class imbalance,1,1,1.0
class imbalance problems,3,2,1.5
imbalance problems these,1,1,1.0
problems these methods,1,1,1.0
these methods combine,1,1,1.0
methods combine the,1,1,1.0
combine the results,1,1,1.0
results of many,1,1,1.0
of many classifiers,1,1,1.0
many classifiers their,1,1,1.0
classifiers their successes,1,1,1.0
their successes attribute,1,1,1.0
successes attribute to,1,1,1.0
attribute to the,1,1,1.0
to the fact,1,1,1.0
fact that their,1,1,1.0
that their base,1,1,1.0
their base learners,1,1,1.0
base learners usually,1,1,1.0
learners usually are,1,1,1.0
usually are of,1,1,1.0
are of diversity,1,1,1.0
of diversity in,1,1,1.0
diversity in principle,1,1,1.0
in principle or,1,1,1.0
principle or induced,1,1,1.0
or induced with,1,1,1.0
induced with various,1,1,1.0
with various class,1,1,1.0
various class distributions,1,1,1.0
class distributions adaboost,1,1,1.0
distributions adaboost introduced,1,1,1.0
adaboost introduced by,1,1,1.0
introduced by freund,1,1,1.0
by freund and,1,1,1.0
freund and schapire,4,2,2.0
and schapire solved,1,1,1.0
schapire solved many,1,1,1.0
solved many of,1,1,1.0
many of the,3,2,1.5
of the practical,1,1,1.0
the practical difficulties,1,1,1.0
practical difficulties of,1,1,1.0
difficulties of the,1,1,1.0
of the earlier,1,1,1.0
the earlier boosting,1,1,1.0
earlier boosting algorithms,1,1,1.0
boosting algorithms initially,1,1,1.0
algorithms initially all,1,1,1.0
initially all weights,1,1,1.0
all weights are,1,1,1.0
weights are set,1,1,1.0
are set equally,1,1,1.0
set equally but,1,1,1.0
equally but on,1,1,1.0
but on each,1,1,1.0
on each round,1,1,1.0
each round the,1,1,1.0
round the weights,1,1,1.0
the weights of,1,1,1.0
weights of incorrectly,1,1,1.0
of incorrectly classified,1,1,1.0
classified examples are,1,1,1.0
examples are increased,1,1,1.0
are increased so,1,1,1.0
increased so that,1,1,1.0
that the weak,1,1,1.0
the weak learner,3,2,1.5
weak learner is,1,1,1.0
learner is forced,1,1,1.0
is forced to,1,1,1.0
forced to focus,1,1,1.0
to focus on,2,2,1.0
focus on the,4,3,1.3333333333333333
on the hard,1,1,1.0
the hard examples,1,1,1.0
hard examples in,1,1,1.0
training set as,2,2,1.0
set as stated,1,1,1.0
as stated in,1,1,1.0
stated in learning,1,1,1.0
in learning by,1,1,1.0
learning by making,1,1,1.0
by making adaboost,1,1,1.0
making adaboost s,1,1,1.0
s rule the,1,1,1.0
rule the resulting,1,1,1.0
than adaboost thus,1,1,1.0
adaboost thus it,1,1,1.0
thus it can,1,1,1.0
used to address,1,1,1.0
to address class,1,1,1.0
address class imbalance,1,1,1.0
imbalance problem scales,1,1,1.0
problem scales examples,1,1,1.0
scales examples in,2,1,2.0
examples in proportion,2,1,2.0
in proportion to,2,1,2.0
proportion to how,2,1,2.0
to how well,2,1,2.0
how well they,2,1,2.0
well they are,2,1,2.0
they are distinguished,2,1,2.0
are distinguished from,2,1,2.0
distinguished from examples,2,1,2.0
from examples and,1,1,1.0
examples and scales,1,1,1.0
and scales examples,1,1,1.0
from examples another,1,1,1.0
examples another algorithm,1,1,1.0
another algorithm that,1,1,1.0
algorithm that uses,1,1,1.0
that uses boosting,1,1,1.0
uses boosting to,1,1,1.0
boosting to address,1,1,1.0
address the class,1,1,1.0
problem is smoteboost,1,1,1.0
is smoteboost this,1,1,1.0
smoteboost this algorithm,1,1,1.0
this algorithm recognizes,1,1,1.0
algorithm recognizes that,1,1,1.0
recognizes that boosting,1,1,1.0
that boosting may,1,1,1.0
boosting may suffer,1,1,1.0
may suffer from,2,2,1.0
suffer from the,1,1,1.0
from the same,1,1,1.0
the same problems,1,1,1.0
same problems as,1,1,1.0
problems as overfitting,1,1,1.0
as overfitting instead,1,1,1.0
overfitting instead of,1,1,1.0
instead of changing,1,1,1.0
of changing the,1,1,1.0
changing the distributions,1,1,1.0
distributions of training,1,1,1.0
of training data,2,1,2.0
training data by,1,1,1.0
data by updating,1,1,1.0
by updating the,1,1,1.0
updating the weights,1,1,1.0
associated with each,1,1,1.0
with each example,1,1,1.0
each example smoteboost,1,1,1.0
example smoteboost alters,1,1,1.0
smoteboost alters the,1,1,1.0
the distributions by,1,1,1.0
distributions by adding,1,1,1.0
by adding new,1,1,1.0
adding new examples,1,1,1.0
new examples of,1,1,1.0
examples of minority,1,1,1.0
smote algorithm experiment,1,1,1.0
algorithm experiment results,1,1,1.0
experiment results indicated,1,1,1.0
results indicated that,1,1,1.0
indicated that the,1,1,1.0
that the experts,1,1,1.0
the experts approach,1,1,1.0
experts approach performs,1,1,1.0
approach performs well,1,1,1.0
performs well generally,1,1,1.0
well generally outperforming,1,1,1.0
generally outperforming adaboost,1,1,1.0
outperforming adaboost with,1,1,1.0
adaboost with respect,1,1,1.0
respect to precision,1,1,1.0
to precision and,1,1,1.0
and recall on,1,1,1.0
recall on text,1,1,1.0
text classification problems,1,1,1.0
problems and doing,1,1,1.0
and doing especially,1,1,1.0
doing especially well,1,1,1.0
especially well at,1,1,1.0
well at covering,1,1,1.0
at covering the,1,1,1.0
covering the minority,1,1,1.0
the minority examples,3,2,1.5
minority examples more,1,1,1.0
examples more detailed,1,1,1.0
more detailed experiments,1,1,1.0
detailed experiments are,1,1,1.0
experiments are presented,1,1,1.0
presented in metacost,1,1,1.0
in metacost is,1,1,1.0
is another ensemble,1,1,1.0
another ensemble method,1,1,1.0
ensemble method it,1,1,1.0
method it begins,1,1,1.0
it begins to,1,1,1.0
internal model then,1,1,1.0
model then estimates,1,1,1.0
then estimates class,1,1,1.0
and then labels,1,1,1.0
then labels the,1,1,1.0
labels the training,1,1,1.0
training set chan,1,1,1.0
set chan and,1,1,1.0
chan and stolfo,2,1,2.0
and stolfo run,1,1,1.0
stolfo run a,1,1,1.0
run a set,1,1,1.0
set of preliminary,1,1,1.0
of preliminary experiments,1,1,1.0
preliminary experiments to,1,1,1.0
experiments to identify,1,1,1.0
identify a good,1,1,1.0
a good class,2,1,2.0
good class distributions,2,1,2.0
distributions and then,1,1,1.0
and then do,1,1,1.0
then do resampling,1,1,1.0
do resampling to,1,1,1.0
resampling to generate,1,1,1.0
to generate multiple,1,1,1.0
generate multiple training,1,1,1.0
multiple training sets,1,1,1.0
training sets with,1,1,1.0
sets with the,1,1,1.0
with the desired,1,1,1.0
the desired class,1,1,1.0
desired class distributions,1,1,1.0
class distributions each,1,1,1.0
distributions each training,1,1,1.0
each training set,2,1,2.0
training set typically,1,1,1.0
set typically includes,1,1,1.0
typically includes all,1,1,1.0
includes all examples,1,1,1.0
all examples and,1,1,1.0
examples and a,1,1,1.0
and a subset,1,1,1.0
a subset of,1,1,1.0
of the examples,1,1,1.0
the examples however,1,1,1.0
examples however each,1,1,1.0
however each example,1,1,1.0
each example is,1,1,1.0
example is guaranteed,1,1,1.0
is guaranteed to,1,1,1.0
guaranteed to occur,1,1,1.0
to occur in,1,1,1.0
occur in at,1,1,1.0
in at least,1,1,1.0
at least one,2,2,1.0
least one training,1,1,1.0
one training set,1,1,1.0
training set so,1,1,1.0
set so no,1,1,1.0
so no data,1,1,1.0
no data is,1,1,1.0
data is wasted,1,1,1.0
is wasted the,1,1,1.0
wasted the learning,1,1,1.0
algorithm is applied,1,1,1.0
is applied to,3,2,1.5
applied to each,1,1,1.0
to each training,1,1,1.0
set and is,1,1,1.0
and is used,1,1,1.0
used to form,1,1,1.0
to form a,2,2,1.0
form a composite,1,1,1.0
a composite learner,1,1,1.0
composite learner from,1,1,1.0
learner from the,1,1,1.0
from the resulting,1,1,1.0
the resulting classifiers,1,1,1.0
resulting classifiers since,1,1,1.0
classifiers since it,1,1,1.0
since it is,1,1,1.0
it is a,2,1,2.0
is a wrapper,1,1,1.0
a wrapper method,1,1,1.0
wrapper method it,1,1,1.0
method it can,2,2,1.0
be used with,1,1,1.0
used with any,1,1,1.0
with any learning,1,1,1.0
any learning method,1,1,1.0
learning method internally,1,1,1.0
method internally the,1,1,1.0
internally the same,1,1,1.0
the same basic,1,1,1.0
same basic approach,1,1,1.0
basic approach for,1,1,1.0
approach for partitioning,1,1,1.0
for partitioning the,1,1,1.0
partitioning the data,1,1,1.0
the data and,3,2,1.5
data and learning,1,1,1.0
and learning multiple,1,1,1.0
learning multiple classifiers,1,1,1.0
multiple classifiers has,1,1,1.0
classifiers has been,1,1,1.0
been used with,1,1,1.0
used with support,1,1,1.0
with support vector,1,1,1.0
vector machines the,1,1,1.0
machines the resulting,1,1,1.0
the resulting svm,1,1,1.0
resulting svm ensembles,1,1,1.0
svm ensembles was,1,1,1.0
ensembles was shown,1,1,1.0
was shown to,1,1,1.0
shown to outperform,1,1,1.0
to outperform both,1,1,1.0
outperform both and,1,1,1.0
both and while,1,1,1.0
and while these,1,1,1.0
while these ensemble,1,1,1.0
these ensemble approaches,1,1,1.0
ensemble approaches are,1,1,1.0
approaches are effective,1,1,1.0
are effective for,1,1,1.0
effective for dealing,1,1,1.0
for dealing with,2,1,2.0
imbalance problem they,1,1,1.0
problem they assume,1,1,1.0
they assume that,1,1,1.0
assume that a,1,1,1.0
class distributions is,1,1,1.0
distributions is known,1,1,1.0
is known this,1,1,1.0
known this can,1,1,1.0
this can be,3,2,1.5
can be estimated,2,2,1.0
be estimated using,1,1,1.0
estimated using some,1,1,1.0
using some preliminary,1,1,1.0
some preliminary runs,1,1,1.0
preliminary runs but,1,1,1.0
runs but it,1,1,1.0
it is time,1,1,1.0
is time consuming,1,1,1.0
time consuming from,1,1,1.0
consuming from the,1,1,1.0
from the style,1,1,1.0
the style constructing,1,1,1.0
style constructing the,1,1,1.0
constructing the training,1,1,1.0
training data sets,1,1,1.0
sets they can,1,1,1.0
viewed as a,1,1,1.0
as a variant,1,1,1.0
a variant of,1,1,1.0
variant of bagging,1,1,1.0
of bagging phua,1,1,1.0
bagging phua et,1,1,1.0
phua et al,1,1,1.0
et al combined,1,1,1.0
al combined bagging,1,1,1.0
combined bagging and,1,1,1.0
bagging and stacking,1,1,1.0
and stacking to,1,1,1.0
stacking to identify,1,1,1.0
to identify the,1,1,1.0
identify the best,1,1,1.0
the best mix,1,1,1.0
best mix of,1,1,1.0
mix of classifiers,1,1,1.0
of classifiers in,2,1,2.0
classifiers in their,1,1,1.0
in their insurance,1,1,1.0
their insurance fraud,1,1,1.0
insurance fraud detection,1,1,1.0
fraud detection domain,1,1,1.0
detection domain they,1,1,1.0
domain they noted,1,1,1.0
noted that bagging,1,1,1.0
that bagging achieves,1,1,1.0
bagging achieves the,1,1,1.0
achieves the best,2,2,1.0
the best besides,1,1,1.0
best besides ensemble,1,1,1.0
besides ensemble learning,1,1,1.0
ensemble learning algorithms,1,1,1.0
learning algorithms of,1,1,1.0
algorithms of boosting,1,1,1.0
of boosting and,1,1,1.0
boosting and bagging,1,1,1.0
and bagging style,1,1,1.0
bagging style kotsiantis,1,1,1.0
style kotsiantis and,1,1,1.0
kotsiantis and pintelas,3,2,1.5
and pintelas used,1,1,1.0
pintelas used three,1,1,1.0
used three agents,1,1,1.0
three agents the,1,1,1.0
agents the first,1,1,1.0
the first learns,1,1,1.0
first learns using,1,1,1.0
learns using naive,1,1,1.0
using naive bayes,1,1,1.0
naive bayes the,1,1,1.0
bayes the second,1,1,1.0
the second using,1,1,1.0
second using and,1,1,1.0
using and the,2,2,1.0
and the third,1,1,1.0
the third using,1,1,1.0
third using on,1,1,1.0
using on a,1,1,1.0
on a filtered,1,1,1.0
a filtered version,1,1,1.0
filtered version of,1,1,1.0
version of training,1,1,1.0
training data and,2,2,1.0
data and combined,1,1,1.0
and combined their,1,1,1.0
combined their predictions,1,1,1.0
their predictions according,1,1,1.0
predictions according to,1,1,1.0
according to a,2,2,1.0
to a voting,1,1,1.0
a voting scheme,1,1,1.0
voting scheme this,1,1,1.0
scheme this technique,1,1,1.0
this technique attempts,1,1,1.0
technique attempts to,1,1,1.0
attempts to achieve,1,1,1.0
to achieve diversity,1,1,1.0
achieve diversity in,1,1,1.0
diversity in the,1,1,1.0
in the errors,1,1,1.0
the errors of,1,1,1.0
errors of the,1,1,1.0
of the academic,1,1,1.0
the academic models,1,1,1.0
academic models by,1,1,1.0
models by using,1,1,1.0
by using different,1,1,1.0
using different learning,2,1,2.0
different learning algorithms,1,1,1.0
learning algorithms the,1,1,1.0
algorithms the intuition,1,1,1.0
the intuition is,1,1,1.0
intuition is that,1,1,1.0
that the models,1,1,1.0
the models generated,1,1,1.0
models generated using,1,1,1.0
generated using different,1,1,1.0
different learning biases,1,1,1.0
learning biases are,1,1,1.0
biases are more,1,1,1.0
are more likely,4,3,1.3333333333333333
likely to make,1,1,1.0
to make errors,1,1,1.0
make errors in,1,1,1.0
errors in different,1,1,1.0
different ways they,1,1,1.0
ways they also,1,1,1.0
they also used,1,1,1.0
also used feature,1,1,1.0
used feature selection,1,1,1.0
feature selection of,1,1,1.0
selection of the,1,1,1.0
training data because,1,1,1.0
data because in,1,1,1.0
because in small,1,1,1.0
in small data,1,1,1.0
small data sets,1,1,1.0
sets the amount,1,1,1.0
the amount of,1,1,1.0
amount of class,1,1,1.0
class imbalance affects,1,1,1.0
imbalance affects more,1,1,1.0
affects more the,1,1,1.0
more the induction,1,1,1.0
the induction and,1,1,1.0
induction and thus,1,1,1.0
and thus feature,1,1,1.0
thus feature selection,1,1,1.0
feature selection makes,1,1,1.0
selection makes the,1,1,1.0
makes the problem,1,1,1.0
the problem less,1,1,1.0
problem less difficult,1,1,1.0
less difficult motivated,1,1,1.0
difficult motivated zheng,1,1,1.0
motivated zheng and,1,1,1.0
zheng and srihari,1,1,1.0
and srihari s,1,1,1.0
srihari s work,1,1,1.0
s work castillo,1,1,1.0
work castillo and,1,1,1.0
and serrano do,1,1,1.0
serrano do not,1,1,1.0
do not particularly,1,1,1.0
selection but make,1,1,1.0
but make it,1,1,1.0
make it a,1,1,1.0
complete framework they,1,1,1.0
framework they use,1,1,1.0
they use a,1,1,1.0
use a classifier,1,1,1.0
a classifier system,1,1,1.0
classifier system to,1,1,1.0
system to construct,1,1,1.0
to construct multiple,1,1,1.0
construct multiple learners,1,1,1.0
multiple learners each,1,1,1.0
learners each doing,1,1,1.0
each doing its,1,1,1.0
doing its own,1,1,1.0
its own feature,1,1,1.0
own feature selection,1,1,1.0
feature selection based,1,1,1.0
selection based on,1,1,1.0
genetic algorithm their,1,1,1.0
algorithm their proposed,1,1,1.0
their proposed system,1,1,1.0
proposed system also,1,1,1.0
system also combines,1,1,1.0
also combines the,1,1,1.0
combines the predictions,1,1,1.0
predictions of each,1,1,1.0
of each learner,1,1,1.0
each learner using,1,1,1.0
learner using genetic,1,1,1.0
using genetic algorithms,1,1,1.0
genetic algorithms evaluation,1,1,1.0
algorithms evaluation metrics,1,1,1.0
evaluation metrics accuracy,1,1,1.0
metrics accuracy is,1,1,1.0
accuracy is the,1,1,1.0
is the most,1,1,1.0
most common evaluation,1,1,1.0
common evaluation metric,1,1,1.0
evaluation metric for,2,1,2.0
metric for most,1,1,1.0
for most traditional,1,1,1.0
most traditional application,1,1,1.0
traditional application but,1,1,1.0
application but accuracy,1,1,1.0
but accuracy is,1,1,1.0
accuracy is not,1,1,1.0
is not suitable,1,1,1.0
not suitable to,1,1,1.0
suitable to evaluate,1,1,1.0
to evaluate imbalanced,1,1,1.0
evaluate imbalanced data,1,1,1.0
data sets since,1,1,1.0
sets since many,1,1,1.0
since many practitioners,1,1,1.0
many practitioners have,1,1,1.0
practitioners have observed,1,1,1.0
have observed that,1,1,1.0
observed that for,1,1,1.0
that for extremely,1,1,1.0
for extremely skewed,1,1,1.0
extremely skewed class,1,1,1.0
distributions the recall,1,1,1.0
the recall of,1,1,1.0
recall of the,2,1,2.0
class is often,1,1,1.0
is often which,1,1,1.0
often which means,1,1,1.0
which means that,3,2,1.5
means that there,1,1,1.0
that there are,4,3,1.3333333333333333
there are no,2,2,1.0
are no classification,1,1,1.0
no classification rules,1,1,1.0
classification rules generated,1,1,1.0
rules generated for,1,1,1.0
generated for the,1,1,1.0
class using terminology,1,1,1.0
using terminology from,1,1,1.0
terminology from information,1,1,1.0
from information retrieval,2,1,2.0
information retrieval the,1,1,1.0
retrieval the minority,1,1,1.0
minority class has,1,1,1.0
class has much,1,1,1.0
has much lower,1,1,1.0
much lower precision,1,1,1.0
lower precision and,1,1,1.0
and recall than,1,1,1.0
recall than the,1,1,1.0
than the majority,2,2,1.0
majority class places,1,1,1.0
class places more,1,1,1.0
places more weight,1,1,1.0
more weight on,1,1,1.0
weight on the,1,1,1.0
on the majority,4,2,2.0
class than on,1,1,1.0
than on minority,1,1,1.0
on minority class,1,1,1.0
minority class which,1,1,1.0
class which makes,1,1,1.0
which makes it,1,1,1.0
makes it difficult,1,1,1.0
it difficult for,1,1,1.0
difficult for a,1,1,1.0
for a classifier,1,1,1.0
a classifier to,1,1,1.0
classifier to perform,1,1,1.0
to perform well,2,2,1.0
perform well on,1,1,1.0
well on the,1,1,1.0
on the minority,3,3,1.0
class for this,1,1,1.0
this reason additional,1,1,1.0
reason additional metrics,1,1,1.0
additional metrics are,1,1,1.0
metrics are coming,1,1,1.0
are coming into,1,1,1.0
coming into widespread,1,1,1.0
into widespread use,1,1,1.0
widespread use in,1,1,1.0
use in recent,1,1,1.0
recent years several,1,1,1.0
years several new,1,1,1.0
several new metrics,1,1,1.0
new metrics have,1,1,1.0
metrics have been,1,1,1.0
been proposed or,1,1,1.0
introduced from other,1,1,1.0
from other domains,1,1,1.0
other domains for,1,1,1.0
domains for imbalanced,1,1,1.0
sets they are,1,1,1.0
they are precision,1,1,1.0
are precision and,2,1,2.0
and recall from,1,1,1.0
recall from information,1,1,1.0
information retrieval domain,1,1,1.0
retrieval domain roc,1,1,1.0
domain roc and,1,1,1.0
roc and auc,2,1,2.0
and auc area,1,1,1.0
auc area under,1,1,1.0
roc curve from,1,1,1.0
curve from medical,1,1,1.0
from medical domain,1,1,1.0
medical domain value,1,1,1.0
domain value maximum,1,1,1.0
value maximum geometry,1,1,1.0
maximum geometry mean,1,1,1.0
geometry mean mgm,1,1,1.0
mean mgm of,1,1,1.0
mgm of the,2,1,2.0
of the accuracy,3,1,3.0
the accuracy on,2,1,2.0
accuracy on the,2,1,2.0
minority class maximum,1,1,1.0
class maximum sum,1,1,1.0
maximum sum ms,1,1,1.0
sum ms of,1,1,1.0
ms of the,1,1,1.0
the accuracy all,1,1,1.0
accuracy all the,1,1,1.0
all the metrics,1,1,1.0
the metrics can,1,1,1.0
metrics can be,1,1,1.0
can be classified,1,1,1.0
be classified into,1,1,1.0
classified into two,1,1,1.0
into two categories,1,1,1.0
two categories metrics,1,1,1.0
categories metrics based,1,1,1.0
metrics based on,2,1,2.0
based on confusion,2,1,2.0
on confusion matrix,2,1,2.0
confusion matrix directly,1,1,1.0
matrix directly and,1,1,1.0
directly and that,1,1,1.0
and that based,1,1,1.0
that based on,1,1,1.0
based on accuracy,1,1,1.0
on accuracy of,1,1,1.0
accuracy of binary,1,1,1.0
of binary classes,1,1,1.0
binary classes or,1,1,1.0
classes or precision,1,1,1.0
or precision and,1,1,1.0
and recall directly,1,1,1.0
recall directly accuracy,1,1,1.0
directly accuracy precision,1,1,1.0
accuracy precision and,1,1,1.0
and recall fp,1,1,1.0
recall fp rate,1,1,1.0
fp rate tp,2,1,2.0
rate tp rate,2,1,2.0
tp rate roc,1,1,1.0
rate roc and,1,1,1.0
and auc fall,1,1,1.0
auc fall into,1,1,1.0
fall into the,2,1,2.0
into the first,1,1,1.0
the first while,1,1,1.0
first while and,1,1,1.0
while and other,1,1,1.0
and other more,1,1,1.0
other more complex,1,1,1.0
more complex metrics,1,1,1.0
complex metrics such,1,1,1.0
metrics such as,2,2,1.0
such as mgm,1,1,1.0
as mgm of,1,1,1.0
minority class ms,1,1,1.0
class ms fall,1,1,1.0
ms fall into,1,1,1.0
into the other,1,1,1.0
the other table,1,1,1.0
other table shows,1,1,1.0
shows the confusion,1,1,1.0
matrix and a,1,1,1.0
and a good,1,1,1.0
a good understanding,1,1,1.0
good understanding to,1,1,1.0
understanding to confusion,1,1,1.0
to confusion matrix,1,1,1.0
confusion matrix will,1,1,1.0
matrix will be,4,2,2.0
will be helpful,2,1,2.0
be helpful table,1,1,1.0
helpful table confusion,1,1,1.0
confusion matrix prediction,1,1,1.0
matrix prediction positive,1,1,1.0
prediction positive negative,1,1,1.0
positive negative real,1,1,1.0
negative real positive,1,1,1.0
real positive tp,1,1,1.0
false negative negative,1,1,1.0
negative negative fp,1,1,1.0
negative fp false,1,1,1.0
true negative as,1,1,1.0
negative as promised,1,1,1.0
as promised at,1,1,1.0
promised at the,1,1,1.0
at the beginning,2,2,1.0
the beginning of,2,2,1.0
beginning of the,1,1,1.0
the paper the,1,1,1.0
paper the class,1,1,1.0
positive and the,1,1,1.0
and the class,3,2,1.5
is negative fig,1,1,1.0
negative fig presents,1,1,1.0
fig presents the,1,1,1.0
presents the most,1,1,1.0
the most well,1,1,1.0
most well known,1,1,1.0
well known evaluation,1,1,1.0
known evaluation metrics,1,1,1.0
evaluation metrics as,1,1,1.0
metrics as shown,1,1,1.0
shown in table,2,2,1.0
in table tp,1,1,1.0
table tp and,1,1,1.0
tp and tn,1,1,1.0
and tn denote,1,1,1.0
tn denote the,1,1,1.0
denote the number,2,1,2.0
number of positive,2,1,2.0
of positive and,1,1,1.0
negative examples that,1,1,1.0
that are classified,1,1,1.0
are classified correctly,1,1,1.0
classified correctly while,1,1,1.0
correctly while fn,1,1,1.0
while fn and,1,1,1.0
fn and fp,1,1,1.0
and fp denote,1,1,1.0
fp denote the,1,1,1.0
number of misclassified,1,1,1.0
of misclassified positive,1,1,1.0
misclassified positive and,1,1,1.0
negative examples respectively,1,1,1.0
examples respectively by,1,1,1.0
respectively by definition,1,1,1.0
by definition accuracy,1,1,1.0
definition accuracy precision,1,1,1.0
accuracy precision fp,1,1,1.0
precision fp rate,1,1,1.0
rate and value,1,1,1.0
and value can,1,1,1.0
value can be,1,1,1.0
can be represented,1,1,1.0
be represented by,1,1,1.0
represented by equations,1,1,1.0
by equations from,1,1,1.0
equations from to,1,1,1.0
from to as,1,1,1.0
to as shown,1,1,1.0
in where precision,1,1,1.0
where precision and,1,1,1.0
and recall are,1,1,1.0
recall are precision,1,1,1.0
and recall of,1,1,1.0
minority class fp,1,1,1.0
class fp rate,1,1,1.0
fp rate denotes,1,1,1.0
rate denotes the,1,1,1.0
denotes the percentage,1,1,1.0
percentage of the,2,1,2.0
of the misclassified,1,1,1.0
the misclassified negative,1,1,1.0
misclassified negative examples,1,1,1.0
negative examples and,1,1,1.0
examples and tp,1,1,1.0
and tp rate,1,1,1.0
rate is the,1,1,1.0
is the percentage,1,1,1.0
of the correctly,1,1,1.0
correctly classified positive,1,1,1.0
classified positive examples,1,1,1.0
positive examples the,1,1,1.0
examples the point,1,1,1.0
the point is,1,1,1.0
point is the,1,1,1.0
is the ideal,1,1,1.0
the ideal point,1,1,1.0
ideal point of,1,1,1.0
point of the,1,1,1.0
of the learners,1,1,1.0
the learners that,1,1,1.0
learners that is,1,1,1.0
that is there,1,1,1.0
is there is,1,1,1.0
is no positive,1,1,1.0
no positive examples,1,1,1.0
positive examples were,1,1,1.0
examples were misclassified,1,1,1.0
were misclassified to,1,1,1.0
misclassified to negative,1,1,1.0
to negative class,1,1,1.0
negative class and,1,1,1.0
class and vice,1,1,1.0
and vice versa,1,1,1.0
vice versa or,1,1,1.0
versa or is,1,1,1.0
or is a,1,1,1.0
is a popular,1,1,1.0
a popular evaluation,1,1,1.0
popular evaluation metric,1,1,1.0
metric for imbalance,1,1,1.0
for imbalance problem,1,1,1.0
imbalance problem it,2,1,2.0
problem it is,1,1,1.0
is a kind,1,1,1.0
a kind of,1,1,1.0
kind of combination,1,1,1.0
of combination of,2,2,1.0
combination of recall,1,1,1.0
of recall and,1,1,1.0
recall and precision,2,1,2.0
and precision which,1,1,1.0
precision which are,1,1,1.0
which are effective,1,1,1.0
are effective metrics,1,1,1.0
effective metrics for,1,1,1.0
metrics for information,1,1,1.0
for information retrieval,1,1,1.0
information retrieval community,1,1,1.0
retrieval community where,1,1,1.0
community where the,1,1,1.0
where the imbalance,1,1,1.0
problem exists is,1,1,1.0
exists is high,1,1,1.0
is high when,1,1,1.0
high when both,1,1,1.0
when both recall,1,1,1.0
both recall and,1,1,1.0
and precision are,1,1,1.0
precision are high,1,1,1.0
are high and,1,1,1.0
high and can,1,1,1.0
and can be,3,2,1.5
can be adjusted,1,1,1.0
be adjusted through,1,1,1.0
adjusted through changing,1,1,1.0
through changing the,1,1,1.0
changing the value,1,1,1.0
the value of,4,3,1.3333333333333333
value of β,3,2,1.5
of β where,1,1,1.0
β where β,1,1,1.0
where β corresponds,1,1,1.0
β corresponds to,1,1,1.0
corresponds to relative,1,1,1.0
to relative importance,1,1,1.0
relative importance of,1,1,1.0
importance of precision,1,1,1.0
of precision recall,1,1,1.0
precision recall for,1,1,1.0
recall for example,1,1,1.0
for example counts,1,1,1.0
example counts both,1,1,1.0
counts both equally,1,1,1.0
both equally while,1,1,1.0
equally while counts,1,1,1.0
while counts recall,1,1,1.0
counts recall twice,1,1,1.0
recall twice as,1,1,1.0
twice as much,1,1,1.0
as much perhaps,1,1,1.0
much perhaps the,1,1,1.0
most common metric,1,1,1.0
common metric to,1,1,1.0
metric to assess,1,1,1.0
to assess overall,1,1,1.0
assess overall classification,1,1,1.0
overall classification performance,1,1,1.0
classification performance is,1,1,1.0
performance is roc,1,1,1.0
is roc analysis,1,1,1.0
roc analysis and,2,2,1.0
analysis and the,1,1,1.0
and the associated,1,1,1.0
the associated use,1,1,1.0
associated use of,1,1,1.0
of the area,2,1,2.0
roc curve auc,1,1,1.0
curve auc in,1,1,1.0
auc in detail,1,1,1.0
in detail roc,1,1,1.0
detail roc curve,1,1,1.0
in which tp,1,1,1.0
which tp rate,1,1,1.0
the and fp,1,1,1.0
and fp rate,1,1,1.0
on the roc,1,1,1.0
roc curves like,1,1,1.0
curves like curves,1,1,1.0
like curves can,1,1,1.0
curves can also,1,1,1.0
can also be,2,2,1.0
also be used,1,1,1.0
to assess different,2,2,1.0
assess different roc,1,1,1.0
different roc curve,1,1,1.0
roc curve depicts,1,1,1.0
curve depicts relative,1,1,1.0
depicts relative between,1,1,1.0
relative between benefits,1,1,1.0
between benefits tp,1,1,1.0
benefits tp rate,1,1,1.0
rate and costs,1,1,1.0
and costs fp,1,1,1.0
costs fp rate,1,1,1.0
fp rate that,1,1,1.0
rate that is,1,1,1.0
of positive examples,1,1,1.0
positive examples correctly,1,1,1.0
examples correctly classified,1,1,1.0
correctly classified can,1,1,1.0
classified can be,1,1,1.0
be increased at,1,1,1.0
increased at the,1,1,1.0
at the expense,1,1,1.0
the expense of,1,1,1.0
expense of introducing,1,1,1.0
of introducing additional,1,1,1.0
introducing additional false,1,1,1.0
additional false positives,1,1,1.0
false positives a,1,1,1.0
positives a major,1,1,1.0
a major disadvantage,1,1,1.0
major disadvantage of,1,1,1.0
disadvantage of roc,1,1,1.0
of roc analysis,2,2,1.0
roc analysis is,1,1,1.0
analysis is that,1,1,1.0
that it does,1,1,1.0
does not deliver,1,1,1.0
not deliver a,1,1,1.0
deliver a single,1,1,1.0
a single easy,1,1,1.0
single easy to,1,1,1.0
easy to use,1,1,1.0
to use performance,1,1,1.0
use performance measure,1,1,1.0
performance measure like,1,1,1.0
measure like accuracy,1,1,1.0
like accuracy directly,1,1,1.0
accuracy directly auc,1,1,1.0
directly auc does,1,1,1.0
auc does not,1,1,1.0
does not place,1,1,1.0
not place more,1,1,1.0
place more emphasis,1,1,1.0
more emphasis on,1,1,1.0
emphasis on one,1,1,1.0
on one class,1,1,1.0
one class over,1,1,1.0
class over the,1,1,1.0
over the other,1,1,1.0
the other so,1,1,1.0
other so it,1,1,1.0
is not biased,1,1,1.0
not biased against,1,1,1.0
biased against the,1,1,1.0
against the minority,1,1,1.0
minority class tnfpfntptntpaccuracy,1,1,1.0
class tnfpfntptntpaccuracy pr,1,1,1.0
tnfpfntptntpaccuracy pr fptptpecison,1,1,1.0
pr fptptpecison re,1,1,1.0
fptptpecison re fntptpcall,1,1,1.0
re fntptpcall tnfpfpratefp,1,1,1.0
fntptpcall tnfpfpratefp fntptpratetp,1,1,1.0
tnfpfpratefp fntptpratetp ecisioncall,1,1,1.0
fntptpratetp ecisioncall ecisioncallf,1,1,1.0
ecisioncall ecisioncallf value,1,1,1.0
ecisioncallf value prre,1,1,1.0
value prre pr,1,1,1.0
prre pr re,1,1,1.0
pr re β,1,1,1.0
re β β,1,1,1.0
β β accuracyaccuracymgm,1,1,1.0
β accuracyaccuracymgm accuracyaccuracyms,1,1,1.0
accuracyaccuracymgm accuracyaccuracyms evaluation,1,1,1.0
accuracyaccuracyms evaluation metrics,1,1,1.0
evaluation metrics based,1,1,1.0
confusion matrix besides,1,1,1.0
matrix besides minimum,1,1,1.0
besides minimum cost,1,1,1.0
minimum cost criterion,1,1,1.0
cost criterion is,1,1,1.0
criterion is also,1,1,1.0
is also used,1,1,1.0
also used to,1,1,1.0
to evaluate the,3,2,1.5
evaluate the performance,1,1,1.0
classifiers in learning,1,1,1.0
in learning from,4,2,2.0
data sets when,1,1,1.0
sets when performing,1,1,1.0
when performing learning,1,1,1.0
performing learning when,1,1,1.0
learning when applying,1,1,1.0
when applying machine,1,1,1.0
applying machine learning,1,1,1.0
learning algorithms to,1,1,1.0
algorithms to real,1,1,1.0
to real world,1,1,1.0
world applications rarely,1,1,1.0
applications rarely would,1,1,1.0
rarely would one,1,1,1.0
would one or,1,1,1.0
one or more,2,2,1.0
or more of,2,2,1.0
more of these,1,1,1.0
of these assumptions,1,1,1.0
these assumptions hold,1,1,1.0
assumptions hold but,1,1,1.0
hold but to,1,1,1.0
but to select,1,1,1.0
to select a,2,1,2.0
select a classifier,2,1,2.0
a classifier certain,1,1,1.0
classifier certain conditions,1,1,1.0
certain conditions must,1,1,1.0
conditions must exist,1,1,1.0
must exist and,1,1,1.0
exist and we,1,1,1.0
and we may,1,1,1.0
we may need,1,1,1.0
may need more,1,1,1.0
need more information,1,1,1.0
more information if,1,1,1.0
information if one,1,1,1.0
if one roc,1,1,1.0
one roc curve,1,1,1.0
roc curve dominates,1,1,1.0
curve dominates all,1,1,1.0
dominates all others,1,1,1.0
all others then,1,1,1.0
others then the,1,1,1.0
then the best,1,1,1.0
the best method,3,2,1.5
best method is,2,2,1.0
method is the,1,1,1.0
is the one,2,2,1.0
the one that,2,2,1.0
one that produced,1,1,1.0
that produced the,1,1,1.0
produced the dominant,1,1,1.0
the dominant curve,2,1,2.0
dominant curve which,1,1,1.0
curve which is,1,1,1.0
which is also,1,1,1.0
is also the,1,1,1.0
also the curve,1,1,1.0
the curve with,1,1,1.0
curve with the,1,1,1.0
with the largest,2,2,1.0
the largest area,1,1,1.0
largest area with,1,1,1.0
area with maximum,1,1,1.0
with maximum auc,1,1,1.0
maximum auc to,1,1,1.0
auc to select,1,1,1.0
a classifier from,1,1,1.0
classifier from the,1,1,1.0
from the dominant,1,1,1.0
dominant curve we,1,1,1.0
curve we need,1,1,1.0
we need additional,1,1,1.0
need additional information,1,1,1.0
additional information such,1,1,1.0
information such as,1,1,1.0
such as a,1,1,1.0
as a target,1,1,1.0
a target fp,1,1,1.0
target fp rate,1,1,1.0
fp rate on,1,1,1.0
rate on the,1,1,1.0
other hand if,1,1,1.0
hand if multiple,1,1,1.0
if multiple curves,1,1,1.0
multiple curves dominate,1,1,1.0
curves dominate in,1,1,1.0
dominate in different,1,1,1.0
in different parts,1,1,1.0
different parts of,1,1,1.0
parts of the,1,1,1.0
of the roc,1,1,1.0
the roc space,4,2,2.0
roc space then,1,1,1.0
space then we,1,1,1.0
then we can,1,1,1.0
we can use,1,1,1.0
can use the,2,2,1.0
use the roc,1,1,1.0
the roc convex,1,1,1.0
roc convex hull,1,1,1.0
convex hull method,1,1,1.0
hull method to,1,1,1.0
method to select,1,1,1.0
to select the,1,1,1.0
select the optimal,1,1,1.0
the optimal classifier,1,1,1.0
optimal classifier relations,1,1,1.0
classifier relations to,1,1,1.0
relations to other,1,1,1.0
to other problems,1,1,1.0
other problems it,1,1,1.0
problems it has,1,1,1.0
it has also,1,1,1.0
also been observed,1,1,1.0
been observed that,1,1,1.0
observed that in,1,1,1.0
that in some,2,2,1.0
in some domains,1,1,1.0
some domains for,1,1,1.0
domains for example,1,1,1.0
example the sick,1,1,1.0
the sick data,1,1,1.0
sick data set,1,1,1.0
data set standard,1,1,1.0
set standard machine,1,1,1.0
learning algorithms are,1,1,1.0
algorithms are capable,1,1,1.0
are capable of,1,1,1.0
capable of inducing,1,1,1.0
of inducing good,1,1,1.0
inducing good classifiers,1,1,1.0
good classifiers even,1,1,1.0
classifiers even using,1,1,1.0
even using highly,1,1,1.0
using highly imbalanced,1,1,1.0
highly imbalanced training,1,1,1.0
imbalanced training sets,4,3,1.3333333333333333
training sets this,1,1,1.0
sets this shows,1,1,1.0
this shows that,1,1,1.0
shows that class,1,1,1.0
that class imbalance,2,1,2.0
is not the,5,3,1.6666666666666667
not the only,3,2,1.5
the only problem,2,1,2.0
only problem responsible,1,1,1.0
problem responsible for,1,1,1.0
for the decrease,1,1,1.0
the decrease in,1,1,1.0
decrease in performance,1,1,1.0
in performance of,1,1,1.0
performance of learning,2,2,1.0
of learning algorithms,2,2,1.0
learning algorithms class,1,1,1.0
algorithms class imbalance,1,1,1.0
only problem to,1,1,1.0
problem to contend,1,1,1.0
to contend with,1,1,1.0
contend with besides,1,1,1.0
with besides the,1,1,1.0
besides the distributions,1,1,1.0
the distributions within,1,1,1.0
distributions within each,1,1,1.0
within each class,1,1,1.0
each class of,1,1,1.0
class of the,1,1,1.0
the data within,1,1,1.0
data within class,1,1,1.0
imbalance are also,1,1,1.0
are also relevant,1,1,1.0
also relevant it,1,1,1.0
relevant it was,1,1,1.0
it was found,1,1,1.0
was found that,1,1,1.0
found that in,1,1,1.0
that in certain,1,1,1.0
in certain cases,1,1,1.0
certain cases addressing,1,1,1.0
cases addressing the,1,1,1.0
addressing the small,1,1,1.0
the small disjuncts,1,1,1.0
small disjuncts problem,1,1,1.0
disjuncts problem with,1,1,1.0
problem with regardless,1,1,1.0
with regardless of,1,1,1.0
regardless of the,2,2,1.0
problem was sufficient,1,1,1.0
was sufficient to,1,1,1.0
sufficient to increase,1,1,1.0
to increase performance,1,1,1.0
increase performance experiments,1,1,1.0
performance experiments by,1,1,1.0
experiments by jo,1,1,1.0
by jo and,1,1,1.0
jo and japkowicz,4,2,2.0
and japkowicz suggested,1,1,1.0
japkowicz suggested that,1,1,1.0
suggested that the,3,2,1.5
that the problem,2,1,2.0
problem is not,3,1,3.0
is not directly,4,2,2.0
not directly caused,2,1,2.0
directly caused by,2,1,2.0
caused by class,3,1,3.0
by class imbalances,1,1,1.0
class imbalances but,1,1,1.0
imbalances but rather,2,2,1.0
but rather that,2,1,2.0
rather that class,2,1,2.0
that class imbalances,1,1,1.0
class imbalances may,1,1,1.0
imbalances may yield,1,1,1.0
may yield small,2,1,2.0
yield small disjuncts,2,1,2.0
small disjuncts which,2,1,2.0
disjuncts which in,2,1,2.0
which in turn,3,2,1.5
in turn will,2,1,2.0
turn will cause,2,1,2.0
will cause degradation,2,1,2.0
cause degradation a,1,1,1.0
degradation a approach,1,1,1.0
a approach was,1,1,1.0
approach was proposed,1,1,1.0
was proposed whose,1,1,1.0
proposed whose idea,1,1,1.0
whose idea is,1,1,1.0
is to consider,1,1,1.0
to consider not,1,1,1.0
consider not only,1,1,1.0
not only the,2,2,1.0
only the imbalance,1,1,1.0
the imbalance but,1,1,1.0
imbalance but also,1,1,1.0
but also the,1,1,1.0
also the class,1,1,1.0
imbalance and to,1,1,1.0
and to the,3,2,1.5
the dataset by,1,1,1.0
dataset by rectifying,1,1,1.0
by rectifying these,1,1,1.0
rectifying these two,1,1,1.0
these two types,1,1,1.0
two types of,1,1,1.0
types of imbalances,1,1,1.0
of imbalances simultaneously,1,1,1.0
imbalances simultaneously the,1,1,1.0
simultaneously the experiments,1,1,1.0
the experiments results,1,1,1.0
experiments results of,1,1,1.0
results of prati,1,1,1.0
of prati et,1,1,1.0
prati et al,1,1,1.0
et al using,1,1,1.0
al using a,1,1,1.0
using a inductive,1,1,1.0
a inductive scheme,1,1,1.0
inductive scheme suggested,1,1,1.0
scheme suggested that,1,1,1.0
is not solely,1,1,1.0
not solely caused,1,1,1.0
solely caused by,1,1,1.0
by class imbalance,2,1,2.0
class imbalance but,2,1,2.0
imbalance but is,1,1,1.0
but is also,1,1,1.0
is also related,1,1,1.0
also related to,1,1,1.0
to the degree,1,1,1.0
the degree of,3,2,1.5
degree of data,1,1,1.0
of data overlapping,1,1,1.0
data overlapping among,1,1,1.0
overlapping among the,1,1,1.0
among the classes,1,1,1.0
the classes it,1,1,1.0
classes it was,1,1,1.0
it was also,1,1,1.0
was also found,1,1,1.0
also found that,1,1,1.0
found that data,1,1,1.0
that data duplication,1,1,1.0
data duplication is,1,1,1.0
duplication is generally,1,1,1.0
is generally harmful,1,1,1.0
generally harmful although,1,1,1.0
harmful although for,1,1,1.0
although for classifiers,1,1,1.0
for classifiers such,1,1,1.0
such as naive,1,1,1.0
as naive bayes,1,1,1.0
naive bayes and,1,1,1.0
bayes and perceptrons,1,1,1.0
and perceptrons with,1,1,1.0
perceptrons with margins,1,1,1.0
with margins high,1,1,1.0
margins high degrees,1,1,1.0
high degrees of,1,1,1.0
degrees of duplication,1,1,1.0
of duplication are,1,1,1.0
duplication are necessary,1,1,1.0
are necessary to,1,1,1.0
necessary to harm,1,1,1.0
to harm classification,1,1,1.0
harm classification it,1,1,1.0
classification it was,1,1,1.0
it was argued,2,1,2.0
was argued that,2,1,2.0
that the reason,1,1,1.0
the reason why,1,1,1.0
reason why class,1,1,1.0
why class imbalances,1,1,1.0
class imbalances and,1,1,1.0
imbalances and overlapping,1,1,1.0
and overlapping classes,1,1,1.0
overlapping classes are,1,1,1.0
classes are related,1,1,1.0
are related is,1,1,1.0
related is that,1,1,1.0
is that misclassification,1,1,1.0
that misclassification often,1,1,1.0
misclassification often occurs,1,1,1.0
often occurs near,1,1,1.0
occurs near class,1,1,1.0
near class boundaries,1,1,1.0
class boundaries where,1,1,1.0
boundaries where overlap,1,1,1.0
where overlap usually,1,1,1.0
overlap usually occurs,1,1,1.0
usually occurs as,1,1,1.0
occurs as well,1,1,1.0
as well weiss,1,1,1.0
well weiss investigated,1,1,1.0
investigated the relation,1,1,1.0
imbalance and training,1,1,1.0
and training set,1,1,1.0
training set size,1,1,1.0
set size experiments,1,1,1.0
size experiments showed,1,1,1.0
showed that while,1,1,1.0
while the position,1,1,1.0
the position of,1,1,1.0
position of the,1,1,1.0
of the best,1,1,1.0
the best class,1,1,1.0
best class distributions,1,1,1.0
class distributions varies,1,1,1.0
distributions varies somewhat,1,1,1.0
varies somewhat with,1,1,1.0
somewhat with size,1,1,1.0
with size in,1,1,1.0
size in many,1,1,1.0
in many cases,1,1,1.0
many cases especially,1,1,1.0
cases especially with,1,1,1.0
especially with error,1,1,1.0
with error rate,1,1,1.0
error rate the,1,1,1.0
rate the variation,1,1,1.0
the variation is,1,1,1.0
variation is small,1,1,1.0
is small which,1,1,1.0
small which gives,1,1,1.0
which gives support,1,1,1.0
gives support to,1,1,1.0
support to the,1,1,1.0
to the notion,1,1,1.0
the notion that,1,1,1.0
notion that there,1,1,1.0
is a best,1,1,1.0
a best marginal,1,1,1.0
best marginal class,1,1,1.0
marginal class distribution,1,1,1.0
class distribution for,2,2,1.0
distribution for a,1,1,1.0
for a learning,1,1,1.0
a learning task,2,2,1.0
learning task the,1,1,1.0
task the results,1,1,1.0
results also indicated,1,1,1.0
also indicated that,1,1,1.0
indicated that for,1,1,1.0
that for any,3,2,1.5
for any fixed,1,1,1.0
any fixed class,1,1,1.0
fixed class distribution,1,1,1.0
class distribution increasing,1,1,1.0
distribution increasing the,1,1,1.0
increasing the size,1,1,1.0
size of the,3,3,1.0
training set always,1,1,1.0
set always leads,1,1,1.0
always leads to,1,1,1.0
leads to improved,1,1,1.0
to improved classifier,1,1,1.0
improved classifier performance,1,1,1.0
classifier performance conclusion,1,1,1.0
performance conclusion learning,1,1,1.0
conclusion learning from,1,1,1.0
sets is an,1,1,1.0
is an important,1,1,1.0
an important issue,1,1,1.0
important issue in,1,1,1.0
issue in machine,1,1,1.0
learning a direct,1,1,1.0
a direct method,1,1,1.0
direct method to,1,1,1.0
method to solve,1,1,1.0
solve the imbalance,1,1,1.0
problem is artificially,1,1,1.0
is artificially balancing,1,1,1.0
artificially balancing the,1,1,1.0
balancing the class,1,1,1.0
distributions and its,1,1,1.0
and its effectiveness,1,1,1.0
its effectiveness has,1,1,1.0
effectiveness has been,1,1,1.0
been empirically analyzed,1,1,1.0
empirically analyzed in,1,1,1.0
analyzed in however,1,1,1.0
in however there,1,1,1.0
however there is,1,1,1.0
there is some,1,1,1.0
some evidence that,1,1,1.0
evidence that the,2,2,1.0
class distributions artificially,1,1,1.0
distributions artificially does,1,1,1.0
artificially does not,1,1,1.0
does not have,1,1,1.0
not have much,1,1,1.0
have much effect,1,1,1.0
much effect on,1,1,1.0
effect on the,2,2,1.0
on the performance,2,2,1.0
of the induced,1,1,1.0
the induced classifier,1,1,1.0
induced classifier since,1,1,1.0
classifier since some,1,1,1.0
since some learning,1,1,1.0
some learning systems,1,1,1.0
learning systems are,1,1,1.0
systems are not,1,1,1.0
are not sensitive,1,1,1.0
not sensitive to,1,1,1.0
sensitive to differences,1,1,1.0
to differences in,1,1,1.0
differences in class,1,1,1.0
in class distributions,1,1,1.0
class distributions it,1,1,1.0
distributions it seems,1,1,1.0
it seems that,1,1,1.0
seems that we,1,1,1.0
that we still,1,1,1.0
we still need,1,1,1.0
still need a,1,1,1.0
need a clearer,1,1,1.0
a clearer and,1,1,1.0
clearer and deeper,1,1,1.0
and deeper understanding,1,1,1.0
deeper understanding of,2,1,2.0
understanding of how,1,1,1.0
of how class,1,1,1.0
how class distribution,1,1,1.0
class distribution affects,1,1,1.0
distribution affects each,1,1,1.0
affects each phase,1,1,1.0
each phase of,1,1,1.0
phase of the,1,1,1.0
learning process for,1,1,1.0
process for more,1,1,1.0
for more learners,1,1,1.0
more learners except,1,1,1.0
learners except decision,1,1,1.0
except decision trees,1,1,1.0
decision trees a,1,1,1.0
trees a deeper,1,1,1.0
a deeper understanding,1,1,1.0
understanding of the,1,1,1.0
of the basics,1,1,1.0
the basics will,1,1,1.0
basics will help,1,1,1.0
will help us,1,1,1.0
help us to,1,1,1.0
us to design,1,1,1.0
to design better,1,1,1.0
design better methods,1,1,1.0
better methods for,1,1,1.0
methods for dealing,1,1,1.0
problem of learning,1,1,1.0
of learning with,1,1,1.0
class distributions as,1,1,1.0
distributions as is,1,1,1.0
as is stated,1,1,1.0
is stated in,1,1,1.0
stated in section,1,1,1.0
in section some,1,1,1.0
section some data,1,1,1.0
some data sets,1,1,1.0
sets are immune,1,1,1.0
are immune to,1,1,1.0
immune to class,1,1,1.0
to class imbalance,1,1,1.0
problem it was,1,1,1.0
imbalance but rather,1,1,1.0
class imbalance may,1,1,1.0
imbalance may yield,1,1,1.0
cause degradation though,1,1,1.0
degradation though maximum,1,1,1.0
though maximum specification,1,1,1.0
maximum specification bias,1,1,1.0
specification bias in,1,1,1.0
bias in induction,1,1,1.0
in induction processes,1,1,1.0
induction processes and,1,1,1.0
processes and dealing,1,1,1.0
and dealing with,1,1,1.0
problems of within,1,1,1.0
of within class,1,1,1.0
imbalance and between,1,1,1.0
and between class,1,1,1.0
class imbalance have,1,1,1.0
imbalance have present,1,1,1.0
have present their,1,1,1.0
present their effectiveness,1,1,1.0
their effectiveness according,1,1,1.0
effectiveness according to,1,1,1.0
according to minority,1,1,1.0
minority class more,1,1,1.0
class more effective,1,1,1.0
more effective methods,1,1,1.0
effective methods are,1,1,1.0
methods are needed,1,1,1.0
are needed current,1,1,1.0
needed current researches,1,1,1.0
current researches on,1,1,1.0
researches on small,1,1,1.0
on small disjuncts,1,1,1.0
small disjuncts are,2,1,2.0
disjuncts are ad,1,1,1.0
are ad hoc,1,1,1.0
ad hoc so,1,1,1.0
hoc so standard,1,1,1.0
so standard metrics,1,1,1.0
standard metrics for,1,1,1.0
metrics for the,1,1,1.0
for the degree,1,1,1.0
degree of small,1,1,1.0
disjuncts are deadly,1,1,1.0
are deadly in,1,1,1.0
deadly in need,1,1,1.0
in need since,1,1,1.0
need since machine,1,1,1.0
since machine learning,1,1,1.0
machine learning is,1,1,1.0
learning is an,1,1,1.0
is an science,1,1,1.0
an science and,1,1,1.0
science and the,1,1,1.0
problem and some,1,1,1.0
other related ones,1,1,1.0
related ones are,1,1,1.0
ones are of,1,1,1.0
are of nature,1,1,1.0
of nature realizing,1,1,1.0
nature realizing to,1,1,1.0
realizing to explore,1,1,1.0
to explore idiographic,1,1,1.0
explore idiographic solutions,1,1,1.0
idiographic solutions for,1,1,1.0
solutions for specific,1,1,1.0
for specific applications,1,1,1.0
specific applications is,1,1,1.0
applications is very,1,1,1.0
is very important,1,1,1.0
very important and,1,1,1.0
important and valuable,1,1,1.0
and valuable for,1,1,1.0
valuable for practitioners,1,1,1.0
for practitioners and,1,1,1.0
practitioners and a,1,1,1.0
and a better,2,2,1.0
a better data,1,1,1.0
better data understanding,1,1,1.0
data understanding and,1,1,1.0
understanding and more,1,1,1.0
and more knowledge,1,1,1.0
more knowledge on,1,1,1.0
knowledge on the,1,1,1.0
on the domain,1,1,1.0
the domain will,1,1,1.0
domain will be,1,1,1.0
be helpful in,1,1,1.0
helpful in the,1,1,1.0
in the process,2,2,1.0
the process references,1,1,1.0
process references kotsiantis,1,1,1.0
references kotsiantis kanellopoulos,1,1,1.0
kotsiantis kanellopoulos and,1,1,1.0
kanellopoulos and pintelas,1,1,1.0
and pintelas handling,1,1,1.0
pintelas handling imbalanced,1,1,1.0
imbalanced datasets a,1,1,1.0
datasets a review,1,1,1.0
a review gests,1,1,1.0
review gests international,1,1,1.0
gests international transactions,1,1,1.0
international transactions on,1,1,1.0
transactions on computer,1,1,1.0
and engineering pp,1,1,1.0
engineering pp visa,1,1,1.0
pp visa and,1,1,1.0
visa and ralescu,1,1,1.0
and ralescu issues,1,1,1.0
ralescu issues in,1,1,1.0
issues in mining,1,1,1.0
imbalanced data review,1,1,1.0
data review paper,1,1,1.0
review paper in,1,1,1.0
paper in proceedings,1,1,1.0
of the sixteen,1,1,1.0
the sixteen midwest,1,1,1.0
sixteen midwest artificial,1,1,1.0
midwest artificial intelligence,1,1,1.0
artificial intelligence and,2,1,2.0
intelligence and cognitive,1,1,1.0
and cognitive science,1,1,1.0
cognitive science conference,1,1,1.0
science conference dayton,1,1,1.0
conference dayton pp,1,1,1.0
dayton pp monard,1,1,1.0
pp monard and,1,1,1.0
monard and batista,1,1,1.0
and batista learning,1,1,1.0
batista learning with,1,1,1.0
skewed class distribution,1,1,1.0
class distribution in,2,2,1.0
distribution in advances,1,1,1.0
in advances in,2,2,1.0
advances in logic,1,1,1.0
in logic artificial,1,1,1.0
logic artificial intelligence,1,1,1.0
intelligence and robotics,1,1,1.0
and robotics sao,1,1,1.0
robotics sao paulo,1,1,1.0
sao paulo sp,1,1,1.0
paulo sp ios,1,1,1.0
sp ios press,1,1,1.0
ios press pp,1,1,1.0
press pp weiss,1,1,1.0
pp weiss mining,2,2,1.0
weiss mining with,2,2,1.0
mining with rarity,2,2,1.0
with rarity a,2,2,1.0
rarity a unifying,2,2,1.0
a unifying framework,2,2,1.0
unifying framework sigkdd,1,1,1.0
framework sigkdd explorations,1,1,1.0
sigkdd explorations pp,7,1,7.0
explorations pp maloof,1,1,1.0
pp maloof learning,2,2,1.0
maloof learning when,2,2,1.0
learning when data,2,2,1.0
when data sets,2,2,1.0
sets are imbalanced,2,2,1.0
are imbalanced and,2,2,1.0
imbalanced and when,2,2,1.0
and when costs,2,2,1.0
when costs are,2,2,1.0
costs are unequal,2,2,1.0
are unequal and,2,2,1.0
unequal and unknown,2,2,1.0
and unknown in,2,2,1.0
unknown in proceedings,1,1,1.0
of the icml,2,1,2.0
the icml workshop,2,1,2.0
icml workshop on,4,1,4.0
workshop on learning,5,1,5.0
data sets ii,9,2,4.5
sets ii pp,4,2,2.0
ii pp stone,1,1,1.0
pp stone and,1,1,1.0
stone and olshen,1,1,1.0
and olshen classification,1,1,1.0
olshen classification and,1,1,1.0
classification and regression,1,1,1.0
and regression trees,1,1,1.0
regression trees chapman,1,1,1.0
trees chapman and,1,1,1.0
chapman and press,1,1,1.0
and press japkowicz,1,1,1.0
press japkowicz class,1,1,1.0
japkowicz class imbalances,4,2,2.0
class imbalances are,2,2,1.0
imbalances are we,2,2,1.0
are we focusing,2,2,1.0
we focusing on,2,2,1.0
on the right,3,3,1.0
the right issue,2,2,1.0
right issue proceedings,1,1,1.0
issue proceedings of,1,1,1.0
the workshop learning,1,1,1.0
workshop learning with,1,1,1.0
learning with imbalanced,2,2,1.0
ii pp laurikkala,1,1,1.0
pp laurikkala improving,1,1,1.0
laurikkala improving identification,1,1,1.0
improving identification of,1,1,1.0
identification of difficult,1,1,1.0
of difficult small,1,1,1.0
difficult small classes,1,1,1.0
small classes by,1,1,1.0
classes by balancing,1,1,1.0
by balancing class,1,1,1.0
balancing class distribution,1,1,1.0
class distribution technical,1,1,1.0
distribution technical report,1,1,1.0
technical report university,1,1,1.0
report university of,1,1,1.0
university of tampere,1,1,1.0
of tampere guo,1,1,1.0
tampere guo and,1,1,1.0
guo and herna,1,1,1.0
and herna learning,1,1,1.0
herna learning from,1,1,1.0
data sets with,2,2,1.0
sets with boosting,2,2,1.0
with boosting and,2,2,1.0
boosting and data,2,2,1.0
and data generation,3,2,1.5
data generation the,3,2,1.5
generation the data,1,1,1.0
the data boosting,1,1,1.0
data boosting approach,1,1,1.0
boosting approach sigkdd,1,1,1.0
approach sigkdd explorations,1,1,1.0
explorations pp weiss,1,1,1.0
pp weiss the,1,1,1.0
weiss the effect,1,1,1.0
effect of small,1,1,1.0
small disjuncts and,1,1,1.0
disjuncts and class,1,1,1.0
and class distribution,1,1,1.0
class distribution on,3,2,1.5
distribution on decision,1,1,1.0
decision tree learning,1,1,1.0
tree learning dissertation,1,1,1.0
learning dissertation department,1,1,1.0
dissertation department of,1,1,1.0
computer science rutgers,1,1,1.0
science rutgers university,1,1,1.0
rutgers university new,1,1,1.0
university new brunswick,1,1,1.0
new brunswick new,1,1,1.0
brunswick new jersey,1,1,1.0
new jersey may,1,1,1.0
jersey may japkowicz,1,1,1.0
may japkowicz learning,1,1,1.0
japkowicz learning from,2,2,1.0
data sets a,2,2,1.0
sets a comparison,2,2,1.0
comparison of various,2,2,1.0
of various strategies,2,2,1.0
various strategies aaai,1,1,1.0
strategies aaai workshop,1,1,1.0
data sets menlo,1,1,1.0
sets menlo park,1,1,1.0
menlo park ca,3,2,1.5
park ca aaai,2,2,1.0
ca aaai press,2,2,1.0
aaai press kotsiantis,1,1,1.0
press kotsiantis and,1,1,1.0
and pintelas mixture,1,1,1.0
pintelas mixture of,1,1,1.0
mixture of expert,1,1,1.0
of expert agents,1,1,1.0
expert agents for,1,1,1.0
agents for handling,1,1,1.0
for handling imbalanced,2,2,1.0
handling imbalanced data,2,2,1.0
data sets annals,1,1,1.0
sets annals of,1,1,1.0
annals of mathematics,1,1,1.0
of mathematics computing,1,1,1.0
mathematics computing teleinformatics,1,1,1.0
computing teleinformatics vol,1,1,1.0
teleinformatics vol pp,1,1,1.0
vol pp hart,1,1,1.0
pp hart the,1,1,1.0
hart the condensed,1,1,1.0
the condensed nearest,1,1,1.0
neighbor rule ieee,1,1,1.0
rule ieee transactions,1,1,1.0
transactions on information,2,2,1.0
on information theory,2,2,1.0
information theory pp,1,1,1.0
theory pp kubat,1,1,1.0
pp kubat and,2,2,1.0
kubat and matwin,3,3,1.0
and matwin addressing,2,2,1.0
matwin addressing the,2,2,1.0
addressing the curse,2,2,1.0
the curse of,3,3,1.0
curse of imbalanced,3,3,1.0
of imbalanced training,3,3,1.0
training sets one,1,1,1.0
sets one sided,1,1,1.0
one sided selection,1,1,1.0
sided selection in,1,1,1.0
selection in proceedings,2,2,1.0
of the fourteenth,2,1,2.0
the fourteenth international,1,1,1.0
fourteenth international conference,1,1,1.0
machine learning nashville,2,2,1.0
learning nashville tennesse,1,1,1.0
nashville tennesse morgan,1,1,1.0
tennesse morgan kaufmann,1,1,1.0
morgan kaufmann pp,1,1,1.0
kaufmann pp chawla,1,1,1.0
pp chawla hall,1,1,1.0
chawla hall bowyer,1,1,1.0
hall bowyer and,1,1,1.0
bowyer and kegelmeyer,1,1,1.0
and kegelmeyer smote,2,2,1.0
oversampling technique journal,1,1,1.0
intelligence research pp,2,1,2.0
research pp han,2,1,2.0
pp han wang,3,2,1.5
han wang and,2,2,1.0
wang and mao,2,2,1.0
and mao smote,1,1,1.0
mao smote a,1,1,1.0
learning in proceedings,3,2,1.5
intelligent computing part,1,1,1.0
computing part i,1,1,1.0
part i lncs,1,1,1.0
i lncs pp,1,1,1.0
lncs pp weiss,1,1,1.0
pp weiss and,1,1,1.0
weiss and provost,2,2,1.0
and provost learning,2,2,1.0
provost learning when,2,2,1.0
learning when training,2,2,1.0
training data are,2,2,1.0
data are costly,2,2,1.0
are costly the,2,2,1.0
costly the effect,2,2,1.0
of class distribution,2,2,1.0
distribution on tree,2,2,1.0
on tree induction,2,2,1.0
tree induction journal,1,1,1.0
induction journal of,1,1,1.0
han wang wen,1,1,1.0
wang wen and,1,1,1.0
wen and wang,1,1,1.0
and wang sampling,1,1,1.0
wang sampling algorithm,1,1,1.0
sampling algorithm based,1,1,1.0
preliminary classification in,1,1,1.0
classification in imbalanced,1,1,1.0
sets learning journal,1,1,1.0
of computer allocations,1,1,1.0
computer allocations in,1,1,1.0
allocations in chinese,1,1,1.0
in chinese taeho,1,1,1.0
chinese taeho jo,1,1,1.0
taeho jo and,2,1,2.0
and japkowicz class,2,2,1.0
class imbalances versus,2,1,2.0
imbalances versus small,1,1,1.0
versus small disjuncts,1,1,1.0
small disjuncts sigkdd,1,1,1.0
disjuncts sigkdd explorations,1,1,1.0
sigkdd explorations volume,1,1,1.0
explorations volume issue,1,1,1.0
volume issue pp,1,1,1.0
issue pp batista,1,1,1.0
pp batista prati,1,1,1.0
batista prati and,1,1,1.0
prati and monard,1,1,1.0
and monard a,1,1,1.0
data sigkdd explorations,3,1,3.0
explorations pp forman,1,1,1.0
pp forman an,1,1,1.0
forman an extensive,1,1,1.0
an extensive empirical,1,1,1.0
extensive empirical study,1,1,1.0
study of feature,1,1,1.0
of feature selection,1,1,1.0
feature selection metrics,1,1,1.0
selection metrics for,1,1,1.0
metrics for text,1,1,1.0
for text classification,1,1,1.0
text classification journal,1,1,1.0
classification journal of,2,1,2.0
journal of machine,6,2,3.0
machine learning research,6,2,3.0
learning research pp,2,1,2.0
research pp mladenic,1,1,1.0
pp mladenic and,1,1,1.0
mladenic and grobelnik,1,1,1.0
and grobelnik feature,1,1,1.0
grobelnik feature selection,1,1,1.0
selection for unbalanced,1,1,1.0
for unbalanced class,1,1,1.0
unbalanced class distribution,1,1,1.0
distribution and naive,1,1,1.0
and naive bayes,1,1,1.0
naive bayes in,1,1,1.0
bayes in proceedings,1,1,1.0
of the sixteenth,2,1,2.0
the sixteenth international,2,1,2.0
sixteenth international conference,2,1,2.0
machine learning pp,6,1,6.0
learning pp zheng,1,1,1.0
pp zheng wu,1,1,1.0
zheng wu and,1,1,1.0
wu and srihari,1,1,1.0
and srihari feature,1,1,1.0
imbalanced data sigkdd,1,1,1.0
explorations pp castillo,1,1,1.0
pp castillo and,1,1,1.0
and serrano a,1,1,1.0
serrano a multistrategy,1,1,1.0
a multistrategy approach,1,1,1.0
multistrategy approach for,1,1,1.0
approach for digital,1,1,1.0
for digital text,1,1,1.0
digital text categorization,1,1,1.0
text categorization from,1,1,1.0
categorization from imbalanced,1,1,1.0
from imbalanced documents,1,1,1.0
imbalanced documents sigkdd,1,1,1.0
documents sigkdd explorations,1,1,1.0
explorations pp van,1,1,1.0
pp van der,1,1,1.0
van der putten,1,1,1.0
der putten and,1,1,1.0
putten and van,1,1,1.0
and van someren,1,1,1.0
van someren a,1,1,1.0
someren a variance,1,1,1.0
a variance analysis,1,1,1.0
variance analysis of,1,1,1.0
analysis of a,3,2,1.5
of a real,1,1,1.0
a real world,1,1,1.0
real world learning,1,1,1.0
world learning problem,1,1,1.0
learning problem the,2,2,1.0
problem the coil,1,1,1.0
the coil challenge,1,1,1.0
coil challenge machine,1,1,1.0
challenge machine learning,1,1,1.0
learning pp wilson,1,1,1.0
pp wilson asymptotic,1,1,1.0
wilson asymptotic properties,1,1,1.0
asymptotic properties of,1,1,1.0
properties of nearest,1,1,1.0
nearest neighbor rules,1,1,1.0
neighbor rules using,1,1,1.0
rules using edited,1,1,1.0
using edited data,1,1,1.0
edited data ieee,1,1,1.0
ieee trans on,1,1,1.0
trans on systems,1,1,1.0
and cybernetics vol,1,1,1.0
cybernetics vol pp,2,2,1.0
vol pp drummond,1,1,1.0
pp drummond and,1,1,1.0
and holte class,1,1,1.0
holte class imbalance,1,1,1.0
imbalance and cost,1,1,1.0
and cost sensitivity,1,1,1.0
cost sensitivity why,1,1,1.0
sensitivity why beats,1,1,1.0
why beats in,1,1,1.0
beats in icml,1,1,1.0
in icml workshop,2,1,2.0
sets ii washington,4,2,2.0
ii washington dc,2,1,2.0
washington dc barandela,1,1,1.0
dc barandela sánchez,1,1,1.0
barandela sánchez garcía,1,1,1.0
sánchez garcía and,1,1,1.0
garcía and rangel,1,1,1.0
and rangel strategies,2,2,1.0
rangel strategies for,2,2,1.0
strategies for learning,2,2,1.0
for learning in,2,2,1.0
learning in class,2,2,1.0
in class imbalance,2,2,1.0
imbalance problems pattern,1,1,1.0
problems pattern recognition,1,1,1.0
pattern recognition pp,2,1,2.0
recognition pp wu,1,1,1.0
pp wu and,3,3,1.0
wu and chang,1,1,1.0
and chang alignment,1,1,1.0
chang alignment for,1,1,1.0
alignment for imbalanced,1,1,1.0
for imbalanced dataset,1,1,1.0
imbalanced dataset learning,1,1,1.0
dataset learning in,1,1,1.0
learning in icml,1,1,1.0
washington dc veropoulos,1,1,1.0
dc veropoulos campbell,1,1,1.0
veropoulos campbell and,1,1,1.0
campbell and cristianini,1,1,1.0
and cristianini controlling,1,1,1.0
cristianini controlling the,1,1,1.0
controlling the sensitivity,1,1,1.0
the sensitivity of,1,1,1.0
sensitivity of support,1,1,1.0
of support vector,2,2,1.0
vector machines in,1,1,1.0
machines in proceedings,2,2,1.0
conference on ai,1,1,1.0
on ai pp,1,1,1.0
ai pp huang,1,1,1.0
pp huang yang,1,1,1.0
huang yang king,1,1,1.0
yang king and,1,1,1.0
king and lyu,1,1,1.0
and lyu learning,1,1,1.0
lyu learning classifiers,1,1,1.0
learning classifiers from,1,1,1.0
classifiers from imbalanced,1,1,1.0
imbalanced data based,1,1,1.0
data based on,1,1,1.0
based on biased,1,1,1.0
on biased minimax,1,1,1.0
probability machine in,1,1,1.0
machine in proceedings,1,1,1.0
of the ieee,2,2,1.0
the ieee computer,1,1,1.0
ieee computer society,1,1,1.0
computer society conference,1,1,1.0
society conference on,1,1,1.0
conference on computer,1,1,1.0
on computer vision,1,1,1.0
vision and pattern,1,1,1.0
and pattern recognition,1,1,1.0
pattern recognition domingos,1,1,1.0
recognition domingos metacost,1,1,1.0
of the fifth,2,2,1.0
the fifth international,1,1,1.0
fifth international conference,1,1,1.0
data mining acm,1,1,1.0
mining acm press,1,1,1.0
acm press pp,1,1,1.0
press pp w,1,1,1.0
pp w f,1,1,1.0
w f a,1,1,1.0
f a n,1,1,1.0
a n s,1,1,1.0
n s j,1,1,1.0
s j s,1,1,1.0
j s t,1,1,1.0
s t o,3,2,1.5
t o l,1,1,1.0
o l f,1,1,1.0
l f o,1,1,1.0
f o j,1,1,1.0
o j z,1,1,1.0
j z h,1,1,1.0
z h a,1,1,1.0
h a n,2,1,2.0
a n g,1,1,1.0
n g a,1,1,1.0
g a n,1,1,1.0
a n d,5,2,2.5
n d p,1,1,1.0
d p k,1,1,1.0
p k c,1,1,1.0
k c h,1,1,1.0
c h a,1,1,1.0
a n adacost,1,1,1.0
n adacost misclassification,1,1,1.0
adacost misclassification boosting,1,1,1.0
misclassification boosting in,1,1,1.0
boosting in proceedings,2,1,2.0
learning pp japkowicz,2,1,2.0
pp japkowicz supervised,1,1,1.0
japkowicz supervised versus,1,1,1.0
supervised versus unsupervised,1,1,1.0
versus unsupervised binary,1,1,1.0
unsupervised binary learning,1,1,1.0
binary learning by,1,1,1.0
learning by feed,1,1,1.0
by feed forward,1,1,1.0
feed forward neural,1,1,1.0
forward neural networks,1,1,1.0
neural networks machine,1,1,1.0
networks machine learning,2,2,1.0
learning pp scholkopf,1,1,1.0
pp scholkopf platt,1,1,1.0
scholkopf platt smola,1,1,1.0
platt smola and,1,1,1.0
smola and williamson,1,1,1.0
and williamson estimating,1,1,1.0
williamson estimating the,1,1,1.0
estimating the support,1,1,1.0
the support of,1,1,1.0
support of a,1,1,1.0
of a dimensional,1,1,1.0
a dimensional distribution,1,1,1.0
dimensional distribution neural,1,1,1.0
distribution neural computation,1,1,1.0
neural computation pp,1,1,1.0
computation pp tax,1,1,1.0
pp tax classification,1,1,1.0
tax classification dissertation,1,1,1.0
classification dissertation delft,1,1,1.0
dissertation delft university,1,1,1.0
delft university of,1,1,1.0
university of technology,1,1,1.0
of technology manevitz,1,1,1.0
technology manevitz and,1,1,1.0
manevitz and yousef,1,1,1.0
and yousef svms,1,1,1.0
yousef svms for,1,1,1.0
svms for document,1,1,1.0
for document classification,1,1,1.0
document classification journal,1,1,1.0
research pp riddle,1,1,1.0
pp riddle segal,1,1,1.0
riddle segal and,1,1,1.0
segal and etzioni,1,1,1.0
and etzioni representation,1,1,1.0
etzioni representation design,1,1,1.0
representation design and,1,1,1.0
design and induction,1,1,1.0
and induction in,1,1,1.0
induction in a,1,1,1.0
in a boeing,1,1,1.0
a boeing manufacturing,1,1,1.0
boeing manufacturing design,1,1,1.0
manufacturing design applied,1,1,1.0
design applied artificial,1,1,1.0
applied artificial intelligence,1,1,1.0
intelligence pp kubat,1,1,1.0
pp kubat holte,1,1,1.0
kubat holte and,2,2,1.0
holte and matwin,2,2,1.0
and matwin learning,1,1,1.0
matwin learning when,1,1,1.0
learning when negative,1,1,1.0
when negative examples,1,1,1.0
negative examples abound,1,1,1.0
examples abound in,1,1,1.0
abound in proceedings,1,1,1.0
of the ninth,1,1,1.0
the ninth european,1,1,1.0
ninth european conference,1,1,1.0
machine learning lnai,1,1,1.0
learning lnai springer,1,1,1.0
lnai springer pp,1,1,1.0
springer pp cohen,1,1,1.0
pp cohen fast,1,1,1.0
cohen fast effective,1,1,1.0
fast effective rule,1,1,1.0
effective rule induction,1,1,1.0
rule induction in,1,1,1.0
induction in proceedings,1,1,1.0
of the twelfth,1,1,1.0
the twelfth international,1,1,1.0
twelfth international conference,1,1,1.0
learning pp tomek,1,1,1.0
pp tomek two,1,1,1.0
tomek two modifications,1,1,1.0
two modifications of,1,1,1.0
modifications of cnn,1,1,1.0
of cnn ieee,1,1,1.0
cnn ieee transactions,1,1,1.0
man and communications,1,1,1.0
and communications pp,1,1,1.0
communications pp raskutti,1,1,1.0
pp raskutti and,1,1,1.0
and kowalczyk extreme,1,1,1.0
kowalczyk extreme rebalancing,1,1,1.0
extreme rebalancing svms,1,1,1.0
rebalancing svms a,1,1,1.0
svms a case,1,1,1.0
a case study,2,1,2.0
case study sigkdd,1,1,1.0
study sigkdd explorations,1,1,1.0
explorations pp freund,1,1,1.0
pp freund and,1,1,1.0
and schapire a,2,2,1.0
system sciences pp,1,1,1.0
sciences pp m,1,1,1.0
pp m v,1,1,1.0
m v j,1,1,1.0
v j o,1,1,1.0
j o s,1,1,1.0
o s h,1,1,1.0
s h i,1,1,1.0
h i v,1,1,1.0
i v k,1,1,1.0
v k u,1,1,1.0
k u m,1,1,1.0
u m a,1,1,1.0
m a r,1,1,1.0
a r a,1,1,1.0
r a n,1,1,1.0
n d r,1,1,1.0
d r c,1,1,1.0
r c a,1,1,1.0
c a g,1,1,1.0
a g a,1,1,1.0
g a r,1,1,1.0
a r w,1,1,1.0
r w a,1,1,1.0
w a l,1,1,1.0
a l evaluating,1,1,1.0
l evaluating boosting,1,1,1.0
evaluating boosting algorithms,1,1,1.0
boosting algorithms to,1,1,1.0
algorithms to classify,1,1,1.0
to classify rare,1,1,1.0
classify rare cases,1,1,1.0
rare cases comparison,1,1,1.0
cases comparison and,1,1,1.0
comparison and improvements,1,1,1.0
and improvements in,1,1,1.0
improvements in proceedings,1,1,1.0
of the first,1,1,1.0
the first ieee,1,1,1.0
first ieee international,1,1,1.0
ieee international conference,2,1,2.0
data mining pp,3,2,1.5
mining pp chawla,1,1,1.0
pp chawla lazarevic,1,1,1.0
lazarevic hall and,2,2,1.0
hall and bowyer,2,2,1.0
and bowyer smoteboost,1,1,1.0
in boosting in,2,2,1.0
of the seventh,1,1,1.0
the seventh european,1,1,1.0
seventh european conference,1,1,1.0
conference on principles,1,1,1.0
on principles and,1,1,1.0
principles and practice,1,1,1.0
and practice of,1,1,1.0
practice of knowledge,1,1,1.0
of knowledge discovery,1,1,1.0
in databases dubrovnik,1,1,1.0
databases dubrovnik croatia,1,1,1.0
dubrovnik croatia pp,1,1,1.0
croatia pp estabrooks,1,1,1.0
pp estabrooks taeho,1,1,1.0
estabrooks taeho jo,1,1,1.0
and japkowicz a,2,1,2.0
japkowicz a multiple,1,1,1.0
a multiple resampling,1,1,1.0
multiple resampling method,1,1,1.0
resampling method for,1,1,1.0
method for learning,1,1,1.0
data sets computational,1,1,1.0
sets computational intelligence,1,1,1.0
computational intelligence pp,1,1,1.0
intelligence pp chan,1,1,1.0
pp chan and,1,1,1.0
and stolfo toward,1,1,1.0
stolfo toward scalable,1,1,1.0
toward scalable learning,1,1,1.0
scalable learning with,1,1,1.0
with class and,1,1,1.0
class and cost,2,2,1.0
and cost distributions,2,2,1.0
cost distributions a,1,1,1.0
distributions a case,1,1,1.0
case study in,1,1,1.0
study in credit,1,1,1.0
in credit card,1,1,1.0
credit card fraud,1,1,1.0
card fraud detection,1,1,1.0
fraud detection in,1,1,1.0
detection in proceedings,1,1,1.0
mining pp yan,1,1,1.0
pp yan liu,1,1,1.0
yan liu jin,1,1,1.0
liu jin and,1,1,1.0
jin and hauptmann,1,1,1.0
and hauptmann on,1,1,1.0
hauptmann on predicting,1,1,1.0
on predicting rare,1,1,1.0
predicting rare classes,1,1,1.0
rare classes with,1,1,1.0
classes with svm,1,1,1.0
with svm ensembles,2,2,1.0
svm ensembles in,2,2,1.0
ensembles in scene,1,1,1.0
in scene classification,1,1,1.0
scene classification in,1,1,1.0
classification in ieee,1,1,1.0
conference on acoustics,1,1,1.0
on acoustics speech,1,1,1.0
acoustics speech and,1,1,1.0
speech and signal,1,1,1.0
and signal processing,1,1,1.0
signal processing phua,1,1,1.0
processing phua and,1,1,1.0
phua and alahakoon,1,1,1.0
and alahakoon minority,1,1,1.0
alahakoon minority report,1,1,1.0
minority report in,1,1,1.0
report in fraud,1,1,1.0
in fraud detection,2,2,1.0
fraud detection classification,1,1,1.0
detection classification of,1,1,1.0
classification of skewed,1,1,1.0
of skewed data,1,1,1.0
skewed data sigkdd,1,1,1.0
explorations pp estabrooks,1,1,1.0
pp estabrooks and,1,1,1.0
estabrooks and japkowicz,1,1,1.0
japkowicz a framework,1,1,1.0
a framework for,1,1,1.0
framework for learning,1,1,1.0
learning from unbalanced,1,1,1.0
from unbalanced data,1,1,1.0
sets in proceedings,1,1,1.0
of the intelligent,1,1,1.0
the intelligent data,1,1,1.0
intelligent data analysis,2,2,1.0
data analysis conference,1,1,1.0
analysis conference pp,1,1,1.0
conference pp bradley,1,1,1.0
pp bradley the,1,1,1.0
bradley the use,1,1,1.0
roc curve in,1,1,1.0
curve in the,1,1,1.0
in the evaluation,1,1,1.0
the evaluation of,1,1,1.0
evaluation of machine,1,1,1.0
learning algorithms pattern,1,1,1.0
algorithms pattern recognition,1,1,1.0
recognition pp provost,1,1,1.0
pp provost and,3,2,1.5
provost and fawcett,3,2,1.5
and fawcett robust,2,2,1.0
fawcett robust classification,1,1,1.0
robust classification for,1,1,1.0
classification for imprecise,1,1,1.0
for imprecise environments,1,1,1.0
imprecise environments machine,1,1,1.0
environments machine learning,1,1,1.0
pp japkowicz in,1,1,1.0
japkowicz in the,1,1,1.0
in the presence,1,1,1.0
the presence of,1,1,1.0
presence of and,1,1,1.0
of and imbalances,1,1,1.0
and imbalances in,1,1,1.0
imbalances in proceedings,1,1,1.0
the fourteenth conference,1,1,1.0
fourteenth conference of,1,1,1.0
conference of the,1,1,1.0
of the canadian,1,1,1.0
the canadian society,1,1,1.0
canadian society for,1,1,1.0
society for computational,1,1,1.0
for computational studies,1,1,1.0
computational studies of,1,1,1.0
studies of intelligence,1,1,1.0
of intelligence pp,1,1,1.0
prati batista and,2,2,1.0
batista and monard,2,2,1.0
and monard class,2,2,1.0
monard class imbalances,2,2,1.0
imbalances versus class,1,1,1.0
versus class overlapping,1,1,1.0
class overlapping an,2,2,1.0
overlapping an analysis,2,2,1.0
of a learning,2,2,1.0
a learning system,2,2,1.0
learning system behavior,2,2,1.0
system behavior in,2,2,1.0
behavior in micai,1,1,1.0
in micai lnai,1,1,1.0
micai lnai pp,1,1,1.0
lnai pp kolez,1,1,1.0
pp kolez chowdhury,1,1,1.0
kolez chowdhury and,1,1,1.0
chowdhury and alspector,1,1,1.0
and alspector data,1,1,1.0
alspector data duplication,1,1,1.0
data duplication an,1,1,1.0
duplication an imbalance,1,1,1.0
an imbalance problem,1,1,1.0
sets ii view,1,1,1.0
ii view publication,1,1,1.0
view publication stats,1,1,1.0
the feature space,41,2,20.5
feature space p,1,1,1.0
space p p,1,1,1.0
p p guti,2,1,2.0
p guti senior,1,1,1.0
guti senior member,1,1,1.0
senior member ieee,2,1,2.0
member ieee p,1,1,1.0
ieee p ti,1,1,1.0
p ti ˇno,2,1,2.0
ti ˇno and,1,1,1.0
ˇno and c,1,1,1.0
and c herv,1,1,1.0
c herv senior,1,1,1.0
herv senior member,1,1,1.0
member ieee imbalanced,1,1,1.0
ieee imbalanced nature,1,1,1.0
imbalanced nature of,3,1,3.0
nature of some,1,1,1.0
of some data,1,1,1.0
some data is,1,1,1.0
data is one,1,1,1.0
the current challenges,1,1,1.0
current challenges for,1,1,1.0
challenges for machine,1,1,1.0
machine learning researchers,2,1,2.0
learning researchers one,1,1,1.0
researchers one common,1,1,1.0
one common approach,1,1,1.0
common approach the,1,1,1.0
approach the minority,1,1,1.0
class through convex,1,1,1.0
through convex combination,1,1,1.0
convex combination of,11,1,11.0
combination of its,1,1,1.0
of its patterns,1,1,1.0
its patterns w,1,1,1.0
patterns w e,1,1,1.0
w e explore,2,1,2.0
e explore the,2,1,2.0
explore the general,1,1,1.0
the general idea,1,1,1.0
general idea of,1,1,1.0
idea of synthetic,1,1,1.0
feature space induced,3,1,3.0
space induced by,3,1,3.0
induced by a,3,1,3.0
by a kernel,3,1,3.0
a kernel function,6,1,6.0
kernel function as,1,1,1.0
function as opposed,1,1,1.0
opposed to input,1,1,1.0
to input space,1,1,1.0
input space if,1,1,1.0
space if the,1,1,1.0
if the kernel,3,1,3.0
kernel function matches,2,1,2.0
function matches the,2,1,2.0
matches the underlying,3,1,3.0
the underlying problem,3,1,3.0
underlying problem the,1,1,1.0
problem the classes,1,1,1.0
the classes will,1,1,1.0
classes will be,1,1,1.0
will be linearly,1,1,1.0
be linearly separable,1,1,1.0
linearly separable and,1,1,1.0
separable and synthetically,1,1,1.0
and synthetically generated,1,1,1.0
synthetically generated patterns,2,1,2.0
generated patterns will,1,1,1.0
patterns will lie,1,1,1.0
will lie on,2,1,2.0
lie on the,3,1,3.0
minority class region,1,1,1.0
class region since,1,1,1.0
region since the,1,1,1.0
since the feature,2,1,2.0
feature space is,7,1,7.0
space is not,3,1,3.0
not directly accessible,2,1,2.0
directly accessible we,1,1,1.0
accessible we use,1,1,1.0
we use the,3,2,1.5
use the empirical,1,1,1.0
the empirical feature,13,1,13.0
empirical feature space,21,1,21.0
feature space a,2,1,2.0
space a euclidean,1,1,1.0
a euclidean space,2,1,2.0
euclidean space isomorphic,1,1,1.0
space isomorphic to,1,1,1.0
isomorphic to the,2,1,2.0
feature space for,1,1,1.0
space for purposes,1,1,1.0
for purposes the,1,1,1.0
purposes the proposed,1,1,1.0
the proposed method,3,2,1.5
proposed method is,1,1,1.0
method is framed,1,1,1.0
is framed in,1,1,1.0
framed in the,1,1,1.0
context of support,1,1,1.0
vector machines where,1,1,1.0
machines where imbalanced,1,1,1.0
where imbalanced datasets,1,1,1.0
imbalanced datasets can,1,1,1.0
datasets can pose,1,1,1.0
can pose a,1,1,1.0
pose a serious,2,1,2.0
a serious hindrance,2,1,2.0
serious hindrance the,1,1,1.0
hindrance the idea,1,1,1.0
idea is investigated,1,1,1.0
is investigated in,1,1,1.0
investigated in three,1,1,1.0
in three scenarios,1,1,1.0
three scenarios sampling,1,1,1.0
scenarios sampling in,1,1,1.0
sampling in the,5,1,5.0
in the full,4,1,4.0
the full and,3,1,3.0
full and empirical,2,1,2.0
and empirical feature,3,1,3.0
empirical feature spaces,3,1,3.0
feature spaces a,1,1,1.0
spaces a kernel,1,1,1.0
a kernel learning,1,1,1.0
kernel learning technique,1,1,1.0
learning technique maximising,1,1,1.0
technique maximising the,1,1,1.0
maximising the data,1,1,1.0
the data class,1,1,1.0
data class separation,1,1,1.0
class separation to,1,1,1.0
separation to study,1,1,1.0
to study the,2,1,2.0
study the inﬂuence,1,1,1.0
the inﬂuence of,3,2,1.5
inﬂuence of the,3,1,3.0
of the feature,6,2,3.0
feature space structure,3,1,3.0
space structure implicitly,1,1,1.0
structure implicitly deﬁned,1,1,1.0
implicitly deﬁned by,1,1,1.0
deﬁned by the,1,1,1.0
by the kernel,4,1,4.0
kernel function a,1,1,1.0
function a uniﬁed,1,1,1.0
a uniﬁed framework,2,1,2.0
uniﬁed framework for,2,1,2.0
framework for preferential,2,1,2.0
for preferential that,1,1,1.0
preferential that spans,1,1,1.0
that spans some,1,1,1.0
spans some of,1,1,1.0
of the previous,1,1,1.0
the previous approaches,1,1,1.0
previous approaches in,1,1,1.0
approaches in the,4,1,4.0
the literature w,1,1,1.0
literature w e,1,1,1.0
w e support,1,1,1.0
e support our,1,1,1.0
support our investigation,1,1,1.0
our investigation with,1,1,1.0
investigation with extensive,1,1,1.0
with extensive experiments,1,1,1.0
extensive experiments over,1,1,1.0
experiments over imbalanced,2,1,2.0
over imbalanced datasets,3,1,3.0
imbalanced datasets index,1,1,1.0
datasets index t,1,1,1.0
index t imbalanced,1,1,1.0
t imbalanced classiﬁcation,1,1,1.0
imbalanced classiﬁcation kernel,1,1,1.0
classiﬁcation kernel methods,1,1,1.0
kernel methods empirical,1,1,1.0
methods empirical feature,1,1,1.0
feature space support,1,1,1.0
space support vector,1,1,1.0
vector machines i,1,1,1.0
machines i n,1,1,1.0
i n t,2,1,2.0
n t ro,1,1,1.0
t ro d,1,1,1.0
ro d u,1,1,1.0
d u c,1,1,1.0
u c t,1,1,1.0
c t i,1,1,1.0
t i o,1,1,1.0
i o n,3,1,3.0
o n classiﬁcation,1,1,1.0
n classiﬁcation methods,1,1,1.0
classiﬁcation methods often,1,1,1.0
methods often conveniently,1,1,1.0
often conveniently assume,1,1,1.0
conveniently assume that,1,1,1.0
that the prior,1,1,1.0
the prior class,1,1,1.0
prior class probability,1,1,1.0
class probability distribution,1,1,1.0
probability distribution is,2,2,1.0
distribution is of,1,1,1.0
is of high,1,1,1.0
of high entropy,1,1,1.0
high entropy however,1,1,1.0
entropy however this,1,1,1.0
however this is,2,1,2.0
this is not,4,1,4.0
not the case,2,2,1.0
the case in,1,1,1.0
case in many,1,1,1.0
in many applications,2,2,1.0
many applications from,1,1,1.0
applications from areas,1,1,1.0
from areas such,1,1,1.0
areas such as,1,1,1.0
medical diagnosis information,1,1,1.0
diagnosis information retrieval,1,1,1.0
information retrieval fraud,1,1,1.0
retrieval fraud detection,1,1,1.0
fraud detection etc,1,1,1.0
detection etc the,1,1,1.0
etc the classiﬁcation,1,1,1.0
the classiﬁcation paradigm,1,1,1.0
classiﬁcation paradigm when,1,1,1.0
paradigm when one,1,1,1.0
when one or,1,1,1.0
one or several,1,1,1.0
or several classes,1,1,1.0
several classes have,1,1,1.0
classes have a,1,1,1.0
have a much,1,1,1.0
a much lower,1,1,1.0
much lower prior,1,1,1.0
lower prior probability,1,1,1.0
prior probability in,1,1,1.0
probability in the,1,1,1.0
set is known,1,1,1.0
known as imbalanced,1,1,1.0
as imbalanced classiﬁcation,1,1,1.0
imbalanced classiﬁcation and,1,1,1.0
classiﬁcation and it,1,1,1.0
and it poses,1,1,1.0
it poses a,1,1,1.0
poses a difﬁcult,1,1,1.0
a difﬁcult challenge,1,1,1.0
difﬁcult challenge for,1,1,1.0
challenge for machine,1,1,1.0
learning researchers because,1,1,1.0
researchers because of,1,1,1.0
because of that,1,1,1.0
of that imbalanced,1,1,1.0
that imbalanced classiﬁcation,1,1,1.0
imbalanced classiﬁcation is,1,1,1.0
classiﬁcation is currently,1,1,1.0
is currently receiving,1,1,1.0
currently receiving a,1,1,1.0
receiving a lot,1,1,1.0
a lot of,1,1,1.0
lot of attention,1,1,1.0
of attention from,1,1,1.0
attention from the,1,1,1.0
from the pattern,1,1,1.0
the pattern recognition,1,1,1.0
machine learning communities,1,1,1.0
learning communities often,1,1,1.0
communities often the,1,1,1.0
often the minority,1,1,1.0
minority class happens,1,1,1.0
class happens to,1,1,1.0
happens to be,1,1,1.0
to be more,4,2,2.0
be more important,1,1,1.0
the majority one,2,1,2.0
majority one but,1,1,1.0
one but it,1,1,1.0
but it may,1,1,1.0
it may also,2,2,1.0
also be much,1,1,1.0
be much more,1,1,1.0
much more difﬁcult,1,1,1.0
more difﬁcult to,2,2,1.0
difﬁcult to model,1,1,1.0
to model due,1,1,1.0
model due to,1,1,1.0
to the low,1,1,1.0
the low number,1,1,1.0
low number of,1,1,1.0
number of available,1,1,1.0
of available samples,1,1,1.0
available samples since,1,1,1.0
samples since most,1,1,1.0
since most traditional,1,1,1.0
most traditional learning,1,1,1.0
traditional learning systems,1,1,1.0
learning systems have,1,1,1.0
systems have been,1,1,1.0
have been designed,1,1,1.0
been designed to,1,1,1.0
designed to work,1,1,1.0
to work on,1,1,1.0
work on balanced,1,1,1.0
on balanced data,1,1,1.0
balanced data they,1,1,1.0
data they will,1,1,1.0
they will usually,1,1,1.0
will usually be,1,1,1.0
usually be focused,1,1,1.0
be focused on,1,1,1.0
focused on improving,1,1,1.0
on improving overall,2,1,2.0
improving overall performance,2,1,2.0
overall performance and,1,1,1.0
performance and be,1,1,1.0
and be biased,1,1,1.0
be biased towards,1,1,1.0
biased towards the,1,1,1.0
towards the majority,1,1,1.0
majority class consequently,1,1,1.0
class consequently harming,1,1,1.0
consequently harming the,1,1,1.0
harming the minority,1,1,1.0
minority one although,1,1,1.0
one although from,1,1,1.0
although from a,1,1,1.0
from a formal,1,1,1.0
a formal deﬁnition,1,1,1.0
formal deﬁnition an,1,1,1.0
deﬁnition an imbalanced,1,1,1.0
an imbalanced the,1,1,1.0
imbalanced the work,1,1,1.0
the work of,2,1,2.0
work of p,2,1,2.0
of p p,1,1,1.0
p guti and,2,1,2.0
guti and herv,2,1,2.0
and herv has,1,1,1.0
herv has been,1,1,1.0
has been subsidized,1,1,1.0
been subsidized by,1,1,1.0
subsidized by the,1,1,1.0
by the project,1,1,1.0
the project of,2,1,2.0
project of the,2,1,2.0
of the spanish,1,1,1.0
the spanish ministerial,1,1,1.0
spanish ministerial commission,1,1,1.0
ministerial commission of,1,1,1.0
commission of science,1,1,1.0
of science and,3,2,1.5
science and t,1,1,1.0
and t echnology,1,1,1.0
t echnology micyt,1,1,1.0
echnology micyt feder,1,1,1.0
micyt feder funds,1,1,1.0
feder funds and,1,1,1.0
funds and the,1,1,1.0
and the project,1,1,1.0
of the junta,1,1,1.0
the junta de,1,1,1.0
junta de andaluc,1,1,1.0
de andaluc spain,1,1,1.0
andaluc spain the,1,1,1.0
spain the work,1,1,1.0
of p tino,1,1,1.0
p tino has,1,1,1.0
tino has been,1,1,1.0
has been supported,1,1,1.0
been supported by,1,1,1.0
supported by epsrc,1,1,1.0
by epsrc grant,1,1,1.0
epsrc grant p,1,1,1.0
grant p ortiz,1,1,1.0
p ortiz p,1,1,1.0
ortiz p guti,1,1,1.0
and herv are,1,1,1.0
herv are with,1,1,1.0
are with the,2,2,1.0
with the department,3,2,1.5
science and numerical,1,1,1.0
and numerical analysis,1,1,1.0
numerical analysis of,1,1,1.0
of the university,2,1,2.0
the university of,2,1,2.0
university of c,1,1,1.0
of c spain,1,1,1.0
c spain email,1,1,1.0
spain email pagutierrez,1,1,1.0
email pagutierrez chervas,1,1,1.0
pagutierrez chervas p,1,1,1.0
chervas p ti,1,1,1.0
ti ˇno is,1,1,1.0
ˇno is with,1,1,1.0
is with the,2,2,1.0
with the school,1,1,1.0
the school of,1,1,1.0
computer science of,1,1,1.0
science of the,1,1,1.0
university of birmingham,1,1,1.0
of birmingham birmingham,1,1,1.0
birmingham birmingham united,1,1,1.0
birmingham united kingdom,1,1,1.0
united kingdom email,1,1,1.0
kingdom email dataset,1,1,1.0
email dataset is,1,1,1.0
dataset is any,1,1,1.0
is any set,1,1,1.0
any set of,1,1,1.0
set of labelled,1,1,1.0
of labelled data,1,1,1.0
labelled data exhibiting,1,1,1.0
data exhibiting an,1,1,1.0
exhibiting an unequal,1,1,1.0
an unequal distribution,2,2,1.0
unequal distribution between,2,2,1.0
distribution between classes,1,1,1.0
between classes it,1,1,1.0
classes it has,1,1,1.0
shown that this,3,1,3.0
that this is,2,1,2.0
the only factor,1,1,1.0
only factor involved,1,1,1.0
factor involved hindering,1,1,1.0
involved hindering the,1,1,1.0
hindering the learning,1,1,1.0
the learning in,1,1,1.0
learning in this,1,1,1.0
in this context,1,1,1.0
this context the,1,1,1.0
context the complexity,1,1,1.0
the complexity of,1,1,1.0
complexity of the,1,1,1.0
the data existence,1,1,1.0
data existence of,1,1,1.0
existence of noisy,1,1,1.0
of noisy and,1,1,1.0
noisy and samples,1,1,1.0
and samples or,1,1,1.0
samples or class,1,1,1.0
or class overlapping,1,1,1.0
class overlapping or,1,1,1.0
overlapping or the,1,1,1.0
or the size,1,1,1.0
training set data,1,1,1.0
set data or,1,1,1.0
data or small,1,1,1.0
or small sample,1,1,1.0
small sample size,1,1,1.0
sample size can,1,1,1.0
size can also,1,1,1.0
also be part,1,1,1.0
be part of,1,1,1.0
of the nature,1,1,1.0
problem the approaches,1,1,1.0
the approaches developed,1,1,1.0
approaches developed over,1,1,1.0
developed over the,1,1,1.0
over the years,1,1,1.0
the years for,1,1,1.0
years for tackling,1,1,1.0
for tackling the,1,1,1.0
tackling the class,1,1,1.0
imbalance problem can,1,1,1.0
problem can be,1,1,1.0
can be categorised,1,1,1.0
be categorised in,1,1,1.0
categorised in two,1,1,1.0
in two groups,1,1,1.0
two groups data,1,1,1.0
groups data approach,1,1,1.0
data approach based,1,1,1.0
based on sampling,1,1,1.0
on sampling methods,1,1,1.0
sampling methods including,1,1,1.0
methods including minority,1,1,1.0
including minority groups,1,1,1.0
minority groups groups,1,1,1.0
groups groups of,1,1,1.0
groups of interesting,1,1,1.0
of interesting rare,1,1,1.0
interesting rare examples,1,1,1.0
rare examples or,1,1,1.0
examples or majority,1,1,1.0
or majority groups,1,1,1.0
majority groups groups,1,1,1.0
groups groups with,1,1,1.0
groups with large,1,1,1.0
with large example,1,1,1.0
large example sizes,1,1,1.0
example sizes the,1,1,1.0
sizes the combination,1,1,1.0
the combination of,2,1,2.0
of both being,1,1,1.0
both being also,1,1,1.0
being also very,1,1,1.0
also very popular,1,1,1.0
very popular algorithm,1,1,1.0
popular algorithm approach,1,1,1.0
algorithm approach forces,1,1,1.0
approach forces the,1,1,1.0
forces the classiﬁer,1,1,1.0
the classiﬁer to,1,1,1.0
classiﬁer to pay,1,1,1.0
to pay more,1,1,1.0
pay more attention,1,1,1.0
class by learning,1,1,1.0
by learning the,1,1,1.0
learning the analysis,1,1,1.0
the analysis made,1,1,1.0
analysis made in,1,1,1.0
made in this,2,1,2.0
this paper is,1,1,1.0
paper is contextualised,1,1,1.0
is contextualised on,1,1,1.0
contextualised on data,1,1,1.0
on data approaches,1,1,1.0
data approaches thus,1,1,1.0
approaches thus a,1,1,1.0
thus a brief,1,1,1.0
a brief discussion,1,1,1.0
brief discussion on,2,2,1.0
discussion on these,1,1,1.0
on these techniques,1,1,1.0
these techniques is,1,1,1.0
techniques is now,1,1,1.0
is now given,1,1,1.0
now given for,1,1,1.0
given for a,1,1,1.0
for a detailed,1,1,1.0
a detailed review,1,1,1.0
detailed review of,1,1,1.0
review of see,1,1,1.0
of see roughly,1,1,1.0
see roughly speaking,1,1,1.0
roughly speaking it,1,1,1.0
speaking it can,1,1,1.0
can be said,5,1,5.0
be said that,3,1,3.0
said that and,1,1,1.0
that and sampling,1,1,1.0
and sampling are,1,1,1.0
sampling are opposite,1,1,1.0
are opposite and,1,1,1.0
opposite and equivalent,1,1,1.0
and equivalent since,1,1,1.0
equivalent since they,1,1,1.0
they are aimed,1,1,1.0
are aimed at,1,1,1.0
aimed at the,1,1,1.0
at the same,3,1,3.0
the same purpose,1,1,1.0
same purpose balance,1,1,1.0
purpose balance the,1,1,1.0
balance the class,1,1,1.0
class distribution but,1,1,1.0
distribution but using,1,1,1.0
but using different,1,1,1.0
using different approaches,1,1,1.0
different approaches formally,1,1,1.0
approaches formally concerns,1,1,1.0
formally concerns to,1,1,1.0
concerns to the,1,1,1.0
to the process,2,1,2.0
the process of,2,1,2.0
process of sampling,1,1,1.0
sampling a distribution,1,1,1.0
a distribution with,1,1,1.0
distribution with a,1,1,1.0
with a signiﬁcantly,1,1,1.0
a signiﬁcantly higher,1,1,1.0
signiﬁcantly higher frequency,1,1,1.0
higher frequency than,1,1,1.0
frequency than the,1,1,1.0
than the given,1,1,1.0
the given one,1,1,1.0
given one and,1,1,1.0
one and to,1,1,1.0
process of reducing,1,1,1.0
of reducing the,1,1,1.0
reducing the frequency,1,1,1.0
frequency of the,1,1,1.0
class in both,1,1,1.0
in both cases,1,1,1.0
both cases the,1,1,1.0
cases the methodologies,1,1,1.0
the methodologies impose,1,1,1.0
methodologies impose a,1,1,1.0
impose a balance,1,1,1.0
a balance in,1,1,1.0
balance in the,1,1,1.0
in the class,1,1,1.0
distribution in order,1,1,1.0
order to avoid,1,1,1.0
to avoid aliasing,1,1,1.0
avoid aliasing and,1,1,1.0
aliasing and focus,1,1,1.0
and focus on,1,1,1.0
on the classiﬁcation,1,1,1.0
the classiﬁcation of,1,1,1.0
classiﬁcation of minority,1,1,1.0
of minority classes,1,1,1.0
minority classes although,1,1,1.0
classes although both,1,1,1.0
although both sampling,1,1,1.0
both sampling and,1,1,1.0
sampling and approaches,1,1,1.0
and approaches have,1,1,1.0
approaches have been,2,1,2.0
have been shown,1,1,1.0
shown to improve,1,1,1.0
to improve classiﬁer,1,1,1.0
improve classiﬁer performance,1,1,1.0
classiﬁer performance over,1,1,1.0
performance over imbalanced,1,1,1.0
imbalanced datasets different,1,1,1.0
datasets different studies,1,1,1.0
different studies suggest,1,1,1.0
studies suggest that,3,1,3.0
suggest that is,1,1,1.0
that is more,1,1,1.0
is more useful,1,1,1.0
more useful than,1,1,1.0
useful than specially,1,1,1.0
than specially for,1,1,1.0
specially for highly,1,1,1.0
for highly imbalanced,3,1,3.0
highly imbalanced and,1,1,1.0
imbalanced and complex,1,1,1.0
and complex datasets,1,1,1.0
complex datasets recall,1,1,1.0
datasets recall that,1,1,1.0
recall that could,1,1,1.0
that could entail,1,1,1.0
could entail a,1,1,1.0
entail a loss,1,1,1.0
a loss of,1,1,1.0
loss of potentially,1,1,1.0
of potentially meaningful,1,1,1.0
potentially meaningful information,1,1,1.0
meaningful information of,1,1,1.0
the dataset concerning,1,1,1.0
dataset concerning the,1,1,1.0
concerning the ﬁrst,1,1,1.0
the ﬁrst idea,2,1,2.0
ﬁrst idea is,2,1,2.0
is to perform,1,1,1.0
to perform a,1,1,1.0
perform a random,1,1,1.0
a random replication,1,1,1.0
of minority data,2,2,1.0
minority data but,1,1,1.0
data but this,1,1,1.0
but this often,1,1,1.0
this often leads,1,1,1.0
often leads to,1,1,1.0
leads to another,1,1,1.0
to another common,1,1,1.0
another common approach,1,1,1.0
common approach is,1,1,1.0
is to generate,1,1,1.0
to generate new,2,1,2.0
generate new synthetic,2,1,2.0
new synthetic patterns,2,1,2.0
synthetic patterns according,1,1,1.0
patterns according to,4,1,4.0
minority class distribution,1,1,1.0
class distribution one,1,1,1.0
distribution one of,1,1,1.0
the most methods,1,1,1.0
most methods to,1,1,1.0
methods to do,1,1,1.0
to do so,2,1,2.0
do so is,1,1,1.0
so is the,1,1,1.0
is the synthetic,1,1,1.0
technique smote based,1,1,1.0
smote based on,1,1,1.0
based on erating,1,1,1.0
on erating new,1,1,1.0
erating new instances,1,1,1.0
new instances by,1,1,1.0
instances by convex,1,1,1.0
by convex combination,9,1,9.0
combination of one,1,1,1.0
of one point,1,1,1.0
one point and,1,1,1.0
point and one,1,1,1.0
and one of,2,1,2.0
one of its,3,1,3.0
its neighbours both,2,1,2.0
neighbours both belonging,3,1,3.0
both belonging to,3,1,3.0
minority class however,1,1,1.0
class however the,1,1,1.0
however the classes,1,1,1.0
the classes in,2,1,2.0
classes in general,1,1,1.0
in general can,1,1,1.0
general can not,1,1,1.0
can not be,2,1,2.0
not be assumed,1,1,1.0
be assumed to,1,1,1.0
assumed to be,2,1,2.0
to be convex,1,1,1.0
be convex and,1,1,1.0
convex and hence,1,1,1.0
and hence smote,1,1,1.0
hence smote does,1,1,1.0
smote does not,1,1,1.0
does not avoid,1,1,1.0
not avoid synthetic,1,1,1.0
avoid synthetic patterns,1,1,1.0
synthetic patterns to,1,1,1.0
patterns to fall,1,1,1.0
to fall inside,1,1,1.0
fall inside majority,1,1,1.0
inside majority regions,1,1,1.0
majority regions therefore,1,1,1.0
regions therefore more,1,1,1.0
therefore more careful,1,1,1.0
more careful techniques,1,1,1.0
careful techniques have,1,1,1.0
developed to prevent,1,1,1.0
to prevent this,1,1,1.0
prevent this issue,1,1,1.0
this issue prevent,1,1,1.0
issue prevent but,1,1,1.0
prevent but solve,1,1,1.0
but solve adaptive,1,1,1.0
solve adaptive synthetic,1,1,1.0
adaptive synthetic and,1,1,1.0
synthetic and sampling,1,1,1.0
and sampling methods,1,1,1.0
sampling methods are,1,1,1.0
methods are examples,1,1,1.0
are examples of,1,1,1.0
examples of more,1,1,1.0
of more powerful,1,1,1.0
more powerful techniques,1,1,1.0
powerful techniques based,1,1,1.0
techniques based on,1,1,1.0
based on extracting,1,1,1.0
on extracting knowledge,1,1,1.0
extracting knowledge from,1,1,1.0
knowledge from the,2,2,1.0
from the data,2,2,1.0
the data to,1,1,1.0
data to analyse,1,1,1.0
to analyse which,2,1,2.0
analyse which patterns,2,1,2.0
which patterns and,1,1,1.0
patterns and regions,1,1,1.0
and regions of,1,1,1.0
regions of the,4,2,2.0
of the space,2,1,2.0
the space are,1,1,1.0
space are more,3,1,3.0
are more suitable,2,1,2.0
more suitable for,3,1,3.0
suitable for sampling,1,1,1.0
for sampling this,1,1,1.0
sampling this will,1,1,1.0
this will be,2,1,2.0
will be referred,1,1,1.0
be referred in,1,1,1.0
referred in the,2,1,2.0
in the paper,1,1,1.0
the paper to,1,1,1.0
paper to as,1,1,1.0
to as preferential,1,1,1.0
as preferential at,1,1,1.0
preferential at the,1,1,1.0
the same time,2,1,2.0
same time kernel,1,1,1.0
time kernel methods,1,1,1.0
kernel methods have,1,1,1.0
have been spreading,1,1,1.0
been spreading rapidly,1,1,1.0
spreading rapidly and,1,1,1.0
rapidly and gaining,1,1,1.0
and gaining acceptance,1,1,1.0
gaining acceptance in,1,1,1.0
acceptance in machine,1,1,1.0
machine learning due,1,1,1.0
learning due to,1,1,1.0
due to their,1,1,1.0
to their good,1,1,1.0
their good generalisation,1,1,1.0
good generalisation ability,2,1,2.0
generalisation ability and,2,1,2.0
ability and determinism,1,1,1.0
and determinism being,1,1,1.0
determinism being one,1,1,1.0
being one of,1,1,1.0
widely used the,1,1,1.0
used the support,1,1,1.0
the support v,1,1,1.0
support v ector,4,1,4.0
v ector machine,1,1,1.0
ector machine svm,1,1,1.0
machine svm however,1,1,1.0
svm however for,1,1,1.0
however for svm,1,1,1.0
for svm imbalanced,1,1,1.0
svm imbalanced data,1,1,1.0
imbalanced data pose,1,1,1.0
data pose a,1,1,1.0
a serious challenge,1,1,1.0
serious challenge due,1,1,1.0
challenge due to,1,1,1.0
to the formulation,1,1,1.0
the formulation of,1,1,1.0
formulation of the,1,1,1.0
of the maximisation,1,1,1.0
the maximisation which,1,1,1.0
maximisation which focus,1,1,1.0
which focus on,1,1,1.0
focus on improving,1,1,1.0
overall performance thus,1,1,1.0
performance thus the,1,1,1.0
thus the combination,1,1,1.0
combination of kernel,3,1,3.0
of kernel methods,2,1,2.0
kernel methods with,1,1,1.0
methods with techniques,1,1,1.0
with techniques for,1,1,1.0
techniques for tackling,1,1,1.0
for tackling class,1,1,1.0
tackling class imbalance,1,1,1.0
imbalance is widely,1,1,1.0
is widely spread,1,1,1.0
widely spread it,1,1,1.0
spread it is,1,1,1.0
it is clear,5,2,2.5
is clear that,5,2,2.5
clear that by,1,1,1.0
that by linear,1,1,1.0
by linear interpolation,1,1,1.0
linear interpolation is,1,1,1.0
interpolation is not,1,1,1.0
is not as,3,1,3.0
not as suitable,1,1,1.0
as suitable when,1,1,1.0
suitable when dealing,1,1,1.0
when dealing with,4,1,4.0
dealing with nonlinear,1,1,1.0
with nonlinear classiﬁers,1,1,1.0
nonlinear classiﬁers as,1,1,1.0
classiﬁers as it,1,1,1.0
as it could,1,1,1.0
it could be,2,1,2.0
could be than,1,1,1.0
be than when,1,1,1.0
than when applying,1,1,1.0
when applying linear,1,1,1.0
applying linear classiﬁers,1,1,1.0
linear classiﬁers however,1,1,1.0
classiﬁers however linearly,1,1,1.0
however linearly separable,1,1,1.0
linearly separable datasets,1,1,1.0
separable datasets are,1,1,1.0
datasets are not,1,1,1.0
are not common,1,1,1.0
not common in,1,1,1.0
common in applications,1,1,1.0
in applications thus,1,1,1.0
applications thus making,1,1,1.0
thus making advisable,1,1,1.0
making advisable the,2,1,2.0
advisable the application,1,1,1.0
application of classiﬁers,1,1,1.0
of classiﬁers able,1,1,1.0
classiﬁers able to,1,1,1.0
able to capture,1,1,1.0
to capture this,1,1,1.0
capture this nonlinearity,1,1,1.0
this nonlinearity besides,1,1,1.0
nonlinearity besides the,1,1,1.0
besides the development,1,1,1.0
the development of,3,2,1.5
development of a,1,1,1.0
of a able,1,1,1.0
a able nonlinear,1,1,1.0
able nonlinear strategy,1,1,1.0
nonlinear strategy could,1,1,1.0
strategy could be,1,1,1.0
could be tricky,1,1,1.0
be tricky thus,1,1,1.0
tricky thus in,1,1,1.0
thus in contrast,1,1,1.0
in contrast to,1,1,1.0
contrast to previous,1,1,1.0
to previous approaches,1,1,1.0
previous approaches we,1,1,1.0
approaches we propose,1,1,1.0
we propose to,1,1,1.0
propose to generate,1,1,1.0
synthetic data by,1,1,1.0
data by convex,1,1,1.0
combination of points,1,1,1.0
of points in,4,2,2.0
in a space,1,1,1.0
a space where,1,1,1.0
space where the,1,1,1.0
where the classes,1,1,1.0
classes are ideally,1,1,1.0
are ideally linearly,1,1,1.0
ideally linearly separated,1,1,1.0
linearly separated making,1,1,1.0
separated making generation,1,1,1.0
making generation of,1,1,1.0
generation of new,1,1,1.0
of new synthetic,1,1,1.0
new synthetic points,1,1,1.0
synthetic points by,1,1,1.0
points by convex,1,1,1.0
combination of the,2,2,1.0
the original points,1,1,1.0
original points belonging,1,1,1.0
points belonging to,1,1,1.0
to the same,1,1,1.0
the same class,1,1,1.0
same class safe,1,1,1.0
class safe this,1,1,1.0
safe this is,1,1,1.0
is done using,1,1,1.0
done using the,1,1,1.0
using the feature,1,1,1.0
kernel function for,2,1,2.0
function for the,1,1,1.0
for the patterns,1,1,1.0
the patterns rather,1,1,1.0
patterns rather than,1,1,1.0
rather than using,1,1,1.0
than using the,1,1,1.0
using the input,1,1,1.0
the input space,19,1,19.0
input space however,2,1,2.0
space however this,2,1,2.0
is not so,1,1,1.0
not so straightforward,1,1,1.0
so straightforward because,1,1,1.0
straightforward because when,1,1,1.0
because when dealing,1,1,1.0
dealing with kernel,2,1,2.0
with kernel methods,3,1,3.0
kernel methods the,1,1,1.0
methods the only,1,1,1.0
the only information,1,1,1.0
only information available,1,1,1.0
information available is,1,1,1.0
available is the,1,1,1.0
is the dot,1,1,1.0
the dot products,5,1,5.0
dot products of,2,1,2.0
products of the,2,1,2.0
of the images,1,1,1.0
the images of,2,1,2.0
images of the,4,1,4.0
of the patterns,6,1,6.0
the patterns t,1,1,1.0
patterns t o,1,1,1.0
t o cope,2,1,2.0
o cope with,2,1,2.0
cope with this,2,1,2.0
with this issue,2,1,2.0
this issue this,1,1,1.0
issue this paper,1,1,1.0
this paper makes,1,1,1.0
paper makes use,1,1,1.0
makes use of,1,1,1.0
of the notion,1,1,1.0
notion of the,1,1,1.0
of the empirical,4,1,4.0
feature space efs,4,1,4.0
space efs which,1,1,1.0
efs which is,1,1,1.0
which is euclidean,1,1,1.0
is euclidean and,1,1,1.0
euclidean and preserves,1,1,1.0
and preserves the,1,1,1.0
preserves the geometrical,1,1,1.0
the geometrical structure,1,1,1.0
geometrical structure of,1,1,1.0
structure of the,2,1,2.0
the original feature,2,1,2.0
original feature space,2,1,2.0
feature space given,1,1,1.0
space given that,1,1,1.0
given that distances,1,1,1.0
that distances and,1,1,1.0
distances and angles,1,1,1.0
and angles in,1,1,1.0
angles in the,1,1,1.0
feature space are,4,1,4.0
space are uniquely,2,1,2.0
are uniquely determined,2,1,2.0
uniquely determined by,2,1,2.0
determined by dot,1,1,1.0
by dot products,1,1,1.0
dot products and,1,1,1.0
products and that,1,1,1.0
and that the,1,1,1.0
that the dot,2,1,2.0
of the corresponding,2,2,1.0
the corresponding images,1,1,1.0
corresponding images are,1,1,1.0
images are the,1,1,1.0
are the original,1,1,1.0
the original kernel,1,1,1.0
original kernel values,1,1,1.0
kernel values the,1,1,1.0
values the main,1,1,1.0
the main motivation,1,1,1.0
main motivation for,1,1,1.0
motivation for performing,1,1,1.0
for performing in,1,1,1.0
performing in the,2,1,2.0
in the efs,18,1,18.0
the efs instead,1,1,1.0
efs instead of,2,1,2.0
instead of in,1,1,1.0
of in the,5,1,5.0
in the input,9,1,9.0
input space is,2,1,2.0
space is the,1,1,1.0
is the hypothesis,1,1,1.0
the hypothesis that,2,1,2.0
hypothesis that the,2,1,2.0
that the feature,1,1,1.0
feature space provide,1,1,1.0
space provide a,1,1,1.0
provide a more,1,1,1.0
a more suitable,6,1,6.0
more suitable space,4,1,4.0
suitable space for,4,1,4.0
space for via,1,1,1.0
for via convex,1,1,1.0
via convex combination,1,1,1.0
convex combination because,2,1,2.0
combination because the,1,1,1.0
because the class,1,1,1.0
the class separation,3,1,3.0
class separation will,1,1,1.0
separation will be,1,1,1.0
will be simpler,1,1,1.0
be simpler and,1,1,1.0
simpler and larger,1,1,1.0
and larger ideally,1,1,1.0
larger ideally due,1,1,1.0
ideally due to,1,1,1.0
to the kernel,2,1,2.0
the kernel trick,3,1,3.0
kernel trick linearly,1,1,1.0
trick linearly separable,1,1,1.0
linearly separable at,1,1,1.0
separable at the,1,1,1.0
same time this,1,1,1.0
time this technique,1,1,1.0
this technique can,1,1,1.0
technique can be,1,1,1.0
seen as a,1,1,1.0
as a general,1,1,1.0
a general nonlinear,1,1,1.0
general nonlinear in,1,1,1.0
nonlinear in the,1,1,1.0
input space due,1,1,1.0
space due to,1,1,1.0
to the application,2,1,2.0
application of the,2,1,2.0
of the nonlinear,1,1,1.0
the nonlinear map,2,1,2.0
nonlinear map φ,1,1,1.0
map φ related,1,1,1.0
φ related to,1,1,1.0
kernel trick and,1,1,1.0
trick and could,1,1,1.0
and could be,1,1,1.0
used in combination,1,1,1.0
in combination with,1,1,1.0
combination with any,1,1,1.0
with any classiﬁer,1,1,1.0
any classiﬁer t,1,1,1.0
classiﬁer t o,1,1,1.0
t o the,1,1,1.0
o the best,1,1,1.0
the best of,1,1,1.0
best of our,1,1,1.0
of our knowledge,1,1,1.0
our knowledge performing,1,1,1.0
knowledge performing in,1,1,1.0
feature space has,1,1,1.0
space has only,1,1,1.0
has only been,1,1,1.0
only been researched,1,1,1.0
been researched in,1,1,1.0
researched in recall,1,1,1.0
in recall that,1,1,1.0
recall that in,1,1,1.0
that in our,1,1,1.0
in our case,1,1,1.0
our case it,1,1,1.0
case it is,2,1,2.0
it is performed,1,1,1.0
performed in the,1,1,1.0
the efs in,2,1,2.0
efs in this,2,1,2.0
in this previous,1,1,1.0
this previous work,1,1,1.0
previous work the,1,1,1.0
work the synthetic,1,1,1.0
synthetic instances were,2,1,2.0
instances were generated,1,1,1.0
were generated by,1,1,1.0
generated by using,1,1,1.0
using the geometric,1,1,1.0
the geometric interpretation,1,1,1.0
geometric interpretation of,1,1,1.0
interpretation of the,1,1,1.0
of the dot,2,1,2.0
dot products in,1,1,1.0
products in the,1,1,1.0
in the kernel,3,1,3.0
the kernel matrix,10,2,5.0
kernel matrix and,2,1,2.0
matrix and the,1,1,1.0
and the of,1,1,1.0
instances were approximated,1,1,1.0
were approximated based,1,1,1.0
approximated based on,1,1,1.0
based on a,3,1,3.0
on a distance,1,1,1.0
a distance relation,1,1,1.0
distance relation between,1,1,1.0
relation between the,1,1,1.0
between the feature,2,2,1.0
feature space and,3,1,3.0
space and the,1,1,1.0
and the input,1,1,1.0
the input one,1,1,1.0
input one since,1,1,1.0
one since inverse,1,1,1.0
since inverse mapping,1,1,1.0
inverse mapping φ,1,1,1.0
mapping φ from,1,1,1.0
φ from the,1,1,1.0
from the feature,1,1,1.0
feature space to,2,1,2.0
space to the,2,1,2.0
to the input,2,1,2.0
is not available,1,1,1.0
not available our,1,1,1.0
available our proposal,1,1,1.0
our proposal is,1,1,1.0
proposal is free,1,1,1.0
is free of,1,1,1.0
free of the,1,1,1.0
of the assumptions,1,1,1.0
the assumptions of,1,1,1.0
assumptions of this,1,1,1.0
of this inverse,1,1,1.0
this inverse mapping,1,1,1.0
inverse mapping approximation,1,1,1.0
mapping approximation the,1,1,1.0
approximation the study,1,1,1.0
the study made,1,1,1.0
study made in,1,1,1.0
this paper intends,1,1,1.0
paper intends to,1,1,1.0
intends to provide,1,1,1.0
to provide an,1,1,1.0
provide an extensive,1,1,1.0
an extensive analysis,1,1,1.0
extensive analysis of,1,1,1.0
analysis of in,1,1,1.0
the efs and,2,1,2.0
efs and can,1,1,1.0
can be subdivided,1,1,1.0
be subdivided in,1,1,1.0
subdivided in three,1,1,1.0
in three sections,1,1,1.0
three sections the,1,1,1.0
sections the ﬁrst,1,1,1.0
the ﬁrst one,2,1,2.0
ﬁrst one deals,1,1,1.0
one deals with,1,1,1.0
deals with the,2,1,2.0
with the issue,1,1,1.0
issue of extending,1,1,1.0
of extending the,1,1,1.0
extending the smote,1,1,1.0
smote algorithm to,1,1,1.0
full and efs,1,1,1.0
and efs the,1,1,1.0
efs the objective,1,1,1.0
the objective is,2,1,2.0
is to test,2,1,2.0
to test whether,3,1,3.0
test whether the,3,1,3.0
whether the efs,1,1,1.0
the efs provides,2,1,2.0
efs provides a,2,1,2.0
provides a more,3,1,3.0
more suitable framework,1,1,1.0
suitable framework for,1,1,1.0
framework for by,1,1,1.0
for by convex,2,1,2.0
combination of patterns,5,1,5.0
of patterns and,1,1,1.0
patterns and to,1,1,1.0
and to deal,1,1,1.0
with the dimensionality,1,1,1.0
the dimensionality of,7,1,7.0
dimensionality of the,9,1,9.0
of the efs,15,1,15.0
the efs the,3,1,3.0
efs the second,1,1,1.0
the second part,1,1,1.0
second part deals,1,1,1.0
part deals with,1,1,1.0
with the kernel,1,1,1.0
kernel function choice,1,1,1.0
function choice since,1,1,1.0
choice since our,1,1,1.0
since our methodology,1,1,1.0
our methodology depends,1,1,1.0
methodology depends on,1,1,1.0
depends on how,1,1,1.0
on how the,1,1,1.0
how the kernel,1,1,1.0
the kernel matches,1,1,1.0
kernel matches the,1,1,1.0
the underlying classiﬁcation,1,1,1.0
underlying classiﬁcation problem,1,1,1.0
classiﬁcation problem and,2,1,2.0
problem and we,1,1,1.0
and we develop,1,1,1.0
we develop a,1,1,1.0
develop a strategy,1,1,1.0
a strategy for,1,1,1.0
strategy for optimising,1,1,1.0
for optimising the,4,1,4.0
optimising the feature,1,1,1.0
feature space based,1,1,1.0
space based on,1,1,1.0
based on analytical,1,1,1.0
on analytical knowledge,1,1,1.0
analytical knowledge using,1,1,1.0
knowledge using the,1,1,1.0
using the notion,1,1,1.0
notion of alignment,1,1,1.0
of alignment ideally,1,1,1.0
alignment ideally a,1,1,1.0
ideally a better,1,1,1.0
a better ﬁtted,1,1,1.0
better ﬁtted kernel,1,1,1.0
ﬁtted kernel will,1,1,1.0
kernel will increase,1,1,1.0
increase the class,1,1,1.0
the class separability,1,1,1.0
class separability providing,1,1,1.0
separability providing a,1,1,1.0
providing a safer,1,1,1.0
a safer environment,1,1,1.0
safer environment for,1,1,1.0
environment for the,1,1,1.0
for the generation,2,1,2.0
generation of synthetic,2,2,1.0
of synthetic patterns,2,1,2.0
synthetic patterns the,2,1,2.0
patterns the last,1,1,1.0
the last part,1,1,1.0
last part of,1,1,1.0
part of this,1,1,1.0
this paper proposes,2,2,1.0
paper proposes a,1,1,1.0
proposes a uniﬁed,1,1,1.0
a uniﬁed adaptive,1,1,1.0
uniﬁed adaptive framework,1,1,1.0
adaptive framework for,1,1,1.0
for preferential generalising,1,1,1.0
preferential generalising several,1,1,1.0
generalising several approaches,1,1,1.0
several approaches in,1,1,1.0
the literature the,1,1,1.0
literature the optimal,1,1,1.0
the optimal svm,2,1,2.0
optimal svm hyperplane,2,1,2.0
svm hyperplane and,2,1,2.0
hyperplane and kernel,1,1,1.0
and kernel learning,1,1,1.0
kernel learning techniques,7,1,7.0
learning techniques are,1,1,1.0
techniques are used,1,1,1.0
are used for,1,1,1.0
used for optimising,1,1,1.0
optimising the synthetically,1,1,1.0
the synthetically generated,1,1,1.0
generated patterns the,1,1,1.0
patterns the objective,1,1,1.0
is to check,1,1,1.0
to check if,1,1,1.0
check if some,1,1,1.0
if some regions,1,1,1.0
some regions of,2,1,2.0
the space can,1,1,1.0
space can be,2,1,2.0
can be more,1,1,1.0
be more useful,1,1,1.0
more useful for,1,1,1.0
useful for than,1,1,1.0
for than others,1,1,1.0
than others t,1,1,1.0
others t o,1,1,1.0
t o test,1,1,1.0
o test the,1,1,1.0
test the different,1,1,1.0
the different hypotheses,1,1,1.0
different hypotheses exposed,1,1,1.0
hypotheses exposed in,1,1,1.0
exposed in this,1,1,1.0
paper we perform,1,1,1.0
we perform a,1,1,1.0
perform a thorough,1,1,1.0
a thorough set,2,1,2.0
thorough set of,2,1,2.0
set of experiments,2,1,2.0
of experiments with,1,1,1.0
experiments with binary,1,1,1.0
with binary imbalanced,1,1,1.0
binary imbalanced datasets,1,1,1.0
imbalanced datasets the,2,1,2.0
datasets the paper,1,1,1.0
follows section ii,1,1,1.0
section ii introduces,1,1,1.0
ii introduces some,1,1,1.0
introduces some useful,1,1,1.0
some useful notions,1,1,1.0
useful notions section,1,1,1.0
notions section iii,1,1,1.0
section iii exposes,1,1,1.0
iii exposes how,1,1,1.0
exposes how to,1,1,1.0
how to perform,1,1,1.0
to perform sampling,1,1,1.0
perform sampling in,1,1,1.0
the efs section,1,1,1.0
efs section iv,1,1,1.0
section iv develops,1,1,1.0
iv develops a,1,1,1.0
develops a new,1,1,1.0
a new methodology,1,1,1.0
new methodology for,1,1,1.0
methodology for kernel,1,1,1.0
for kernel learning,2,1,2.0
kernel learning section,1,1,1.0
learning section v,1,1,1.0
section v proposes,1,1,1.0
v proposes a,1,1,1.0
proposes a general,1,1,1.0
a general preferential,1,1,1.0
general preferential framework,1,1,1.0
preferential framework section,1,1,1.0
framework section vi,1,1,1.0
section vi exposes,1,1,1.0
vi exposes the,1,1,1.0
exposes the tal,1,1,1.0
the tal study,1,1,1.0
tal study and,1,1,1.0
study and analyses,1,1,1.0
and analyses the,2,1,2.0
analyses the results,1,1,1.0
results obtained and,1,1,1.0
obtained and ﬁnally,1,1,1.0
and ﬁnally section,1,1,1.0
ﬁnally section vii,1,1,1.0
section vii outlines,1,1,1.0
vii outlines some,1,1,1.0
outlines some conclusions,1,1,1.0
some conclusions and,1,1,1.0
conclusions and future,1,1,1.0
and future work,1,1,1.0
future work ii,1,1,1.0
work ii b,1,1,1.0
ii b ac,1,1,1.0
b ac k,1,1,1.0
ac k g,1,1,1.0
k g ro,1,1,1.0
g ro u,1,1,1.0
ro u n,1,1,1.0
u n d,1,1,1.0
n d this,1,1,1.0
d this section,1,1,1.0
this section is,3,1,3.0
section is intended,1,1,1.0
is intended to,3,1,3.0
intended to introduce,1,1,1.0
to introduce the,1,1,1.0
introduce the notation,1,1,1.0
the notation used,1,1,1.0
notation used throughout,1,1,1.0
used throughout all,1,1,1.0
throughout all the,1,1,1.0
all the paper,1,1,1.0
paper and to,1,1,1.0
to provide some,1,1,1.0
provide some previous,1,1,1.0
some previous notions,1,1,1.0
previous notions about,1,1,1.0
notions about svm,1,1,1.0
about svm classiﬁers,1,1,1.0
svm classiﬁers and,1,1,1.0
classiﬁers and the,1,1,1.0
and the empirical,1,1,1.0
feature space consider,1,1,1.0
space consider a,1,1,1.0
consider a sample,1,1,1.0
a sample d,1,1,1.0
sample d xi,1,1,1.0
d xi y,1,1,1.0
xi y i,1,1,1.0
y i m,2,2,1.0
i m x,1,1,1.0
m x y,1,1,1.0
x y generated,1,1,1.0
y generated from,1,1,1.0
generated from a,1,1,1.0
from a unknown,1,1,1.0
a unknown joint,1,1,1.0
unknown joint distribution,1,1,1.0
joint distribution p,1,1,1.0
distribution p x,1,1,1.0
p x y,1,1,1.0
x y where,1,1,1.0
y where x,1,1,1.0
where x rd,1,1,1.0
x rd y,1,1,1.0
rd y the,1,1,1.0
y the goal,1,1,1.0
goal in binary,1,1,1.0
in binary classiﬁcation,1,1,1.0
binary classiﬁcation is,1,1,1.0
classiﬁcation is to,1,1,1.0
is to assign,1,1,1.0
to assign an,1,1,1.0
assign an input,1,1,1.0
an input vector,1,1,1.0
input vector x,1,1,1.0
vector x to,1,1,1.0
x to one,1,1,1.0
to one of,1,1,1.0
one of classes,1,1,1.0
of classes denote,1,1,1.0
classes denote by,1,1,1.0
denote by xtr,1,1,1.0
by xtr and,1,1,1.0
xtr and xts,1,1,1.0
and xts the,1,1,1.0
xts the sets,1,1,1.0
the sets of,1,1,1.0
sets of training,1,1,1.0
of training and,1,1,1.0
training and testing,3,2,1.5
and testing inputs,1,1,1.0
testing inputs respectively,1,1,1.0
inputs respectively furthermore,1,1,1.0
respectively furthermore we,1,1,1.0
furthermore we will,2,1,2.0
we will mark,1,1,1.0
will mark by,1,1,1.0
mark by subscript,1,1,1.0
by subscript and,1,1,1.0
subscript and to,1,1,1.0
to the sets,1,1,1.0
the sets containing,1,1,1.0
sets containing inputs,1,1,1.0
containing inputs from,1,1,1.0
inputs from the,1,1,1.0
the positive and,2,1,2.0
and negative class,1,1,1.0
negative class respectively,1,1,1.0
class respectively for,1,1,1.0
respectively for a,1,1,1.0
for a set,1,1,1.0
a set x,1,1,1.0
set x we,1,1,1.0
x we denote,1,1,1.0
we denote by,2,1,2.0
denote by x,1,1,1.0
by x the,1,1,1.0
x the design,1,1,1.0
the design matrix,4,1,4.0
design matrix storing,3,1,3.0
matrix storing points,2,1,2.0
storing points of,2,1,2.0
points of x,1,1,1.0
of x as,1,1,1.0
x as rows,1,1,1.0
as rows reproducing,1,1,1.0
rows reproducing kernels,1,1,1.0
reproducing kernels often,1,1,1.0
kernels often referred,1,1,1.0
often referred as,1,1,1.0
referred as mercer,1,1,1.0
as mercer kernels,1,1,1.0
mercer kernels are,1,1,1.0
kernels are functions,1,1,1.0
are functions k,1,1,1.0
functions k x,1,1,1.0
k x x,1,1,1.0
x x r,1,1,1.0
x r which,1,1,1.0
r which for,1,1,1.0
which for all,1,1,1.0
for all pattern,1,1,1.0
all pattern sets,1,1,1.0
pattern sets xm,1,1,1.0
sets xm give,1,1,1.0
xm give rise,1,1,1.0
give rise to,1,1,1.0
rise to semideﬁnite,1,1,1.0
to semideﬁnite positive,1,1,1.0
semideﬁnite positive matrices,1,1,1.0
positive matrices where,1,1,1.0
matrices where kij,1,1,1.0
where kij k,1,1,1.0
kij k xi,1,1,1.0
k xi xj,5,1,5.0
xi xj kernel,1,1,1.0
xj kernel functions,1,1,1.0
kernel functions allow,1,1,1.0
functions allow us,1,1,1.0
allow us to,1,1,1.0
us to derive,1,1,1.0
to derive nonlinear,1,1,1.0
derive nonlinear classiﬁers,1,1,1.0
nonlinear classiﬁers by,1,1,1.0
classiﬁers by reducing,1,1,1.0
by reducing them,1,1,1.0
reducing them to,1,1,1.0
them to linear,1,1,1.0
to linear ones,1,1,1.0
linear ones but,1,1,1.0
ones but in,1,1,1.0
but in some,1,1,1.0
in some hilbert,1,1,1.0
some hilbert space,1,1,1.0
hilbert space h,2,1,2.0
space h nonlinearly,1,1,1.0
h nonlinearly related,1,1,1.0
nonlinearly related to,1,1,1.0
input space and,2,1,2.0
space and furnished,1,1,1.0
and furnished with,1,1,1.0
furnished with a,1,1,1.0
with a dot,1,1,1.0
a dot product,1,1,1.0
dot product k,1,1,1.0
product k xi,1,1,1.0
xi xj xi,1,1,1.0
xj xi φ,1,1,1.0
xi φ xj,1,1,1.0
φ xj the,1,1,1.0
xj the use,1,1,1.0
of this kernel,1,1,1.0
this kernel function,2,1,2.0
kernel function instead,1,1,1.0
function instead of,1,1,1.0
instead of the,3,2,1.5
the dot product,6,1,6.0
dot product in,2,1,2.0
product in rm,1,1,1.0
in rm corresponds,1,1,1.0
rm corresponds to,1,1,1.0
corresponds to using,1,1,1.0
to using a,1,1,1.0
using a usually,1,1,1.0
a usually nonlinear,1,1,1.0
usually nonlinear mapping,1,1,1.0
nonlinear mapping of,1,1,1.0
mapping of patterns,1,1,1.0
of patterns from,1,1,1.0
patterns from x,1,1,1.0
from x to,1,1,1.0
x to a,1,1,1.0
to a or,1,1,1.0
a or dimensional,1,1,1.0
or dimensional hilbert,1,1,1.0
dimensional hilbert space,1,1,1.0
space h such,1,1,1.0
h such that,1,1,1.0
such that φ,1,1,1.0
that φ x,1,1,1.0
φ x h,1,1,1.0
x h where,1,1,1.0
h where the,1,1,1.0
where the separation,1,1,1.0
the separation would,1,1,1.0
separation would ideally,1,1,1.0
would ideally be,1,1,1.0
ideally be easier,1,1,1.0
be easier and,1,1,1.0
easier and take,1,1,1.0
and take the,1,1,1.0
take the dot,1,1,1.0
dot product there,1,1,1.0
product there kernel,1,1,1.0
there kernel machines,1,1,1.0
kernel machines trained,1,1,1.0
machines trained on,1,1,1.0
trained on d,1,1,1.0
on d do,1,1,1.0
d do not,1,1,1.0
do not operate,1,1,1.0
not operate on,1,1,1.0
operate on the,1,1,1.0
on the whole,1,1,1.0
the whole of,1,1,1.0
whole of h,1,1,1.0
of h but,1,1,1.0
h but on,1,1,1.0
but on its,1,1,1.0
on its subset,1,1,1.0
its subset f,1,1,1.0
subset f span,1,1,1.0
f span φ,1,1,1.0
span φ φ,1,1,1.0
φ φ xm,1,1,1.0
φ xm which,1,1,1.0
xm which we,1,1,1.0
which we will,1,1,1.0
we will refer,1,1,1.0
will refer to,1,1,1.0
refer to as,1,1,1.0
to as the,2,1,2.0
as the feature,1,1,1.0
feature space such,1,1,1.0
space such that,1,1,1.0
such that f,1,1,1.0
that f h,1,1,1.0
f h note,1,1,1.0
h note that,1,1,1.0
note that f,1,1,1.0
that f is,1,1,1.0
f is at,1,1,1.0
is at most,1,1,1.0
at most an,1,1,1.0
most an linear,1,1,1.0
an linear support,1,1,1.0
linear support v,1,1,1.0
v ector machines,3,1,3.0
ector machines svm,2,1,2.0
machines svm is,1,1,1.0
svm is perhaps,1,1,1.0
is perhaps the,1,1,1.0
most common kernel,1,1,1.0
common kernel method,1,1,1.0
kernel method for,1,1,1.0
method for statistical,1,1,1.0
for statistical pattern,1,1,1.0
statistical pattern recognition,1,1,1.0
pattern recognition due,1,1,1.0
recognition due to,1,1,1.0
due to its,1,1,1.0
to its good,1,1,1.0
its good generalisation,1,1,1.0
ability and freedom,1,1,1.0
and freedom from,1,1,1.0
freedom from local,1,1,1.0
from local minima,1,1,1.0
local minima the,1,1,1.0
minima the basic,1,1,1.0
behind this technique,1,1,1.0
this technique is,1,1,1.0
technique is the,2,2,1.0
is the separation,1,1,1.0
the separation of,2,1,2.0
separation of two,1,1,1.0
two different classes,1,1,1.0
different classes through,1,1,1.0
classes through a,1,1,1.0
through a hyperplane,1,1,1.0
a hyperplane which,1,1,1.0
hyperplane which is,1,1,1.0
which is speciﬁed,1,1,1.0
is speciﬁed by,1,1,1.0
speciﬁed by a,1,1,1.0
by a normal,1,1,1.0
a normal vector,1,1,1.0
normal vector w,1,1,1.0
vector w and,1,1,1.0
w and a,2,1,2.0
and a bias,2,1,2.0
a bias the,1,1,1.0
bias the optimal,1,1,1.0
the optimal separating,1,1,1.0
optimal separating hyperplane,1,1,1.0
separating hyperplane is,1,1,1.0
hyperplane is the,1,1,1.0
the one which,1,1,1.0
one which maximises,1,1,1.0
which maximises the,2,1,2.0
maximises the distance,2,1,2.0
distance between the,2,2,1.0
between the hyperplane,1,1,1.0
the hyperplane and,1,1,1.0
hyperplane and the,1,1,1.0
and the nearest,1,1,1.0
the nearest points,1,1,1.0
nearest points in,1,1,1.0
points in both,1,1,1.0
in both classes,1,1,1.0
both classes called,1,1,1.0
classes called margin,1,1,1.0
called margin beyond,1,1,1.0
margin beyond the,1,1,1.0
beyond the application,1,1,1.0
application of kernel,1,1,1.0
of kernel techniques,1,1,1.0
kernel techniques to,1,1,1.0
techniques to allow,1,1,1.0
to allow decision,1,1,1.0
allow decision discriminants,1,1,1.0
decision discriminants the,1,1,1.0
discriminants the kernel,1,1,1.0
kernel trick another,1,1,1.0
trick another generalisation,1,1,1.0
another generalisation was,1,1,1.0
generalisation was made,1,1,1.0
was made to,1,1,1.0
made to replace,1,1,1.0
to replace hard,1,1,1.0
replace hard margins,1,1,1.0
hard margins with,1,1,1.0
margins with soft,1,1,1.0
with soft margins,1,1,1.0
soft margins using,1,1,1.0
margins using the,1,1,1.0
using the ξi,1,1,1.0
the ξi in,1,1,1.0
ξi in order,1,1,1.0
order to deal,1,1,1.0
deal with overlapping,1,1,1.0
with overlapping classes,1,1,1.0
overlapping classes therefore,1,1,1.0
classes therefore this,1,1,1.0
therefore this algorithm,1,1,1.0
this algorithm seeks,1,1,1.0
algorithm seeks for,1,1,1.0
seeks for a,1,1,1.0
for a classiﬁer,1,1,1.0
a classiﬁer f,1,1,1.0
classiﬁer f rd,1,1,1.0
f rd r,1,1,1.0
rd r of,1,1,1.0
r of the,1,1,1.0
of the form,1,1,1.0
the form f,1,1,1.0
form f x,1,1,1.0
f x w,1,1,1.0
x w φ,1,1,1.0
w φ x,1,1,1.0
φ x b,1,1,1.0
x b φ,1,1,1.0
b φ being,1,1,1.0
φ being the,1,1,1.0
being the mapping,1,1,1.0
the mapping function,1,1,1.0
mapping function induced,1,1,1.0
function induced by,1,1,1.0
induced by the,3,1,3.0
the kernel that,2,1,2.0
kernel that minimises,2,1,2.0
that minimises the,2,1,2.0
minimises the objective,1,1,1.0
the objective function,1,1,1.0
objective function c,1,1,1.0
function c ξi,1,1,1.0
c ξi for,1,1,1.0
ξi for some,1,1,1.0
for some parameter,1,1,1.0
some parameter c,1,1,1.0
parameter c subject,1,1,1.0
c subject to,1,1,1.0
subject to the,2,1,2.0
to the constraints,2,1,2.0
the constraints yi,2,1,2.0
constraints yi w,2,1,2.0
yi w φ,2,1,2.0
w φ xi,2,1,2.0
φ xi b,2,1,2.0
xi b ξi,2,1,2.0
b ξi ξ,2,1,2.0
ξi ξ i,2,1,2.0
ξ i m,2,1,2.0
i m it,1,1,1.0
m it is,1,1,1.0
clear that using,1,1,1.0
that using svms,1,1,1.0
using svms the,1,1,1.0
svms the maximization,1,1,1.0
the maximization paradigm,1,1,1.0
maximization paradigm poses,1,1,1.0
paradigm poses a,2,1,2.0
poses a serious,2,1,2.0
serious hindrance for,1,1,1.0
hindrance for imbalanced,1,1,1.0
for imbalanced datasets,3,2,1.5
datasets the main,1,1,1.0
the main reason,1,1,1.0
main reason for,1,1,1.0
reason for this,1,1,1.0
for this is,1,1,1.0
this is that,1,1,1.0
is that svm,1,1,1.0
that svm optimisation,1,1,1.0
svm optimisation is,1,1,1.0
optimisation is focused,1,1,1.0
is focused on,2,2,1.0
focused on overall,1,1,1.0
on overall error,1,1,1.0
overall error therefore,1,1,1.0
error therefore they,1,1,1.0
therefore they are,1,1,1.0
they are inherently,1,1,1.0
are inherently biased,1,1,1.0
inherently biased toward,1,1,1.0
biased toward the,1,1,1.0
toward the majority,1,1,1.0
in the worst,1,1,1.0
the worst case,1,1,1.0
worst case for,1,1,1.0
case for a,1,1,1.0
for a noisy,1,1,1.0
a noisy and,1,1,1.0
noisy and highly,1,1,1.0
and highly imbalanced,1,1,1.0
highly imbalanced dataset,1,1,1.0
imbalanced dataset the,1,1,1.0
dataset the svm,1,1,1.0
the svm paradigm,3,1,3.0
svm paradigm is,1,1,1.0
paradigm is very,1,1,1.0
is very likely,1,1,1.0
very likely to,1,1,1.0
likely to obtain,1,1,1.0
to obtain a,1,1,1.0
obtain a trivial,2,1,2.0
a trivial classiﬁer,2,1,2.0
trivial classiﬁer the,1,1,1.0
classiﬁer the one,1,1,1.0
one that classiﬁes,1,1,1.0
that classiﬁes all,1,1,1.0
classiﬁes all the,1,1,1.0
all the patterns,3,1,3.0
the patterns in,1,1,1.0
patterns in the,2,1,2.0
class a solution,1,1,1.0
a solution that,1,1,1.0
solution that as,1,1,1.0
that as said,1,1,1.0
as said if,1,1,1.0
said if the,1,1,1.0
if the imbalance,1,1,1.0
the imbalance is,1,1,1.0
imbalance is severe,1,1,1.0
is severe could,1,1,1.0
severe could provide,1,1,1.0
could provide the,1,1,1.0
provide the minimal,1,1,1.0
the minimal error,1,1,1.0
minimal error t,1,1,1.0
error t o,1,1,1.0
this issue several,1,1,1.0
issue several studies,1,1,1.0
several studies in,1,1,1.0
studies in the,1,1,1.0
in the machine,2,1,2.0
machine learning literature,2,1,2.0
learning literature have,1,1,1.0
literature have explored,1,1,1.0
have explored different,1,1,1.0
explored different solutions,1,1,1.0
different solutions to,1,1,1.0
to the imbalanced,1,1,1.0
the imbalanced classiﬁcation,1,1,1.0
imbalanced classiﬁcation problem,1,1,1.0
classiﬁcation problem considering,1,1,1.0
problem considering the,1,1,1.0
considering the svm,1,1,1.0
svm paradigm most,1,1,1.0
paradigm most of,1,1,1.0
most of them,1,1,1.0
of them are,1,1,1.0
them are based,1,1,1.0
based on classiﬁcation,1,1,1.0
on classiﬁcation ensembles,1,1,1.0
classiﬁcation ensembles and,1,1,1.0
ensembles and kernel,1,1,1.0
and kernel optimisation,1,1,1.0
kernel optimisation techniques,1,1,1.0
optimisation techniques among,1,1,1.0
techniques among others,1,1,1.0
among others however,1,1,1.0
others however some,1,1,1.0
however some studies,1,1,1.0
some studies suggest,1,1,1.0
suggest that sampling,1,1,1.0
that sampling is,1,1,1.0
sampling is not,1,1,1.0
not as effective,1,1,1.0
as effective as,1,1,1.0
effective as in,1,1,1.0
as in this,1,1,1.0
this case because,1,1,1.0
case because of,1,1,1.0
because of the,4,2,2.0
the potential loss,1,1,1.0
potential loss of,1,1,1.0
loss of information,1,1,1.0
of information on,1,1,1.0
information on the,1,1,1.0
the class boundaries,2,1,2.0
class boundaries which,1,1,1.0
boundaries which is,1,1,1.0
which is crucial,1,1,1.0
is crucial for,1,1,1.0
crucial for the,1,1,1.0
for the svm,1,1,1.0
the svm solution,2,1,2.0
svm solution synthetic,1,1,1.0
solution synthetic minority,1,1,1.0
technique smote as,1,1,1.0
smote as stated,1,1,1.0
as stated one,1,1,1.0
stated one of,1,1,1.0
widely used techniques,1,1,1.0
used techniques for,1,1,1.0
techniques for sampling,1,1,1.0
for sampling is,1,1,1.0
sampling is the,1,1,1.0
is the smote,1,1,1.0
smote algorithm the,2,2,1.0
algorithm the process,1,1,1.0
the process is,2,1,2.0
process is very,1,1,1.0
is very simple,1,1,1.0
very simple the,1,1,1.0
simple the method,1,1,1.0
the method consists,1,1,1.0
method consists on,1,1,1.0
consists on generating,1,1,1.0
on generating new,1,1,1.0
generating new instances,1,1,1.0
new instances on,1,1,1.0
instances on the,1,1,1.0
on the line,2,1,2.0
the line that,1,1,1.0
line that connects,1,1,1.0
that connects one,1,1,1.0
connects one randomly,1,1,1.0
one randomly chosen,2,2,1.0
randomly chosen point,1,1,1.0
chosen point with,1,1,1.0
point with one,1,1,1.0
with one of,1,1,1.0
of its nearest,1,1,1.0
its nearest neighbours,1,1,1.0
nearest neighbours both,1,1,1.0
minority class therefore,1,1,1.0
class therefore this,1,1,1.0
therefore this methodology,1,1,1.0
this methodology relies,1,1,1.0
methodology relies on,1,1,1.0
relies on a,1,1,1.0
on a convex,1,1,1.0
a convex combination,2,1,2.0
combination of two,1,1,1.0
of two patterns,1,1,1.0
two patterns note,1,1,1.0
patterns note that,2,1,2.0
note that with,1,1,1.0
that with this,1,1,1.0
with this approach,1,1,1.0
this approach new,1,1,1.0
approach new patterns,1,1,1.0
new patterns could,1,1,1.0
patterns could lie,1,1,1.0
could lie inside,1,1,1.0
lie inside the,1,1,1.0
inside the majority,1,1,1.0
majority class region,4,1,4.0
class region although,1,1,1.0
region although choosing,1,1,1.0
although choosing a,1,1,1.0
choosing a correct,1,1,1.0
a correct value,1,1,1.0
correct value for,1,1,1.0
value for the,2,2,1.0
for the k,1,1,1.0
the k parameter,1,1,1.0
k parameter of,1,1,1.0
parameter of the,1,1,1.0
of the neighbours,4,1,4.0
the neighbours method,2,1,2.0
neighbours method could,1,1,1.0
method could avoid,1,1,1.0
could avoid this,1,1,1.0
avoid this to,1,1,1.0
this to happen,1,1,1.0
to happen in,1,1,1.0
happen in some,1,1,1.0
some cases empirical,1,1,1.0
cases empirical feature,1,1,1.0
space efs w,1,1,1.0
efs w e,1,1,1.0
w e can,1,1,1.0
e can endow,1,1,1.0
can endow an,1,1,1.0
endow an r,1,1,1.0
an r m,1,1,1.0
r m space,1,1,1.0
m space f,1,1,1.0
space f with,1,1,1.0
f with an,1,1,1.0
with an orthonormal,1,1,1.0
an orthonormal basis,3,1,3.0
orthonormal basis ug,1,1,1.0
basis ug g,1,1,1.0
ug g b,1,1,1.0
g b b,1,1,1.0
b b r,1,1,1.0
b r satisfying,1,1,1.0
r satisfying orthogonality,1,1,1.0
satisfying orthogonality normalisation,1,1,1.0
orthogonality normalisation and,1,1,1.0
normalisation and completeness,1,1,1.0
and completeness consider,1,1,1.0
completeness consider the,1,1,1.0
consider the set,2,1,2.0
the set e,2,1,2.0
set e ϕ,1,1,1.0
e ϕ v,1,1,1.0
ϕ v f,1,1,1.0
v f where,1,1,1.0
f where ϕ,1,1,1.0
where ϕ v,1,1,1.0
ϕ v the,1,1,1.0
v the map,1,1,1.0
the map ϕ,1,1,1.0
map ϕ is,1,1,1.0
ϕ is an,1,1,1.0
is an isometric,1,1,1.0
an isometric isomorphism,1,1,1.0
isometric isomorphism of,1,1,1.0
isomorphism of f,1,1,1.0
of f and,1,1,1.0
f and e,1,1,1.0
and e a,1,1,1.0
e a bijective,1,1,1.0
a bijective linear,2,1,2.0
bijective linear mapping,2,1,2.0
linear mapping such,1,1,1.0
mapping such that,1,1,1.0
such that the,2,2,1.0
dot products are,1,1,1.0
products are preserved,1,1,1.0
are preserved v,1,1,1.0
preserved v ϕ,1,1,1.0
v ϕ when,1,1,1.0
ϕ when f,1,1,1.0
when f is,1,1,1.0
f is the,1,1,1.0
is the feature,1,1,1.0
feature space the,2,1,2.0
space the set,1,1,1.0
set e is,1,1,1.0
e is referred,1,1,1.0
is referred to,2,1,2.0
referred to as,3,1,3.0
to as empirical,1,1,1.0
as empirical feature,1,1,1.0
space efs consider,1,1,1.0
efs consider a,1,1,1.0
consider a set,1,1,1.0
set of training,1,1,1.0
of training points,1,1,1.0
training points xi,1,1,1.0
points xi m,1,1,1.0
xi m x,1,1,1.0
m x then,1,1,1.0
x then when,1,1,1.0
then when working,1,1,1.0
when working with,1,1,1.0
working with kernel,1,1,1.0
kernel methods we,1,1,1.0
methods we use,1,1,1.0
we use a,2,1,2.0
use a kernel,1,1,1.0
kernel function k,1,1,1.0
function k to,1,1,1.0
k to map,1,1,1.0
to map the,1,1,1.0
map the patterns,1,1,1.0
the patterns to,2,1,2.0
patterns to the,3,1,3.0
feature space f,2,1,2.0
space f and,1,1,1.0
f and thus,1,1,1.0
and thus obtain,1,1,1.0
thus obtain a,1,1,1.0
obtain a gram,1,1,1.0
a gram matrix,1,1,1.0
gram matrix k,2,1,2.0
matrix k with,1,1,1.0
k with rank,1,1,1.0
with rank r,1,1,1.0
rank r r,1,1,1.0
r r the,1,1,1.0
r the nonlinear,1,1,1.0
nonlinear map from,1,1,1.0
map from the,1,1,1.0
from the input,1,1,1.0
input space to,1,1,1.0
to the euclidean,3,2,1.5
the euclidean space,1,1,1.0
euclidean space φ,1,1,1.0
space φ e,1,1,1.0
φ e r,6,1,6.0
e r x,1,1,1.0
r x rr,1,1,1.0
x rr which,1,1,1.0
rr which preserves,1,1,1.0
which preserves the,1,1,1.0
preserves the feature,1,1,1.0
space structure is,1,1,1.0
structure is referred,1,1,1.0
as the empirical,1,1,1.0
the empirical kernel,11,1,11.0
empirical kernel map,10,1,10.0
kernel map the,1,1,1.0
map the efs,1,1,1.0
the efs e,5,1,5.0
efs e is,1,1,1.0
e is chosen,1,1,1.0
is chosen so,1,1,1.0
chosen so as,1,1,1.0
so as to,1,1,1.0
as to preserve,1,1,1.0
to preserve the,1,1,1.0
preserve the dot,1,1,1.0
dot product information,1,1,1.0
product information about,1,1,1.0
information about f,1,1,1.0
about f contained,1,1,1.0
f contained in,1,1,1.0
contained in k,1,1,1.0
in k to,1,1,1.0
k to be,1,1,1.0
to be isometric,1,1,1.0
be isometric isomorphic,1,1,1.0
isometric isomorphic to,1,1,1.0
to the embedded,1,1,1.0
the embedded feature,1,1,1.0
embedded feature space,1,1,1.0
space f h,1,1,1.0
f h in,1,1,1.0
h in this,1,1,1.0
in this sense,2,1,2.0
this sense it,1,1,1.0
sense it can,1,1,1.0
said that the,2,1,2.0
that the empirical,1,1,1.0
kernel map corresponds,1,1,1.0
map corresponds to,1,1,1.0
corresponds to a,2,1,2.0
to a bijective,1,1,1.0
linear mapping ϕ,1,1,1.0
mapping ϕ f,1,1,1.0
ϕ f e,1,1,1.0
f e a,1,1,1.0
e a graphical,1,1,1.0
a graphical representation,2,1,2.0
graphical representation of,2,1,2.0
representation of the,6,2,3.0
of the input,3,1,3.0
input space feature,2,1,2.0
space feature space,2,1,2.0
space efs and,1,1,1.0
efs and mappings,1,1,1.0
and mappings between,1,1,1.0
mappings between these,1,1,1.0
between these spaces,1,1,1.0
these spaces is,1,1,1.0
spaces is shown,1,1,1.0
is shown in,2,2,1.0
shown in fig,3,2,1.5
in fig fig,1,1,1.0
fig fig representation,1,1,1.0
fig representation of,2,2,1.0
the relation and,1,1,1.0
relation and mapping,1,1,1.0
and mapping between,1,1,1.0
mapping between input,1,1,1.0
between input space,1,1,1.0
space and empirical,1,1,1.0
feature space any,1,1,1.0
space any given,1,1,1.0
any given gram,1,1,1.0
given gram matrix,1,1,1.0
matrix k of,2,1,2.0
k of rank,2,1,2.0
of rank r,2,1,2.0
rank r can,1,1,1.0
r can be,2,1,2.0
can be diagonalised,1,1,1.0
be diagonalised as,1,1,1.0
diagonalised as follows,1,1,1.0
as follows λ,1,1,1.0
follows λ pt,1,1,1.0
λ pt where,1,1,1.0
pt where t,1,1,1.0
where t is,2,1,2.0
t is the,2,1,2.0
is the transpose,1,1,1.0
the transpose operation,1,1,1.0
transpose operation λ,1,1,1.0
operation λ is,1,1,1.0
λ is a,1,1,1.0
is a diagonal,1,1,1.0
a diagonal matrix,1,1,1.0
diagonal matrix containing,1,1,1.0
matrix containing the,1,1,1.0
containing the r,1,1,1.0
the r nonzero,1,1,1.0
r nonzero eigenvalues,1,1,1.0
nonzero eigenvalues of,1,1,1.0
eigenvalues of k,1,1,1.0
of k in,1,1,1.0
k in decreasing,1,1,1.0
in decreasing order,1,1,1.0
decreasing order λ,1,1,1.0
order λ r,1,1,1.0
λ r and,1,1,1.0
r and p,1,1,1.0
and p is,1,1,1.0
p is a,2,1,2.0
is a unitary,2,1,2.0
a unitary matrix,2,1,2.0
unitary matrix that,1,1,1.0
matrix that consists,1,1,1.0
that consists of,1,1,1.0
consists of the,1,1,1.0
of the eigenvectors,1,1,1.0
the eigenvectors associated,1,1,1.0
eigenvectors associated to,1,1,1.0
associated to those,1,1,1.0
to those r,1,1,1.0
those r eigenvalues,1,1,1.0
r eigenvalues ur,1,1,1.0
eigenvalues ur constituting,1,1,1.0
ur constituting an,1,1,1.0
constituting an orthonormal,1,1,1.0
orthonormal basis of,2,1,2.0
basis of rr,1,1,1.0
of rr then,1,1,1.0
rr then the,1,1,1.0
then the empirical,1,1,1.0
kernel map is,1,1,1.0
map is deﬁned,1,1,1.0
is deﬁned as,3,2,1.5
deﬁned as φ,1,1,1.0
as φ e,1,1,1.0
e r xi,3,1,3.0
r xi λ,1,1,1.0
xi λ pt,1,1,1.0
λ pt k,2,1,2.0
pt k xi,1,1,1.0
k xi k,1,1,1.0
xi k xi,1,1,1.0
k xi xm,1,1,1.0
xi xm consider,1,1,1.0
xm consider the,1,1,1.0
the set φ,1,1,1.0
set φ e,1,1,1.0
e r φ,1,1,1.0
r φ e,1,1,1.0
e r xm,1,1,1.0
r xm of,1,1,1.0
xm of the,1,1,1.0
the efs images,2,1,2.0
efs images of,2,1,2.0
the training points,2,1,2.0
training points let,1,1,1.0
points let be,1,1,1.0
let be the,1,1,1.0
be the design,1,1,1.0
matrix storing φ,1,1,1.0
storing φ e,1,1,1.0
r xi as,1,1,1.0
xi as rows,1,1,1.0
as rows it,1,1,1.0
rows it is,1,1,1.0
it is easy,1,1,1.0
is easy to,1,1,1.0
easy to check,1,1,1.0
to check that,1,1,1.0
check that the,1,1,1.0
that the standard,2,1,2.0
the standard dot,1,1,1.0
standard dot product,1,1,1.0
dot product matrix,1,1,1.0
product matrix of,1,1,1.0
matrix of φ,1,1,1.0
of φ e,2,1,2.0
r xi i,1,1,1.0
xi i m,3,2,1.5
i m evaluated,1,1,1.0
m evaluated in,1,1,1.0
evaluated in e,1,1,1.0
in e is,1,1,1.0
e is k,1,1,1.0
is k writing,1,1,1.0
k writing z,1,1,1.0
writing z λ,1,1,1.0
z λ pt,1,1,1.0
pt k we,1,1,1.0
k we obtain,1,1,1.0
we obtain ztz,1,1,1.0
obtain ztz pλp,1,1,1.0
ztz pλp tpλ,1,1,1.0
pλp tpλ t,1,1,1.0
tpλ t since,1,1,1.0
t since the,1,1,1.0
since the distances,1,1,1.0
the distances and,1,1,1.0
distances and the,1,1,1.0
and the angles,1,1,1.0
the angles of,1,1,1.0
angles of the,1,1,1.0
of the m,2,2,1.0
the m vectors,1,1,1.0
m vectors φ,1,1,1.0
vectors φ xi,1,1,1.0
φ xi i,1,1,1.0
i m in,1,1,1.0
m in the,1,1,1.0
determined by the,1,1,1.0
by the note,1,1,1.0
the note that,1,1,1.0
note that p,1,1,1.0
that p is,1,1,1.0
unitary matrix and,1,1,1.0
matrix and k,1,1,1.0
and k a,1,1,1.0
k a symmetric,1,1,1.0
a symmetric dot,1,1,1.0
symmetric dot product,1,1,1.0
dot product xi,1,1,1.0
product xi xj,1,1,1.0
xi xj k,2,1,2.0
xj k xi,2,1,2.0
k xi xi,1,1,1.0
xi xi k,1,1,1.0
xi k xj,1,1,1.0
k xj xj,1,1,1.0
xj xj xi,1,1,1.0
xj xi xj,1,1,1.0
xi xj the,1,1,1.0
xj the training,1,1,1.0
training data have,1,1,1.0
data have the,1,1,1.0
the same geometrical,1,1,1.0
same geometrical ture,1,1,1.0
geometrical ture in,1,1,1.0
ture in both,1,1,1.0
in both spaces,1,1,1.0
both spaces f,1,1,1.0
spaces f and,1,1,1.0
f and however,1,1,1.0
and however recall,1,1,1.0
however recall that,1,1,1.0
recall that the,2,1,2.0
that the map,1,1,1.0
the map φ,1,1,1.0
map φ into,1,1,1.0
φ into the,1,1,1.0
into the feature,1,1,1.0
space is nonlinear,1,1,1.0
is nonlinear therefore,1,1,1.0
nonlinear therefore each,1,1,1.0
therefore each point,1,1,1.0
each point in,1,1,1.0
point in the,1,1,1.0
in the span,1,1,1.0
the span of,1,1,1.0
span of the,1,1,1.0
of the mapped,1,1,1.0
the mapped input,1,1,1.0
mapped input data,1,1,1.0
input data would,1,1,1.0
data would not,1,1,1.0
would not necessarily,1,1,1.0
not necessarily be,1,1,1.0
necessarily be the,1,1,1.0
be the image,1,1,1.0
the image of,1,1,1.0
image of some,1,1,1.0
of some input,1,1,1.0
some input pattern,1,1,1.0
input pattern this,1,1,1.0
pattern this is,1,1,1.0
this is known,1,1,1.0
as the preimage,1,1,1.0
the preimage problem,1,1,1.0
preimage problem this,1,1,1.0
problem this problem,1,1,1.0
this problem also,1,1,1.0
problem also appears,1,1,1.0
also appears when,1,1,1.0
appears when using,1,1,1.0
using the empirical,1,1,1.0
kernel map because,1,1,1.0
map because it,1,1,1.0
because it also,1,1,1.0
it also corresponds,1,1,1.0
also corresponds to,1,1,1.0
to a nonlinear,1,1,1.0
a nonlinear transformation,1,1,1.0
nonlinear transformation note,1,1,1.0
transformation note that,1,1,1.0
note that this,2,1,2.0
is not a,3,1,3.0
not a problem,2,1,2.0
a problem for,1,1,1.0
problem for the,1,1,1.0
for the of,1,1,1.0
the of minority,1,1,1.0
class since the,1,1,1.0
since the linear,1,1,1.0
the linear decision,1,1,1.0
linear decision boundary,1,1,1.0
boundary is built,1,1,1.0
is built in,1,1,1.0
built in the,1,1,1.0
space and if,1,1,1.0
and if the,1,1,1.0
if the classes,1,1,1.0
classes are almost,1,1,1.0
are almost linearly,1,1,1.0
almost linearly separable,1,1,1.0
linearly separable in,1,1,1.0
separable in the,1,1,1.0
feature space doing,1,1,1.0
space doing local,1,1,1.0
doing local convex,1,1,1.0
local convex combination,1,1,1.0
convex combination is,2,1,2.0
combination is reasonable,1,1,1.0
is reasonable whether,1,1,1.0
reasonable whether the,1,1,1.0
whether the of,1,1,1.0
the synthetic points,1,1,1.0
synthetic points exist,1,1,1.0
points exist or,1,1,1.0
exist or not,1,1,1.0
or not iii,1,1,1.0
not iii s,1,1,1.0
iii s y,1,1,1.0
s y n,2,2,1.0
y n t,1,1,1.0
n t h,2,1,2.0
t h e,3,1,3.0
h e t,1,1,1.0
e t i,1,1,1.0
t i c,1,1,1.0
i c ov,1,1,1.0
c ov e,1,1,1.0
ov e r,3,1,3.0
e r a,3,1,3.0
r a m,4,1,4.0
a m p,3,1,3.0
m p l,3,1,3.0
p l i,3,1,3.0
l i n,3,1,3.0
i n g,5,1,5.0
n g b,1,1,1.0
g b y,1,1,1.0
b y c,1,1,1.0
y c o,1,1,1.0
c o n,2,1,2.0
o n v,1,1,1.0
n v e,1,1,1.0
v e x,1,1,1.0
e x c,1,1,1.0
x c o,1,1,1.0
c o m,1,1,1.0
o m b,1,1,1.0
m b i,1,1,1.0
b i nat,1,1,1.0
i nat i,1,1,1.0
nat i o,1,1,1.0
o n i,1,1,1.0
n i n,2,1,2.0
h e efs,1,1,1.0
e efs the,1,1,1.0
efs the main,1,1,1.0
the main hypothesis,1,1,1.0
main hypothesis in,1,1,1.0
hypothesis in this,1,1,1.0
section is that,1,1,1.0
that the efs,4,1,4.0
the efs provide,1,1,1.0
efs provide us,1,1,1.0
provide us with,2,1,2.0
us with a,2,1,2.0
with a more,2,1,2.0
more suitable class,1,1,1.0
suitable class distribution,1,1,1.0
distribution for it,1,1,1.0
for it is,1,1,1.0
clear that when,1,1,1.0
that when classes,1,1,1.0
when classes are,1,1,1.0
classes are nonlinearly,1,1,1.0
are nonlinearly separable,1,1,1.0
nonlinearly separable which,1,1,1.0
separable which may,1,1,1.0
which may occur,1,1,1.0
may occur in,1,1,1.0
occur in the,1,1,1.0
input space one,1,1,1.0
space one should,1,1,1.0
one should be,1,1,1.0
should be very,1,1,1.0
be very careful,1,1,1.0
very careful when,1,1,1.0
careful when creating,1,1,1.0
when creating synthetic,1,1,1.0
creating synthetic patterns,1,1,1.0
synthetic patterns by,1,1,1.0
patterns by convex,1,1,1.0
combination because these,1,1,1.0
because these could,1,1,1.0
these could lie,1,1,1.0
could lie on,1,1,1.0
class region however,1,1,1.0
region however if,1,1,1.0
however if the,1,1,1.0
if the data,2,2,1.0
data are linearly,1,1,1.0
are linearly separable,1,1,1.0
linearly separable a,1,1,1.0
separable a statement,1,1,1.0
a statement that,1,1,1.0
statement that will,1,1,1.0
that will be,1,1,1.0
will be true,1,1,1.0
be true if,1,1,1.0
true if the,1,1,1.0
the underlying learning,1,1,1.0
underlying learning problem,1,1,1.0
learning problem by,1,1,1.0
problem by convex,1,1,1.0
of patterns is,1,1,1.0
patterns is not,1,1,1.0
a problem t,1,1,1.0
problem t o,2,1,2.0
t o illustrate,1,1,1.0
o illustrate this,1,1,1.0
illustrate this consider,1,1,1.0
this consider fig,1,1,1.0
consider fig where,1,1,1.0
fig where a,1,1,1.0
where a toy,1,1,1.0
a toy nonlinearly,1,1,1.0
toy nonlinearly separable,1,1,1.0
nonlinearly separable dataset,1,1,1.0
separable dataset have,1,1,1.0
dataset have been,1,1,1.0
have been represented,1,1,1.0
been represented by,1,1,1.0
represented by the,1,1,1.0
by the φ,1,1,1.0
the φ e,2,1,2.0
φ e transformation,1,1,1.0
e transformation using,1,1,1.0
transformation using a,1,1,1.0
using a gaussian,1,1,1.0
a gaussian kernel,1,1,1.0
gaussian kernel retaining,1,1,1.0
kernel retaining only,1,1,1.0
retaining only two,1,1,1.0
only two dominant,1,1,1.0
two dominant dimensions,1,1,1.0
dominant dimensions imbalanced,1,1,1.0
dimensions imbalanced donut,1,1,1.0
imbalanced donut toy,1,1,1.0
donut toy dataset,1,1,1.0
toy dataset separable,1,1,1.0
dataset separable projection,1,1,1.0
separable projection of,1,1,1.0
projection of the,1,1,1.0
the imbalanced toy,1,1,1.0
imbalanced toy data,1,1,1.0
toy data by,1,1,1.0
by the empirical,1,1,1.0
kernel map linearly,1,1,1.0
map linearly separable,1,1,1.0
linearly separable fig,1,1,1.0
separable fig synthethic,1,1,1.0
fig synthethic dataset,1,1,1.0
synthethic dataset representing,1,1,1.0
dataset representing a,1,1,1.0
representing a linearly,1,1,1.0
a linearly separable,1,1,1.0
linearly separable classiﬁcation,1,1,1.0
separable classiﬁcation problem,1,1,1.0
problem and their,1,1,1.0
and their tion,1,1,1.0
their tion to,1,1,1.0
tion to the,1,1,1.0
to the dominant,2,1,2.0
the dominant dimensions,2,1,2.0
dominant dimensions of,3,1,3.0
dimensions of the,3,1,3.0
efs e induced,1,1,1.0
e induced by,1,1,1.0
by the gaussian,2,1,2.0
the gaussian kernel,4,1,4.0
gaussian kernel function,2,1,2.0
kernel function linearly,2,1,2.0
function linearly separable,2,1,2.0
linearly separable problem,2,1,2.0
separable problem reduced,1,1,1.0
problem reduced empirical,1,1,1.0
reduced empirical feature,3,1,3.0
feature space in,5,1,5.0
space in this,1,1,1.0
in this subsection,4,1,4.0
this subsection we,2,1,2.0
subsection we present,1,1,1.0
we present a,2,2,1.0
present a reduced,1,1,1.0
a reduced version,1,1,1.0
reduced version of,1,1,1.0
the efs where,1,1,1.0
efs where we,1,1,1.0
where we select,1,1,1.0
we select the,2,1,2.0
select the q,1,1,1.0
the q q,1,1,1.0
q q r,3,1,3.0
q r dominant,1,1,1.0
r dominant dimensions,1,1,1.0
dominant dimensions to,1,1,1.0
dimensions to approximate,1,1,1.0
approximate the kernel,1,1,1.0
kernel matrix in,2,2,1.0
matrix in relation,1,1,1.0
relation to classiﬁcation,1,1,1.0
to classiﬁcation it,1,1,1.0
classiﬁcation it has,1,1,1.0
has been argued,1,1,1.0
been argued that,1,1,1.0
argued that most,1,1,1.0
that most decisive,1,1,1.0
most decisive information,1,1,1.0
decisive information can,1,1,1.0
information can be,1,1,1.0
can be contained,1,1,1.0
be contained in,1,1,1.0
contained in a,1,1,1.0
in a subspace,1,1,1.0
a subspace of,1,1,1.0
subspace of the,1,1,1.0
feature space under,1,1,1.0
space under the,1,1,1.0
under the assumption,1,1,1.0
the assumption of,1,1,1.0
assumption of smooth,1,1,1.0
of smooth kernels,1,1,1.0
smooth kernels matching,1,1,1.0
kernels matching the,1,1,1.0
matching the underlying,1,1,1.0
underlying problem however,1,1,1.0
problem however for,1,1,1.0
however for the,2,1,2.0
case of svms,1,1,1.0
of svms the,1,1,1.0
svms the capacity,1,1,1.0
the capacity control,1,1,1.0
capacity control inclusion,1,1,1.0
control inclusion of,1,1,1.0
inclusion of slacks,1,1,1.0
of slacks variables,1,1,1.0
slacks variables and,1,1,1.0
variables and dimensions,1,1,1.0
and dimensions associated,1,1,1.0
dimensions associated with,1,1,1.0
with the highest,1,1,1.0
the highest eigenvalues,1,1,1.0
highest eigenvalues of,1,1,1.0
eigenvalues of the,1,1,1.0
of the gram,1,1,1.0
the gram matrix,1,1,1.0
gram matrix parameter,1,1,1.0
matrix parameter for,1,1,1.0
parameter for preventing,1,1,1.0
for preventing is,1,1,1.0
preventing is alent,1,1,1.0
is alent to,1,1,1.0
alent to some,1,1,1.0
to some form,1,1,1.0
some form of,1,1,1.0
form of regularisation,1,1,1.0
of regularisation so,1,1,1.0
regularisation so that,1,1,1.0
so that denoising,1,1,1.0
that denoising is,1,1,1.0
denoising is not,1,1,1.0
is not necessary,1,1,1.0
not necessary although,1,1,1.0
necessary although it,1,1,1.0
although it could,1,1,1.0
could be very,1,1,1.0
be very useful,1,1,1.0
very useful for,1,1,1.0
useful for unregularised,1,1,1.0
for unregularised methods,1,1,1.0
unregularised methods in,1,1,1.0
section we test,1,1,1.0
we test whether,1,1,1.0
test whether a,1,1,1.0
whether a minority,1,1,1.0
in the reduced,5,1,5.0
the reduced dimensionality,1,1,1.0
reduced dimensionality efs,2,1,2.0
dimensionality efs as,1,1,1.0
efs as opposed,1,1,1.0
opposed to in,1,1,1.0
to in the,1,1,1.0
the full efs,1,1,1.0
full efs can,1,1,1.0
efs can be,1,1,1.0
can be beneﬁcial,2,1,2.0
be beneﬁcial one,1,1,1.0
beneﬁcial one motivation,1,1,1.0
one motivation for,1,1,1.0
motivation for in,1,1,1.0
for in reduced,1,1,1.0
in reduced dimensionality,1,1,1.0
dimensionality efs is,1,1,1.0
efs is that,1,1,1.0
that the procedure,1,1,1.0
the procedure relies,1,1,1.0
procedure relies on,1,1,1.0
relies on distances,2,1,2.0
on distances in,1,1,1.0
distances in the,1,1,1.0
the efs to,1,1,1.0
efs to perform,1,1,1.0
to perform the,1,1,1.0
perform the neighbourhood,1,1,1.0
the neighbourhood analysis,1,1,1.0
neighbourhood analysis roughly,1,1,1.0
analysis roughly speaking,1,1,1.0
roughly speaking these,1,1,1.0
speaking these distances,1,1,1.0
these distances have,1,1,1.0
distances have been,1,1,1.0
have been proven,1,1,1.0
been proven to,2,1,2.0
proven to be,1,1,1.0
to be misleading,1,1,1.0
be misleading as,1,1,1.0
misleading as the,1,1,1.0
as the data,1,1,1.0
the data dimensionality,1,1,1.0
data dimensionality increases,1,1,1.0
dimensionality increases making,1,1,1.0
increases making more,1,1,1.0
making more probable,1,1,1.0
more probable that,1,1,1.0
probable that the,1,1,1.0
that the neighbours,1,1,1.0
the neighbours are,1,1,1.0
neighbours are chosen,1,1,1.0
are chosen in,1,1,1.0
chosen in a,1,1,1.0
in a random,1,1,1.0
a random fashion,1,1,1.0
random fashion it,1,1,1.0
fashion it is,1,1,1.0
it is that,1,1,1.0
for any real,1,1,1.0
any real symmetric,1,1,1.0
real symmetric m,1,1,1.0
symmetric m m,1,1,1.0
m m matrix,1,1,1.0
m matrix k,1,1,1.0
rank r we,1,1,1.0
r we can,1,1,1.0
we can ﬁnd,1,1,1.0
can ﬁnd its,1,1,1.0
ﬁnd its real,1,1,1.0
its real nonzero,1,1,1.0
real nonzero eigenvalues,1,1,1.0
nonzero eigenvalues λr,1,1,1.0
eigenvalues λr and,1,1,1.0
λr and the,1,1,1.0
and the corresponding,2,2,1.0
the corresponding orthonormal,1,1,1.0
corresponding orthonormal eigenvectors,1,1,1.0
orthonormal eigenvectors ur,1,1,1.0
eigenvectors ur so,1,1,1.0
ur so that,1,1,1.0
so that k,1,1,1.0
that k r,1,1,1.0
k r λiuiut,1,1,1.0
r λiuiut i,1,1,1.0
λiuiut i in,2,1,2.0
i in this,1,1,1.0
this case the,5,1,5.0
case the best,1,1,1.0
the best q,1,1,1.0
best q r,1,1,1.0
q r approximation,1,1,1.0
r approximation to,1,1,1.0
approximation to k,1,1,1.0
to k is,1,1,1.0
k is kq,1,1,1.0
is kq q,1,1,1.0
kq q λiuiut,1,1,1.0
q λiuiut i,1,1,1.0
in the sense,2,1,2.0
the sense that,2,1,2.0
sense that it,2,1,2.0
that it minimises,1,1,1.0
it minimises f,1,1,1.0
minimises f over,1,1,1.0
f over all,1,1,1.0
over all q,1,1,1.0
all q matrices,1,1,1.0
q matrices where,1,1,1.0
matrices where f,1,1,1.0
where f denotes,1,1,1.0
f denotes the,1,1,1.0
denotes the frobenius,1,1,1.0
the frobenius norm,1,1,1.0
frobenius norm this,1,1,1.0
norm this concept,1,1,1.0
this concept can,1,1,1.0
concept can be,1,1,1.0
be said to,2,1,2.0
be the main,1,1,1.0
the main idea,1,1,1.0
main idea for,1,1,1.0
idea for the,1,1,1.0
for the reduced,1,1,1.0
the reduced efs,3,1,3.0
reduced efs instead,1,1,1.0
instead of working,1,1,1.0
of working in,1,1,1.0
working in the,1,1,1.0
efs e we,1,1,1.0
e we can,1,1,1.0
we can operate,1,1,1.0
can operate in,1,1,1.0
operate in its,1,1,1.0
in its lower,1,1,1.0
its lower dimensional,1,1,1.0
lower dimensional subspace,1,1,1.0
dimensional subspace e,1,1,1.0
subspace e q,1,1,1.0
e q where,1,1,1.0
q where the,1,1,1.0
where the kernel,1,1,1.0
kernel matrix has,1,1,1.0
matrix has the,1,1,1.0
has the form,1,1,1.0
the form k,1,1,1.0
form k q,1,1,1.0
k q p,1,1,1.0
q p q,2,1,2.0
p q λ,1,1,1.0
q λ q,1,1,1.0
λ q p,1,1,1.0
p q t,1,1,1.0
q t q,1,1,1.0
t q r,1,1,1.0
q r where,2,1,2.0
r where p,1,1,1.0
where p q,1,1,1.0
p q and,1,1,1.0
q and λ,1,1,1.0
and λ q,1,1,1.0
λ q consist,1,1,1.0
q consist of,1,1,1.0
consist of the,1,1,1.0
of the ﬁrst,2,1,2.0
the ﬁrst q,1,1,1.0
ﬁrst q columns,1,1,1.0
q columns of,1,1,1.0
columns of p,1,1,1.0
of p and,1,1,1.0
p and λ,1,1,1.0
and λ respectively,1,1,1.0
λ respectively consider,1,1,1.0
respectively consider the,1,1,1.0
consider the preimage,1,1,1.0
the preimage f,1,1,1.0
preimage f q,1,1,1.0
f q of,1,1,1.0
q of e,1,1,1.0
of e q,1,1,1.0
e q under,1,1,1.0
q under the,1,1,1.0
under the isomorphism,1,1,1.0
the isomorphism ϕ,2,1,2.0
isomorphism ϕ let,1,1,1.0
ϕ let uj,1,1,1.0
let uj q,1,1,1.0
uj q be,1,1,1.0
q be an,1,1,1.0
be an orthonormal,1,1,1.0
basis of f,1,1,1.0
of f q,1,1,1.0
f q given,1,1,1.0
q given v,1,1,1.0
given v f,1,1,1.0
v f its,1,1,1.0
f its projection,1,1,1.0
its projection onto,1,1,1.0
projection onto f,1,1,1.0
onto f q,1,1,1.0
f q is,1,1,1.0
q is obtained,1,1,1.0
is obtained as,1,1,1.0
obtained as uj,1,1,1.0
as uj q,1,1,1.0
uj q the,1,1,1.0
q the isomorphism,1,1,1.0
isomorphism ϕ from,1,1,1.0
ϕ from f,1,1,1.0
from f to,1,1,1.0
f to e,1,1,1.0
to e carries,1,1,1.0
e carries the,1,1,1.0
carries the structure,1,1,1.0
the structure over,1,1,1.0
structure over ϕ,1,1,1.0
over ϕ v,1,1,1.0
ϕ v e,1,1,1.0
v e is,1,1,1.0
e is projected,1,1,1.0
is projected onto,1,1,1.0
projected onto e,1,1,1.0
onto e q,1,1,1.0
e q as,1,1,1.0
q as v,1,1,1.0
as v ϕ,1,1,1.0
v ϕ uj,2,1,2.0
ϕ uj q,1,1,1.0
uj q moreover,1,1,1.0
q moreover for,1,1,1.0
moreover for all,1,1,1.0
for all j,1,1,1.0
all j q,1,1,1.0
j q uj,1,1,1.0
q uj v,1,1,1.0
uj v ϕ,1,1,1.0
ϕ uj therefore,1,1,1.0
uj therefore we,1,1,1.0
therefore we could,1,1,1.0
we could deﬁne,1,1,1.0
could deﬁne the,1,1,1.0
deﬁne the kernel,1,1,1.0
the kernel associated,1,1,1.0
kernel associated with,1,1,1.0
with the reduced,1,1,1.0
reduced efs by,1,1,1.0
efs by k,1,1,1.0
by k q,1,1,1.0
k q xi,1,1,1.0
q xi xj,1,1,1.0
xi xj φ,1,1,1.0
xj φ e,2,1,2.0
φ e q,25,1,25.0
e q xi,10,1,10.0
q xi φ,4,1,4.0
xi φ e,6,1,6.0
e q xj,2,1,2.0
q xj e,1,1,1.0
xj e which,1,1,1.0
e which for,1,1,1.0
which for q,1,1,1.0
for q being,1,1,1.0
q being the,1,1,1.0
being the rank,1,1,1.0
the rank of,2,1,2.0
rank of k,1,1,1.0
of k will,1,1,1.0
k will correspond,1,1,1.0
will correspond to,2,1,2.0
correspond to synthetic,1,1,1.0
to synthetic minority,1,1,1.0
synthetic minority in,1,1,1.0
minority in the,1,1,1.0
the reduced or,2,1,2.0
reduced or rank,1,1,1.0
or rank efs,1,1,1.0
rank efs once,1,1,1.0
efs once that,1,1,1.0
once that the,1,1,1.0
that the notion,1,1,1.0
notion of efs,1,1,1.0
of efs has,1,1,1.0
efs has been,1,1,1.0
has been introduced,1,1,1.0
been introduced this,1,1,1.0
introduced this subsection,1,1,1.0
this subsection will,1,1,1.0
subsection will show,1,1,1.0
will show the,1,1,1.0
show the main,1,1,1.0
the main steps,1,1,1.0
main steps to,1,1,1.0
steps to extend,1,1,1.0
to extend a,1,1,1.0
extend a algorithm,1,1,1.0
a algorithm to,1,1,1.0
algorithm to this,1,1,1.0
to this space,1,1,1.0
this space concerning,1,1,1.0
space concerning the,1,1,1.0
concerning the training,1,1,1.0
the training phase,1,1,1.0
training phase the,1,1,1.0
phase the ﬁrst,1,1,1.0
the ﬁrst step,3,2,1.5
ﬁrst step of,1,1,1.0
step of the,1,1,1.0
of the proposed,5,2,2.5
the proposed methodology,1,1,1.0
proposed methodology corresponds,1,1,1.0
methodology corresponds to,1,1,1.0
to the computation,1,1,1.0
the computation of,2,1,2.0
computation of the,2,1,2.0
the training kernel,2,1,2.0
training kernel matrix,3,1,3.0
kernel matrix k,4,1,4.0
matrix k through,1,1,1.0
k through a,1,1,1.0
through a predeﬁned,1,1,1.0
a predeﬁned kernel,1,1,1.0
predeﬁned kernel function,1,1,1.0
kernel function then,1,1,1.0
function then the,1,1,1.0
then the reduced,1,1,1.0
reduced or empirical,1,1,1.0
or empirical kernel,1,1,1.0
kernel map φ,4,1,4.0
map φ e,4,1,4.0
e q q,2,1,2.0
q r can,1,1,1.0
can be computed,1,1,1.0
be computed via,1,1,1.0
computed via the,1,1,1.0
via the eigenvector,1,1,1.0
the eigenvector decomposition,1,1,1.0
eigenvector decomposition of,1,1,1.0
decomposition of this,1,1,1.0
of this training,1,1,1.0
this training kernel,1,1,1.0
matrix k eq,1,1,1.0
k eq as,1,1,1.0
eq as said,1,1,1.0
as said let,1,1,1.0
said let z,1,1,1.0
let z be,1,1,1.0
z be the,1,1,1.0
be the set,2,2,1.0
the set generated,1,1,1.0
set generated by,1,1,1.0
generated by applying,1,1,1.0
by applying the,1,1,1.0
applying the φ,1,1,1.0
e q transformation,1,1,1.0
q transformation to,1,1,1.0
transformation to the,2,1,2.0
the training patterns,4,1,4.0
training patterns and,1,1,1.0
patterns and the,2,1,2.0
and the design,2,1,2.0
points of z,1,1,1.0
of z as,1,1,1.0
z as rows,1,1,1.0
as rows in,1,1,1.0
rows in the,1,1,1.0
in the second,1,1,1.0
the second step,1,1,1.0
second step the,1,1,1.0
step the process,1,1,1.0
process is w,1,1,1.0
is w e,1,1,1.0
w e assume,1,1,1.0
e assume that,1,1,1.0
that the singular,1,1,1.0
the singular values,1,1,1.0
singular values are,1,1,1.0
values are performed,1,1,1.0
are performed over,1,1,1.0
performed over the,1,1,1.0
minority class images,1,1,1.0
class images of,1,1,1.0
images of this,1,1,1.0
of this z,1,1,1.0
this z matrix,1,1,1.0
z matrix resulting,1,1,1.0
matrix resulting in,1,1,1.0
resulting in the,1,1,1.0
generation of n,1,1,1.0
of n new,1,1,1.0
n new synthetic,1,1,1.0
new synthetic images,1,1,1.0
synthetic images arranged,1,1,1.0
images arranged in,1,1,1.0
arranged in the,1,1,1.0
in the set,1,1,1.0
set s and,1,1,1.0
s and the,1,1,1.0
design matrix more,1,1,1.0
matrix more speciﬁcally,1,1,1.0
more speciﬁcally as,1,1,1.0
speciﬁcally as the,1,1,1.0
as the standard,1,1,1.0
the standard smote,2,1,2.0
standard smote algorithm,1,1,1.0
smote algorithm has,1,1,1.0
algorithm has been,1,1,1.0
has been chosen,1,1,1.0
been chosen for,1,1,1.0
chosen for each,2,1,2.0
for each new,1,1,1.0
each new synthetic,1,1,1.0
new synthetic instance,1,1,1.0
synthetic instance will,1,1,1.0
instance will be,1,1,1.0
will be generated,2,2,1.0
generated using a,1,1,1.0
using a linear,1,1,1.0
a linear interpolation,1,1,1.0
linear interpolation between,1,1,1.0
interpolation between pattern,1,1,1.0
between pattern xi,1,1,1.0
pattern xi and,1,1,1.0
xi and one,1,1,1.0
minority class at,1,1,1.0
class at every,1,1,1.0
at every step,1,1,1.0
every step j,1,1,1.0
step j n,1,1,1.0
j n we,1,1,1.0
n we create,1,1,1.0
we create a,1,1,1.0
create a point,1,1,1.0
a point sj,1,1,1.0
point sj in,1,1,1.0
sj in e,1,1,1.0
in e q,1,1,1.0
e q by,1,1,1.0
q by picking,1,1,1.0
by picking at,1,1,1.0
picking at random,1,1,1.0
at random a,1,1,1.0
random a minority,1,1,1.0
minority class point,1,1,1.0
class point xi,1,1,1.0
point xi and,1,1,1.0
xi and calculating,1,1,1.0
and calculating sj,1,1,1.0
calculating sj φ,1,1,1.0
sj φ e,1,1,1.0
e q ˆ,5,1,5.0
q ˆ xi,5,1,5.0
ˆ xi φ,4,1,4.0
q xi δ,2,1,2.0
xi δ where,2,1,2.0
δ where φ,1,1,1.0
where φ e,1,1,1.0
ˆ xi is,2,1,2.0
xi is one,2,1,2.0
the neighbours for,1,1,1.0
neighbours for φ,1,1,1.0
for φ e,1,1,1.0
q xi in,2,1,2.0
xi in the,2,1,2.0
efs e q,2,1,2.0
e q and,3,1,3.0
q and δ,1,1,1.0
and δ is,1,1,1.0
δ is a,2,1,2.0
random number generated,2,1,2.0
number generated from,2,1,2.0
generated from the,2,1,2.0
from the uniform,2,1,2.0
the uniform distribution,3,1,3.0
uniform distribution u,2,1,2.0
distribution u for,1,1,1.0
u for simplicity,1,1,1.0
for simplicity we,2,1,2.0
simplicity we the,1,1,1.0
we the minority,1,1,1.0
minority class so,1,1,1.0
class so that,1,1,1.0
that the two,1,1,1.0
two classes become,1,1,1.0
classes become balanced,1,1,1.0
become balanced from,1,1,1.0
balanced from the,1,1,1.0
from the deﬁnition,1,1,1.0
the deﬁnition of,1,1,1.0
deﬁnition of the,1,1,1.0
the efs we,1,1,1.0
efs we know,1,1,1.0
we know that,1,1,1.0
know that ϕ,1,1,1.0
that ϕ sj,1,1,1.0
ϕ sj f,1,1,1.0
sj f q,1,1,1.0
f q the,1,1,1.0
q the representation,1,1,1.0
the representation of,2,1,2.0
of the new,2,1,2.0
the new pattern,1,1,1.0
new pattern in,1,1,1.0
pattern in the,1,1,1.0
feature space will,1,1,1.0
space will be,1,1,1.0
will be unique,1,1,1.0
be unique and,1,1,1.0
unique and will,1,1,1.0
and will lie,1,1,1.0
the line between,1,1,1.0
line between ϕ,1,1,1.0
between ϕ xi,1,1,1.0
ϕ xi and,1,1,1.0
xi and ϕ,1,1,1.0
and ϕ ˆ,1,1,1.0
ϕ ˆ xi,1,1,1.0
ˆ xi ϕ,1,1,1.0
xi ϕ is,1,1,1.0
ϕ is a,1,1,1.0
is a linear,1,1,1.0
a linear map,1,1,1.0
linear map recall,1,1,1.0
map recall that,1,1,1.0
that the norms,1,1,1.0
the norms and,1,1,1.0
norms and distances,1,1,1.0
and distances are,1,1,1.0
distances are preserved,1,1,1.0
are preserved e,1,1,1.0
preserved e q,1,1,1.0
q xi ˆ,1,1,1.0
xi ˆ xi,1,1,1.0
xi φ xi,2,1,2.0
φ xi and,1,1,1.0
xi and so,1,1,1.0
and so are,1,1,1.0
so are the,1,1,1.0
are the angles,1,1,1.0
the angles φ,1,1,1.0
angles φ e,1,1,1.0
ˆ xi t,1,1,1.0
xi t φ,1,1,1.0
t φ e,1,1,1.0
q xi sj,1,1,1.0
xi sj xi,1,1,1.0
sj xi φ,1,1,1.0
xi φ ˆ,1,1,1.0
φ ˆ xi,1,1,1.0
φ xi ϕ,1,1,1.0
xi ϕ sj,1,1,1.0
ϕ sj as,1,1,1.0
sj as a,1,1,1.0
as a consequence,1,1,1.0
a consequence if,1,1,1.0
consequence if φ,1,1,1.0
if φ e,1,1,1.0
the neighbours of,1,1,1.0
neighbours of φ,1,1,1.0
the efs this,1,1,1.0
efs this will,1,1,1.0
be so in,1,1,1.0
so in the,1,1,1.0
feature space as,1,1,1.0
space as well,1,1,1.0
as well the,1,1,1.0
well the third,1,1,1.0
the third step,1,1,1.0
third step is,1,1,1.0
step is the,1,1,1.0
is the execution,1,1,1.0
the execution of,1,1,1.0
execution of the,1,1,1.0
the learning machine,2,1,2.0
learning machine over,1,1,1.0
machine over the,1,1,1.0
over the set,1,1,1.0
the set ϕ,1,1,1.0
set ϕ z,1,1,1.0
ϕ z s,2,1,2.0
z s f,1,1,1.0
s f q,1,1,1.0
f q in,1,1,1.0
q in this,1,1,1.0
this case there,1,1,1.0
case there are,1,1,1.0
there are two,1,1,1.0
are two different,1,1,1.0
two different possibilities,1,1,1.0
different possibilities to,1,1,1.0
possibilities to consider,1,1,1.0
to consider first,1,1,1.0
consider first we,1,1,1.0
first we could,1,1,1.0
we could employ,1,1,1.0
could employ the,1,1,1.0
employ the efs,1,1,1.0
the efs as,1,1,1.0
efs as a,1,1,1.0
as a new,2,2,1.0
a new representation,1,1,1.0
new representation for,1,1,1.0
representation for the,1,1,1.0
for the data,2,1,2.0
data and use,2,2,1.0
and use the,3,2,1.5
use the classiﬁcation,1,1,1.0
the classiﬁcation algorithm,1,1,1.0
classiﬁcation algorithm in,1,1,1.0
algorithm in this,1,1,1.0
in this new,1,1,1.0
this new space,1,1,1.0
new space as,1,1,1.0
space as done,1,1,1.0
as done in,3,1,3.0
done in other,3,1,3.0
in other works,3,1,3.0
other works this,1,1,1.0
works this idea,1,1,1.0
this idea will,1,1,1.0
idea will provide,1,1,1.0
will provide us,1,1,1.0
a more easily,1,1,1.0
more easily separable,1,1,1.0
easily separable and,1,1,1.0
separable and balanced,1,1,1.0
and balanced space,1,1,1.0
balanced space than,1,1,1.0
space than the,1,1,1.0
input space which,1,1,1.0
space which could,1,1,1.0
which could indeed,1,1,1.0
could indeed be,1,1,1.0
indeed be used,1,1,1.0
be used for,5,1,5.0
used for any,2,1,2.0
for any learning,1,1,1.0
any learning machine,1,1,1.0
learning machine independently,1,1,1.0
machine independently of,1,1,1.0
independently of being,1,1,1.0
of being kernelized,1,1,1.0
being kernelized or,1,1,1.0
kernelized or not,1,1,1.0
or not however,1,1,1.0
not however when,1,1,1.0
however when dealing,1,1,1.0
dealing with a,1,1,1.0
with a kernel,1,1,1.0
kernel function it,1,1,1.0
function it could,1,1,1.0
it could actually,1,1,1.0
could actually be,1,1,1.0
actually be more,1,1,1.0
be more advisable,1,1,1.0
more advisable to,2,1,2.0
advisable to recompute,1,1,1.0
to recompute the,1,1,1.0
recompute the dot,1,1,1.0
dot products between,1,1,1.0
products between patterns,1,1,1.0
between patterns create,1,1,1.0
patterns create a,1,1,1.0
create a new,1,1,1.0
a new sampled,1,1,1.0
new sampled kernel,1,1,1.0
sampled kernel matrix,1,1,1.0
kernel matrix due,1,1,1.0
matrix due to,1,1,1.0
to the high,1,1,1.0
the high number,1,1,1.0
high number of,1,1,1.0
features the dimensionality,1,1,1.0
the efs which,1,1,1.0
efs which in,1,1,1.0
which in most,1,1,1.0
in most of,4,1,4.0
of the cases,3,1,3.0
the cases will,1,1,1.0
cases will increase,1,1,1.0
increase the computational,1,1,1.0
the computational cost,2,1,2.0
computational cost of,2,1,2.0
cost of the,1,1,1.0
learning machine considered,1,1,1.0
machine considered t,1,1,1.0
considered t o,1,1,1.0
t o do,5,1,5.0
o do so,5,1,5.0
do so synthetic,1,1,1.0
so synthetic samples,1,1,1.0
synthetic samples will,1,1,1.0
samples will be,1,1,1.0
will be used,2,1,2.0
used to complete,1,1,1.0
to complete the,2,1,2.0
complete the kernel,1,1,1.0
kernel matrix by,2,1,2.0
matrix by obtaining,1,1,1.0
by obtaining their,1,1,1.0
obtaining their dot,1,1,1.0
their dot product,1,1,1.0
product in the,1,1,1.0
the efs with,1,1,1.0
efs with respect,1,1,1.0
to the rest,2,1,2.0
the rest of,3,2,1.5
rest of the,1,1,1.0
training patterns using,1,1,1.0
patterns using this,1,1,1.0
using this approach,1,1,1.0
this approach the,1,1,1.0
approach the sampled,1,1,1.0
the sampled training,1,1,1.0
sampled training gram,1,1,1.0
training gram matrix,1,1,1.0
gram matrix will,2,1,2.0
will be composed,2,1,2.0
be composed as,2,1,2.0
composed as follows,2,1,2.0
as follows z,1,1,1.0
follows z zt,1,1,1.0
z zt z,1,1,1.0
zt z st,1,1,1.0
z st s,1,1,1.0
st s zt,1,1,1.0
s zt s,1,1,1.0
zt s st,1,1,1.0
s st note,1,1,1.0
st note that,1,1,1.0
note that for,4,1,4.0
for any number,1,1,1.0
any number of,1,1,1.0
number of dominant,3,1,3.0
of dominant dimensions,4,1,4.0
dominant dimensions q,1,1,1.0
dimensions q for,1,1,1.0
q for the,1,1,1.0
for the empirical,1,1,1.0
e q the,1,1,1.0
q the kernel,1,1,1.0
kernel matrix obtained,2,1,2.0
matrix obtained will,1,1,1.0
obtained will be,1,1,1.0
will be positive,1,1,1.0
be positive semideﬁnite,1,1,1.0
positive semideﬁnite furthermore,1,1,1.0
semideﬁnite furthermore since,1,1,1.0
furthermore since we,1,1,1.0
since we are,3,2,1.5
we are generating,1,1,1.0
are generating new,1,1,1.0
generating new patterns,1,1,1.0
new patterns by,1,1,1.0
patterns by a,1,1,1.0
by a linear,1,1,1.0
a linear combination,1,1,1.0
linear combination of,1,1,1.0
combination of other,1,1,1.0
of other patterns,1,1,1.0
other patterns in,1,1,1.0
the dataset the,1,1,1.0
dataset the empirical,1,1,1.0
empirical kernel maps,1,1,1.0
kernel maps associated,1,1,1.0
maps associated to,1,1,1.0
associated to ϕ,1,1,1.0
to ϕ z,2,1,2.0
ϕ z and,1,1,1.0
z and to,1,1,1.0
and to ϕ,1,1,1.0
z s can,1,1,1.0
s can be,1,1,1.0
to be equivalent,1,1,1.0
be equivalent for,1,1,1.0
equivalent for the,1,1,1.0
for the generalisation,2,1,2.0
the generalisation phase,1,1,1.0
generalisation phase the,1,1,1.0
phase the same,1,1,1.0
the same steps,1,1,1.0
same steps are,1,1,1.0
steps are considered,1,1,1.0
are considered to,1,1,1.0
considered to complete,1,1,1.0
complete the test,2,1,2.0
the test kernel,2,1,2.0
test kernel matrix,2,1,2.0
kernel matrix considering,1,1,1.0
matrix considering that,1,1,1.0
considering that the,1,1,1.0
of the test,2,1,2.0
the test patterns,2,1,2.0
test patterns are,1,1,1.0
patterns are derived,1,1,1.0
are derived using,1,1,1.0
derived using the,1,1,1.0
using the same,1,1,1.0
the same φ,1,1,1.0
same φ e,1,1,1.0
e q map,1,1,1.0
q map considering,1,1,1.0
map considering only,1,1,1.0
considering only the,1,1,1.0
only the training,1,1,1.0
training data note,1,1,1.0
data note that,2,1,2.0
note that in,2,1,2.0
that in this,2,1,2.0
this case we,2,2,1.0
case we will,1,1,1.0
we will compute,1,1,1.0
will compute the,1,1,1.0
compute the dot,1,1,1.0
dot product between,3,1,3.0
product between train,1,1,1.0
between train and,1,1,1.0
train and test,1,1,1.0
and test patterns,1,1,1.0
test patterns and,2,1,2.0
patterns and between,1,1,1.0
and between test,1,1,1.0
between test and,1,1,1.0
test and synthetic,1,1,1.0
and synthetic patterns,1,1,1.0
patterns the sampled,1,1,1.0
the sampled test,1,1,1.0
sampled test gram,1,1,1.0
test gram matrix,1,1,1.0
as follows t,1,1,1.0
follows t z,1,1,1.0
t z tt,1,1,1.0
z tt s,1,1,1.0
tt s tt,1,1,1.0
s tt where,1,1,1.0
tt where t,1,1,1.0
is the representation,1,1,1.0
the representation in,1,1,1.0
representation in the,1,1,1.0
the efs of,1,1,1.0
efs of the,1,1,1.0
patterns and t,1,1,1.0
and t corresponds,1,1,1.0
t corresponds to,1,1,1.0
number of test,1,1,1.0
of test patterns,1,1,1.0
test patterns note,1,1,1.0
note that these,1,1,1.0
that these new,1,1,1.0
these new kernel,1,1,1.0
new kernel matrices,1,1,1.0
kernel matrices and,1,1,1.0
matrices and can,1,1,1.0
for any algorithm,1,1,1.0
any algorithm a,1,1,1.0
algorithm a summary,1,1,1.0
a summary of,1,1,1.0
summary of this,1,1,1.0
of this method,2,2,1.0
be seen in,4,1,4.0
seen in fig,2,1,2.0
in fig algorithm,1,1,1.0
fig algorithm synthetic,1,1,1.0
algorithm synthetic in,1,1,1.0
in the empirical,7,1,7.0
feature space input,1,1,1.0
space input training,1,1,1.0
input training patterns,1,1,1.0
training patterns xtr,1,1,1.0
patterns xtr training,1,1,1.0
xtr training targets,1,1,1.0
training targets ytr,1,1,1.0
targets ytr and,1,1,1.0
ytr and testing,1,1,1.0
and testing patterns,1,1,1.0
testing patterns xts,1,1,1.0
patterns xts output,1,1,1.0
xts output t,1,1,1.0
output t esting,1,1,1.0
t esting targets,1,1,1.0
esting targets yts,1,1,1.0
targets yts compute,1,1,1.0
yts compute kernel,1,1,1.0
compute kernel matrix,1,1,1.0
kernel matrix ktr,1,1,1.0
matrix ktr for,1,1,1.0
ktr for training,1,1,1.0
for training patterns,1,1,1.0
training patterns compute,1,1,1.0
patterns compute the,1,1,1.0
compute the empirical,1,1,1.0
e q via,1,1,1.0
q via ktr,1,1,1.0
via ktr map,1,1,1.0
ktr map training,1,1,1.0
map training patterns,1,1,1.0
training patterns to,1,1,1.0
to the efs,2,1,2.0
the efs using,2,1,2.0
efs using φ,2,1,2.0
using φ e,2,1,2.0
q and obtain,2,1,2.0
and obtain their,2,1,2.0
obtain their new,2,1,2.0
their new tation,2,1,2.0
new tation z,1,1,1.0
tation z generate,1,1,1.0
z generate synthetic,1,1,1.0
generate synthetic patterns,1,1,1.0
synthetic patterns s,1,1,1.0
patterns s using,1,1,1.0
s using the,1,1,1.0
using the new,1,1,1.0
the new representation,1,1,1.0
new representation z,1,1,1.0
representation z of,1,1,1.0
z of the,1,1,1.0
training patterns complete,1,1,1.0
patterns complete the,1,1,1.0
complete the train,1,1,1.0
the train kernel,1,1,1.0
train kernel matrix,1,1,1.0
kernel matrix with,4,1,4.0
matrix with the,3,1,3.0
with the dot,2,1,2.0
product between patterns,2,1,2.0
between patterns eq,2,1,2.0
patterns eq train,1,1,1.0
eq train the,1,1,1.0
train the learning,1,1,1.0
learning algorithm with,1,1,1.0
algorithm with kernel,1,1,1.0
with kernel matrix,1,1,1.0
matrix and obtain,1,1,1.0
and obtain a,1,1,1.0
obtain a plane,1,1,1.0
a plane w,1,1,1.0
plane w and,1,1,1.0
a bias term,1,1,1.0
bias term b,1,1,1.0
term b map,1,1,1.0
b map testing,1,1,1.0
map testing patterns,1,1,1.0
testing patterns to,1,1,1.0
new tation complete,1,1,1.0
tation complete the,1,1,1.0
patterns eq predict,1,1,1.0
eq predict yts,1,1,1.0
predict yts using,1,1,1.0
yts using and,1,1,1.0
and the model,1,1,1.0
the model w,1,1,1.0
model w b,1,1,1.0
w b eq,1,1,1.0
b eq fig,1,1,1.0
eq fig different,1,1,1.0
fig different steps,1,1,1.0
different steps for,1,1,1.0
steps for the,1,1,1.0
for the kernel,1,1,1.0
the kernel algorithm,1,1,1.0
kernel algorithm as,1,1,1.0
algorithm as mentioned,1,1,1.0
as mentioned before,1,1,1.0
mentioned before our,1,1,1.0
before our points,1,1,1.0
our points in,1,1,1.0
points in the,4,2,2.0
feature space may,1,1,1.0
space may not,1,1,1.0
may not have,1,1,1.0
not have preimages,1,1,1.0
have preimages in,1,1,1.0
preimages in the,1,1,1.0
however this does,1,1,1.0
this does not,1,1,1.0
does not pose,1,1,1.0
not pose a,1,1,1.0
pose a methodological,1,1,1.0
a methodological problem,1,1,1.0
methodological problem since,1,1,1.0
problem since the,1,1,1.0
since the class,1,1,1.0
class separation is,1,1,1.0
separation is formulated,1,1,1.0
is formulated in,1,1,1.0
formulated in the,2,2,1.0
feature space iv,1,1,1.0
space iv o,1,1,1.0
iv o p,1,1,1.0
o p t,1,1,1.0
p t i,1,1,1.0
t i m,1,1,1.0
i m i,1,1,1.0
m i s,1,1,1.0
i s i,1,1,1.0
s i n,1,1,1.0
n g t,1,1,1.0
g t h,1,1,1.0
h e f,1,1,1.0
e f e,2,1,2.0
f e at,1,1,1.0
e at u,1,1,1.0
at u r,1,1,1.0
u r e,1,1,1.0
r e s,2,1,2.0
e s pac,1,1,1.0
s pac e,1,1,1.0
pac e b,1,1,1.0
e b y,1,1,1.0
b y k,1,1,1.0
y k e,1,1,1.0
k e r,1,1,1.0
e r n,1,1,1.0
r n e,1,1,1.0
n e l,1,1,1.0
e l l,1,1,1.0
l l e,1,1,1.0
l e a,1,1,1.0
e a r,1,1,1.0
a r n,1,1,1.0
r n i,1,1,1.0
n g f,1,1,1.0
g f o,1,1,1.0
f o r,2,1,2.0
o r ov,1,1,1.0
r ov e,1,1,1.0
n g as,2,1,2.0
g as stated,2,1,2.0
as stated before,4,1,4.0
stated before our,1,1,1.0
before our ﬁrst,1,1,1.0
our ﬁrst hypothesis,1,1,1.0
ﬁrst hypothesis was,1,1,1.0
hypothesis was that,1,1,1.0
was that in,1,1,1.0
that in the,3,1,3.0
the efs was,2,1,2.0
efs was more,1,1,1.0
was more advisable,1,1,1.0
more advisable if,1,1,1.0
advisable if the,1,1,1.0
kernel function matched,1,1,1.0
function matched the,1,1,1.0
matched the underlying,1,1,1.0
underlying problem in,1,1,1.0
problem in the,1,1,1.0
it can asymptotically,1,1,1.0
can asymptotically represent,1,1,1.0
asymptotically represent the,1,1,1.0
represent the function,1,1,1.0
the function to,1,1,1.0
function to be,1,1,1.0
to be learned,1,1,1.0
be learned and,1,1,1.0
learned and is,1,1,1.0
and is sufﬁciently,1,1,1.0
is sufﬁciently smooth,1,1,1.0
sufﬁciently smooth in,1,1,1.0
smooth in this,1,1,1.0
section we propose,2,1,2.0
we propose a,2,1,2.0
propose a method,1,1,1.0
a method for,1,1,1.0
method for kernel,1,1,1.0
kernel learning that,1,1,1.0
learning that would,1,1,1.0
that would ideally,1,1,1.0
would ideally provide,1,1,1.0
ideally provide a,1,1,1.0
provide a clearer,1,1,1.0
a clearer class,1,1,1.0
clearer class separation,1,1,1.0
class separation in,1,1,1.0
separation in the,1,1,1.0
space to analyse,1,1,1.0
to analyse its,1,1,1.0
analyse its effect,1,1,1.0
its effect in,1,1,1.0
effect in the,1,1,1.0
in the method,2,2,1.0
the method ideally,1,1,1.0
method ideally we,1,1,1.0
ideally we would,1,1,1.0
we would like,2,2,1.0
would like to,4,2,2.0
like to ﬁnd,1,1,1.0
to ﬁnd the,4,2,2.0
ﬁnd the kernel,1,1,1.0
minimises the true,1,1,1.0
the true risk,1,1,1.0
true risk of,1,1,1.0
risk of a,1,1,1.0
of a classiﬁer,1,1,1.0
a classiﬁer for,3,2,1.5
classiﬁer for a,1,1,1.0
for a speciﬁc,1,1,1.0
a speciﬁc dataset,1,1,1.0
speciﬁc dataset unfortunately,1,1,1.0
dataset unfortunately the,1,1,1.0
unfortunately the risk,1,1,1.0
the risk is,1,1,1.0
risk is not,1,1,1.0
is not accessible,1,1,1.0
not accessible therefore,1,1,1.0
accessible therefore different,1,1,1.0
therefore different analytical,1,1,1.0
different analytical bounds,1,1,1.0
analytical bounds for,1,1,1.0
bounds for the,2,1,2.0
the generalisation error,1,1,1.0
generalisation error have,1,1,1.0
error have been,1,1,1.0
been developed in,2,1,2.0
developed in the,2,1,2.0
learning literature with,1,1,1.0
literature with the,1,1,1.0
with the aim,1,1,1.0
aim of better,1,1,1.0
of better suiting,1,1,1.0
better suiting a,1,1,1.0
suiting a given,1,1,1.0
given dataset in,1,1,1.0
dataset in the,3,2,1.5
the kernel machine,2,1,2.0
kernel machine literature,1,1,1.0
machine literature a,1,1,1.0
literature a considerable,1,1,1.0
a considerable interest,1,1,1.0
considerable interest has,1,1,1.0
interest has been,1,1,1.0
has been devoted,1,1,1.0
been devoted to,1,1,1.0
devoted to learning,1,1,1.0
to learning the,1,1,1.0
learning the optimal,1,1,1.0
the optimal kernel,1,1,1.0
optimal kernel given,1,1,1.0
kernel given a,1,1,1.0
given a ular,1,1,1.0
a ular classiﬁcation,1,1,1.0
ular classiﬁcation task,1,1,1.0
classiﬁcation task as,1,1,1.0
task as opposed,1,1,1.0
opposed to imposing,1,1,1.0
to imposing them,1,1,1.0
imposing them one,1,1,1.0
them one of,1,1,1.0
of the prominent,1,1,1.0
the prominent approaches,1,1,1.0
prominent approaches in,1,1,1.0
approaches in kernel,1,1,1.0
in kernel learning,1,1,1.0
kernel learning is,1,1,1.0
learning is centred,1,1,1.0
is centred target,1,1,1.0
centred target alignment,1,1,1.0
target alignment kt,1,1,1.0
alignment kt a,1,1,1.0
kt a centred,1,1,1.0
a centred kt,1,1,1.0
centred kt a,4,1,4.0
kt a is,3,1,3.0
a is data,1,1,1.0
is data distribution,1,1,1.0
data distribution independent,1,1,1.0
distribution independent making,1,1,1.0
independent making it,1,1,1.0
making it particularly,1,1,1.0
it particularly suitable,1,1,1.0
particularly suitable for,1,1,1.0
suitable for imbalanced,1,1,1.0
for imbalanced classiﬁcation,1,1,1.0
imbalanced classiﬁcation note,1,1,1.0
classiﬁcation note that,1,1,1.0
note that kt,1,1,1.0
that kt a,2,1,2.0
a is related,1,1,1.0
to the fisher,2,1,2.0
the fisher criterion,2,1,2.0
fisher criterion which,1,1,1.0
criterion which maximises,1,1,1.0
distance between different,1,1,1.0
between different classes,1,1,1.0
classes and minimises,1,1,1.0
and minimises the,1,1,1.0
minimises the within,1,1,1.0
within class distance,1,1,1.0
class distance this,1,1,1.0
distance this can,1,1,1.0
can be a,1,1,1.0
be a useful,1,1,1.0
a useful property,1,1,1.0
useful property of,1,1,1.0
property of the,1,1,1.0
space in which,1,1,1.0
in which to,1,1,1.0
which to perform,1,1,1.0
to perform class,1,1,1.0
perform class minority,1,1,1.0
class minority patterns,1,1,1.0
minority patterns would,1,1,1.0
patterns would be,1,1,1.0
would be far,1,1,1.0
be far from,1,1,1.0
class region and,1,1,1.0
region and closely,1,1,1.0
and closely clustered,1,1,1.0
closely clustered together,1,1,1.0
clustered together kt,1,1,1.0
together kt a,1,1,1.0
kt a optimises,1,1,1.0
a optimises the,1,1,1.0
optimises the kernel,1,1,1.0
the kernel by,1,1,1.0
kernel by aligning,1,1,1.0
by aligning it,1,1,1.0
aligning it to,1,1,1.0
it to the,1,1,1.0
to the ideal,1,1,1.0
the ideal kernel,3,1,3.0
ideal kernel matrix,3,1,3.0
kernel matrix ki,2,1,2.0
matrix ki which,1,1,1.0
ki which will,1,1,1.0
which will submit,1,1,1.0
will submit the,1,1,1.0
submit the structure,1,1,1.0
the structure ki,1,1,1.0
structure ki xi,1,1,1.0
ki xi xj,1,1,1.0
xi xj if,1,1,1.0
xj if yi,1,1,1.0
if yi yj,1,1,1.0
yi yj otherwise,1,1,1.0
yj otherwise where,1,1,1.0
otherwise where yi,1,1,1.0
where yi is,1,1,1.0
yi is the,1,1,1.0
is the target,1,1,1.0
the target of,1,1,1.0
target of pattern,1,1,1.0
of pattern xi,1,1,1.0
pattern xi xtr,1,1,1.0
xi xtr in,1,1,1.0
xtr in this,1,1,1.0
this sense ki,1,1,1.0
sense ki will,1,1,1.0
ki will provide,1,1,1.0
will provide information,1,1,1.0
provide information about,1,1,1.0
information about which,1,1,1.0
about which patterns,1,1,1.0
which patterns should,3,1,3.0
patterns should be,3,1,3.0
should be considered,1,1,1.0
to be similar,1,1,1.0
be similar when,1,1,1.0
similar when performing,1,1,1.0
when performing a,1,1,1.0
performing a learning,1,1,1.0
learning task thus,1,1,1.0
task thus the,1,1,1.0
thus the problem,1,1,1.0
problem of ﬁnding,1,1,1.0
of ﬁnding an,1,1,1.0
ﬁnding an optimal,1,1,1.0
an optimal kernel,1,1,1.0
optimal kernel k,1,1,1.0
kernel k is,1,1,1.0
k is changed,1,1,1.0
is changed to,1,1,1.0
changed to the,1,1,1.0
to the one,1,1,1.0
the one of,1,1,1.0
one of ﬁnding,1,1,1.0
of ﬁnding a,1,1,1.0
ﬁnding a good,1,1,1.0
a good approximation,1,1,1.0
good approximation k,1,1,1.0
approximation k for,1,1,1.0
k for the,1,1,1.0
for the ideal,1,1,1.0
matrix ki given,1,1,1.0
ki given a,1,1,1.0
given a family,1,1,1.0
a family of,1,1,1.0
family of kernel,1,1,1.0
of kernel functions,1,1,1.0
kernel functions this,1,1,1.0
functions this formulation,1,1,1.0
this formulation allows,1,1,1.0
formulation allows to,1,1,1.0
allows to separate,1,1,1.0
to separate the,1,1,1.0
separate the optimisation,1,1,1.0
the optimisation from,1,1,1.0
optimisation from kernel,1,1,1.0
from kernel machine,1,1,1.0
kernel machine learning,1,1,1.0
learning and to,1,1,1.0
and to reduce,1,1,1.0
to reduce the,2,2,1.0
reduce the increase,1,1,1.0
the increase in,1,1,1.0
increase in the,1,1,1.0
in the tional,1,1,1.0
the tional cost,1,1,1.0
tional cost of,1,1,1.0
cost of learning,1,1,1.0
of learning more,1,1,1.0
learning more complex,1,1,1.0
more complex kernels,2,1,2.0
complex kernels given,1,1,1.0
kernels given that,1,1,1.0
given that the,1,1,1.0
that the kernel,1,1,1.0
kernel machine will,1,1,1.0
machine will be,1,1,1.0
will be unaffected,1,1,1.0
be unaffected by,1,1,1.0
unaffected by this,1,1,1.0
by this higher,1,1,1.0
this higher complexity,1,1,1.0
higher complexity as,1,1,1.0
complexity as said,1,1,1.0
as said before,4,1,4.0
said before concerning,1,1,1.0
before concerning imbalanced,1,1,1.0
concerning imbalanced classiﬁcation,1,1,1.0
imbalanced classiﬁcation ous,1,1,1.0
classiﬁcation ous studies,1,1,1.0
ous studies have,1,1,1.0
studies have noted,1,1,1.0
have noted several,1,1,1.0
noted several issues,1,1,1.0
several issues in,1,1,1.0
issues in kt,1,1,1.0
in kt a,1,1,1.0
kt a for,2,1,2.0
a for different,1,1,1.0
for different pattern,1,1,1.0
different pattern distributions,1,1,1.0
pattern distributions but,1,1,1.0
distributions but a,1,1,1.0
but a recent,1,1,1.0
a recent study,1,1,1.0
recent study has,1,1,1.0
study has shown,1,1,1.0
has shown that,1,1,1.0
that this can,1,1,1.0
can be solved,1,1,1.0
be solved by,1,1,1.0
solved by the,1,1,1.0
by the use,1,1,1.0
use of centred,1,1,1.0
of centred kernel,1,1,1.0
centred kernel matrices,1,1,1.0
kernel matrices the,1,1,1.0
matrices the notion,1,1,1.0
notion of centred,1,1,1.0
of centred alignment,1,1,1.0
centred alignment ac,1,1,1.0
alignment ac between,1,1,1.0
ac between k,1,1,1.0
between k and,1,1,1.0
k and ki,1,1,1.0
and ki is,1,1,1.0
ki is deﬁned,1,1,1.0
deﬁned as ac,1,1,1.0
as ac k,1,1,1.0
ac k ki,1,1,1.0
k ki kic,1,1,1.0
ki kic kic,1,1,1.0
kic kic where,1,1,1.0
kic where kc,1,1,1.0
where kc denotes,1,1,1.0
kc denotes the,1,1,1.0
denotes the centred,1,1,1.0
the centred version,1,1,1.0
centred version of,1,1,1.0
version of kernel,1,1,1.0
of kernel matrix,1,1,1.0
matrix k and,2,1,2.0
k and is,2,1,2.0
and is computed,1,1,1.0
is computed as,1,1,1.0
computed as kc,1,1,1.0
as kc k,1,1,1.0
kc k m,1,1,1.0
k m m,2,1,2.0
m m k,1,1,1.0
m k m,1,1,1.0
m m being,1,1,1.0
m being m,1,1,1.0
being m a,1,1,1.0
m a matrix,1,1,1.0
a matrix with,1,1,1.0
matrix with all,1,1,1.0
with all elements,1,1,1.0
all elements equal,1,1,1.0
elements equal to,1,1,1.0
equal to m,1,1,1.0
to m centred,1,1,1.0
m centred kt,1,1,1.0
a is maximised,1,1,1.0
is maximised when,1,1,1.0
maximised when a,1,1,1.0
when a kernel,1,1,1.0
a kernel reﬂect,1,1,1.0
kernel reﬂect the,1,1,1.0
reﬂect the criminant,1,1,1.0
the criminant properties,1,1,1.0
criminant properties of,1,1,1.0
properties of the,1,1,1.0
the data used,1,1,1.0
data used to,1,1,1.0
used to deﬁne,1,1,1.0
to deﬁne the,1,1,1.0
deﬁne the ideal,1,1,1.0
ideal kernel consider,1,1,1.0
kernel consider a,1,1,1.0
consider a kernel,1,1,1.0
kernel function depending,1,1,1.0
function depending on,1,1,1.0
depending on a,1,1,1.0
on a vector,1,1,1.0
a vector of,2,1,2.0
vector of eters,1,1,1.0
of eters α,1,1,1.0
eters α because,1,1,1.0
α because of,1,1,1.0
of the differentiability,1,1,1.0
the differentiability of,1,1,1.0
differentiability of ac,1,1,1.0
of ac with,1,1,1.0
ac with respect,1,1,1.0
respect to these,2,1,2.0
to these kernel,2,1,2.0
these kernel parameters,2,1,2.0
kernel parameters α,2,1,2.0
parameters α a,1,1,1.0
α a gradient,1,1,1.0
a gradient ascent,1,1,1.0
gradient ascent algorithm,1,1,1.0
ascent algorithm can,1,1,1.0
algorithm can be,1,1,1.0
used to maximise,1,1,1.0
to maximise the,1,1,1.0
maximise the alignment,1,1,1.0
the alignment between,1,1,1.0
alignment between the,1,1,1.0
between the kernel,2,1,2.0
kernel matrix constructed,1,1,1.0
matrix constructed kα,1,1,1.0
constructed kα and,1,1,1.0
kα and the,1,1,1.0
and the ideal,1,1,1.0
the ideal one,2,1,2.0
ideal one ki,1,1,1.0
one ki as,1,1,1.0
ki as follows,1,1,1.0
as follows α,1,1,1.0
follows α arg,1,1,1.0
α arg max,1,1,1.0
arg max α,1,1,1.0
max α ac,1,1,1.0
α ac kα,1,1,1.0
ac kα ki,2,1,2.0
kα ki the,1,1,1.0
ki the alignment,1,1,1.0
the alignment derivative,1,1,1.0
alignment derivative with,1,1,1.0
derivative with respect,1,1,1.0
parameters α is,1,1,1.0
α is ac,1,1,1.0
is ac kα,1,1,1.0
kα ki α,1,1,1.0
ki α kα,1,1,1.0
α kα α,1,1,1.0
kα α kic,1,1,1.0
α kic c,1,1,1.0
kic c kic,1,1,1.0
c kic c,1,1,1.0
kic c kα,1,1,1.0
c kα α,1,1,1.0
kα α c,1,1,1.0
α c f,1,1,1.0
c f where,1,1,1.0
f where and,1,1,1.0
where and for,1,1,1.0
and for arbitrary,1,1,1.0
for arbitrary matrices,1,1,1.0
arbitrary matrices and,1,1,1.0
matrices and it,1,1,1.0
and it holds,1,1,1.0
it holds that,1,1,1.0
holds that which,1,1,1.0
that which simpliﬁes,1,1,1.0
which simpliﬁes the,2,2,1.0
simpliﬁes the computation,1,1,1.0
the computation in,1,1,1.0
computation in this,1,1,1.0
paper we will,1,1,1.0
we will consider,1,1,1.0
will consider a,1,1,1.0
consider a generalised,1,1,1.0
a generalised gaussian,1,1,1.0
generalised gaussian kernel,5,1,5.0
gaussian kernel with,4,1,4.0
kernel with covariance,1,1,1.0
with covariance structure,1,1,1.0
covariance structure deﬁned,1,1,1.0
structure deﬁned by,1,1,1.0
deﬁned by a,1,1,1.0
by a positive,1,1,1.0
a positive semideﬁnite,1,1,1.0
positive semideﬁnite matrix,1,1,1.0
semideﬁnite matrix q,1,1,1.0
matrix q k,1,1,1.0
q k xi,1,1,1.0
xi xj exp,2,1,2.0
xj exp xi,2,1,2.0
exp xi xj,2,1,2.0
xi xj tq,1,1,1.0
xj tq xi,1,1,1.0
tq xi xj,1,1,1.0
xi xj as,1,1,1.0
xj as usual,1,1,1.0
as usual the,1,1,1.0
usual the matrix,1,1,1.0
the matrix q,1,1,1.0
matrix q will,1,1,1.0
q will be,1,1,1.0
will be parametrised,1,1,1.0
be parametrised by,1,1,1.0
parametrised by utu,1,1,1.0
by utu where,1,1,1.0
utu where u,1,1,1.0
where u is,1,1,1.0
u is a,1,1,1.0
is a d,1,1,1.0
a d d,1,1,1.0
d d matrix,1,1,1.0
d matrix d,1,1,1.0
matrix d being,1,1,1.0
d being the,1,1,1.0
being the dimensionality,1,1,1.0
input space therefore,1,1,1.0
space therefore we,1,1,1.0
therefore we can,1,1,1.0
we can equivalently,1,1,1.0
can equivalently restate,1,1,1.0
equivalently restate our,1,1,1.0
restate our problem,1,1,1.0
our problem as,1,1,1.0
problem as learning,1,1,1.0
as learning the,1,1,1.0
learning the best,1,1,1.0
the best matrix,1,1,1.0
best matrix u,1,1,1.0
matrix u k,1,1,1.0
u k xi,1,1,1.0
xi xj tutu,1,1,1.0
xj tutu xi,1,1,1.0
tutu xi xj,1,1,1.0
xi xj now,1,1,1.0
xj now we,1,1,1.0
now we can,1,1,1.0
we can compute,1,1,1.0
can compute the,1,1,1.0
compute the derivative,1,1,1.0
the derivative of,1,1,1.0
derivative of the,1,1,1.0
of the kernel,7,1,7.0
the kernel with,1,1,1.0
kernel with respect,1,1,1.0
to the entries,1,1,1.0
the entries of,2,1,2.0
entries of the,2,1,2.0
of the u,2,1,2.0
the u matrix,2,1,2.0
u matrix xi,1,1,1.0
matrix xi xj,1,1,1.0
xi xj u,1,1,1.0
xj u xi,1,1,1.0
u xi xj,1,1,1.0
xi xj t,1,1,1.0
xj t xi,1,1,1.0
t xi xj,1,1,1.0
xi xj therefore,1,1,1.0
xj therefore we,1,1,1.0
therefore we will,1,1,1.0
we will optimise,1,1,1.0
will optimise a,1,1,1.0
optimise a vector,1,1,1.0
vector of parameters,1,1,1.0
of parameters α,1,1,1.0
parameters α composed,1,1,1.0
α composed of,1,1,1.0
composed of the,1,1,1.0
of the entries,1,1,1.0
u matrix it,1,1,1.0
it is important,1,1,1.0
important to note,1,1,1.0
to note that,3,2,1.5
note that some,1,1,1.0
that some attempts,1,1,1.0
some attempts have,1,1,1.0
attempts have been,1,1,1.0
made to establish,1,1,1.0
to establish learning,1,1,1.0
establish learning bounds,1,1,1.0
learning bounds for,2,1,2.0
for the gaussian,1,1,1.0
kernel with several,1,1,1.0
with several parameters,1,1,1.0
several parameters when,1,1,1.0
parameters when considering,1,1,1.0
when considering large,1,1,1.0
considering large margin,1,1,1.0
large margin classiﬁers,1,1,1.0
margin classiﬁers these,1,1,1.0
classiﬁers these studies,1,1,1.0
these studies suggest,1,1,1.0
suggest that the,1,1,1.0
that the interaction,1,1,1.0
the interaction between,1,1,1.0
interaction between the,1,1,1.0
between the margin,1,1,1.0
the margin and,1,1,1.0
margin and the,1,1,1.0
and the complexity,1,1,1.0
the complexity measure,1,1,1.0
complexity measure of,1,1,1.0
measure of the,1,1,1.0
the kernel class,1,1,1.0
kernel class is,1,1,1.0
class is plicative,1,1,1.0
is plicative thus,1,1,1.0
plicative thus discouraging,1,1,1.0
thus discouraging the,1,1,1.0
discouraging the development,1,1,1.0
development of techniques,1,1,1.0
of techniques for,1,1,1.0
for the optimisation,3,1,3.0
the optimisation of,8,1,8.0
optimisation of more,1,1,1.0
of more complex,1,1,1.0
complex kernels however,1,1,1.0
kernels however recent,1,1,1.0
however recent developments,1,1,1.0
recent developments have,1,1,1.0
developments have shown,1,1,1.0
have shown that,2,2,1.0
that this interaction,1,1,1.0
this interaction is,1,1,1.0
interaction is additive,1,1,1.0
is additive up,1,1,1.0
additive up to,1,1,1.0
up to log,1,1,1.0
to log factors,1,1,1.0
log factors rather,1,1,1.0
factors rather than,1,1,1.0
rather than multiplicative,1,1,1.0
than multiplicative yielding,1,1,1.0
multiplicative yielding then,1,1,1.0
yielding then stronger,1,1,1.0
then stronger bounds,1,1,1.0
stronger bounds therefore,1,1,1.0
bounds therefore the,1,1,1.0
therefore the number,2,2,1.0
number of patterns,4,1,4.0
of patterns needed,1,1,1.0
patterns needed to,1,1,1.0
needed to obtain,1,1,1.0
to obtain the,2,2,1.0
obtain the same,1,1,1.0
the same estimation,1,1,1.0
same estimation error,1,1,1.0
estimation error with,1,1,1.0
error with the,1,1,1.0
with the same,1,1,1.0
the same probability,1,1,1.0
same probability for,1,1,1.0
probability for a,1,1,1.0
for a kernel,1,1,1.0
a kernel compared,1,1,1.0
kernel compared to,1,1,1.0
to a spherical,1,1,1.0
a spherical one,1,1,1.0
spherical one grows,1,1,1.0
one grows slowly,1,1,1.0
grows slowly and,1,1,1.0
slowly and directly,1,1,1.0
and directly depends,1,1,1.0
directly depends on,1,1,1.0
number of parameters,1,1,1.0
of parameters t,1,1,1.0
parameters t o,1,1,1.0
t o demonstrate,1,1,1.0
o demonstrate the,1,1,1.0
demonstrate the usefulness,1,1,1.0
the usefulness of,1,1,1.0
usefulness of learning,1,1,1.0
of learning the,1,1,1.0
learning the kernels,1,1,1.0
the kernels we,1,1,1.0
kernels we present,1,1,1.0
we present in,1,1,1.0
present in fig,1,1,1.0
in fig a,1,1,1.0
fig a graphical,1,1,1.0
representation of three,1,1,1.0
of three dimensional,1,1,1.0
three dimensional toy,1,1,1.0
dimensional toy datasets,1,1,1.0
toy datasets and,1,1,1.0
datasets and their,1,1,1.0
and their mapping,1,1,1.0
their mapping φ,1,1,1.0
mapping φ e,1,1,1.0
φ e using,1,1,1.0
e using a,1,1,1.0
using a ical,1,1,1.0
a ical gaussian,1,1,1.0
ical gaussian kernel,1,1,1.0
kernel with q,1,1,1.0
with q id,1,1,1.0
q id an,1,1,1.0
id an optimised,1,1,1.0
an optimised spherical,3,1,3.0
optimised spherical gaussian,2,1,2.0
spherical gaussian kernel,8,1,8.0
gaussian kernel obtained,1,1,1.0
kernel obtained through,1,1,1.0
obtained through centred,1,1,1.0
through centred kt,1,1,1.0
kt a and,1,1,1.0
a and an,1,1,1.0
and an optimised,1,1,1.0
an optimised generalised,3,1,3.0
optimised generalised gaussian,2,1,2.0
gaussian kernel summarising,1,1,1.0
kernel summarising kernel,1,1,1.0
summarising kernel learning,1,1,1.0
kernel learning will,1,1,1.0
learning will be,1,1,1.0
will be applied,1,1,1.0
be applied before,1,1,1.0
applied before the,1,1,1.0
before the procedure,1,1,1.0
the procedure to,1,1,1.0
procedure to learn,1,1,1.0
to learn a,1,1,1.0
learn a suitable,1,1,1.0
a suitable kernel,1,1,1.0
suitable kernel kα,1,1,1.0
kernel kα for,1,1,1.0
kα for the,1,1,1.0
the data representation,1,1,1.0
data representation after,1,1,1.0
representation after this,1,1,1.0
after this the,1,1,1.0
this the efs,1,1,1.0
the efs φ,1,1,1.0
efs φ e,1,1,1.0
e q associated,1,1,1.0
q associated to,1,1,1.0
associated to this,2,1,2.0
to this kernel,1,1,1.0
this kernel kα,1,1,1.0
kernel kα will,1,1,1.0
kα will be,1,1,1.0
will be computed,1,1,1.0
be computed and,1,1,1.0
then the images,1,1,1.0
training patterns for,1,1,1.0
patterns for the,1,1,1.0
minority class contained,1,1,1.0
class contained in,1,1,1.0
contained in the,1,1,1.0
in the z,1,1,1.0
the z matrix,1,1,1.0
z matrix will,1,1,1.0
will be for,1,1,1.0
be for comparison,1,1,1.0
for comparison purposes,1,1,1.0
comparison purposes we,1,1,1.0
purposes we will,1,1,1.0
we will also,1,1,1.0
will also test,1,1,1.0
also test the,1,1,1.0
test the optimization,1,1,1.0
the optimization of,1,1,1.0
optimization of a,1,1,1.0
of a spherical,2,1,2.0
a spherical gaussian,3,1,3.0
kernel with one,1,1,1.0
with one kernel,1,1,1.0
one kernel parameter,1,1,1.0
kernel parameter via,1,1,1.0
parameter via alignment,1,1,1.0
via alignment v,1,1,1.0
alignment v u,1,1,1.0
v u n,1,1,1.0
u n i,1,1,1.0
n i fi,1,1,1.0
i fi e,1,1,1.0
fi e d,1,1,1.0
e d f,1,1,1.0
d f r,1,1,1.0
f r a,1,1,1.0
a m e,1,1,1.0
m e wo,1,1,1.0
e wo r,1,1,1.0
wo r k,1,1,1.0
r k f,1,1,1.0
k f o,1,1,1.0
o r p,1,1,1.0
r p r,1,1,1.0
p r e,1,1,1.0
r e f,1,1,1.0
f e r,2,1,2.0
e r e,3,2,1.5
r e n,2,1,2.0
e n t,1,1,1.0
n t i,1,1,1.0
t i a,2,2,1.0
i a l,1,1,1.0
a l ov,1,1,1.0
l ov e,1,1,1.0
stated before several,1,1,1.0
before several approaches,1,1,1.0
several approaches have,1,1,1.0
the literature for,2,1,2.0
literature for handling,1,1,1.0
data and a,1,1,1.0
and a large,1,1,1.0
number of these,1,1,1.0
of these contributions,1,1,1.0
these contributions are,1,1,1.0
contributions are based,1,1,1.0
based on analysing,1,1,1.0
on analysing the,1,1,1.0
analysing the patterns,1,1,1.0
the patterns which,1,1,1.0
patterns which could,1,1,1.0
which could be,1,1,1.0
could be more,1,1,1.0
be more suitable,1,1,1.0
suitable for giving,1,1,1.0
for giving rise,1,1,1.0
giving rise to,1,1,1.0
rise to approaches,1,1,1.0
to approaches based,1,1,1.0
approaches based on,1,1,1.0
based on on,1,1,1.0
on on the,1,1,1.0
class boundary or,3,1,3.0
boundary or in,1,1,1.0
or in the,1,1,1.0
in the within,1,1,1.0
within class safe,1,1,1.0
class safe region,1,1,1.0
safe region these,1,1,1.0
region these techniques,1,1,1.0
these techniques are,1,1,1.0
techniques are commonly,1,1,1.0
are commonly referred,1,1,1.0
commonly referred to,1,1,1.0
to as weighted,1,1,1.0
as weighted however,1,1,1.0
weighted however to,1,1,1.0
however to our,1,1,1.0
to our best,1,1,1.0
our best knowledge,1,1,1.0
best knowledge there,1,1,1.0
knowledge there is,1,1,1.0
is no principled,1,1,1.0
no principled method,1,1,1.0
principled method for,1,1,1.0
method for choosing,1,1,1.0
for choosing the,1,1,1.0
choosing the region,1,1,1.0
the region of,1,1,1.0
region of the,1,1,1.0
used for sampling,1,1,1.0
for sampling in,3,1,3.0
sampling in this,1,1,1.0
propose a new,1,1,1.0
a new adaptive,1,1,1.0
new adaptive weighted,1,1,1.0
adaptive weighted technique,1,1,1.0
weighted technique that,1,1,1.0
technique that naturally,1,1,1.0
that naturally spans,1,1,1.0
naturally spans unweighted,1,1,1.0
spans unweighted and,1,1,1.0
unweighted and weighted,1,1,1.0
and weighted methods,1,1,1.0
weighted methods both,1,1,1.0
methods both on,1,1,1.0
both on the,1,1,1.0
on the boundary,1,1,1.0
the boundary and,1,1,1.0
boundary and within,1,1,1.0
within class t,1,1,1.0
class t o,1,1,1.0
do so our,1,1,1.0
so our approach,1,1,1.0
our approach will,1,1,1.0
approach will take,1,1,1.0
will take advantage,1,1,1.0
advantage of the,1,1,1.0
of the spatial,1,1,1.0
the spatial distribution,4,1,4.0
spatial distribution of,4,1,4.0
the patterns according,3,1,3.0
to the optimal,4,1,4.0
the optimal hyperplane,2,1,2.0
optimal hyperplane obtained,1,1,1.0
hyperplane obtained from,1,1,1.0
from the svm,1,1,1.0
svm solution knowledge,1,1,1.0
solution knowledge extraction,1,1,1.0
knowledge extraction spatial,1,1,1.0
extraction spatial distribution,1,1,1.0
the patterns w,1,1,1.0
patterns w eighted,1,1,1.0
w eighted techniques,1,1,1.0
eighted techniques are,1,1,1.0
techniques are based,1,1,1.0
on the idea,2,2,1.0
idea that not,1,1,1.0
not all the,1,1,1.0
the patterns of,1,1,1.0
patterns of the,1,1,1.0
the dataset are,1,1,1.0
dataset are equally,1,1,1.0
are equally important,1,1,1.0
equally important and,1,1,1.0
important and suitable,1,1,1.0
and suitable for,1,1,1.0
suitable for and,2,1,2.0
for and therefore,1,1,1.0
and therefore they,1,1,1.0
therefore they should,1,1,1.0
they should fig,1,1,1.0
should fig synthethic,1,1,1.0
fig synthethic datasets,1,1,1.0
synthethic datasets representing,1,1,1.0
datasets representing separable,1,1,1.0
representing separable classiﬁcation,1,1,1.0
separable classiﬁcation problems,1,1,1.0
classiﬁcation problems and,1,1,1.0
problems and their,1,1,1.0
and their transformation,1,1,1.0
their transformation to,1,1,1.0
the efs induced,1,1,1.0
efs induced by,1,1,1.0
separable problem contribute,1,1,1.0
problem contribute equally,1,1,1.0
contribute equally to,1,1,1.0
equally to the,1,1,1.0
to the new,1,1,1.0
the new synthetic,2,1,2.0
synthetic data one,1,1,1.0
data one of,1,1,1.0
the ﬁrst steps,1,1,1.0
ﬁrst steps of,1,1,1.0
steps of these,1,1,1.0
of these methodologies,1,1,1.0
these methodologies corresponds,1,1,1.0
methodologies corresponds to,1,1,1.0
to the identiﬁcation,1,1,1.0
the identiﬁcation of,1,1,1.0
identiﬁcation of the,1,1,1.0
of the useful,1,1,1.0
the useful patterns,1,1,1.0
useful patterns to,1,1,1.0
patterns to be,2,1,2.0
used for most,1,1,1.0
of the approaches,1,1,1.0
the approaches in,1,1,1.0
the literature do,1,1,1.0
literature do so,1,1,1.0
do so by,1,1,1.0
so by analysing,1,1,1.0
by analysing local,1,1,1.0
analysing local neighbourhood,1,1,1.0
local neighbourhood of,1,1,1.0
neighbourhood of points,1,1,1.0
this paper however,1,1,1.0
paper however we,1,1,1.0
however we will,1,1,1.0
we will derive,1,1,1.0
will derive a,1,1,1.0
derive a weighted,1,1,1.0
a weighted technique,1,1,1.0
weighted technique considering,1,1,1.0
technique considering the,1,1,1.0
considering the spatial,1,1,1.0
the patterns with,1,1,1.0
patterns with respect,2,1,2.0
svm hyperplane in,1,1,1.0
hyperplane in particular,1,1,1.0
particular the patterns,1,1,1.0
used for will,1,1,1.0
for will be,1,1,1.0
will be selected,1,1,1.0
be selected based,1,1,1.0
selected based on,2,1,2.0
based on their,2,1,2.0
on their position,1,1,1.0
their position and,1,1,1.0
position and distance,1,1,1.0
and distance to,1,1,1.0
distance to the,1,1,1.0
optimal hyperplane however,1,1,1.0
hyperplane however as,1,1,1.0
however as stated,1,1,1.0
stated before the,1,1,1.0
before the optimisation,1,1,1.0
optimisation of the,5,1,5.0
of the svm,1,1,1.0
svm paradigm poses,1,1,1.0
a serious problem,1,1,1.0
serious problem for,1,1,1.0
problem for anced,1,1,1.0
for anced datasets,1,1,1.0
anced datasets therefore,1,1,1.0
datasets therefore for,1,1,1.0
therefore for the,1,1,1.0
purpose of weighted,1,1,1.0
of weighted sampling,1,1,1.0
weighted sampling we,1,1,1.0
sampling we use,1,1,1.0
use the approach,1,1,1.0
the approach giving,1,1,1.0
approach giving more,1,1,1.0
giving more importance,1,1,1.0
more importance to,1,1,1.0
importance to errors,1,1,1.0
to errors committed,1,1,1.0
errors committed by,1,1,1.0
committed by patterns,1,1,1.0
by patterns belonging,1,1,1.0
patterns belonging to,4,1,4.0
class the svm,1,1,1.0
the svm approach,1,1,1.0
svm approach consists,1,1,1.0
approach consists of,1,1,1.0
consists of introducing,1,1,1.0
of introducing different,1,1,1.0
introducing different penalty,1,1,1.0
different penalty factors,1,1,1.0
penalty factors and,1,1,1.0
factors and for,1,1,1.0
and for the,2,1,2.0
for the positive,2,1,2.0
and negative svm,1,1,1.0
negative svm slack,1,1,1.0
svm slack variables,1,1,1.0
slack variables during,1,1,1.0
variables during training,1,1,1.0
during training the,1,1,1.0
training the primal,1,1,1.0
the primal svm,1,1,1.0
primal svm problem,1,1,1.0
svm problem is,1,1,1.0
problem is transformed,1,1,1.0
is transformed into,1,1,1.0
transformed into ξi,1,1,1.0
into ξi ξi,1,1,1.0
ξi ξi subject,1,1,1.0
ξi subject to,1,1,1.0
i m for,1,1,1.0
m for simplicity,1,1,1.0
simplicity we will,1,1,1.0
we will set,1,1,1.0
will set where,1,1,1.0
set where is,1,1,1.0
where is assumed,1,1,1.0
is assumed to,1,1,1.0
be the minority,1,1,1.0
class is the,1,1,1.0
of patterns belonging,2,1,2.0
belonging to class,2,1,2.0
to class and,1,1,1.0
and the number,5,2,2.5
class the ratio,1,1,1.0
the ratio is,1,1,1.0
ratio is usually,1,1,1.0
is usually known,1,1,1.0
usually known as,1,1,1.0
as the imbalanced,1,1,1.0
the imbalanced ratio,3,2,1.5
imbalanced ratio as,1,1,1.0
ratio as stated,1,1,1.0
stated before each,1,1,1.0
before each synthetically,1,1,1.0
each synthetically generated,1,1,1.0
synthetically generated point,1,1,1.0
generated point sz,1,1,1.0
point sz e,1,1,1.0
sz e q,1,1,1.0
e q z,1,1,1.0
q z n,1,1,1.0
z n in,1,1,1.0
n in the,2,2,1.0
minority class represented,1,1,1.0
class represented by,1,1,1.0
represented by training,1,1,1.0
by training samples,1,1,1.0
training samples xtr,1,1,1.0
samples xtr is,1,1,1.0
xtr is generated,1,1,1.0
is generated by,2,2,1.0
generated by ﬁrst,1,1,1.0
by ﬁrst picking,1,1,1.0
ﬁrst picking a,1,1,1.0
picking a pair,1,1,1.0
a pair of,2,2,1.0
pair of points,1,1,1.0
of points xi,1,1,1.0
points xi and,2,1,2.0
xi and xj,4,1,4.0
and xj from,1,1,1.0
xj from xtr,1,1,1.0
from xtr and,1,1,1.0
xtr and then,1,1,1.0
and then constructing,1,1,1.0
then constructing their,1,1,1.0
constructing their convex,1,1,1.0
their convex combination,1,1,1.0
convex combination in,1,1,1.0
combination in the,1,1,1.0
e q sz,1,1,1.0
q sz φ,1,1,1.0
sz φ e,1,1,1.0
q xj φ,1,1,1.0
δ where δ,1,1,1.0
where δ is,1,1,1.0
distribution u optimisation,1,1,1.0
u optimisation of,1,1,1.0
of the procedure,1,1,1.0
the procedure the,1,1,1.0
procedure the points,1,1,1.0
the points xi,1,1,1.0
and xj will,1,1,1.0
xj will be,1,1,1.0
will be randomly,1,1,1.0
be randomly selected,1,1,1.0
randomly selected based,1,1,1.0
on their relative,1,1,1.0
their relative position,1,1,1.0
relative position in,1,1,1.0
position in the,1,1,1.0
feature space with,1,1,1.0
space with respect,1,1,1.0
to the separating,1,1,1.0
the separating hyperplane,2,1,2.0
separating hyperplane because,1,1,1.0
hyperplane because the,1,1,1.0
because the norm,1,1,1.0
the norm of,1,1,1.0
norm of w,1,1,1.0
of w is,1,1,1.0
w is the,1,1,1.0
is the signed,1,1,1.0
the signed distance,1,1,1.0
signed distance of,1,1,1.0
distance of φ,1,1,1.0
of φ xi,1,1,1.0
φ xi f,1,1,1.0
xi f q,1,1,1.0
f q from,1,1,1.0
q from the,1,1,1.0
from the hyperplane,1,1,1.0
the hyperplane is,1,1,1.0
hyperplane is given,1,1,1.0
given by f,1,1,1.0
by f xi,1,1,1.0
f xi xi,1,1,1.0
xi xi note,1,1,1.0
xi note that,2,1,2.0
note that if,1,1,1.0
that if φ,1,1,1.0
if φ xi,2,1,2.0
φ xi is,1,1,1.0
xi is on,1,1,1.0
is on the,1,1,1.0
the right side,1,1,1.0
right side of,1,1,1.0
side of the,2,1,2.0
of the hyperplane,2,1,2.0
the hyperplane f,1,1,1.0
hyperplane f xi,1,1,1.0
f xi is,1,1,1.0
xi is positive,1,1,1.0
is positive otherwise,1,1,1.0
positive otherwise it,1,1,1.0
otherwise it is,1,1,1.0
it is negative,1,1,1.0
is negative w,1,1,1.0
negative w e,1,1,1.0
w e will,2,1,2.0
e will represent,1,1,1.0
will represent the,1,1,1.0
represent the selection,1,1,1.0
the selection process,1,1,1.0
selection process as,1,1,1.0
process as draws,1,1,1.0
as draws from,1,1,1.0
draws from a,1,1,1.0
from a multinomial,1,1,1.0
a multinomial distribution,1,1,1.0
multinomial distribution over,1,1,1.0
distribution over xtr,2,1,2.0
over xtr patterns,1,1,1.0
xtr patterns belonging,1,1,1.0
class with natural,1,1,1.0
with natural parameters,1,1,1.0
natural parameters µi,1,1,1.0
parameters µi f,1,1,1.0
µi f xi,1,1,1.0
f xi where,1,1,1.0
xi where β,1,1,1.0
where β r,1,1,1.0
β r is,1,1,1.0
r is a,1,1,1.0
is a scale,1,1,1.0
a scale parameter,1,1,1.0
scale parameter using,1,1,1.0
parameter using the,1,1,1.0
using the link,1,1,1.0
the link function,1,1,1.0
link function the,1,1,1.0
function the probability,1,1,1.0
the probability of,3,2,1.5
probability of picking,1,1,1.0
of picking xi,1,1,1.0
picking xi xtr,1,1,1.0
xi xtr is,1,1,1.0
xtr is p,1,1,1.0
is p xi,1,1,1.0
p xi exp,1,1,1.0
xi exp xi,1,1,1.0
exp xi exp,1,1,1.0
xi exp x,1,1,1.0
exp x if,1,1,1.0
x if φ,1,1,1.0
φ xi lies,1,1,1.0
xi lies on,1,1,1.0
lies on the,1,1,1.0
on the separating,1,1,1.0
separating hyperplane then,1,1,1.0
hyperplane then f,1,1,1.0
then f xi,1,1,1.0
f xi note,1,1,1.0
note that when,1,1,1.0
that when β,2,1,2.0
when β points,4,1,4.0
β points deep,1,1,1.0
points deep within,1,1,1.0
deep within the,1,1,1.0
within the minority,2,1,2.0
to be picked,2,1,2.0
be picked when,2,1,2.0
picked when β,2,1,2.0
β points closer,1,1,1.0
points closer to,1,1,1.0
boundary or lying,1,1,1.0
or lying inside,1,1,1.0
lying inside the,1,1,1.0
inside the opposite,1,1,1.0
the opposite class,1,1,1.0
opposite class are,1,1,1.0
class are preferred,1,1,1.0
are preferred and,2,1,2.0
preferred and when,2,1,2.0
and when β,2,1,2.0
when β all,2,1,2.0
β all the,2,1,2.0
all the points,2,1,2.0
the points are,2,1,2.0
points are equally,2,1,2.0
are equally likely,2,1,2.0
equally likely to,3,2,1.5
to be chosen,2,1,2.0
be chosen as,1,1,1.0
chosen as this,1,1,1.0
as this will,1,1,1.0
this will correspond,1,1,1.0
to the uniform,1,1,1.0
uniform distribution over,1,1,1.0
over xtr this,1,1,1.0
xtr this approach,1,1,1.0
this approach naturally,1,1,1.0
approach naturally spans,1,1,1.0
naturally spans different,1,1,1.0
spans different approaches,1,1,1.0
different approaches to,1,1,1.0
approaches to weighted,1,1,1.0
to weighted and,1,1,1.0
weighted and unweighted,1,1,1.0
and unweighted previously,1,1,1.0
unweighted previously introduced,1,1,1.0
previously introduced in,1,1,1.0
introduced in the,2,2,1.0
literature for selecting,1,1,1.0
for selecting the,1,1,1.0
selecting the pairs,1,1,1.0
the pairs xi,1,1,1.0
pairs xi xj,1,1,1.0
xi xj xtr,1,1,1.0
xj xtr we,1,1,1.0
xtr we could,1,1,1.0
we could use,1,1,1.0
could use two,1,1,1.0
use two different,1,1,1.0
two different ideas,1,1,1.0
different ideas pick,1,1,1.0
ideas pick xi,1,1,1.0
pick xi and,1,1,1.0
and xj independently,2,1,2.0
xj independently with,1,1,1.0
independently with respect,1,1,1.0
to the bution,1,1,1.0
the bution of,1,1,1.0
bution of eq,1,1,1.0
of eq pick,1,1,1.0
eq pick xi,1,1,1.0
pick xi according,1,1,1.0
xi according to,1,1,1.0
to the distribution,1,1,1.0
distribution of eq,1,1,1.0
of eq and,1,1,1.0
eq and select,1,1,1.0
and select xj,1,1,1.0
select xj using,1,1,1.0
xj using neighbours,1,1,1.0
using neighbours method,1,1,1.0
neighbours method in,1,1,1.0
method in most,1,1,1.0
of the weighted,1,1,1.0
the weighted approaches,1,1,1.0
weighted approaches in,1,1,1.0
the literature they,1,1,1.0
literature they make,1,1,1.0
they make use,1,1,1.0
make use of,1,1,1.0
neighbours method because,1,1,1.0
method because they,1,1,1.0
because they obtain,1,1,1.0
they obtain the,1,1,1.0
obtain the spatial,2,1,2.0
spatial distribution information,1,1,1.0
distribution information of,1,1,1.0
to their neighbourhood,1,1,1.0
their neighbourhood however,1,1,1.0
neighbourhood however for,1,1,1.0
however for this,1,1,1.0
for this approach,1,1,1.0
this approach note,1,1,1.0
approach note that,1,1,1.0
note that it,1,1,1.0
it is actually,1,1,1.0
is actually more,1,1,1.0
actually more advisable,1,1,1.0
advisable to select,1,1,1.0
to select xi,1,1,1.0
select xi and,1,1,1.0
xj independently according,1,1,1.0
independently according to,1,1,1.0
to the probability,1,1,1.0
the probability distribution,1,1,1.0
probability distribution obtained,1,1,1.0
distribution obtained because,1,1,1.0
obtained because otherwise,1,1,1.0
because otherwise the,1,1,1.0
otherwise the effect,1,1,1.0
effect of the,2,1,2.0
of the preferential,1,1,1.0
the preferential learning,1,1,1.0
preferential learning in,1,1,1.0
learning in the,1,1,1.0
in the sampling,2,2,1.0
the sampling process,2,1,2.0
sampling process could,2,1,2.0
process could be,2,1,2.0
could be smoothed,1,1,1.0
be smoothed picking,1,1,1.0
smoothed picking points,1,1,1.0
picking points by,1,1,1.0
points by the,1,1,1.0
by the neighbours,1,1,1.0
the neighbours approach,1,1,1.0
neighbours approach may,1,1,1.0
approach may differ,1,1,1.0
may differ to,1,1,1.0
differ to a,1,1,1.0
to a large,1,1,1.0
a large extent,1,1,1.0
large extent to,1,1,1.0
extent to the,1,1,1.0
the selection made,1,1,1.0
selection made with,1,1,1.0
made with the,1,1,1.0
with the probability,1,1,1.0
the probability function,1,1,1.0
probability function based,1,1,1.0
function based on,1,1,1.0
on the arguments,1,1,1.0
the arguments in,1,1,1.0
arguments in section,1,1,1.0
in section iii,2,2,1.0
section iii of,1,1,1.0
iii of the,1,1,1.0
space is done,1,1,1.0
is done through,1,1,1.0
done through sampling,1,1,1.0
through sampling in,1,1,1.0
the efs note,1,1,1.0
efs note that,1,1,1.0
note that the,2,1,2.0
that the patterns,1,1,1.0
the patterns preferred,1,1,1.0
patterns preferred for,1,1,1.0
preferred for sampling,1,1,1.0
input space could,1,1,1.0
space could not,1,1,1.0
could not be,1,1,1.0
be the ones,1,1,1.0
the ones preferred,1,1,1.0
ones preferred in,1,1,1.0
preferred in the,1,1,1.0
feature space therefore,1,1,1.0
space therefore the,1,1,1.0
therefore the use,1,1,1.0
the efs is,3,1,3.0
efs is needed,1,1,1.0
is needed for,1,1,1.0
needed for this,1,1,1.0
for this methodology,1,1,1.0
this methodology as,1,1,1.0
methodology as well,1,1,1.0
as well t,2,1,2.0
well t o,2,1,2.0
t o optimise,1,1,1.0
o optimise the,1,1,1.0
optimise the β,1,1,1.0
the β values,1,1,1.0
β values as,1,1,1.0
values as different,1,1,1.0
as different β,1,1,1.0
different β values,5,1,5.0
β values will,1,1,1.0
values will induce,1,1,1.0
will induce different,1,1,1.0
induce different synthetic,1,1,1.0
different synthetic patterns,1,1,1.0
synthetic patterns we,1,1,1.0
patterns we will,1,1,1.0
we will test,3,1,3.0
will test two,1,1,1.0
test two approaches,1,1,1.0
two approaches the,1,1,1.0
approaches the ﬁrst,2,1,2.0
is to use,3,1,3.0
to use a,1,1,1.0
use a single,1,1,1.0
a single value,1,1,1.0
single value of,1,1,1.0
of β found,1,1,1.0
β found by,1,1,1.0
found by over,1,1,1.0
by over a,1,1,1.0
over a set,1,1,1.0
set of p,1,1,1.0
of p predeﬁned,1,1,1.0
p predeﬁned β,1,1,1.0
predeﬁned β values,1,1,1.0
β values the,1,1,1.0
values the second,1,1,1.0
the second idea,1,1,1.0
second idea is,1,1,1.0
to use multiple,1,1,1.0
use multiple β,1,1,1.0
multiple β values,1,1,1.0
β values within,1,1,1.0
values within the,1,1,1.0
within the framework,1,1,1.0
the framework of,1,1,1.0
framework of multiple,1,1,1.0
of multiple kernel,1,1,1.0
multiple kernel learning,4,1,4.0
kernel learning mkl,1,1,1.0
learning mkl a,1,1,1.0
mkl a combination,1,1,1.0
combination of different,1,1,1.0
of different kernel,1,1,1.0
different kernel matrices,1,1,1.0
kernel matrices for,1,1,1.0
matrices for a,1,1,1.0
for a particular,1,1,1.0
a particular value,1,1,1.0
particular value of,1,1,1.0
of β we,1,1,1.0
β we denote,1,1,1.0
denote by the,1,1,1.0
matrix obtained on,1,1,1.0
obtained on the,1,1,1.0
on the extended,1,1,1.0
the extended data,1,1,1.0
extended data sample,1,1,1.0
data sample ing,1,1,1.0
sample ing points,1,1,1.0
ing points obtained,1,1,1.0
points obtained using,1,1,1.0
obtained using β,1,1,1.0
using β w,1,1,1.0
β w e,1,1,1.0
w e ﬁx,1,1,1.0
e ﬁx a,1,1,1.0
ﬁx a set,1,1,1.0
set of β,1,1,1.0
of β values,1,1,1.0
β values β,1,1,1.0
values β p,1,1,1.0
β p and,1,1,1.0
p and compute,1,1,1.0
and compute the,1,1,1.0
compute the kernel,1,1,1.0
the kernel matrices,1,1,1.0
kernel matrices p,1,1,1.0
matrices p then,1,1,1.0
p then using,1,1,1.0
then using kt,1,1,1.0
using kt a,1,1,1.0
kt a we,2,1,2.0
a we could,1,1,1.0
we could derive,1,1,1.0
could derive a,1,1,1.0
derive a kernel,1,1,1.0
a kernel matrix,1,1,1.0
kernel matrix p,1,1,1.0
matrix p ωk,1,1,1.0
p ωk k,1,1,1.0
ωk k with,1,1,1.0
k with ωk,1,1,1.0
with ωk and,1,1,1.0
ωk and p,1,1,1.0
and p ωk,1,1,1.0
p ωk convex,1,1,1.0
ωk convex combination,1,1,1.0
of kernel matrices,2,1,2.0
kernel matrices k,1,1,1.0
matrices k by,1,1,1.0
k by multiple,1,1,1.0
by multiple kernel,1,1,1.0
learning techniques thus,1,1,1.0
techniques thus this,1,1,1.0
thus this strategy,1,1,1.0
this strategy will,1,1,1.0
strategy will be,1,1,1.0
will be more,2,2,1.0
be more ﬂexible,1,1,1.0
more ﬂexible than,1,1,1.0
ﬂexible than the,1,1,1.0
than the validation,1,1,1.0
the validation one,1,1,1.0
validation one because,1,1,1.0
one because we,1,1,1.0
because we can,1,1,1.0
we can optimise,1,1,1.0
can optimise a,1,1,1.0
optimise a combination,1,1,1.0
kernel matrices instead,1,1,1.0
matrices instead of,1,1,1.0
instead of restricting,1,1,1.0
of restricting the,1,1,1.0
restricting the solution,1,1,1.0
the solution to,2,1,2.0
solution to only,1,1,1.0
to only choosing,1,1,1.0
only choosing the,1,1,1.0
choosing the best,1,1,1.0
the best performing,3,1,3.0
best performing one,1,1,1.0
performing one for,1,1,1.0
one for the,1,1,1.0
the optimisation we,1,1,1.0
optimisation we will,1,1,1.0
we will need,1,1,1.0
will need to,1,1,1.0
need to deﬁne,1,1,1.0
to deﬁne an,1,1,1.0
deﬁne an extended,1,1,1.0
an extended ideal,1,1,1.0
extended ideal kernel,1,1,1.0
matrix by introducing,1,1,1.0
by introducing the,1,1,1.0
introducing the information,1,1,1.0
the information of,1,1,1.0
synthetic patterns recall,1,1,1.0
patterns recall that,1,1,1.0
recall that all,1,1,1.0
that all these,1,1,1.0
all these patterns,1,1,1.0
these patterns will,1,1,1.0
patterns will belong,1,1,1.0
will belong to,1,1,1.0
class the optimisation,1,1,1.0
the optimisation problem,1,1,1.0
optimisation problem to,1,1,1.0
problem to solve,1,1,1.0
to solve in,1,1,1.0
solve in this,1,1,1.0
this case will,1,1,1.0
case will be,1,1,1.0
will be the,1,1,1.0
be the following,1,1,1.0
the following max,1,1,1.0
following max ω,1,1,1.0
max ω c,1,1,1.0
ω c f,1,1,1.0
c f c,1,1,1.0
f c where,1,1,1.0
c where m,1,1,1.0
where m ω,1,1,1.0
m ω note,1,1,1.0
ω note that,1,1,1.0
note that since,1,1,1.0
that since we,1,1,1.0
we are trying,1,1,1.0
trying to align,1,1,1.0
to align the,1,1,1.0
align the real,1,1,1.0
the real kernel,1,1,1.0
real kernel matrix,1,1,1.0
with the ideal,1,1,1.0
ideal one the,1,1,1.0
one the value,1,1,1.0
value of f,1,1,1.0
of f does,1,1,1.0
f does not,1,1,1.0
does not change,1,1,1.0
not change and,1,1,1.0
change and it,1,1,1.0
and it can,1,1,1.0
can be obviated,1,1,1.0
be obviated in,1,1,1.0
obviated in the,1,1,1.0
in the optimisation,1,1,1.0
the optimisation process,1,1,1.0
optimisation process the,1,1,1.0
process the quadratic,1,1,1.0
the quadratic programming,1,1,1.0
quadratic programming qp,1,1,1.0
programming qp optimization,1,1,1.0
qp optimization problem,1,1,1.0
optimization problem associated,1,1,1.0
problem associated can,1,1,1.0
associated can be,1,1,1.0
in fig shows,1,1,1.0
fig shows the,5,2,2.5
shows the representation,1,1,1.0
training data for,2,1,2.0
data for the,2,1,2.0
dataset in different,1,1,1.0
in different efs,1,1,1.0
different efs using,1,1,1.0
efs using the,1,1,1.0
using the formation,1,1,1.0
the formation φ,1,1,1.0
formation φ e,1,1,1.0
φ e original,1,1,1.0
e original efs,1,1,1.0
original efs efs,1,1,1.0
efs efs for,1,1,1.0
efs for β,1,1,1.0
for β and,1,1,1.0
β and β,1,1,1.0
and β and,1,1,1.0
β and optimised,1,1,1.0
and optimised through,1,1,1.0
optimised through mkl,1,1,1.0
through mkl in,1,1,1.0
mkl in this,1,1,1.0
case the difference,2,1,2.0
the difference between,3,2,1.5
difference between for,1,1,1.0
between for different,1,1,1.0
for different β,3,1,3.0
β values could,1,1,1.0
values could be,1,1,1.0
could be difﬁcult,1,1,1.0
be difﬁcult to,1,1,1.0
difﬁcult to appreciate,1,1,1.0
to appreciate however,1,1,1.0
appreciate however for,1,1,1.0
of the optimised,1,1,1.0
the optimised efs,1,1,1.0
optimised efs one,1,1,1.0
efs one can,1,1,1.0
one can note,1,1,1.0
can note that,1,1,1.0
class separation increases,1,1,1.0
separation increases and,1,1,1.0
increases and the,1,1,1.0
and the within,1,1,1.0
within class decreases,1,1,1.0
class decreases recall,1,1,1.0
decreases recall that,1,1,1.0
recall that kt,1,1,1.0
kt a was,1,1,1.0
a was related,1,1,1.0
was related to,1,1,1.0
fisher criterion fig,1,1,1.0
criterion fig empirical,1,1,1.0
fig empirical feature,2,1,2.0
feature spaces for,3,2,1.5
spaces for the,2,1,2.0
the dataset associated,2,1,2.0
dataset associated to,2,1,2.0
associated to the,2,1,2.0
original data for,2,1,2.0
data for different,2,1,2.0
β values and,2,1,2.0
values and optimised,2,1,2.0
and optimised in,1,1,1.0
optimised in the,1,1,1.0
in the same,2,2,1.0
the same vein,1,1,1.0
same vein fig,1,1,1.0
vein fig shows,1,1,1.0
shows the case,1,1,1.0
and the transformation,1,1,1.0
the transformation φ,1,1,1.0
transformation φ e,1,1,1.0
φ e in,1,1,1.0
e in this,1,1,1.0
the difference for,1,1,1.0
difference for the,1,1,1.0
for the procedure,1,1,1.0
the procedure when,1,1,1.0
procedure when using,1,1,1.0
when using different,1,1,1.0
using different β,1,1,1.0
β values can,1,1,1.0
values can be,1,1,1.0
can be easily,1,1,1.0
be easily appreciated,1,1,1.0
easily appreciated vi,1,1,1.0
appreciated vi e,1,1,1.0
vi e x,1,1,1.0
e x p,1,1,1.0
x p e,1,1,1.0
p e r,1,1,1.0
e r i,1,1,1.0
r i m,1,1,1.0
i m e,1,1,1.0
m e n,1,1,1.0
e n ta,1,1,1.0
n ta l,1,1,1.0
ta l r,1,1,1.0
l r e,1,1,1.0
e s u,1,1,1.0
s u lt,1,1,1.0
u lt s,1,1,1.0
lt s the,1,1,1.0
s the proposed,1,1,1.0
the proposed methodologies,1,1,1.0
proposed methodologies have,1,1,1.0
methodologies have been,1,1,1.0
have been tested,1,1,1.0
been tested considering,1,1,1.0
tested considering support,1,1,1.0
considering support v,1,1,1.0
machines svm and,1,1,1.0
svm and the,1,1,1.0
and the smote,1,1,1.0
smote algorithm binary,1,1,1.0
algorithm binary datasets,1,1,1.0
binary datasets from,1,1,1.0
datasets from the,2,2,1.0
from the uci,2,2,1.0
the uci repository,1,1,1.0
uci repository with,1,1,1.0
repository with different,1,1,1.0
with different imbalance,1,1,1.0
imbalance ratios proportion,1,1,1.0
ratios proportion of,1,1,1.0
proportion of majority,1,1,1.0
of majority patterns,1,1,1.0
majority patterns with,1,1,1.0
respect to minority,1,1,1.0
to minority ones,1,1,1.0
minority ones have,1,1,1.0
ones have been,1,1,1.0
have been used,1,1,1.0
used to test,1,1,1.0
to test the,1,1,1.0
test the performance,1,1,1.0
of the methods,4,1,4.0
the methods in,1,1,1.0
methods in different,1,1,1.0
in different situations,1,1,1.0
different situations the,1,1,1.0
situations the characteristics,1,1,1.0
the characteristics of,2,2,1.0
characteristics of these,2,2,1.0
these datasets can,1,1,1.0
datasets can be,1,1,1.0
seen in t,1,1,1.0
in t able,1,1,1.0
t able as,1,1,1.0
able as done,1,1,1.0
other works some,1,1,1.0
works some multiclass,1,1,1.0
some multiclass datasets,1,1,1.0
multiclass datasets have,1,1,1.0
datasets have also,1,1,1.0
have also been,1,1,1.0
also been considered,1,1,1.0
been considered by,1,1,1.0
considered by grouping,1,1,1.0
by grouping some,1,1,1.0
grouping some classes,1,1,1.0
some classes represents,1,1,1.0
classes represents the,1,1,1.0
represents the ecoli,1,1,1.0
the ecoli dataset,1,1,1.0
ecoli dataset when,1,1,1.0
dataset when considering,1,1,1.0
when considering class,1,1,1.0
considering class versus,1,1,1.0
class versus the,1,1,1.0
versus the rest,1,1,1.0
the rest and,1,1,1.0
rest and is,1,1,1.0
and is the,2,1,2.0
is the yeast,1,1,1.0
the yeast dataset,1,1,1.0
yeast dataset when,1,1,1.0
dataset when grouping,1,1,1.0
when grouping classes,1,1,1.0
grouping classes and,1,1,1.0
classes and versus,1,1,1.0
and versus classes,1,1,1.0
versus classes and,1,1,1.0
classes and in,1,1,1.0
and in order,1,1,1.0
order to obtain,2,2,1.0
to obtain higher,1,1,1.0
obtain higher imbalance,1,1,1.0
higher imbalance ratio,1,1,1.0
imbalance ratio ir,1,1,1.0
ratio ir values,1,1,1.0
ir values a,1,1,1.0
values a stratiﬁed,1,1,1.0
a stratiﬁed dietterich,1,1,1.0
stratiﬁed dietterich technique,1,1,1.0
dietterich technique was,1,1,1.0
technique was performed,1,1,1.0
was performed to,1,1,1.0
performed to divide,1,1,1.0
to divide the,1,1,1.0
divide the data,1,1,1.0
and the results,2,1,2.0
results are taken,1,1,1.0
are taken as,1,1,1.0
taken as mean,1,1,1.0
as mean fig,1,1,1.0
mean fig empirical,1,1,1.0
and optimised standard,1,1,1.0
optimised standard deviation,1,1,1.0
standard deviation of,2,1,2.0
deviation of the,2,1,2.0
of the selected,1,1,1.0
the selected measures,1,1,1.0
selected measures as,1,1,1.0
measures as done,1,1,1.0
as done elsewhere,1,1,1.0
done elsewhere each,1,1,1.0
elsewhere each experiment,1,1,1.0
each experiment over,1,1,1.0
experiment over each,1,1,1.0
over each data,1,1,1.0
each data partition,1,1,1.0
data partition has,1,1,1.0
partition has been,1,1,1.0
has been repeated,1,1,1.0
been repeated times,1,1,1.0
repeated times using,1,1,1.0
times using a,1,1,1.0
using a different,1,1,1.0
a different seed,1,1,1.0
different seed to,1,1,1.0
seed to obtain,1,1,1.0
to obtain more,1,1,1.0
obtain more robust,1,1,1.0
more robust at,1,1,1.0
robust at the,1,1,1.0
at the end,1,1,1.0
the end of,1,1,1.0
end of the,1,1,1.0
of the execution,1,1,1.0
the execution we,1,1,1.0
execution we will,1,1,1.0
we will have,1,1,1.0
will have results,1,1,1.0
have results for,1,1,1.0
results for each,1,1,1.0
for each dataset,5,2,2.5
each dataset the,1,1,1.0
dataset the gaussian,1,1,1.0
gaussian kernel was,1,1,1.0
kernel was used,1,1,1.0
was used the,1,1,1.0
used the kernel,1,1,1.0
the kernel width,1,1,1.0
kernel width and,1,1,1.0
width and the,1,1,1.0
and the cost,1,1,1.0
the cost parameter,1,1,1.0
cost parameter of,1,1,1.0
parameter of svm,1,1,1.0
of svm were,1,1,1.0
svm were selected,1,1,1.0
were selected within,1,1,1.0
selected within the,1,1,1.0
within the values,2,1,2.0
the values by,1,1,1.0
values by means,1,1,1.0
means of a,1,1,1.0
of a nested,1,1,1.0
a nested method,1,1,1.0
nested method applied,1,1,1.0
method applied to,1,1,1.0
set as done,1,1,1.0
other works the,1,1,1.0
works the number,1,1,1.0
synthetic patterns generated,1,1,1.0
patterns generated was,1,1,1.0
generated was that,1,1,1.0
was that needed,1,1,1.0
that needed to,1,1,1.0
needed to balance,1,1,1.0
balance the distributions,1,1,1.0
the distributions after,1,1,1.0
distributions after applying,1,1,1.0
after applying the,1,1,1.0
applying the process,1,1,1.0
the process the,2,1,2.0
process the number,1,1,1.0
of majority and,1,1,1.0
and minority patterns,1,1,1.0
minority patterns were,1,1,1.0
patterns were the,1,1,1.0
were the same,1,1,1.0
the same k,1,1,1.0
same k nearest,1,1,1.0
k nearest neighbours,1,1,1.0
nearest neighbours were,1,1,1.0
neighbours were evaluated,1,1,1.0
were evaluated to,1,1,1.0
evaluated to generate,1,1,1.0
generate synthetic samples,1,1,1.0
synthetic samples in,1,1,1.0
samples in order,1,1,1.0
order to minimise,1,1,1.0
to minimise the,1,1,1.0
minimise the chance,1,1,1.0
the chance that,1,1,1.0
chance that synthetic,1,1,1.0
that synthetic patterns,2,1,2.0
synthetic patterns are,2,1,2.0
patterns are generated,1,1,1.0
generated in the,1,1,1.0
class region when,1,1,1.0
region when using,1,1,1.0
using the standard,1,1,1.0
standard smote technique,1,1,1.0
smote technique the,1,1,1.0
technique the results,1,1,1.0
results have been,1,1,1.0
have been reported,1,1,1.0
been reported in,1,1,1.0
reported in terms,2,1,2.0
terms of two,1,1,1.0
of two metrics,1,1,1.0
two metrics one,1,1,1.0
metrics one of,1,1,1.0
one of them,1,1,1.0
of them specially,1,1,1.0
them specially designed,1,1,1.0
specially designed to,1,1,1.0
designed to deal,1,1,1.0
deal with imbalanced,2,1,2.0
imbalanced data the,1,1,1.0
the accuracy metric,1,1,1.0
accuracy metric acc,1,1,1.0
metric acc which,1,1,1.0
acc which sponds,1,1,1.0
which sponds to,1,1,1.0
sponds to the,1,1,1.0
to the ratio,1,1,1.0
ratio of correctly,2,1,2.0
of correctly classiﬁed,4,2,2.0
correctly classiﬁed patterns,2,1,2.0
classiﬁed patterns and,1,1,1.0
patterns and measures,1,1,1.0
and measures overall,1,1,1.0
measures overall performance,1,1,1.0
overall performance for,1,1,1.0
performance for imbalanced,1,1,1.0
imbalanced datasets this,1,1,1.0
datasets this metric,1,1,1.0
this metric may,1,1,1.0
metric may not,1,1,1.0
be the best,1,1,1.0
the best option,1,1,1.0
best option since,1,1,1.0
option since the,1,1,1.0
since the ﬁcation,1,1,1.0
the ﬁcation of,1,1,1.0
ﬁcation of the,1,1,1.0
class may be,1,1,1.0
may be compromised,1,1,1.0
be compromised for,1,1,1.0
compromised for the,1,1,1.0
for the sake,2,1,2.0
the sake of,2,1,2.0
sake of the,1,1,1.0
majority one it,1,1,1.0
one it does,1,1,1.0
does not distinguish,1,1,1.0
not distinguish between,1,1,1.0
distinguish between the,1,1,1.0
between the numbers,1,1,1.0
the numbers of,1,1,1.0
numbers of correctly,1,1,1.0
correctly classiﬁed examples,1,1,1.0
classiﬁed examples of,1,1,1.0
examples of each,1,1,1.0
of each class,1,1,1.0
each class and,1,1,1.0
class and we,1,1,1.0
and we could,1,1,1.0
we could therefore,1,1,1.0
could therefore obtain,1,1,1.0
therefore obtain a,1,1,1.0
trivial classiﬁer always,1,1,1.0
classiﬁer always outputting,1,1,1.0
always outputting the,1,1,1.0
outputting the majority,1,1,1.0
majority class the,1,1,1.0
class the geometric,1,1,1.0
the geometric mean,1,1,1.0
geometric mean of,1,1,1.0
mean of the,2,2,1.0
of the sensitivities,1,1,1.0
the sensitivities gm,1,1,1.0
sensitivities gm sp,1,1,1.0
gm sp sn,1,1,1.0
sp sn where,1,1,1.0
sn where sp,1,1,1.0
where sp is,1,1,1.0
sp is the,1,1,1.0
is the sensitivity,2,1,2.0
the sensitivity for,2,1,2.0
sensitivity for the,2,1,2.0
positive class ratio,1,1,1.0
classiﬁed patterns sidering,1,1,1.0
patterns sidering only,1,1,1.0
sidering only this,1,1,1.0
only this class,1,1,1.0
this class and,1,1,1.0
class and sn,1,1,1.0
and sn is,1,1,1.0
sn is the,1,1,1.0
for the negative,1,1,1.0
the negative one,1,1,1.0
negative one the,1,1,1.0
one the measure,1,1,1.0
the measure for,1,1,1.0
measure for the,1,1,1.0
for the parameter,2,2,1.0
the parameter selection,1,1,1.0
parameter selection was,1,1,1.0
selection was gm,1,1,1.0
was gm given,1,1,1.0
gm given its,1,1,1.0
given its robustness,1,1,1.0
its robustness and,1,1,1.0
robustness and extended,1,1,1.0
and extended use,1,1,1.0
extended use for,1,1,1.0
use for imbalanced,1,1,1.0
imbalanced data note,1,1,1.0
that this metric,1,1,1.0
this metric gives,1,1,1.0
metric gives much,1,1,1.0
gives much importance,1,1,1.0
much importance to,1,1,1.0
importance to recall,1,1,1.0
to recall that,1,1,1.0
recall that synthetic,1,1,1.0
patterns are randomly,1,1,1.0
are randomly generated,1,1,1.0
randomly generated t,1,1,1.0
generated t able,1,1,1.0
t able i,1,1,1.0
able i datasets,1,1,1.0
i datasets used,1,1,1.0
datasets used for,1,1,1.0
used for the,2,1,2.0
for the experiments,2,1,2.0
the experiments n,1,1,1.0
experiments n corresponds,1,1,1.0
n corresponds to,1,1,1.0
to the total,1,1,1.0
the total number,1,1,1.0
total number of,1,1,1.0
of patterns d,1,1,1.0
patterns d to,1,1,1.0
d to the,1,1,1.0
to the dimensionality,1,1,1.0
space and ir,1,1,1.0
and ir to,1,1,1.0
ir to the,1,1,1.0
to the imbalance,1,1,1.0
the imbalance ratio,1,1,1.0
imbalance ratio dataset,1,1,1.0
ratio dataset n,1,1,1.0
dataset n d,2,1,2.0
n d ir,2,1,2.0
d ir dataset,1,1,1.0
ir dataset n,1,1,1.0
d ir wisconsin,1,1,1.0
ir wisconsin pima,1,1,1.0
wisconsin pima haberman,1,1,1.0
pima haberman classes,1,1,1.0
haberman classes being,1,1,1.0
classes being therefore,1,1,1.0
being therefore sensitive,1,1,1.0
therefore sensitive to,1,1,1.0
sensitive to trivial,1,1,1.0
to trivial classiﬁers,1,1,1.0
trivial classiﬁers if,1,1,1.0
classiﬁers if sp,1,1,1.0
if sp then,1,1,1.0
sp then gm,1,1,1.0
then gm independently,1,1,1.0
gm independently of,1,1,1.0
independently of the,1,1,1.0
of the value,1,1,1.0
value of sn,1,1,1.0
of sn the,1,1,1.0
sn the source,1,1,1.0
the source codes,1,1,1.0
source codes in,1,1,1.0
codes in matlab,1,1,1.0
in matlab for,1,1,1.0
matlab for the,1,1,1.0
for the methods,2,1,2.0
the methods developed,1,1,1.0
methods developed in,1,1,1.0
developed in this,1,1,1.0
this paper are,2,2,1.0
paper are available,2,2,1.0
are available together,1,1,1.0
available together with,1,1,1.0
together with the,1,1,1.0
with the datasets,1,1,1.0
the datasets partitions,1,1,1.0
datasets partitions and,1,1,1.0
partitions and the,1,1,1.0
the results on,2,2,1.0
results on the,2,2,1.0
on the website,1,1,1.0
the website associated,1,1,1.0
website associated with,1,1,1.0
associated with this,1,1,1.0
with this paper,1,1,1.0
paper the purpose,1,1,1.0
purpose of this,2,1,2.0
of this section,3,2,1.5
section is the,1,1,1.0
is the ﬁrst,1,1,1.0
the ﬁrst iment,1,1,1.0
ﬁrst iment is,1,1,1.0
iment is intended,1,1,1.0
intended to test,2,1,2.0
whether the empirical,1,1,1.0
kernel map provides,1,1,1.0
map provides a,1,1,1.0
space for than,2,1,2.0
for than the,2,1,2.0
input space when,1,1,1.0
space when dealing,1,1,1.0
kernel methods and,1,1,1.0
methods and analyses,1,1,1.0
analyses the effect,1,1,1.0
of the number,3,2,1.5
number of dimensions,2,1,2.0
of dimensions chosen,1,1,1.0
dimensions chosen for,1,1,1.0
chosen for the,1,1,1.0
for the inﬂuence,1,1,1.0
of the concentration,1,1,1.0
the concentration of,2,1,2.0
concentration of spectral,1,1,1.0
of spectral properties,1,1,1.0
spectral properties the,1,1,1.0
properties the second,1,1,1.0
the second experimental,1,1,1.0
second experimental subsection,1,1,1.0
experimental subsection will,1,1,1.0
subsection will complement,1,1,1.0
will complement the,1,1,1.0
complement the proach,1,1,1.0
the proach proposing,1,1,1.0
proach proposing a,1,1,1.0
proposing a new,1,1,1.0
a new kernel,1,1,1.0
new kernel learning,1,1,1.0
kernel learning algorithm,2,1,2.0
algorithm to optimise,1,1,1.0
to optimise a,1,1,1.0
optimise a more,1,1,1.0
a more ﬂexible,5,1,5.0
more ﬂexible kernel,4,1,4.0
ﬂexible kernel function,1,1,1.0
kernel function which,1,1,1.0
function which would,1,1,1.0
which would ideally,1,1,1.0
would ideally better,1,1,1.0
ideally better ﬁt,1,1,1.0
better ﬁt the,1,1,1.0
ﬁt the data,1,1,1.0
the data the,1,1,1.0
data the purpose,1,1,1.0
of this experiment,1,1,1.0
this experiment is,2,2,1.0
experiment is to,1,1,1.0
whether the kernel,1,1,1.0
kernel function chosen,1,1,1.0
function chosen inﬂuences,1,1,1.0
chosen inﬂuences the,1,1,1.0
inﬂuences the results,1,1,1.0
the results and,1,1,1.0
results and how,1,1,1.0
and how optimising,1,1,1.0
how optimising this,1,1,1.0
optimising this kernel,1,1,1.0
kernel function the,1,1,1.0
function the synthetic,1,1,1.0
the synthetic generated,1,1,1.0
synthetic generated data,1,1,1.0
generated data will,1,1,1.0
data will be,1,1,1.0
will be better,1,1,1.0
be better adapted,1,1,1.0
better adapted to,1,1,1.0
adapted to the,1,1,1.0
to the classiﬁcation,1,1,1.0
the classiﬁcation problem,3,2,1.5
classiﬁcation problem finally,1,1,1.0
problem finally the,1,1,1.0
finally the third,1,1,1.0
the third experiment,1,1,1.0
third experiment focuses,1,1,1.0
experiment focuses on,1,1,1.0
on the case,1,1,1.0
case of weighted,1,1,1.0
of weighted or,1,1,1.0
weighted or preferential,1,1,1.0
or preferential to,1,1,1.0
preferential to analyse,1,1,1.0
be more prone,2,1,2.0
more prone to,2,1,2.0
prone to be,2,1,2.0
to be and,2,2,1.0
be and test,1,1,1.0
and test a,1,1,1.0
test a new,1,1,1.0
a new multiple,1,1,1.0
new multiple kernel,1,1,1.0
learning algorithm for,1,1,1.0
algorithm for optimising,1,1,1.0
optimising the generated,1,1,1.0
the generated patterns,2,1,2.0
generated patterns t,1,1,1.0
patterns t able,1,1,1.0
t able ii,3,1,3.0
able ii contains,1,1,1.0
ii contains information,1,1,1.0
contains information about,1,1,1.0
information about all,1,1,1.0
about all of,1,1,1.0
the methods used,1,1,1.0
methods used for,1,1,1.0
used for this,1,1,1.0
for this experimentation,1,1,1.0
this experimentation and,1,1,1.0
experimentation and a,1,1,1.0
and a brief,1,1,1.0
a brief summary,1,1,1.0
brief summary mean,1,1,1.0
summary mean and,1,1,1.0
mean and standard,2,1,2.0
of the mean,3,1,3.0
the mean results,1,1,1.0
mean results obtained,1,1,1.0
results obtained along,1,1,1.0
obtained along the,1,1,1.0
along the datasets,1,1,1.0
the datasets used,2,2,1.0
datasets used apart,1,1,1.0
used apart from,1,1,1.0
apart from the,1,1,1.0
from the fact,1,1,1.0
that the svm,1,1,1.0
the svm without,1,1,1.0
svm without performs,1,1,1.0
without performs poorly,1,1,1.0
performs poorly for,1,1,1.0
poorly for minority,1,1,1.0
for minority classes,1,1,1.0
minority classes it,1,1,1.0
classes it can,1,1,1.0
be seen that,15,1,15.0
seen that the,5,1,5.0
standard deviation is,1,1,1.0
deviation is very,1,1,1.0
is very high,1,1,1.0
very high in,1,1,1.0
high in gm,1,1,1.0
in gm indicating,1,1,1.0
gm indicating large,1,1,1.0
indicating large ﬂuctuations,1,1,1.0
large ﬂuctuations in,1,1,1.0
ﬂuctuations in the,1,1,1.0
the results one,1,1,1.0
results one can,1,1,1.0
one can also,1,1,1.0
can also see,1,1,1.0
also see that,2,2,1.0
see that the,3,2,1.5
that the optimisation,1,1,1.0
optimisation of a,1,1,1.0
gaussian kernel by,1,1,1.0
kernel by kt,1,1,1.0
by kt a,2,1,2.0
kt a osk,1,1,1.0
a osk does,1,1,1.0
osk does not,1,1,1.0
does not lead,1,1,1.0
not lead to,1,1,1.0
lead to very,1,1,1.0
to very good,1,1,1.0
very good results,2,1,2.0
good results and,1,1,1.0
results and a,1,1,1.0
a better option,1,1,1.0
better option is,1,1,1.0
option is to,1,1,1.0
to use instead,1,1,1.0
use instead or,1,1,1.0
instead or a,1,1,1.0
or a more,1,1,1.0
more ﬂexible http,1,1,1.0
ﬂexible http kernel,1,1,1.0
http kernel as,1,1,1.0
kernel as the,1,1,1.0
as the one,3,1,3.0
the one used,2,1,2.0
one used in,1,1,1.0
used in ogk,1,1,1.0
in ogk further,1,1,1.0
ogk further information,1,1,1.0
further information about,1,1,1.0
about the results,1,1,1.0
the results will,1,1,1.0
results will be,1,1,1.0
will be extracted,1,1,1.0
be extracted using,1,1,1.0
extracted using statistical,1,1,1.0
using statistical tests,1,1,1.0
statistical tests the,1,1,1.0
tests the complete,1,1,1.0
the complete results,1,1,1.0
complete results for,1,1,1.0
results for all,3,2,1.5
for all of,4,2,2.0
the methods can,1,1,1.0
methods can be,1,1,1.0
seen in the,1,1,1.0
in the webpage,1,1,1.0
the webpage associated,1,1,1.0
webpage associated to,1,1,1.0
to this paper,1,1,1.0
this paper including,1,1,1.0
paper including the,1,1,1.0
including the individual,1,1,1.0
the individual results,1,1,1.0
individual results for,1,1,1.0
all the datasets,2,1,2.0
sake of comparison,1,1,1.0
of comparison we,1,1,1.0
comparison we included,1,1,1.0
we included the,1,1,1.0
included the results,1,1,1.0
results obtained by,2,1,2.0
obtained by a,1,1,1.0
by a majority,1,1,1.0
majority class rule,2,1,2.0
class rule mcr,1,1,1.0
rule mcr classiﬁer,1,1,1.0
mcr classiﬁer as,1,1,1.0
classiﬁer as a,1,1,1.0
a baseline result,1,1,1.0
baseline result a,1,1,1.0
result a na,1,1,1.0
a na rule,1,1,1.0
na rule that,1,1,1.0
rule that classify,1,1,1.0
that classify all,1,1,1.0
the patterns as,1,1,1.0
patterns as belonging,1,1,1.0
as belonging to,1,1,1.0
majority class from,1,1,1.0
from the results,5,1,5.0
results of mcr,1,1,1.0
of mcr it,1,1,1.0
mcr it can,1,1,1.0
seen that acc,1,1,1.0
that acc is,1,1,1.0
acc is not,1,1,1.0
not a suitable,1,1,1.0
a suitable metric,1,1,1.0
suitable metric to,1,1,1.0
metric to take,1,1,1.0
to take into,1,1,1.0
take into account,1,1,1.0
into account since,1,1,1.0
account since this,1,1,1.0
since this trivial,1,1,1.0
this trivial methodology,1,1,1.0
trivial methodology achieves,1,1,1.0
methodology achieves the,1,1,1.0
the best results,1,1,1.0
best results in,1,1,1.0
results in some,3,1,3.0
some cases haberman,1,1,1.0
cases haberman and,1,1,1.0
haberman and in,1,1,1.0
and in the,3,2,1.5
the following subsections,1,1,1.0
following subsections we,1,1,1.0
subsections we will,1,1,1.0
we will perform,1,1,1.0
will perform three,1,1,1.0
perform three differentiated,1,1,1.0
three differentiated statistical,1,1,1.0
differentiated statistical tests,1,1,1.0
statistical tests to,1,1,1.0
tests to validate,1,1,1.0
to validate the,1,1,1.0
validate the previously,1,1,1.0
the previously stated,1,1,1.0
previously stated hypotheses,1,1,1.0
stated hypotheses t,1,1,1.0
hypotheses t able,1,1,1.0
able ii abbreviation,1,1,1.0
ii abbreviation for,1,1,1.0
abbreviation for all,1,1,1.0
the methods considered,2,1,2.0
methods considered for,1,1,1.0
considered for the,1,1,1.0
for the experimentation,1,1,1.0
the experimentation and,1,1,1.0
experimentation and mean,1,1,1.0
and mean and,1,1,1.0
standard deviation results,1,1,1.0
deviation results meansd,1,1,1.0
results meansd for,1,1,1.0
meansd for all,1,1,1.0
the datasets algorithm,1,1,1.0
datasets algorithm acc,1,1,1.0
algorithm acc gm,1,1,1.0
acc gm majority,1,1,1.0
gm majority class,1,1,1.0
class rule classiﬁer,1,1,1.0
rule classiﬁer mcr,1,1,1.0
classiﬁer mcr svm,1,1,1.0
mcr svm without,1,1,1.0
svm without sv,1,1,1.0
without sv m,1,1,1.0
sv m svm,1,1,1.0
m svm applying,1,1,1.0
svm applying in,1,1,1.0
applying in the,1,1,1.0
input space ois,2,1,2.0
space ois svm,1,1,1.0
ois svm with,1,1,1.0
svm with in,2,1,2.0
with in the,2,1,2.0
feature space oef,1,1,1.0
space oef s,1,1,1.0
oef s svm,1,1,1.0
s svm with,2,1,2.0
the reduced empirical,2,1,2.0
feature space oref,1,1,1.0
space oref s,1,1,1.0
oref s svm,1,1,1.0
svm with an,4,1,4.0
with an optimised,4,1,4.0
optimised spherical kernel,1,1,1.0
spherical kernel for,1,1,1.0
kernel for sampling,2,1,2.0
for sampling osk,1,1,1.0
sampling osk svm,1,1,1.0
osk svm with,1,1,1.0
optimised generalised kernel,1,1,1.0
generalised kernel for,1,1,1.0
for sampling ogk,1,1,1.0
sampling ogk svm,1,1,1.0
ogk svm with,1,1,1.0
svm with via,2,1,2.0
with via tial,1,1,1.0
via tial learning,1,1,1.0
tial learning ocp,1,1,1.0
learning ocp l,1,1,1.0
ocp l svm,1,1,1.0
l svm with,1,1,1.0
with via preferential,1,1,1.0
via preferential multiple,1,1,1.0
preferential multiple nel,1,1,1.0
multiple nel learning,1,1,1.0
nel learning op,1,1,1.0
learning op mkl,1,1,1.0
op mkl the,2,1,2.0
mkl the best,1,1,1.0
method is in,1,1,1.0
is in bold,1,1,1.0
in bold face,1,1,1.0
bold face and,1,1,1.0
face and the,1,1,1.0
and the second,2,1,2.0
the second one,2,1,2.0
second one in,1,1,1.0
one in italics,1,1,1.0
in italics first,1,1,1.0
italics first experiment,1,1,1.0
first experiment in,1,1,1.0
experiment in the,1,1,1.0
subsection we will,1,1,1.0
we will validate,1,1,1.0
will validate the,1,1,1.0
validate the hypothesis,1,1,1.0
efs is a,1,1,1.0
is a more,1,1,1.0
input space furthermore,1,1,1.0
space furthermore we,1,1,1.0
will test whether,1,1,1.0
test whether by,1,1,1.0
whether by optimising,1,1,1.0
by optimising the,1,1,1.0
optimising the dimensionality,1,1,1.0
dimensionality of this,1,1,1.0
of this space,1,1,1.0
this space the,1,1,1.0
space the generated,1,1,1.0
generated patterns are,1,1,1.0
patterns are more,1,1,1.0
are more adequate,1,1,1.0
more adequate for,1,1,1.0
adequate for the,1,1,1.0
for the classiﬁcation,2,2,1.0
classiﬁcation problem t,1,1,1.0
do so we,3,1,3.0
so we will,1,1,1.0
will test four,1,1,1.0
test four different,1,1,1.0
four different approaches,1,1,1.0
different approaches sv,1,1,1.0
approaches sv m,1,1,1.0
sv m ois,5,1,5.0
m ois oef,3,1,3.0
ois oef s,4,1,4.0
oef s and,7,1,7.0
s and oref,6,1,6.0
and oref s,6,1,6.0
oref s see,1,1,1.0
s see t,1,1,1.0
see t able,4,1,4.0
able ii for,1,1,1.0
ii for the,1,1,1.0
for the meaning,1,1,1.0
the meaning of,1,1,1.0
meaning of the,1,1,1.0
of the acronyms,1,1,1.0
the acronyms as,1,1,1.0
acronyms as said,1,1,1.0
said before we,1,1,1.0
before we discarded,1,1,1.0
we discarded all,1,1,1.0
discarded all dimensions,1,1,1.0
all dimensions that,1,1,1.0
dimensions that correspond,1,1,1.0
that correspond to,1,1,1.0
correspond to zero,1,1,1.0
to zero eigenvalues,1,1,1.0
zero eigenvalues for,1,1,1.0
eigenvalues for the,1,1,1.0
for the computation,1,1,1.0
the efs for,4,1,4.0
efs for oef,1,1,1.0
for oef s,3,1,3.0
oef s furthermore,1,1,1.0
s furthermore we,1,1,1.0
furthermore we performed,1,1,1.0
we performed a,1,1,1.0
performed a nested,1,1,1.0
a nested validation,1,1,1.0
nested validation over,1,1,1.0
validation over the,1,1,1.0
over the training,1,1,1.0
the training sets,1,1,1.0
training sets of,1,1,1.0
sets of the,1,1,1.0
dominant dimensions when,1,1,1.0
dimensions when considering,1,1,1.0
when considering in,1,1,1.0
considering in the,1,1,1.0
reduced efs oref,1,1,1.0
efs oref s,1,1,1.0
oref s t,1,1,1.0
so we considered,1,1,1.0
we considered the,1,1,1.0
considered the following,1,1,1.0
the following values,1,1,1.0
following values for,1,1,1.0
for the q,1,1,1.0
the q value,1,1,1.0
q value of,1,1,1.0
value of the,4,2,2.0
r where r,1,1,1.0
where r is,1,1,1.0
r is the,1,1,1.0
is the original,1,1,1.0
the original rank,1,1,1.0
original rank of,1,1,1.0
rank of the,2,1,2.0
is the ﬂoor,1,1,1.0
the ﬂoor function,1,1,1.0
ﬂoor function it,1,1,1.0
function it can,1,1,1.0
that the results,1,1,1.0
the results in,7,2,3.5
results in gm,1,1,1.0
in gm for,1,1,1.0
gm for sv,1,1,1.0
for sv m,2,1,2.0
sv m are,1,1,1.0
m are in,1,1,1.0
are in general,1,1,1.0
in general very,1,1,1.0
general very poor,1,1,1.0
very poor analyse,1,1,1.0
poor analyse for,1,1,1.0
analyse for example,1,1,1.0
example the case,1,1,1.0
of the haberman,1,1,1.0
the haberman and,1,1,1.0
haberman and datasets,1,1,1.0
and datasets concerning,1,1,1.0
datasets concerning the,1,1,1.0
concerning the ois,1,1,1.0
the ois method,2,1,2.0
ois method it,1,1,1.0
seen that in,2,1,2.0
some cases the,2,1,2.0
cases the results,2,1,2.0
results of oef,1,1,1.0
of oef s,1,1,1.0
oef s are,1,1,1.0
s are much,1,1,1.0
are much better,1,1,1.0
much better analyse,1,1,1.0
better analyse the,1,1,1.0
analyse the result,1,1,1.0
the result of,5,2,2.5
result of the,2,1,2.0
the dataset where,1,1,1.0
dataset where sv,1,1,1.0
where sv m,1,1,1.0
sv m even,1,1,1.0
m even obtained,1,1,1.0
even obtained better,1,1,1.0
obtained better results,2,1,2.0
better results or,1,1,1.0
results or the,1,1,1.0
or the case,1,1,1.0
dataset in relation,1,1,1.0
to the effect,1,1,1.0
effect of controlling,1,1,1.0
of controlling the,1,1,1.0
controlling the dimensionality,1,1,1.0
the dimensionality it,1,1,1.0
dimensionality it can,1,1,1.0
seen that oref,1,1,1.0
that oref s,1,1,1.0
oref s generally,1,1,1.0
s generally yielded,1,1,1.0
generally yielded similar,1,1,1.0
yielded similar or,1,1,1.0
similar or better,1,1,1.0
or better performance,1,1,1.0
performance than oef,1,1,1.0
than oef s,3,1,3.0
oef s see,1,1,1.0
s see the,1,1,1.0
see the result,1,1,1.0
of the and,2,1,2.0
the and datasets,2,1,2.0
and datasets two,1,1,1.0
datasets two examples,1,1,1.0
two examples which,1,1,1.0
examples which will,1,1,1.0
which will be,4,2,2.0
will be afterwards,1,1,1.0
be afterwards analysed,1,1,1.0
afterwards analysed when,1,1,1.0
analysed when taking,1,1,1.0
when taking acc,1,1,1.0
taking acc into,1,1,1.0
acc into account,1,1,1.0
into account it,1,1,1.0
account it can,1,1,1.0
that the three,1,1,1.0
the three methods,2,1,2.0
three methods obtain,1,1,1.0
methods obtain very,2,1,2.0
obtain very similar,2,1,2.0
very similar values,1,1,1.0
similar values although,1,1,1.0
values although oef,1,1,1.0
although oef s,2,1,2.0
oref s obtain,1,1,1.0
s obtain better,1,1,1.0
obtain better results,1,1,1.0
better results in,3,1,3.0
some cases t,1,1,1.0
cases t able,1,1,1.0
t able iii,3,1,3.0
able iii shows,1,1,1.0
iii shows the,1,1,1.0
shows the test,1,1,1.0
the test mean,1,1,1.0
test mean rankings,1,1,1.0
mean rankings for,1,1,1.0
rankings for the,1,1,1.0
for the best,1,1,1.0
best method and,1,1,1.0
method and for,1,1,1.0
for the worst,1,1,1.0
the worst for,1,1,1.0
worst for the,1,1,1.0
methods considered in,2,1,2.0
considered in this,2,1,2.0
in this experiment,1,1,1.0
this experiment along,1,1,1.0
experiment along all,1,1,1.0
along all of,1,1,1.0
the datasets in,1,1,1.0
datasets in terms,1,1,1.0
terms of acc,1,1,1.0
of acc and,1,1,1.0
acc and gm,3,1,3.0
and gm the,1,1,1.0
gm the results,1,1,1.0
show that sv,1,1,1.0
that sv m,1,1,1.0
sv m is,1,1,1.0
m is the,1,1,1.0
is the best,2,2,1.0
best performing method,1,1,1.0
performing method for,1,1,1.0
method for acc,2,1,2.0
for acc but,1,1,1.0
acc but the,1,1,1.0
but the worst,1,1,1.0
the worst performing,1,1,1.0
worst performing when,1,1,1.0
performing when considering,1,1,1.0
when considering a,1,1,1.0
considering a metric,1,1,1.0
a metric that,1,1,1.0
metric that takes,1,1,1.0
that takes into,1,1,1.0
takes into account,1,1,1.0
into account the,1,1,1.0
account the imbalanced,1,1,1.0
the imbalanced nature,2,1,2.0
the data gm,1,1,1.0
data gm furthermore,1,1,1.0
gm furthermore it,1,1,1.0
furthermore it is,1,1,1.0
it is shown,1,1,1.0
is shown that,1,1,1.0
shown that both,1,1,1.0
that both approaches,1,1,1.0
both approaches for,1,1,1.0
approaches for sampling,1,1,1.0
the efs oef,1,1,1.0
efs oef s,1,1,1.0
oref s outperfomed,1,1,1.0
s outperfomed the,1,1,1.0
outperfomed the results,1,1,1.0
results obtained when,1,1,1.0
obtained when in,1,1,1.0
when in the,1,1,1.0
space ois finally,1,1,1.0
ois finally it,1,1,1.0
finally it can,2,1,2.0
seen that controlling,1,1,1.0
that controlling the,1,1,1.0
controlling the efs,1,1,1.0
the efs dimensionality,2,1,2.0
efs dimensionality we,1,1,1.0
dimensionality we improve,1,1,1.0
we improve the,1,1,1.0
improve the results,1,1,1.0
results in most,1,1,1.0
most cases as,1,1,1.0
cases as the,1,1,1.0
as the oref,1,1,1.0
the oref s,3,1,3.0
oref s method,3,1,3.0
s method obtained,2,1,2.0
method obtained better,1,1,1.0
obtained better mean,1,1,1.0
better mean results,1,1,1.0
mean results than,1,1,1.0
results than oef,2,1,2.0
oef s t,1,1,1.0
t o quantify,1,1,1.0
o quantify whether,1,1,1.0
quantify whether a,1,1,1.0
whether a statistical,1,1,1.0
a statistical difference,1,1,1.0
statistical difference exists,1,1,1.0
difference exists among,1,1,1.0
exists among the,1,1,1.0
among the algorithms,1,1,1.0
the algorithms a,1,1,1.0
algorithms a procedure,1,1,1.0
a procedure is,1,1,1.0
procedure is employed,1,1,1.0
is employed to,1,1,1.0
employed to compare,1,1,1.0
to compare multiple,1,1,1.0
compare multiple classiﬁers,1,1,1.0
multiple classiﬁers in,1,1,1.0
classiﬁers in multiple,1,1,1.0
in multiple datasets,1,1,1.0
multiple datasets t,1,1,1.0
datasets t able,1,1,1.0
able iii also,1,1,1.0
iii also shows,1,1,1.0
also shows the,1,1,1.0
shows the result,1,1,1.0
result of applying,2,1,2.0
of applying the,2,1,2.0
applying the statistical,1,1,1.0
the statistical friedman,1,1,1.0
statistical friedman s,1,1,1.0
friedman s test,7,1,7.0
s test for,2,1,2.0
test for a,1,1,1.0
for a signiﬁcance,1,1,1.0
a signiﬁcance level,2,2,1.0
signiﬁcance level of,3,2,1.5
level of α,2,2,1.0
of α to,1,1,1.0
α to the,1,1,1.0
to the mean,1,1,1.0
the mean acc,1,1,1.0
mean acc and,1,1,1.0
and gm rankings,1,1,1.0
gm rankings the,1,1,1.0
rankings the test,1,1,1.0
the test rejects,1,1,1.0
test rejects the,1,1,1.0
rejects the that,1,1,1.0
the that all,2,1,2.0
that all algorithms,1,1,1.0
all algorithms perform,1,1,1.0
algorithms perform similarly,2,1,2.0
perform similarly in,1,1,1.0
similarly in mean,1,1,1.0
in mean ranking,1,1,1.0
mean ranking for,1,1,1.0
ranking for both,1,1,1.0
for both metrics,1,1,1.0
both metrics note,1,1,1.0
metrics note that,1,1,1.0
that for gm,1,1,1.0
for gm the,1,1,1.0
gm the differences,1,1,1.0
the differences are,2,2,1.0
differences are larger,1,1,1.0
are larger t,1,1,1.0
larger t able,1,1,1.0
able iii mean,1,1,1.0
iii mean ranking,1,1,1.0
mean ranking results,6,1,6.0
ranking results for,3,1,3.0
results for sv,1,1,1.0
oref s ranking,1,1,1.0
s ranking sv,1,1,1.0
ranking sv m,1,1,1.0
oef s oref,1,1,1.0
s oref s,2,1,2.0
oref s acc,2,1,2.0
s acc gm,3,1,3.0
acc gm friedman,3,1,3.0
gm friedman s,3,1,3.0
s test conﬁdence,3,1,3.0
test conﬁdence interval,3,1,3.0
conﬁdence interval f,3,1,3.0
interval f α,3,1,3.0
f α gm,3,1,3.0
α gm on,1,1,1.0
gm on the,1,1,1.0
on the basis,2,1,2.0
the basis of,2,1,2.0
basis of this,1,1,1.0
of this rejection,1,1,1.0
this rejection and,1,1,1.0
rejection and following,1,1,1.0
and following the,2,2,1.0
following the guidelines,1,1,1.0
the guidelines of,1,1,1.0
guidelines of we,1,1,1.0
of we consider,1,1,1.0
we consider the,1,1,1.0
consider the best,1,1,1.0
best performing methods,1,1,1.0
performing methods in,1,1,1.0
methods in gm,1,1,1.0
in gm the,1,1,1.0
gm the two,1,1,1.0
the two proposals,1,1,1.0
two proposals oef,1,1,1.0
proposals oef s,1,1,1.0
oref s as,2,1,2.0
s as control,2,1,2.0
as control methods,3,1,3.0
control methods for,1,1,1.0
for the test,1,1,1.0
the test and,1,1,1.0
test and we,1,1,1.0
and we compare,2,1,2.0
we compare them,1,1,1.0
compare them to,1,1,1.0
the rest according,1,1,1.0
rest according to,1,1,1.0
to their rankings,1,1,1.0
their rankings it,1,1,1.0
rankings it has,1,1,1.0
has been noted,1,1,1.0
been noted that,1,1,1.0
the approach of,2,1,2.0
approach of paring,1,1,1.0
of paring all,1,1,1.0
paring all classiﬁers,1,1,1.0
all classiﬁers to,2,1,2.0
classiﬁers to each,1,1,1.0
to each other,1,1,1.0
each other in,1,1,1.0
other in a,1,1,1.0
in a test,1,1,1.0
a test is,1,1,1.0
test is not,1,1,1.0
not as sensitive,1,1,1.0
as sensitive as,1,1,1.0
sensitive as the,1,1,1.0
as the approach,1,1,1.0
approach of comparing,1,1,1.0
of comparing all,1,1,1.0
comparing all classiﬁers,1,1,1.0
classiﬁers to a,1,1,1.0
to a given,1,1,1.0
a given classiﬁer,1,1,1.0
given classiﬁer control,1,1,1.0
classiﬁer control method,1,1,1.0
control method one,1,1,1.0
method one approach,1,1,1.0
one approach to,1,1,1.0
approach to this,1,1,1.0
to this latter,1,1,1.0
this latter type,1,1,1.0
latter type of,1,1,1.0
type of comparison,1,1,1.0
of comparison is,1,1,1.0
comparison is the,1,1,1.0
is the holm,1,1,1.0
the holm s,3,1,3.0
holm s test,5,1,5.0
s test the,2,1,2.0
test the test,2,1,2.0
the test statistics,1,1,1.0
test statistics for,1,1,1.0
statistics for comparing,1,1,1.0
for comparing the,1,1,1.0
comparing the and,1,1,1.0
the and method,1,1,1.0
and method using,1,1,1.0
method using this,1,1,1.0
using this procedure,1,1,1.0
this procedure is,1,1,1.0
procedure is z,1,1,1.0
is z k,1,1,1.0
z k where,1,1,1.0
k where k,1,1,1.0
where k is,2,2,1.0
k is the,1,1,1.0
number of algorithms,1,1,1.0
of algorithms n,1,1,1.0
algorithms n is,1,1,1.0
n is the,1,1,1.0
of datasets and,1,1,1.0
datasets and ri,1,1,1.0
and ri is,1,1,1.0
ri is the,1,1,1.0
is the mean,1,1,1.0
the mean ranking,3,1,3.0
mean ranking of,1,1,1.0
ranking of the,1,1,1.0
method the z,1,1,1.0
the z value,1,1,1.0
z value is,1,1,1.0
value is used,1,1,1.0
used to ﬁnd,1,1,1.0
ﬁnd the corresponding,1,1,1.0
the corresponding probability,1,1,1.0
corresponding probability from,1,1,1.0
probability from the,1,1,1.0
from the table,1,1,1.0
the table of,1,1,1.0
table of normal,1,1,1.0
of normal distribution,1,1,1.0
normal distribution which,1,1,1.0
distribution which is,2,2,1.0
which is compared,1,1,1.0
is compared with,1,1,1.0
compared with an,1,1,1.0
with an appropriate,1,1,1.0
an appropriate level,1,1,1.0
appropriate level of,1,1,1.0
level of signiﬁcance,1,1,1.0
of signiﬁcance α,1,1,1.0
signiﬁcance α holm,1,1,1.0
α holm s,1,1,1.0
s test adjusts,1,1,1.0
test adjusts the,1,1,1.0
adjusts the α,1,1,1.0
the α value,1,1,1.0
α value in,1,1,1.0
value in order,1,1,1.0
compensate for multiple,1,1,1.0
for multiple comparisons,3,1,3.0
multiple comparisons this,1,1,1.0
comparisons this is,1,1,1.0
done in a,1,1,1.0
in a procedure,1,1,1.0
a procedure that,1,1,1.0
procedure that sequentially,1,1,1.0
that sequentially tests,1,1,1.0
sequentially tests the,1,1,1.0
tests the hypotheses,1,1,1.0
the hypotheses ordered,1,1,1.0
hypotheses ordered by,1,1,1.0
ordered by their,1,1,1.0
by their signiﬁcance,1,1,1.0
their signiﬁcance w,1,1,1.0
signiﬁcance w e,1,1,1.0
e will denote,1,1,1.0
will denote the,1,1,1.0
denote the ordered,1,1,1.0
the ordered by,1,1,1.0
ordered by p,1,1,1.0
by p p,1,1,1.0
p p k,1,1,1.0
p k so,1,1,1.0
k so that,1,1,1.0
so that pk,1,1,1.0
that pk holm,1,1,1.0
pk holm s,1,1,1.0
s test compares,1,1,1.0
test compares each,1,1,1.0
compares each pi,1,1,1.0
each pi with,1,1,1.0
pi with α,1,1,1.0
with α holm,1,1,1.0
α holm from,1,1,1.0
holm from the,1,1,1.0
the most signiﬁcant,1,1,1.0
most signiﬁcant p,1,1,1.0
signiﬁcant p value,1,1,1.0
p value if,1,1,1.0
value if is,1,1,1.0
if is below,1,1,1.0
is below k,1,1,1.0
below k the,1,1,1.0
k the hypothesis,1,1,1.0
the hypothesis is,1,1,1.0
hypothesis is rejected,2,1,2.0
is rejected and,1,1,1.0
rejected and we,1,1,1.0
we compare with,1,1,1.0
compare with k,1,1,1.0
with k if,1,1,1.0
k if the,1,1,1.0
if the second,1,1,1.0
the second hypothesis,1,1,1.0
second hypothesis is,1,1,1.0
is rejected the,1,1,1.0
rejected the test,1,1,1.0
the test proceeds,1,1,1.0
test proceeds with,1,1,1.0
proceeds with the,1,1,1.0
with the third,1,1,1.0
the third and,1,1,1.0
third and so,1,1,1.0
so on as,1,1,1.0
on as soon,1,1,1.0
as soon as,1,1,1.0
soon as a,1,1,1.0
as a certain,1,1,1.0
a certain null,1,1,1.0
certain null hypothesis,1,1,1.0
null hypothesis can,2,2,1.0
hypothesis can not,1,1,1.0
not be rejected,1,1,1.0
be rejected all,1,1,1.0
rejected all the,1,1,1.0
the remaining hypotheses,1,1,1.0
remaining hypotheses are,1,1,1.0
hypotheses are retained,1,1,1.0
are retained as,1,1,1.0
retained as well,1,1,1.0
t o analyse,2,1,2.0
o analyse the,2,1,2.0
analyse the results,1,1,1.0
from the holm,1,1,1.0
s test see,1,1,1.0
test see t,1,1,1.0
t able iv,2,1,2.0
able iv for,1,1,1.0
iv for the,1,1,1.0
for the oef,1,1,1.0
the oef s,1,1,1.0
oef s method,1,1,1.0
s method the,2,1,2.0
method the test,1,1,1.0
the test concluded,3,1,3.0
test concluded that,3,1,3.0
concluded that there,3,1,3.0
that there were,2,1,2.0
there were statistically,2,1,2.0
were statistically signiﬁcant,2,1,2.0
statistically signiﬁcant differences,5,1,5.0
signiﬁcant differences with,1,1,1.0
differences with sv,1,1,1.0
with sv m,2,1,2.0
sv m for,2,1,2.0
m for acc,1,1,1.0
for acc note,1,1,1.0
acc note that,1,1,1.0
this case sv,1,1,1.0
case sv m,1,1,1.0
sv m obtained,2,1,2.0
m obtained better,2,1,2.0
better results sv,1,1,1.0
results sv m,1,1,1.0
m for gm,1,1,1.0
for gm and,1,1,1.0
gm and ois,1,1,1.0
and ois for,1,1,1.0
ois for gm,1,1,1.0
for gm as,1,1,1.0
gm as well,1,1,1.0
as well this,1,1,1.0
well this indicates,1,1,1.0
this indicates that,1,1,1.0
indicates that although,1,1,1.0
that although oef,1,1,1.0
oef s obtained,1,1,1.0
s obtained worst,1,1,1.0
obtained worst results,1,1,1.0
worst results for,1,1,1.0
results for acc,1,1,1.0
for acc in,1,1,1.0
acc in comparison,1,1,1.0
in comparison with,1,1,1.0
comparison with sv,1,1,1.0
sv m the,1,1,1.0
m the results,1,1,1.0
the results for,3,2,1.5
results for gm,1,1,1.0
for gm are,1,1,1.0
gm are signiﬁcantly,1,1,1.0
are signiﬁcantly better,1,1,1.0
signiﬁcantly better with,1,1,1.0
better with comparison,1,1,1.0
with comparison to,1,1,1.0
comparison to sv,1,1,1.0
to sv m,2,1,2.0
sv m and,2,1,2.0
m and ois,2,1,2.0
and ois therefore,1,1,1.0
ois therefore giving,1,1,1.0
therefore giving evidence,1,1,1.0
giving evidence that,1,1,1.0
space for sampling,1,1,1.0
for sampling by,1,1,1.0
sampling by convex,1,1,1.0
of patterns concerning,1,1,1.0
patterns concerning the,1,1,1.0
concerning the oref,1,1,1.0
method the same,1,1,1.0
the same results,1,1,1.0
same results are,1,1,1.0
results are obtained,1,1,1.0
are obtained but,1,1,1.0
obtained but there,1,1,1.0
but there are,1,1,1.0
there are also,1,1,1.0
are also signiﬁcant,1,1,1.0
also signiﬁcant differences,1,1,1.0
signiﬁcant differences when,1,1,1.0
differences when considering,1,1,1.0
when considering the,1,1,1.0
considering the ois,1,1,1.0
ois method for,1,1,1.0
for acc which,1,1,1.0
acc which could,1,1,1.0
which could indicate,1,1,1.0
could indicate that,1,1,1.0
indicate that in,1,1,1.0
feature space can,1,1,1.0
be beneﬁcial with,1,1,1.0
beneﬁcial with other,1,1,1.0
with other purposes,1,1,1.0
other purposes for,1,1,1.0
purposes for example,1,1,1.0
for example for,1,1,1.0
example for ensuring,1,1,1.0
for ensuring the,1,1,1.0
ensuring the class,1,1,1.0
class boundaries t,1,1,1.0
boundaries t able,1,1,1.0
able iv results,1,1,1.0
iv results of,1,1,1.0
results of the,4,2,2.0
of the holm,3,1,3.0
the holm procedure,3,1,3.0
holm procedure using,3,1,3.0
procedure using oef,1,1,1.0
using oef s,1,1,1.0
control methods cms,1,1,1.0
methods cms when,1,1,1.0
cms when compared,1,1,1.0
when compared to,6,2,3.0
compared to sv,1,1,1.0
and ois corrected,1,1,1.0
ois corrected α,1,1,1.0
corrected α values,3,1,3.0
α values compared,3,1,3.0
values compared method,3,1,3.0
compared method and,3,1,3.0
method and all,2,1,2.0
and all of,2,1,2.0
all of them,2,1,2.0
of them ordered,2,1,2.0
them ordered by,2,1,2.0
ordered by the,3,1,3.0
by the number,3,1,3.0
number of comparison,3,1,3.0
of comparison i,3,1,3.0
comparison i cm,3,1,3.0
i cm oef,1,1,1.0
cm oef s,1,1,1.0
oef s acc,1,1,1.0
acc gm i,2,1,2.0
gm i α,5,1,5.0
i α method,5,1,5.0
α method pi,5,1,5.0
method pi method,2,1,2.0
pi method pi,2,1,2.0
method pi sv,2,1,2.0
pi sv m,2,1,2.0
sv m sv,2,1,2.0
m sv m,2,1,2.0
m ois ois,2,1,2.0
ois ois oref,1,1,1.0
ois oref s,1,1,1.0
oref s oref,1,1,1.0
oref s cm,1,1,1.0
s cm oref,1,1,1.0
cm oref s,1,1,1.0
ois ois oef,1,1,1.0
oef s oef,1,1,1.0
s oef s,1,1,1.0
oef s win,2,1,2.0
s win or,2,1,2.0
win or lose,4,1,4.0
or lose with,4,1,4.0
lose with statistical,4,1,4.0
with statistical signiﬁcant,3,1,3.0
statistical signiﬁcant difference,2,1,2.0
signiﬁcant difference for,2,1,2.0
difference for α,3,1,3.0
for α in,1,1,1.0
α in relation,1,1,1.0
the optimal dimensionality,2,1,2.0
optimal dimensionality of,3,1,3.0
the efs it,1,1,1.0
efs it can,1,1,1.0
that the decay,1,1,1.0
the decay rate,1,1,1.0
decay rate of,1,1,1.0
rate of the,2,1,2.0
of the eigenvalues,1,1,1.0
the eigenvalues is,1,1,1.0
eigenvalues is related,1,1,1.0
to the smoothness,1,1,1.0
the smoothness of,1,1,1.0
smoothness of the,1,1,1.0
the kernel and,2,1,2.0
kernel and the,2,1,2.0
number of necessary,1,1,1.0
of necessary dimensions,1,1,1.0
necessary dimensions depends,1,1,1.0
dimensions depends on,1,1,1.0
on the interplay,1,1,1.0
the interplay between,1,1,1.0
interplay between the,1,1,1.0
and the dataset,1,1,1.0
case the mean,1,1,1.0
the mean value,2,1,2.0
mean value obtained,1,1,1.0
from the step,2,1,2.0
the step for,1,1,1.0
step for the,1,1,1.0
for the number,2,2,1.0
of dimensions was,1,1,1.0
dimensions was more,1,1,1.0
was more speciﬁcally,1,1,1.0
more speciﬁcally fig,1,1,1.0
speciﬁcally fig shows,1,1,1.0
shows the histogram,2,1,2.0
the histogram of,2,1,2.0
histogram of the,3,1,3.0
of the optimal,1,1,1.0
efs for all,2,1,2.0
the datasets tested,1,1,1.0
datasets tested where,1,1,1.0
tested where it,1,1,1.0
where it can,1,1,1.0
that in most,1,1,1.0
the cases is,1,1,1.0
cases is enough,1,1,1.0
is enough to,1,1,1.0
enough to contain,1,1,1.0
to contain all,1,1,1.0
contain all the,1,1,1.0
all the relevant,1,1,1.0
the relevant information,1,1,1.0
relevant information about,1,1,1.0
about the dataset,1,1,1.0
the dataset as,2,2,1.0
dataset as said,1,1,1.0
said before one,1,1,1.0
before one of,1,1,1.0
of the hypothesis,2,2,1.0
the hypothesis for,1,1,1.0
hypothesis for controlling,1,1,1.0
for controlling the,1,1,1.0
controlling the mensionality,1,1,1.0
the mensionality of,1,1,1.0
mensionality of the,1,1,1.0
efs was that,1,1,1.0
was that our,1,1,1.0
that our algorithm,1,1,1.0
our algorithm relies,1,1,1.0
algorithm relies on,1,1,1.0
on distances computed,1,1,1.0
distances computed in,1,1,1.0
computed in the,1,1,1.0
efs for computing,1,1,1.0
for computing nearest,1,1,1.0
computing nearest neighbours,1,1,1.0
nearest neighbours and,1,1,1.0
neighbours and choosing,1,1,1.0
and choosing which,1,1,1.0
choosing which patterns,1,1,1.0
which patterns to,1,1,1.0
patterns to distances,1,1,1.0
to distances which,1,1,1.0
distances which may,1,1,1.0
which may bear,1,1,1.0
may bear less,1,1,1.0
bear less neighbourhood,1,1,1.0
less neighbourhood information,1,1,1.0
neighbourhood information as,1,1,1.0
information as the,1,1,1.0
as the efs,2,1,2.0
efs dimensionality increases,1,1,1.0
dimensionality increases fig,1,1,1.0
increases fig shows,1,1,1.0
histogram of distances,2,1,2.0
of distances between,2,1,2.0
distances between pairs,1,1,1.0
between pairs of,1,1,1.0
pairs of patterns,1,1,1.0
of patterns for,2,1,2.0
patterns for different,2,1,2.0
for different values,1,1,1.0
different values of,1,1,1.0
values of the,2,1,2.0
of the dimensionality,2,1,2.0
efs and for,1,1,1.0
and for two,1,1,1.0
for two datasets,1,1,1.0
two datasets where,1,1,1.0
datasets where the,2,1,2.0
where the oref,1,1,1.0
method obtained much,1,1,1.0
obtained much better,1,1,1.0
much better results,2,1,2.0
better results than,2,1,2.0
s and where,1,1,1.0
and where this,1,1,1.0
where this spectral,1,1,1.0
this spectral erties,1,1,1.0
spectral erties phenomenon,1,1,1.0
erties phenomenon can,1,1,1.0
phenomenon can be,1,1,1.0
can be appreciated,1,1,1.0
be appreciated note,1,1,1.0
appreciated note that,1,1,1.0
that for the,1,1,1.0
dataset using all,1,1,1.0
using all of,1,1,1.0
of the dimensions,1,1,1.0
the dimensions corresponds,1,1,1.0
dimensions corresponds to,1,1,1.0
corresponds to in,1,1,1.0
to in an,1,1,1.0
in an almost,1,1,1.0
an almost randomly,1,1,1.0
almost randomly fashion,1,1,1.0
randomly fashion r,1,1,1.0
fashion r fig,1,1,1.0
r fig histogram,1,1,1.0
fig histogram of,3,1,3.0
the mean optimal,1,1,1.0
mean optimal dimensionality,1,1,1.0
for all datasets,1,1,1.0
all datasets the,1,1,1.0
datasets the abscissa,1,1,1.0
the abscissa axis,2,1,2.0
abscissa axis represents,2,1,2.0
axis represents the,2,1,2.0
represents the mean,1,1,1.0
mean value over,1,1,1.0
value over the,1,1,1.0
over the results,1,1,1.0
results for the,3,2,1.5
for the rate,1,1,1.0
the rate of,1,1,1.0
of the rank,1,1,1.0
kernel matrix the,1,1,1.0
matrix the ordinate,1,1,1.0
the ordinate axis,2,1,2.0
ordinate axis shows,1,1,1.0
axis shows the,1,1,1.0
shows the number,1,1,1.0
of datasets where,2,1,2.0
datasets where this,1,1,1.0
where this value,1,1,1.0
this value was,1,1,1.0
value was selected,2,1,2.0
was selected from,2,1,2.0
selected from the,2,1,2.0
the step as,1,1,1.0
step as the,1,1,1.0
as the neighbours,1,1,1.0
the neighbours rule,1,1,1.0
neighbours rule will,1,1,1.0
rule will not,1,1,1.0
not be very,1,1,1.0
be very precise,1,1,1.0
very precise since,1,1,1.0
precise since most,1,1,1.0
since most of,1,1,1.0
of the distances,1,1,1.0
the distances between,1,1,1.0
distances between pair,2,1,2.0
between pair of,2,1,2.0
pair of patterns,2,1,2.0
of patterns are,1,1,1.0
patterns are similar,1,1,1.0
are similar fig,1,1,1.0
similar fig histogram,1,1,1.0
for different dimensionality,1,1,1.0
different dimensionality values,1,1,1.0
dimensionality values of,1,1,1.0
efs the abscissa,1,1,1.0
distance between two,1,1,1.0
between two patterns,1,1,1.0
two patterns and,1,1,1.0
and the ordinate,1,1,1.0
ordinate axis the,1,1,1.0
axis the occurrence,1,1,1.0
the occurrence of,1,1,1.0
occurrence of each,1,1,1.0
of each distance,1,1,1.0
each distance from,1,1,1.0
distance from the,2,2,1.0
the results several,1,1,1.0
results several conclusions,1,1,1.0
several conclusions can,2,1,2.0
conclusions can be,2,1,2.0
can be drawn,2,1,2.0
be drawn firstly,1,1,1.0
drawn firstly by,1,1,1.0
firstly by convex,1,1,1.0
combination is more,1,1,1.0
is more suitable,1,1,1.0
more suitable in,1,1,1.0
suitable in an,1,1,1.0
in an ideally,1,1,1.0
an ideally linearly,1,1,1.0
ideally linearly separable,1,1,1.0
linearly separable space,1,1,1.0
separable space such,1,1,1.0
space such as,1,1,1.0
efs the method,1,1,1.0
the method obtains,2,1,2.0
method obtains better,2,1,2.0
obtains better results,1,1,1.0
results in metrics,1,1,1.0
in metrics that,1,1,1.0
metrics that consider,1,1,1.0
that consider the,1,1,1.0
consider the imbalanced,1,1,1.0
the data without,1,1,1.0
data without compromising,1,1,1.0
without compromising the,1,1,1.0
compromising the overall,1,1,1.0
the overall accuracy,1,1,1.0
overall accuracy however,1,1,1.0
accuracy however in,1,1,1.0
however in the,1,1,1.0
input space does,1,1,1.0
space does not,1,1,1.0
does not achieve,1,1,1.0
not achieve this,1,1,1.0
achieve this balance,1,1,1.0
this balance indicating,1,1,1.0
balance indicating that,1,1,1.0
indicating that a,1,1,1.0
that a convex,1,1,1.0
of patterns in,1,1,1.0
patterns in a,1,1,1.0
in a possibly,1,1,1.0
a possibly nonlinearly,1,1,1.0
possibly nonlinearly separable,1,1,1.0
nonlinearly separable space,1,1,1.0
separable space could,1,1,1.0
space could generate,1,1,1.0
could generate patterns,1,1,1.0
generate patterns in,1,1,1.0
patterns in unwanted,1,1,1.0
in unwanted areas,1,1,1.0
unwanted areas concerning,1,1,1.0
areas concerning the,1,1,1.0
concerning the optimisation,1,1,1.0
optimisation of dominant,1,1,1.0
dominant dimensions for,1,1,1.0
dimensions for the,1,1,1.0
for the feature,1,1,1.0
feature space this,1,1,1.0
space this methodology,1,1,1.0
this methodology improves,1,1,1.0
methodology improves the,1,1,1.0
improves the results,1,1,1.0
some cases thus,1,1,1.0
cases thus encouraging,1,1,1.0
thus encouraging further,1,1,1.0
encouraging further development,1,1,1.0
further development of,1,1,1.0
development of an,1,1,1.0
of an analytical,1,1,1.0
an analytical method,1,1,1.0
analytical method to,1,1,1.0
method to do,1,1,1.0
do so second,1,1,1.0
so second experiment,1,1,1.0
second experiment inﬂuence,1,1,1.0
experiment inﬂuence of,1,1,1.0
function for this,1,1,1.0
experiment we compare,1,1,1.0
we compare three,1,1,1.0
compare three different,1,1,1.0
three different proposals,1,1,1.0
different proposals ﬁrstly,1,1,1.0
proposals ﬁrstly oef,1,1,1.0
ﬁrstly oef s,1,1,1.0
oef s which,1,1,1.0
s which will,1,1,1.0
be used as,1,1,1.0
a baseline method,1,1,1.0
baseline method to,1,1,1.0
method to if,1,1,1.0
to if the,1,1,1.0
if the optimisation,1,1,1.0
kernel function leads,1,1,1.0
function leads to,1,1,1.0
leads to better,1,1,1.0
to better results,2,1,2.0
better results secondly,1,1,1.0
results secondly svm,1,1,1.0
secondly svm with,1,1,1.0
gaussian kernel the,1,1,1.0
kernel the same,1,1,1.0
the same kernel,1,1,1.0
same kernel than,1,1,1.0
kernel than for,1,1,1.0
than for oef,1,1,1.0
oef s but,1,1,1.0
s but optimised,1,1,1.0
but optimised through,1,1,1.0
optimised through kt,1,1,1.0
through kt a,2,1,2.0
a for performing,1,1,1.0
for performing the,2,1,2.0
performing the in,1,1,1.0
the in the,1,1,1.0
feature space osk,1,1,1.0
space osk and,1,1,1.0
osk and ﬁnally,1,1,1.0
and ﬁnally svm,1,1,1.0
ﬁnally svm with,1,1,1.0
gaussian kernel in,1,1,1.0
kernel in the,2,1,2.0
feature space ogk,1,1,1.0
space ogk in,1,1,1.0
ogk in this,1,1,1.0
in this work,1,1,1.0
this work the,1,1,1.0
work the irprop,1,1,1.0
the irprop algorithm,1,1,1.0
irprop algorithm is,1,1,1.0
algorithm is used,1,1,1.0
used to optimise,1,1,1.0
to optimise the,1,1,1.0
optimise the aforementioned,1,1,1.0
the aforementioned centred,1,1,1.0
aforementioned centred kt,1,1,1.0
kt a because,1,1,1.0
a because of,1,1,1.0
because of its,1,1,1.0
of its robustness,1,1,1.0
its robustness the,1,1,1.0
robustness the gradient,1,1,1.0
the gradient norm,1,1,1.0
gradient norm stopping,1,1,1.0
norm stopping criterion,1,1,1.0
stopping criterion was,1,1,1.0
criterion was set,1,1,1.0
was set to,1,1,1.0
set to and,1,1,1.0
to and the,1,1,1.0
and the maximum,1,1,1.0
the maximum number,1,1,1.0
maximum number of,1,1,1.0
number of conjugate,1,1,1.0
of conjugate gradient,1,1,1.0
conjugate gradient steps,1,1,1.0
gradient steps to,1,1,1.0
steps to for,1,1,1.0
to for the,1,1,1.0
optimisation of ogk,1,1,1.0
of ogk we,1,1,1.0
ogk we also,1,1,1.0
we also included,1,1,1.0
also included a,1,1,1.0
included a γ,1,1,1.0
a γ parameter,1,1,1.0
γ parameter as,1,1,1.0
parameter as an,1,1,1.0
as an additional,1,1,1.0
an additional parameter,1,1,1.0
additional parameter in,1,1,1.0
parameter in the,1,1,1.0
in the generalised,1,1,1.0
the generalised gaussian,2,1,2.0
gaussian kernel which,1,1,1.0
kernel which will,1,1,1.0
which will indeed,1,1,1.0
will indeed make,1,1,1.0
indeed make the,1,1,1.0
make the parameters,1,1,1.0
the parameters initialisation,1,1,1.0
parameters initialisation easier,1,1,1.0
initialisation easier the,1,1,1.0
easier the initial,1,1,1.0
the initial point,1,1,1.0
initial point for,1,1,1.0
point for γ,1,1,1.0
for γ for,1,1,1.0
γ for all,1,1,1.0
the methods tested,1,1,1.0
methods tested was,1,1,1.0
tested was chosen,1,1,1.0
was chosen from,1,1,1.0
chosen from the,1,1,1.0
from the set,2,2,1.0
the set analysing,1,1,1.0
set analysing the,1,1,1.0
analysing the best,1,1,1.0
the best result,1,1,1.0
best result in,1,1,1.0
result in alignment,1,1,1.0
in alignment for,1,1,1.0
alignment for the,1,1,1.0
for the three,2,1,2.0
the three values,1,1,1.0
three values the,1,1,1.0
values the q,1,1,1.0
the q matrix,1,1,1.0
q matrix for,1,1,1.0
matrix for the,1,1,1.0
for the generalised,1,1,1.0
gaussian kernel is,2,1,2.0
kernel is initialised,1,1,1.0
is initialised as,1,1,1.0
initialised as the,1,1,1.0
as the pseudoinverse,1,1,1.0
the pseudoinverse of,1,1,1.0
pseudoinverse of the,1,1,1.0
of the covariance,1,1,1.0
the covariance of,1,1,1.0
covariance of the,1,1,1.0
training points q,1,1,1.0
points q cov,1,1,1.0
q cov xtr,1,1,1.0
cov xtr to,1,1,1.0
xtr to address,1,1,1.0
address the problem,1,1,1.0
problem of covariance,1,1,1.0
of covariance matrices,1,1,1.0
covariance matrices once,1,1,1.0
matrices once the,1,1,1.0
once the kernel,1,1,1.0
the kernel has,1,1,1.0
kernel has been,3,1,3.0
has been optimised,1,1,1.0
been optimised via,1,1,1.0
optimised via kt,1,1,1.0
via kt a,1,1,1.0
a we optimise,1,1,1.0
we optimise the,1,1,1.0
optimise the c,1,1,1.0
the c parameter,1,1,1.0
c parameter using,1,1,1.0
parameter using validation,1,1,1.0
using validation within,1,1,1.0
validation within the,1,1,1.0
the values this,1,1,1.0
values this two,1,1,1.0
this two stage,1,1,1.0
two stage optimisation,1,1,1.0
stage optimisation method,1,1,1.0
optimisation method is,1,1,1.0
method is also,1,1,1.0
is also referred,1,1,1.0
also referred in,1,1,1.0
the literature as,1,1,1.0
literature as method,1,1,1.0
as method from,1,1,1.0
method from the,1,1,1.0
the results that,1,1,1.0
results that can,1,1,1.0
found in the,1,1,1.0
in the website,1,1,1.0
the website one,1,1,1.0
website one can,1,1,1.0
one can see,6,2,3.0
see that osk,1,1,1.0
that osk and,1,1,1.0
osk and ogk,2,1,2.0
and ogk obtained,1,1,1.0
ogk obtained in,1,1,1.0
obtained in some,1,1,1.0
some cases better,1,1,1.0
cases better results,1,1,1.0
results in acc,1,1,1.0
in acc than,1,1,1.0
acc than sv,1,1,1.0
than sv m,1,1,1.0
sv m this,1,1,1.0
m this could,1,1,1.0
this could be,1,1,1.0
could be due,1,1,1.0
be due to,1,1,1.0
the kernel optimisation,1,1,1.0
kernel optimisation through,1,1,1.0
optimisation through kt,1,1,1.0
kt a which,1,1,1.0
a which selected,1,1,1.0
which selected a,1,1,1.0
selected a more,1,1,1.0
a more optimal,1,1,1.0
more optimal kernel,1,1,1.0
optimal kernel than,1,1,1.0
kernel than the,1,1,1.0
than the method,1,1,1.0
the method analysing,1,1,1.0
method analysing gm,1,1,1.0
analysing gm it,1,1,1.0
gm it can,1,1,1.0
of the spherical,1,1,1.0
the spherical gaussian,2,1,2.0
kernel is not,1,1,1.0
is not satisfactory,1,1,1.0
not satisfactory in,1,1,1.0
satisfactory in optimising,1,1,1.0
in optimising the,1,1,1.0
optimising the spherical,1,1,1.0
the spherical kernel,1,1,1.0
spherical kernel a,1,1,1.0
kernel a methodology,1,1,1.0
a methodology should,1,1,1.0
methodology should be,1,1,1.0
should be preferred,2,1,2.0
be preferred to,1,1,1.0
preferred to kt,1,1,1.0
to kt t,1,1,1.0
kt t o,1,1,1.0
t o see,1,1,1.0
o see this,1,1,1.0
see this analyse,1,1,1.0
this analyse the,1,1,1.0
analyse the case,1,1,1.0
and datasets where,1,1,1.0
datasets where although,1,1,1.0
where although osk,1,1,1.0
although osk incorporates,1,1,1.0
osk incorporates a,1,1,1.0
incorporates a stage,1,1,1.0
a stage sv,1,1,1.0
stage sv m,1,1,1.0
obtained better gm,1,1,1.0
better gm results,2,1,2.0
gm results finally,1,1,1.0
results finally it,1,1,1.0
seen that ogk,1,1,1.0
that ogk yielded,1,1,1.0
ogk yielded a,1,1,1.0
yielded a much,1,1,1.0
a much better,1,1,1.0
much better mance,1,1,1.0
better mance in,1,1,1.0
mance in most,1,1,1.0
the cases analyse,1,1,1.0
cases analyse the,1,1,1.0
analyse the dataset,1,1,1.0
the dataset demonstrating,1,1,1.0
dataset demonstrating therefore,1,1,1.0
demonstrating therefore that,1,1,1.0
therefore that a,1,1,1.0
that a more,1,1,1.0
ﬂexible kernel combined,1,1,1.0
kernel combined with,1,1,1.0
combined with kernel,1,1,1.0
with kernel learning,1,1,1.0
learning techniques could,2,1,2.0
techniques could optimise,1,1,1.0
could optimise the,1,1,1.0
optimise the separation,1,1,1.0
separation of the,1,1,1.0
classes in the,1,1,1.0
space a necessary,1,1,1.0
a necessary condition,1,1,1.0
necessary condition for,1,1,1.0
condition for by,1,1,1.0
of patterns as,2,1,2.0
patterns as done,1,1,1.0
as done before,1,1,1.0
done before t,1,1,1.0
before t able,1,1,1.0
t able v,2,1,2.0
able v shows,1,1,1.0
v shows the,2,2,1.0
shows the mean,2,1,2.0
three methods considered,1,1,1.0
this subsection and,1,1,1.0
subsection and the,1,1,1.0
and the result,1,1,1.0
applying the friedman,1,1,1.0
the friedman s,2,1,2.0
the test accepted,1,1,1.0
test accepted the,2,1,2.0
accepted the that,2,1,2.0
that all of,1,1,1.0
of the algorithms,1,1,1.0
the algorithms perform,2,1,2.0
perform similarly for,1,1,1.0
similarly for acc,1,1,1.0
for acc and,3,1,3.0
acc and rejected,2,1,2.0
and rejected it,2,1,2.0
rejected it for,2,1,2.0
it for gm,2,1,2.0
for gm from,2,1,2.0
gm from the,1,1,1.0
results obtained it,2,1,2.0
obtained it can,3,2,1.5
seen that when,1,1,1.0
when using a,2,1,2.0
using a spherical,1,1,1.0
gaussian kernel as,1,1,1.0
kernel as in,1,1,1.0
as in oef,1,1,1.0
in oef s,1,1,1.0
oef s optimised,1,1,1.0
s optimised through,1,1,1.0
optimised through and,1,1,1.0
through and osk,1,1,1.0
and osk optimised,1,1,1.0
osk optimised by,1,1,1.0
optimised by kt,1,1,1.0
kt a the,1,1,1.0
a the results,1,1,1.0
results are comparable,1,1,1.0
are comparable and,1,1,1.0
comparable and the,1,1,1.0
and the methods,1,1,1.0
the methods obtain,1,1,1.0
very similar mean,1,1,1.0
similar mean ranking,1,1,1.0
ranking results in,1,1,1.0
results in this,3,2,1.5
this case it,1,1,1.0
clear that the,1,1,1.0
that the method,1,1,1.0
obtains better gm,1,1,1.0
gm results as,1,1,1.0
results as this,1,1,1.0
as this is,1,1,1.0
is the metric,1,1,1.0
the metric used,1,1,1.0
metric used for,1,1,1.0
for the parameters,1,1,1.0
the parameters selection,1,1,1.0
parameters selection stage,1,1,1.0
selection stage however,1,1,1.0
stage however when,1,1,1.0
however when using,1,1,1.0
using a more,1,1,1.0
ﬂexible kernel such,2,1,2.0
kernel such as,2,1,2.0
the one considered,1,1,1.0
one considered in,1,1,1.0
considered in the,1,1,1.0
in the ogk,1,1,1.0
the ogk method,1,1,1.0
ogk method the,1,1,1.0
method the results,1,1,1.0
the results can,1,1,1.0
results can be,1,1,1.0
can be signiﬁcantly,1,1,1.0
be signiﬁcantly improved,1,1,1.0
signiﬁcantly improved note,1,1,1.0
improved note that,1,1,1.0
note that applying,1,1,1.0
that applying to,1,1,1.0
applying to the,1,1,1.0
to the generalised,1,1,1.0
the generalised kernel,1,1,1.0
generalised kernel could,1,1,1.0
kernel could possibly,1,1,1.0
could possibly improve,1,1,1.0
possibly improve gm,1,1,1.0
improve gm results,1,1,1.0
gm results but,1,1,1.0
results but the,1,1,1.0
but the computational,1,1,1.0
the computational task,1,1,1.0
computational task required,1,1,1.0
task required would,1,1,1.0
required would be,1,1,1.0
would be infeasible,1,1,1.0
be infeasible on,1,1,1.0
infeasible on the,1,1,1.0
basis of friedman,1,1,1.0
of friedman s,1,1,1.0
s test rejection,1,1,1.0
test rejection the,1,1,1.0
rejection the holm,1,1,1.0
the holm test,1,1,1.0
holm test t,1,1,1.0
test t able,1,1,1.0
able v mean,1,1,1.0
v mean ranking,1,1,1.0
results for oef,1,1,1.0
oef s osk,2,1,2.0
s osk and,1,1,1.0
and ogk ranking,1,1,1.0
ogk ranking oef,1,1,1.0
ranking oef s,2,1,2.0
s osk ogk,1,1,1.0
osk ogk acc,1,1,1.0
ogk acc gm,1,1,1.0
α gm for,1,1,1.0
gm for multiple,1,1,1.0
multiple comparisons has,2,1,2.0
comparisons has been,2,1,2.0
been applied see,1,1,1.0
applied see t,1,1,1.0
t able vi,2,1,2.0
able vi and,1,1,1.0
vi and the,1,1,1.0
and the test,1,1,1.0
signiﬁcant differences for,5,1,5.0
differences for gm,2,1,2.0
for gm when,2,1,2.0
gm when considering,1,1,1.0
when considering osk,1,1,1.0
considering osk and,1,1,1.0
osk and oef,2,1,2.0
and oef s,2,1,2.0
oef s as,1,1,1.0
s as said,1,1,1.0
as said there,1,1,1.0
said there were,1,1,1.0
there were no,1,1,1.0
were no statistically,1,1,1.0
no statistically signiﬁcant,1,1,1.0
differences for acc,1,1,1.0
for acc t,1,1,1.0
acc t able,2,1,2.0
able vi results,1,1,1.0
vi results of,1,1,1.0
procedure using ogk,1,1,1.0
using ogk as,1,1,1.0
ogk as the,1,1,1.0
as the control,1,1,1.0
the control method,1,1,1.0
control method when,1,1,1.0
method when compared,1,1,1.0
compared to osk,1,1,1.0
to osk and,1,1,1.0
oef s corrected,1,1,1.0
s corrected α,1,1,1.0
i cm ogk,1,1,1.0
cm ogk gm,1,1,1.0
ogk gm i,1,1,1.0
method pi osk,1,1,1.0
pi osk oef,1,1,1.0
osk oef s,1,1,1.0
statistical signiﬁcant differences,1,1,1.0
differences for α,2,1,2.0
for α the,1,1,1.0
α the results,1,1,1.0
this subsection show,1,1,1.0
subsection show that,1,1,1.0
show that in,1,1,1.0
efs is affected,1,1,1.0
is affected by,1,1,1.0
kernel function although,1,1,1.0
function although spherical,1,1,1.0
although spherical gaussian,1,1,1.0
gaussian kernel has,2,1,2.0
has been proven,1,1,1.0
proven to show,1,1,1.0
to show promising,1,1,1.0
show promising results,1,1,1.0
promising results in,1,1,1.0
the previous subsection,1,1,1.0
previous subsection kernel,1,1,1.0
subsection kernel which,1,1,1.0
kernel which is,1,1,1.0
which is indeed,1,1,1.0
is indeed a,1,1,1.0
indeed a complex,1,1,1.0
a complex issue,1,1,1.0
complex issue shows,1,1,1.0
issue shows much,1,1,1.0
shows much better,1,1,1.0
better results when,1,1,1.0
results when ploying,1,1,1.0
when ploying a,1,1,1.0
ploying a more,1,1,1.0
one used therefore,1,1,1.0
used therefore different,1,1,1.0
therefore different kernel,1,1,1.0
different kernel learning,1,1,1.0
techniques could be,2,1,2.0
could be explored,1,1,1.0
be explored in,2,1,2.0
explored in the,2,1,2.0
in the future,2,1,2.0
the future for,1,1,1.0
future for the,1,1,1.0
purpose of in,1,1,1.0
the efs third,1,1,1.0
efs third experiment,1,1,1.0
third experiment preferential,1,1,1.0
experiment preferential this,1,1,1.0
preferential this experimental,1,1,1.0
this experimental subsection,1,1,1.0
experimental subsection is,1,1,1.0
subsection is intended,1,1,1.0
to test if,1,1,1.0
test if there,1,1,1.0
if there are,1,1,1.0
there are patterns,1,1,1.0
are patterns which,1,1,1.0
patterns which are,1,1,1.0
which are more,1,1,1.0
for and if,1,1,1.0
and if a,1,1,1.0
if a general,1,1,1.0
a general adaptive,1,1,1.0
general adaptive approach,1,1,1.0
adaptive approach yielding,1,1,1.0
approach yielding solutions,1,1,1.0
yielding solutions based,1,1,1.0
solutions based on,1,1,1.0
based on unweighted,1,1,1.0
on unweighted borderline,1,1,1.0
unweighted borderline weighted,1,1,1.0
borderline weighted or,1,1,1.0
weighted or safe,1,1,1.0
or safe level,1,1,1.0
safe level weighted,1,1,1.0
level weighted could,1,1,1.0
weighted could achieve,1,1,1.0
could achieve better,1,1,1.0
achieve better results,1,1,1.0
results than standard,1,1,1.0
than standard unweighted,1,1,1.0
standard unweighted t,1,1,1.0
unweighted t o,1,1,1.0
so we compare,1,1,1.0
we compare oef,1,1,1.0
compare oef s,1,1,1.0
oef s to,1,1,1.0
s to two,1,1,1.0
to two different,1,1,1.0
two different approaches,1,1,1.0
different approaches the,1,1,1.0
ﬁrst one based,1,1,1.0
one based on,2,1,2.0
on a strategy,1,1,1.0
a strategy ocp,1,1,1.0
strategy ocp l,1,1,1.0
ocp l and,5,1,5.0
l and the,1,1,1.0
second one based,1,1,1.0
based on kernel,1,1,1.0
on kernel learning,1,1,1.0
learning techniques op,2,1,2.0
techniques op mkl,2,1,2.0
op mkl as,2,1,2.0
mkl as said,1,1,1.0
said before to,1,1,1.0
before to test,1,1,1.0
to test this,1,1,1.0
test this idea,1,1,1.0
this idea we,1,1,1.0
idea we ﬁrst,1,1,1.0
we ﬁrst obtain,1,1,1.0
ﬁrst obtain the,1,1,1.0
the patterns based,1,1,1.0
patterns based on,1,1,1.0
on a svm,1,1,1.0
a svm hyperplane,1,1,1.0
hyperplane and we,1,1,1.0
and we use,1,1,1.0
use a parametrised,1,1,1.0
a parametrised link,1,1,1.0
parametrised link function,1,1,1.0
link function eq,1,1,1.0
function eq to,1,1,1.0
eq to assign,1,1,1.0
to assign different,1,1,1.0
assign different probabilities,1,1,1.0
different probabilities of,1,1,1.0
probabilities of being,1,1,1.0
of being to,1,1,1.0
being to the,1,1,1.0
to the patterns,1,1,1.0
according to this,3,2,1.5
to this spatial,1,1,1.0
this spatial distribution,1,1,1.0
spatial distribution this,1,1,1.0
distribution this parametrisation,1,1,1.0
this parametrisation is,1,1,1.0
parametrisation is made,1,1,1.0
is made using,1,1,1.0
made using a,1,1,1.0
using a β,1,1,1.0
a β scale,1,1,1.0
β scale parameter,1,1,1.0
scale parameter which,1,1,1.0
parameter which will,1,1,1.0
will be optimised,1,1,1.0
be optimised through,1,1,1.0
optimised through ocp,1,1,1.0
through ocp l,1,1,1.0
ocp l within,1,1,1.0
l within a,1,1,1.0
within a set,1,1,1.0
set of values,1,1,1.0
of values and,1,1,1.0
values and through,1,1,1.0
and through kernel,1,1,1.0
through kernel learning,1,1,1.0
op mkl for,1,1,1.0
mkl for the,1,1,1.0
the experiments we,1,1,1.0
experiments we select,1,1,1.0
select the set,1,1,1.0
the set β,1,1,1.0
set β analysing,1,1,1.0
β analysing the,1,1,1.0
analysing the results,1,1,1.0
seen that both,2,1,2.0
that both ocp,1,1,1.0
both ocp l,2,1,2.0
l and op,4,1,4.0
and op mkl,4,1,4.0
op mkl obtain,1,1,1.0
mkl obtain very,1,1,1.0
obtain very competitive,1,1,1.0
very competitive results,1,1,1.0
competitive results both,1,1,1.0
results both for,1,1,1.0
both for acc,1,1,1.0
and gm for,1,1,1.0
gm for some,1,1,1.0
for some cases,1,1,1.0
results obtained are,1,1,1.0
obtained are equal,1,1,1.0
are equal since,1,1,1.0
equal since op,1,1,1.0
since op mkl,1,1,1.0
op mkl also,1,1,1.0
mkl also includes,1,1,1.0
also includes the,1,1,1.0
includes the solutions,1,1,1.0
the solutions of,1,1,1.0
solutions of ocp,1,1,1.0
of ocp once,1,1,1.0
ocp once again,1,1,1.0
once again t,1,1,1.0
again t able,1,1,1.0
t able vii,2,1,2.0
able vii shows,1,1,1.0
vii shows the,1,1,1.0
ranking results when,1,1,1.0
results when comparing,1,1,1.0
when comparing these,1,1,1.0
comparing these two,1,1,1.0
these two approaches,1,1,1.0
two approaches to,1,1,1.0
approaches to the,1,1,1.0
to the standard,1,1,1.0
the standard posed,1,1,1.0
standard posed technique,1,1,1.0
posed technique oef,1,1,1.0
technique oef s,1,1,1.0
oef s in,1,1,1.0
s in this,1,1,1.0
case the friedman,1,1,1.0
s test accepted,1,1,1.0
the that the,1,1,1.0
that the algorithms,1,1,1.0
algorithms perform larly,1,1,1.0
perform larly for,1,1,1.0
larly for acc,1,1,1.0
gm from these,1,1,1.0
from these results,2,1,2.0
these results it,2,1,2.0
results it can,2,1,2.0
that both methods,1,1,1.0
both methods outperform,1,1,1.0
methods outperform the,1,1,1.0
outperform the standard,1,1,1.0
the standard proposal,1,1,1.0
standard proposal or,1,1,1.0
proposal or at,1,1,1.0
or at least,1,1,1.0
at least yield,1,1,1.0
least yield similar,1,1,1.0
yield similar performance,1,1,1.0
similar performance when,1,1,1.0
performance when considering,1,1,1.0
when considering acc,1,1,1.0
considering acc t,1,1,1.0
able vii mean,1,1,1.0
vii mean ranking,1,1,1.0
ranking results obtained,1,1,1.0
obtained by oef,1,1,1.0
by oef s,1,1,1.0
oef s ocp,3,1,3.0
s ocp l,3,1,3.0
op mkl ranking,1,1,1.0
mkl ranking oef,1,1,1.0
ocp l op,1,1,1.0
l op mkl,1,1,1.0
op mkl acc,1,1,1.0
mkl acc gm,1,1,1.0
α gm the,1,1,1.0
gm the holm,1,1,1.0
test for multiple,1,1,1.0
has been also,1,1,1.0
been also plied,1,1,1.0
also plied see,1,1,1.0
plied see t,1,1,1.0
t able viii,2,1,2.0
able viii for,1,1,1.0
viii for both,1,1,1.0
for both ocp,1,1,1.0
mkl the test,1,1,1.0
there are statistically,2,1,2.0
are statistically signiﬁcant,2,1,2.0
gm when compared,1,1,1.0
compared to oef,1,1,1.0
to oef s,1,1,1.0
oef s indicating,1,1,1.0
s indicating that,1,1,1.0
indicating that preferential,1,1,1.0
that preferential is,1,1,1.0
preferential is preferable,1,1,1.0
is preferable over,1,1,1.0
preferable over the,1,1,1.0
over the uniform,1,1,1.0
the uniform one,1,1,1.0
uniform one although,1,1,1.0
one although the,1,1,1.0
although the strategy,1,1,1.0
the strategy obtains,1,1,1.0
strategy obtains very,1,1,1.0
obtains very good,1,1,1.0
good results the,1,1,1.0
results the multiple,1,1,1.0
the multiple kernel,1,1,1.0
multiple kernel strategy,1,1,1.0
kernel strategy yields,1,1,1.0
strategy yields slightly,1,1,1.0
yields slightly better,1,1,1.0
slightly better performance,1,1,1.0
better performance there,1,1,1.0
performance there are,1,1,1.0
for α t,2,1,2.0
α t able,1,1,1.0
able viii results,1,1,1.0
viii results of,1,1,1.0
procedure using ocp,1,1,1.0
using ocp l,1,1,1.0
mkl as control,1,1,1.0
control methods when,1,1,1.0
methods when compared,1,1,1.0
compared to other,4,2,2.0
to other methods,2,2,1.0
other methods corrected,1,1,1.0
methods corrected α,1,1,1.0
method and ordered,1,1,1.0
and ordered by,1,1,1.0
i cm op,1,1,1.0
cm op mkl,1,1,1.0
op mkl gm,1,1,1.0
mkl gm i,1,1,1.0
method pi oef,2,1,2.0
pi oef s,2,1,2.0
ocp l cm,1,1,1.0
l cm ocp,1,1,1.0
cm ocp l,1,1,1.0
ocp l gm,1,1,1.0
l gm i,1,1,1.0
oef s op,1,1,1.0
s op mkl,1,1,1.0
op mkl win,1,1,1.0
mkl win or,1,1,1.0
with statistical cant,1,1,1.0
statistical cant difference,1,1,1.0
cant difference for,1,1,1.0
for α win,1,1,1.0
α win or,1,1,1.0
α t o,1,1,1.0
analyse the most,1,1,1.0
the most appropriate,1,1,1.0
most appropriate region,1,1,1.0
appropriate region for,1,1,1.0
region for we,1,1,1.0
for we analyse,1,1,1.0
we analyse the,1,1,1.0
analyse the optimal,1,1,1.0
the optimal β,1,1,1.0
optimal β values,1,1,1.0
β values obtained,1,1,1.0
values obtained from,1,1,1.0
obtained from see,1,1,1.0
from see fig,1,1,1.0
see fig for,1,1,1.0
fig for the,1,1,1.0
for the histogram,1,1,1.0
the histogram recall,1,1,1.0
histogram recall that,1,1,1.0
recall that when,1,1,1.0
β points within,1,1,1.0
points within the,1,1,1.0
β points on,1,1,1.0
points on the,1,1,1.0
boundary or even,1,1,1.0
or even on,1,1,1.0
even on the,1,1,1.0
the other side,1,1,1.0
other side of,1,1,1.0
the hyperplane are,1,1,1.0
hyperplane are preferred,1,1,1.0
be chosen it,1,1,1.0
chosen it can,1,1,1.0
seen that for,1,1,1.0
that for most,1,1,1.0
for most datasets,1,1,1.0
most datasets within,1,1,1.0
datasets within interior,1,1,1.0
within interior of,1,1,1.0
interior of the,1,1,1.0
class is preferable,1,1,1.0
is preferable moreover,1,1,1.0
preferable moreover note,1,1,1.0
moreover note that,1,1,1.0
that for a,1,1,1.0
for a relatively,1,1,1.0
a relatively large,1,1,1.0
relatively large number,1,1,1.0
of datasets the,1,1,1.0
datasets the choice,1,1,1.0
the choice is,1,1,1.0
choice is uniform,1,1,1.0
is uniform however,1,1,1.0
uniform however this,1,1,1.0
however this could,1,1,1.0
this could mean,1,1,1.0
could mean that,1,1,1.0
mean that uniform,1,1,1.0
that uniform sampling,1,1,1.0
uniform sampling could,1,1,1.0
sampling could be,1,1,1.0
could be feasible,1,1,1.0
be feasible in,1,1,1.0
feasible in some,1,1,1.0
some cases for,1,1,1.0
cases for in,1,1,1.0
feature space because,1,1,1.0
space because of,1,1,1.0
of the improved,2,1,2.0
the improved data,1,1,1.0
improved data separation,1,1,1.0
data separation finally,1,1,1.0
separation finally to,1,1,1.0
finally to study,1,1,1.0
study the computational,1,1,1.0
cost of preferential,1,1,1.0
of preferential we,1,1,1.0
preferential we included,1,1,1.0
we included a,1,1,1.0
included a small,1,1,1.0
a small comparison,1,1,1.0
small comparison of,1,1,1.0
comparison of oefs,1,1,1.0
of oefs and,1,1,1.0
oefs and opmkl,1,1,1.0
and opmkl using,1,1,1.0
opmkl using only,1,1,1.0
using only a,1,1,1.0
only a data,1,1,1.0
a data partition,1,1,1.0
data partition the,1,1,1.0
partition the dataset,1,1,1.0
the dataset chosen,1,1,1.0
dataset chosen is,1,1,1.0
chosen is haberman,1,1,1.0
is haberman and,1,1,1.0
haberman and the,1,1,1.0
and the time,1,1,1.0
the time is,1,1,1.0
time is reported,1,1,1.0
is reported in,1,1,1.0
terms of seconds,1,1,1.0
of seconds fig,1,1,1.0
seconds fig histogram,1,1,1.0
the mean values,1,1,1.0
mean values for,1,1,1.0
for the beta,1,1,1.0
the beta parameter,1,1,1.0
beta parameter used,1,1,1.0
parameter used in,1,1,1.0
process the x,1,1,1.0
the x coordinate,1,1,1.0
x coordinate represents,1,1,1.0
coordinate represents the,1,1,1.0
represents the different,1,1,1.0
the different mean,1,1,1.0
different mean β,1,1,1.0
mean β values,1,1,1.0
β values chosen,1,1,1.0
values chosen for,1,1,1.0
each dataset related,1,1,1.0
dataset related to,1,1,1.0
related to preferential,1,1,1.0
to preferential and,1,1,1.0
preferential and the,1,1,1.0
and the y,1,1,1.0
the y the,1,1,1.0
y the number,1,1,1.0
where the value,1,1,1.0
the value was,1,1,1.0
from the process,1,1,1.0
the process needed,1,1,1.0
process needed to,1,1,1.0
needed to the,1,1,1.0
the data of,1,1,1.0
data of parameters,1,1,1.0
of parameters is,1,1,1.0
parameters is not,1,1,1.0
is not considered,1,1,1.0
not considered according,1,1,1.0
considered according to,1,1,1.0
to this the,1,1,1.0
this the results,1,1,1.0
results are the,1,1,1.0
are the following,1,1,1.0
the following for,1,1,1.0
following for oefs,1,1,1.0
for oefs and,1,1,1.0
oefs and for,1,1,1.0
and for opmkl,1,1,1.0
for opmkl from,1,1,1.0
opmkl from these,1,1,1.0
that the computational,2,2,1.0
the computational time,3,2,1.5
computational time is,1,1,1.0
time is affordable,1,1,1.0
is affordable vii,1,1,1.0
affordable vii c,1,1,1.0
vii c o,1,1,1.0
o n c,1,1,1.0
n c l,1,1,1.0
c l u,1,1,1.0
l u s,1,1,1.0
u s i,1,1,1.0
s i o,1,1,1.0
o n s,1,1,1.0
n s this,1,1,1.0
s this paper,1,1,1.0
this paper explores,1,1,1.0
paper explores the,1,1,1.0
explores the notion,1,1,1.0
notion of in,1,1,1.0
function to deal,1,1,1.0
with imbalanced classiﬁcation,1,1,1.0
imbalanced classiﬁcation problems,1,1,1.0
classiﬁcation problems since,1,1,1.0
problems since the,1,1,1.0
directly accessible the,1,1,1.0
accessible the empirical,1,1,1.0
space is used,1,1,1.0
is used a,1,1,1.0
used a euclidean,1,1,1.0
euclidean space that,1,1,1.0
space that preserves,1,1,1.0
that preserves the,1,1,1.0
preserves the structure,1,1,1.0
the structure of,1,1,1.0
space is tackled,1,1,1.0
is tackled by,1,1,1.0
tackled by convex,1,1,1.0
by convex tion,1,1,1.0
convex tion of,1,1,1.0
tion of patterns,1,1,1.0
patterns as usually,1,1,1.0
as usually done,1,1,1.0
usually done in,1,1,1.0
done in the,1,1,1.0
in the and,1,1,1.0
the and we,1,1,1.0
and we focus,1,1,1.0
we focus on,1,1,1.0
on the paradigm,1,1,1.0
the paradigm of,1,1,1.0
paradigm of kernel,1,1,1.0
kernel methods w,1,1,1.0
methods w e,1,1,1.0
explore the ideas,1,1,1.0
the ideas of,1,1,1.0
ideas of in,1,1,1.0
space the optimisation,1,1,1.0
feature space by,1,1,1.0
space by kernel,1,1,1.0
by kernel learning,1,1,1.0
kernel learning and,1,1,1.0
learning and the,1,1,1.0
and the notion,1,1,1.0
notion of preferential,1,1,1.0
of preferential which,1,1,1.0
preferential which analyses,1,1,1.0
which analyses which,1,1,1.0
analyses which patterns,1,1,1.0
to be from,1,1,1.0
be from the,1,1,1.0
results of a,1,1,1.0
of a thorough,1,1,1.0
of experiments over,1,1,1.0
imbalanced datasets several,1,1,1.0
datasets several conclusions,1,1,1.0
be drawn ﬁrstly,1,1,1.0
drawn ﬁrstly in,1,1,1.0
ﬁrstly in the,1,1,1.0
space is seen,1,1,1.0
seen to yield,1,1,1.0
to yield better,1,1,1.0
yield better performance,1,1,1.0
performance than in,1,1,1.0
than in the,1,1,1.0
input space secondly,1,1,1.0
space secondly the,1,1,1.0
secondly the control,1,1,1.0
the control of,1,1,1.0
control of the,1,1,1.0
feature space could,2,1,2.0
space could lead,1,1,1.0
could lead to,1,1,1.0
lead to better,1,1,1.0
better results thirdly,1,1,1.0
results thirdly the,1,1,1.0
thirdly the kernel,1,1,1.0
the kernel used,1,1,1.0
kernel used inﬂuence,1,1,1.0
used inﬂuence the,1,1,1.0
inﬂuence the solution,1,1,1.0
solution to a,1,1,1.0
to a great,1,1,1.0
a great extent,1,1,1.0
great extent making,1,1,1.0
extent making advisable,1,1,1.0
advisable the optimisation,1,1,1.0
space structure although,1,1,1.0
structure although the,1,1,1.0
although the spherical,1,1,1.0
shown to perform,1,1,1.0
perform well for,1,1,1.0
well for several,1,1,1.0
for several cases,1,1,1.0
several cases and,1,1,1.0
cases and ﬁnally,1,1,1.0
and ﬁnally that,1,1,1.0
ﬁnally that there,1,1,1.0
that there exist,1,1,1.0
there exist some,2,2,1.0
exist some regions,1,1,1.0
the dataset which,1,1,1.0
dataset which should,1,1,1.0
which should be,1,1,1.0
be preferred for,1,1,1.0
preferred for and,1,1,1.0
for and that,1,1,1.0
and that multiple,1,1,1.0
that multiple kernel,1,1,1.0
learning techniques should,1,1,1.0
techniques should be,1,1,1.0
should be explored,1,1,1.0
the future with,1,1,1.0
future with the,1,1,1.0
with the purpose,2,1,2.0
purpose of the,1,1,1.0
of the authors,1,1,1.0
the authors would,1,1,1.0
authors would also,1,1,1.0
would also like,2,2,1.0
also like to,2,2,1.0
like to stress,1,1,1.0
to stress several,1,1,1.0
stress several lines,1,1,1.0
several lines of,1,1,1.0
lines of future,1,1,1.0
of future work,1,1,1.0
future work firstly,1,1,1.0
work firstly an,1,1,1.0
firstly an analytical,1,1,1.0
an analytical methodology,1,1,1.0
analytical methodology for,1,1,1.0
methodology for optimising,1,1,1.0
optimising the number,1,1,1.0
space could be,1,1,1.0
could be developed,2,1,2.0
be developed with,1,1,1.0
developed with the,1,1,1.0
purpose of secondly,1,1,1.0
of secondly considering,1,1,1.0
secondly considering a,1,1,1.0
considering a unique,1,1,1.0
a unique methodology,1,1,1.0
unique methodology combining,1,1,1.0
methodology combining the,1,1,1.0
combining the techniques,1,1,1.0
techniques proposed in,2,2,1.0
proposed in this,1,1,1.0
this paper could,1,1,1.0
paper could be,1,1,1.0
could be accomplished,1,1,1.0
be accomplished to,1,1,1.0
accomplished to analyse,1,1,1.0
to analyse how,1,1,1.0
analyse how these,1,1,1.0
how these methods,1,1,1.0
these methods could,1,1,1.0
methods could complement,1,1,1.0
could complement each,1,1,1.0
complement each other,1,1,1.0
each other furthermore,1,1,1.0
other furthermore in,1,1,1.0
furthermore in the,1,1,1.0
context of kernel,1,1,1.0
of kernel learning,1,1,1.0
kernel learning the,1,1,1.0
learning the sampling,1,1,1.0
could be incorporated,1,1,1.0
be incorporated in,1,1,1.0
incorporated in the,1,1,1.0
the kernel learning,1,1,1.0
kernel learning stage,1,1,1.0
learning stage to,1,1,1.0
stage to search,1,1,1.0
to search the,1,1,1.0
search the more,1,1,1.0
the more suitable,1,1,1.0
more suitable representation,1,1,1.0
suitable representation for,1,1,1.0
representation for performing,1,1,1.0
performing the not,1,1,1.0
the not only,1,1,1.0
only the better,1,1,1.0
the better class,1,1,1.0
better class separation,1,1,1.0
class separation finally,1,1,1.0
separation finally other,1,1,1.0
finally other intelligent,1,1,1.0
other intelligent optimisation,1,1,1.0
intelligent optimisation techniques,1,1,1.0
optimisation techniques could,1,1,1.0
be developed for,1,1,1.0
developed for the,1,1,1.0
generation of the,1,1,1.0
the synthetic patterns,1,1,1.0
synthetic patterns re,1,1,1.0
patterns re f,1,1,1.0
re f e,1,1,1.0
e n c,1,1,1.0
n c e,1,1,1.0
c e s,1,1,1.0
e s he,1,1,1.0
s he and,1,1,1.0
he and garcia,2,2,1.0
and garcia learning,2,2,1.0
data engineering vol,3,1,3.0
engineering vol pp,3,1,3.0
vol pp japkowicz,1,1,1.0
pp japkowicz and,1,1,1.0
japkowicz and stephen,1,1,1.0
and stephen the,1,1,1.0
stephen the class,1,1,1.0
imbalance problem a,1,1,1.0
problem a tematic,1,1,1.0
a tematic study,1,1,1.0
tematic study intelligent,1,1,1.0
study intelligent data,1,1,1.0
data analysis vol,1,1,1.0
analysis vol no,1,1,1.0
vol no pp,36,2,18.0
no pp v,2,2,1.0
pp v chawla,4,2,2.0
v chawla w,1,1,1.0
chawla w bowyer,1,1,1.0
w bowyer hall,1,1,1.0
bowyer hall and,2,2,1.0
hall and w,1,1,1.0
and w p,1,1,1.0
w p kegelmeyer,1,1,1.0
p kegelmeyer smote,1,1,1.0
journal of artiﬁcial,1,1,1.0
of artiﬁcial intelligence,1,1,1.0
artiﬁcial intelligence research,1,1,1.0
intelligence research vol,1,1,1.0
research vol no,3,1,3.0
no pp y,2,2,1.0
pp y t,1,1,1.0
y t ang,1,1,1.0
t ang y,1,1,1.0
ang y zhang,1,1,1.0
y zhang v,1,1,1.0
zhang v chawla,1,1,1.0
v chawla and,2,2,1.0
chawla and krasser,1,1,1.0
and krasser svms,1,1,1.0
krasser svms modeling,1,1,1.0
svms modeling for,1,1,1.0
modeling for highly,1,1,1.0
highly imbalanced classiﬁcation,1,1,1.0
imbalanced classiﬁcation ieee,1,1,1.0
classiﬁcation ieee transactions,2,1,2.0
and cybernetics p,2,1,2.0
cybernetics p art,2,1,2.0
p art b,1,1,1.0
art b cybernetics,1,1,1.0
b cybernetics vol,1,1,1.0
vol pp nguyen,1,1,1.0
pp nguyen w,1,1,1.0
nguyen w cooper,1,1,1.0
w cooper and,1,1,1.0
cooper and kamei,1,1,1.0
and kamei borderline,1,1,1.0
borderline for imbalanced,1,1,1.0
imbalanced data classiﬁcation,2,2,1.0
data classiﬁcation international,1,1,1.0
classiﬁcation international journal,1,1,1.0
soft data p,1,1,1.0
data p aradigms,1,1,1.0
p aradigms vol,1,1,1.0
aradigms vol no,1,1,1.0
no pp bunkhumpornpat,1,1,1.0
pp bunkhumpornpat sinapiromsaran,1,1,1.0
bunkhumpornpat sinapiromsaran and,1,1,1.0
sinapiromsaran and lursinsap,1,1,1.0
and lursinsap minority,1,1,1.0
imbalanced problem in,1,1,1.0
of the p,2,1,2.0
the p conference,1,1,1.0
p conference on,1,1,1.0
data mining bangkok,1,1,1.0
mining bangkok thailand,1,1,1.0
bangkok thailand april,1,1,1.0
thailand april pp,1,1,1.0
april pp he,1,1,1.0
pp he y,2,2,1.0
he y bai,2,2,1.0
y bai garcia,2,2,1.0
bai garcia and,2,2,1.0
garcia and li,2,2,1.0
and li adasyn,2,2,1.0
neural networks hong,1,1,1.0
networks hong kong,1,1,1.0
hong kong china,2,1,2.0
kong china pp,2,1,2.0
china pp barua,1,1,1.0
pp barua islam,2,1,2.0
barua islam and,1,1,1.0
islam and murase,1,1,1.0
and murase a,1,1,1.0
murase a novel,1,1,1.0
a novel synthetic,1,1,1.0
novel synthetic minority,1,1,1.0
oversampling technique for,2,1,2.0
technique for imbalanced,2,1,2.0
imbalanced data set,2,1,2.0
set learning in,1,1,1.0
learning in ings,1,1,1.0
on neural information,5,1,5.0
information processing shanghai,1,1,1.0
processing shanghai china,1,1,1.0
shanghai china nov,1,1,1.0
china nov pp,2,1,2.0
nov pp barua,1,1,1.0
barua islam y,1,1,1.0
islam y ao,1,1,1.0
y ao and,1,1,1.0
ao and murase,1,1,1.0
and murase mwmote,1,1,1.0
murase mwmote majority,1,1,1.0
mwmote majority weighted,1,1,1.0
majority weighted minority,1,1,1.0
weighted minority oversampling,1,1,1.0
set learning ieee,1,1,1.0
learning ieee transactions,1,1,1.0
vol pp galar,1,1,1.0
pp galar fern,2,1,2.0
galar fern barrenechea,2,1,2.0
fern barrenechea bustince,1,1,1.0
barrenechea bustince and,1,1,1.0
bustince and f,1,1,1.0
and f herrera,2,1,2.0
f herrera a,1,1,1.0
p art c,1,1,1.0
art c vol,1,1,1.0
c vol pp,1,1,1.0
vol pp jul,1,1,1.0
pp jul gantner,1,1,1.0
jul gantner and,1,1,1.0
gantner and learning,1,1,1.0
and learning methods,1,1,1.0
learning methods for,1,1,1.0
methods for imbalanced,1,1,1.0
imbalanced data in,1,1,1.0
of the national,1,1,1.0
the national joint,1,1,1.0
national joint conference,2,2,1.0
neural networks barcelona,1,1,1.0
networks barcelona spain,1,1,1.0
barcelona spain july,1,1,1.0
spain july pp,1,1,1.0
july pp boser,1,1,1.0
pp boser guyon,1,1,1.0
boser guyon and,1,1,1.0
guyon and v,1,1,1.0
and v v,2,1,2.0
v v apnik,2,1,2.0
v apnik a,1,1,1.0
apnik a training,1,1,1.0
a training algorithm,1,1,1.0
training algorithm for,1,1,1.0
algorithm for optimal,1,1,1.0
for optimal margin,1,1,1.0
optimal margin classiﬁers,1,1,1.0
margin classiﬁers in,1,1,1.0
classiﬁers in proceedings,1,1,1.0
the fifth annual,1,1,1.0
fifth annual acm,1,1,1.0
annual acm w,1,1,1.0
acm w orkshop,1,1,1.0
w orkshop on,1,1,1.0
orkshop on computational,1,1,1.0
on computational learning,1,1,1.0
computational learning theory,1,1,1.0
learning theory pittsburgh,2,1,2.0
theory pittsburgh p,1,1,1.0
pittsburgh p a,1,1,1.0
p a july,1,1,1.0
a july pp,1,1,1.0
july pp cortes,1,1,1.0
pp cortes and,1,1,1.0
cortes and v,1,1,1.0
v apnik networks,1,1,1.0
apnik networks machine,1,1,1.0
machine learning vol,1,1,1.0
learning vol no,1,1,1.0
no pp zeng,1,1,1.0
pp zeng and,1,1,1.0
zeng and gao,1,1,1.0
and gao improving,1,1,1.0
gao improving svm,1,1,1.0
improving svm classiﬁcation,1,1,1.0
svm classiﬁcation with,1,1,1.0
classiﬁcation with imbalance,1,1,1.0
with imbalance data,1,1,1.0
imbalance data set,1,1,1.0
data set in,1,1,1.0
set in proceedings,1,1,1.0
information processing bangkok,1,1,1.0
processing bangkok thailand,1,1,1.0
bangkok thailand pp,1,1,1.0
thailand pp scholkopf,1,1,1.0
pp scholkopf and,1,1,1.0
scholkopf and smola,1,1,1.0
and smola learning,1,1,1.0
smola learning with,1,1,1.0
learning with kernels,1,1,1.0
with kernels support,1,1,1.0
kernels support v,1,1,1.0
ector machines regularization,1,1,1.0
machines regularization optimization,1,1,1.0
regularization optimization and,1,1,1.0
optimization and beyond,1,1,1.0
and beyond cambridge,1,1,1.0
beyond cambridge ma,1,1,1.0
cambridge ma usa,1,1,1.0
ma usa mit,1,1,1.0
usa mit press,1,1,1.0
mit press sch,1,1,1.0
press sch mika,1,1,1.0
sch mika burges,1,1,1.0
mika burges p,1,1,1.0
burges p knirsch,1,1,1.0
p knirsch m,1,1,1.0
knirsch m r,1,1,1.0
m r and,1,1,1.0
r and smola,1,1,1.0
and smola input,1,1,1.0
smola input space,1,1,1.0
input space versus,1,1,1.0
space versus feature,1,1,1.0
versus feature space,1,1,1.0
space in based,1,1,1.0
in based methods,1,1,1.0
based methods ieee,1,1,1.0
methods ieee transactions,2,1,2.0
neural networks vol,15,2,7.5
networks vol pp,3,1,3.0
vol pp xiong,1,1,1.0
pp xiong swamy,1,1,1.0
xiong swamy and,1,1,1.0
swamy and ahmad,1,1,1.0
and ahmad optimizing,1,1,1.0
ahmad optimizing the,1,1,1.0
optimizing the kernel,1,1,1.0
the kernel in,1,1,1.0
feature space ieee,1,1,1.0
space ieee transactions,1,1,1.0
vol pp march,1,1,1.0
pp march cristianini,1,1,1.0
march cristianini kandola,1,1,1.0
cristianini kandola elisseeff,1,1,1.0
kandola elisseeff and,1,1,1.0
elisseeff and aylor,1,1,1.0
and aylor on,1,1,1.0
aylor on target,1,1,1.0
on target alignment,1,1,1.0
target alignment in,1,1,1.0
alignment in proceedings,1,1,1.0
processing systems v,2,1,2.0
systems v ancouver,2,1,2.0
v ancouver canada,2,1,2.0
ancouver canada pp,1,1,1.0
canada pp cortes,1,1,1.0
pp cortes mohri,1,1,1.0
cortes mohri and,1,1,1.0
mohri and rostamizadeh,1,1,1.0
and rostamizadeh algorithms,1,1,1.0
rostamizadeh algorithms for,1,1,1.0
algorithms for learning,2,2,1.0
for learning kernels,1,1,1.0
learning kernels based,1,1,1.0
kernels based on,1,1,1.0
based on centered,1,1,1.0
on centered alignment,1,1,1.0
centered alignment journal,1,1,1.0
alignment journal of,1,1,1.0
learning research vol,4,1,4.0
no pp akbani,1,1,1.0
pp akbani kwek,1,1,1.0
akbani kwek and,1,1,1.0
kwek and japkowicz,1,1,1.0
and japkowicz applying,1,1,1.0
japkowicz applying support,1,1,1.0
applying support vector,1,1,1.0
vector machines to,1,1,1.0
machines to imbalanced,1,1,1.0
to imbalanced datasets,1,1,1.0
imbalanced datasets in,3,2,1.5
datasets in proceedings,1,1,1.0
of the european,1,1,1.0
the european conference,1,1,1.0
machine learning pisa,1,1,1.0
learning pisa italy,1,1,1.0
pisa italy pp,1,1,1.0
italy pp y,1,1,1.0
pp y liu,2,2,1.0
y liu an,1,1,1.0
liu an and,1,1,1.0
an and huang,1,1,1.0
and huang boosting,1,1,1.0
huang boosting prediction,1,1,1.0
boosting prediction accuracy,1,1,1.0
prediction accuracy on,1,1,1.0
accuracy on balanced,1,1,1.0
on balanced datasets,1,1,1.0
balanced datasets with,1,1,1.0
datasets with svm,1,1,1.0
ensembles in proceedings,1,1,1.0
the p asia,1,1,1.0
p asia conference,1,1,1.0
asia conference on,1,1,1.0
data mining singapore,1,1,1.0
mining singapore april,1,1,1.0
singapore april pp,1,1,1.0
april pp p,1,1,1.0
pp p kang,1,1,1.0
p kang and,1,1,1.0
kang and cho,1,1,1.0
and cho eus,1,1,1.0
cho eus svms,1,1,1.0
eus svms ensemble,1,1,1.0
svms ensemble of,1,1,1.0
ensemble of svms,1,1,1.0
of svms for,1,1,1.0
svms for data,1,1,1.0
for data imbalance,1,1,1.0
data imbalance problems,1,1,1.0
imbalance problems in,1,1,1.0
problems in proceedings,1,1,1.0
information processing hong,1,1,1.0
processing hong kong,1,1,1.0
china pp hong,1,1,1.0
pp hong chen,2,2,1.0
hong chen and,2,2,1.0
chen and harris,2,2,1.0
and harris a,2,2,1.0
harris a classiﬁer,2,2,1.0
classiﬁer for imbalanced,2,2,1.0
data sets ieee,2,2,1.0
sets ieee transactions,1,1,1.0
vol pp wu,1,1,1.0
wu and y,4,2,2.0
and y chang,3,2,1.5
y chang kba,2,2,1.0
chang kba kernel,2,2,1.0
kba kernel boundary,2,2,1.0
kernel boundary alignment,2,2,1.0
boundary alignment ering,1,1,1.0
alignment ering imbalanced,1,1,1.0
ering imbalanced data,1,1,1.0
imbalanced data distribution,3,2,1.5
data distribution ieee,2,2,1.0
distribution ieee transactions,1,1,1.0
vol pp june,1,1,1.0
pp june adaptive,1,1,1.0
june adaptive conformal,1,1,1.0
adaptive conformal transformation,1,1,1.0
conformal transformation for,1,1,1.0
transformation for anced,1,1,1.0
for anced data,3,2,1.5
anced data learning,1,1,1.0
data learning in,1,1,1.0
of the t,1,1,1.0
the t wentieth,1,1,1.0
t wentieth international,1,1,1.0
wentieth international conference,1,1,1.0
machine learning w,1,1,1.0
learning w ashington,1,1,1.0
w ashington usa,1,1,1.0
ashington usa pp,1,1,1.0
usa pp y,1,1,1.0
pp y uan,1,1,1.0
y uan li,1,1,1.0
uan li and,1,1,1.0
li and zhang,2,2,1.0
and zhang learning,2,2,1.0
zhang learning concepts,2,2,1.0
learning concepts from,2,2,1.0
concepts from large,2,2,1.0
from large scale,2,2,1.0
large scale imbalanced,2,2,1.0
scale imbalanced data,2,2,1.0
sets using support,2,2,1.0
using support cluster,2,2,1.0
support cluster machines,3,2,1.5
cluster machines in,2,2,1.0
of the annual,2,1,2.0
the annual acm,1,1,1.0
annual acm international,1,1,1.0
acm international conference,1,1,1.0
conference on multimedia,1,1,1.0
on multimedia new,1,1,1.0
multimedia new y,1,1,1.0
y ork usa,1,1,1.0
ork usa pp,1,1,1.0
usa pp t,1,1,1.0
pp t cover,1,1,1.0
t cover and,1,1,1.0
cover and p,1,1,1.0
and p hart,1,1,1.0
p hart nearest,1,1,1.0
hart nearest neighbor,1,1,1.0
nearest neighbor pattern,1,1,1.0
neighbor pattern classiﬁcation,1,1,1.0
pattern classiﬁcation ieee,1,1,1.0
information theory vol,1,1,1.0
theory vol pp,1,1,1.0
vol pp w,1,1,1.0
pp w johnson,1,1,1.0
w johnson and,1,1,1.0
johnson and riess,1,1,1.0
and riess numerical,1,1,1.0
riess numerical analysis,1,1,1.0
numerical analysis reading,1,1,1.0
analysis reading mass,1,1,1.0
reading mass esley,1,1,1.0
mass esley pub,1,1,1.0
esley pub t,1,1,1.0
pub t y,1,1,1.0
t y kwok,1,1,1.0
y kwok and,1,1,1.0
kwok and w,1,1,1.0
and w tsang,1,1,1.0
w tsang the,1,1,1.0
tsang the problem,1,1,1.0
the problem in,1,1,1.0
problem in kernel,1,1,1.0
in kernel methods,1,1,1.0
kernel methods ieee,1,1,1.0
networks vol no,12,2,6.0
no pp nov,1,1,1.0
pp nov braun,1,1,1.0
nov braun buhmann,1,1,1.0
braun buhmann and,1,1,1.0
buhmann and m,1,1,1.0
and m on,1,1,1.0
m on relevant,1,1,1.0
on relevant dimensions,1,1,1.0
relevant dimensions in,1,1,1.0
dimensions in kernel,1,1,1.0
in kernel feature,1,1,1.0
kernel feature spaces,1,1,1.0
feature spaces journal,1,1,1.0
spaces journal of,1,1,1.0
no pp smola,1,1,1.0
pp smola sch,1,1,1.0
smola sch and,1,1,1.0
sch and m,1,1,1.0
and m the,1,1,1.0
m the connection,1,1,1.0
the connection between,1,1,1.0
connection between regularization,1,1,1.0
between regularization operators,1,1,1.0
regularization operators and,1,1,1.0
operators and support,1,1,1.0
and support vector,1,1,1.0
support vector kernels,1,1,1.0
vector kernels neural,1,1,1.0
kernels neural networks,1,1,1.0
no pp jun,8,2,4.0
pp jun ledoux,1,1,1.0
jun ledoux the,1,1,1.0
ledoux the concentration,1,1,1.0
concentration of measure,1,1,1.0
of measure phenomenon,1,1,1.0
measure phenomenon ser,1,1,1.0
phenomenon ser ical,1,1,1.0
ser ical surveys,1,1,1.0
ical surveys and,1,1,1.0
surveys and monographs,1,1,1.0
and monographs american,1,1,1.0
monographs american mathematical,1,1,1.0
american mathematical society,1,1,1.0
mathematical society giannopoulos,1,1,1.0
society giannopoulos and,1,1,1.0
giannopoulos and v,1,1,1.0
and v milman,1,1,1.0
v milman concentration,1,1,1.0
milman concentration property,1,1,1.0
concentration property on,1,1,1.0
property on probability,1,1,1.0
on probability spaces,1,1,1.0
probability spaces advances,1,1,1.0
spaces advances in,1,1,1.0
advances in mathematics,1,1,1.0
in mathematics vol,1,1,1.0
mathematics vol no,1,1,1.0
no pp abe,1,1,1.0
pp abe and,1,1,1.0
abe and onishi,1,1,1.0
and onishi sparse,1,1,1.0
onishi sparse least,1,1,1.0
sparse least squares,1,1,1.0
least squares support,1,1,1.0
squares support vector,1,1,1.0
support vector regressors,1,1,1.0
vector regressors trained,1,1,1.0
regressors trained in,1,1,1.0
trained in the,1,1,1.0
space in proceedings,2,1,2.0
conference on artiﬁcial,1,1,1.0
on artiﬁcial neural,1,1,1.0
artiﬁcial neural networks,1,1,1.0
neural networks san,1,1,1.0
networks san sebasti,1,1,1.0
san sebasti spain,1,1,1.0
sebasti spain june,1,1,1.0
spain june pp,1,1,1.0
june pp xiong,1,1,1.0
pp xiong a,1,1,1.0
xiong a uniﬁed,1,1,1.0
framework for kernelization,1,1,1.0
for kernelization the,1,1,1.0
kernelization the empirical,1,1,1.0
empirical kernel feature,1,1,1.0
kernel feature space,1,1,1.0
of the chinese,1,1,1.0
the chinese conference,1,1,1.0
chinese conference on,1,1,1.0
conference on p,1,1,1.0
on p attern,1,1,1.0
p attern recognition,3,1,3.0
attern recognition nanjing,1,1,1.0
recognition nanjing china,1,1,1.0
nanjing china nov,1,1,1.0
nov pp ramona,1,1,1.0
pp ramona richard,1,1,1.0
ramona richard and,1,1,1.0
richard and david,1,1,1.0
and david multiclass,1,1,1.0
david multiclass feature,1,1,1.0
multiclass feature selection,1,1,1.0
feature selection with,1,1,1.0
selection with kernel,1,1,1.0
with kernel criteria,1,1,1.0
kernel criteria ieee,1,1,1.0
criteria ieee transactions,1,1,1.0
learning systems vol,1,1,1.0
vol pp lanckriet,1,1,1.0
pp lanckriet cristianini,1,1,1.0
lanckriet cristianini p,1,1,1.0
cristianini p bartlett,1,1,1.0
p bartlett ghaoui,1,1,1.0
bartlett ghaoui and,1,1,1.0
ghaoui and jordan,1,1,1.0
and jordan learning,1,1,1.0
jordan learning the,1,1,1.0
learning the kernel,1,1,1.0
matrix with semideﬁnite,1,1,1.0
with semideﬁnite programming,1,1,1.0
semideﬁnite programming journal,1,1,1.0
programming journal of,1,1,1.0
research vol pp,2,1,2.0
vol pp srebro,1,1,1.0
pp srebro and,1,1,1.0
srebro and learning,1,1,1.0
and learning bounds,1,1,1.0
bounds for support,1,1,1.0
for support vector,1,1,1.0
support vector chines,2,2,1.0
vector chines with,1,1,1.0
chines with learned,1,1,1.0
with learned kernels,1,1,1.0
learned kernels in,1,1,1.0
kernels in proceedings,1,1,1.0
the annual conference,1,1,1.0
annual conference on,1,1,1.0
conference on learning,1,1,1.0
on learning theory,1,1,1.0
theory pittsburgh usa,1,1,1.0
pittsburgh usa june,1,1,1.0
usa june pp,1,1,1.0
june pp lichman,1,1,1.0
pp lichman uci,1,1,1.0
learning repository online,2,2,1.0
repository online a,1,1,1.0
online a vailable,1,1,1.0
a vailable http,1,1,1.0
vailable http kubat,1,1,1.0
http kubat and,1,1,1.0
training sets selection,2,2,1.0
sets selection in,2,2,1.0
the international ence,1,1,1.0
international ence on,1,1,1.0
ence on machine,1,1,1.0
learning nashville usa,1,1,1.0
nashville usa july,1,1,1.0
usa july pp,1,1,1.0
july pp barandela,1,1,1.0
pp barandela s,1,1,1.0
barandela s v,1,1,1.0
s v garc,1,1,1.0
v garc and,1,1,1.0
garc and rangel,1,1,1.0
imbalance problems p,1,1,1.0
problems p attern,1,1,1.0
attern recognition vol,2,1,2.0
recognition vol no,2,1,2.0
no pp galar,1,1,1.0
fern barrenechea and,1,1,1.0
barrenechea and f,1,1,1.0
f herrera eusboost,1,1,1.0
herrera eusboost enhancing,1,1,1.0
eusboost enhancing ensembles,1,1,1.0
enhancing ensembles for,1,1,1.0
ensembles for highly,1,1,1.0
highly imbalanced by,1,1,1.0
imbalanced by evolutionary,1,1,1.0
by evolutionary undersampling,1,1,1.0
evolutionary undersampling p,1,1,1.0
undersampling p attern,1,1,1.0
no pp demsar,1,1,1.0
pp demsar statistical,1,1,1.0
demsar statistical comparisons,1,1,1.0
statistical comparisons of,2,2,1.0
comparisons of classiﬁers,2,2,1.0
of classiﬁers over,2,2,1.0
classiﬁers over multiple,2,2,1.0
over multiple data,2,2,1.0
multiple data sets,2,2,1.0
data sets journal,1,1,1.0
sets journal of,1,1,1.0
vol pp igel,1,1,1.0
pp igel and,1,1,1.0
igel and h,1,1,1.0
and h empirical,1,1,1.0
h empirical evaluation,1,1,1.0
empirical evaluation of,1,1,1.0
evaluation of the,1,1,1.0
the improved rprop,1,1,1.0
improved rprop learning,1,1,1.0
rprop learning algorithms,1,1,1.0
learning algorithms neurocomputing,1,1,1.0
algorithms neurocomputing vol,1,1,1.0
neurocomputing vol pp,1,1,1.0
vol pp chapelle,1,1,1.0
pp chapelle and,1,1,1.0
chapelle and rakotomamonjy,1,1,1.0
and rakotomamonjy second,1,1,1.0
rakotomamonjy second order,1,1,1.0
second order optimization,1,1,1.0
order optimization of,1,1,1.0
optimization of kernel,1,1,1.0
of kernel parameters,1,1,1.0
kernel parameters in,1,1,1.0
parameters in in,1,1,1.0
in in proceedings,1,1,1.0
vol no october,10,1,10.0
no october ramoboost,1,1,1.0
october ramoboost ranked,1,1,1.0
ramoboost ranked minority,1,1,1.0
ranked minority oversampling,10,1,10.0
minority oversampling in,10,1,10.0
oversampling in boosting,10,1,10.0
in boosting sheng,1,1,1.0
boosting sheng chen,1,1,1.0
sheng chen student,1,1,1.0
chen student member,1,1,1.0
student member ieee,1,1,1.0
member ieee haibo,1,1,1.0
ieee haibo he,1,1,1.0
haibo he member,1,1,1.0
he member ieee,1,1,1.0
member ieee and,1,1,1.0
ieee and edwardo,1,1,1.0
and edwardo garcia,1,1,1.0
edwardo garcia in,1,1,1.0
garcia in recent,1,1,1.0
recent years learning,1,1,1.0
years learning from,1,1,1.0
imbalanced data has,1,1,1.0
data has attracted,1,1,1.0
has attracted growing,1,1,1.0
attracted growing attention,1,1,1.0
growing attention from,1,1,1.0
attention from both,1,1,1.0
from both academia,1,1,1.0
both academia and,1,1,1.0
academia and industry,1,1,1.0
and industry due,1,1,1.0
industry due to,1,1,1.0
to the explosive,1,1,1.0
the explosive growth,1,1,1.0
explosive growth of,1,1,1.0
growth of applications,1,1,1.0
of applications that,1,1,1.0
applications that use,1,1,1.0
that use and,1,1,1.0
use and produce,1,1,1.0
and produce imbalanced,1,1,1.0
produce imbalanced data,1,1,1.0
imbalanced data however,1,1,1.0
data however because,1,1,1.0
however because of,1,1,1.0
of the complex,1,1,1.0
the complex characteristics,1,1,1.0
complex characteristics of,1,1,1.0
characteristics of imbalanced,1,1,1.0
imbalanced data many,1,1,1.0
data many solutions,1,1,1.0
many solutions struggle,1,1,1.0
solutions struggle to,1,1,1.0
struggle to provide,1,1,1.0
to provide robust,1,1,1.0
provide robust efﬁciency,1,1,1.0
robust efﬁciency in,1,1,1.0
efﬁciency in applications,1,1,1.0
in applications in,1,1,1.0
applications in an,1,1,1.0
in an effort,1,1,1.0
an effort to,1,1,1.0
effort to address,1,1,1.0
to address this,1,1,1.0
address this problem,1,1,1.0
this problem this,1,1,1.0
problem this paper,1,1,1.0
this paper presents,1,1,1.0
paper presents ranked,1,1,1.0
presents ranked minority,1,1,1.0
ranked minority sampling,1,1,1.0
minority sampling in,1,1,1.0
sampling in boosting,1,1,1.0
in boosting ramoboost,1,1,1.0
boosting ramoboost which,1,1,1.0
ramoboost which is,2,1,2.0
is a ramo,2,1,2.0
a ramo technique,2,1,2.0
ramo technique based,1,1,1.0
technique based on,1,1,1.0
the idea of,1,1,1.0
idea of adaptive,1,1,1.0
of adaptive synthetic,1,1,1.0
adaptive synthetic data,1,1,1.0
synthetic data generation,6,1,6.0
data generation in,2,1,2.0
generation in an,1,1,1.0
in an ensemble,1,1,1.0
an ensemble learning,1,1,1.0
ensemble learning system,1,1,1.0
learning system brieﬂy,1,1,1.0
system brieﬂy ramoboost,1,1,1.0
brieﬂy ramoboost adaptively,1,1,1.0
ramoboost adaptively ranks,1,1,1.0
adaptively ranks minority,1,1,1.0
ranks minority class,1,1,1.0
class instances at,1,1,1.0
instances at each,1,1,1.0
at each learning,1,1,1.0
each learning iteration,1,1,1.0
learning iteration according,1,1,1.0
iteration according to,1,1,1.0
to a sampling,1,1,1.0
a sampling probability,1,1,1.0
sampling probability distribution,2,1,2.0
probability distribution that,1,1,1.0
distribution that is,1,1,1.0
that is based,1,1,1.0
on the underlying,1,1,1.0
the underlying data,1,1,1.0
underlying data distribution,1,1,1.0
data distribution and,3,1,3.0
distribution and can,1,1,1.0
and can adaptively,1,1,1.0
can adaptively shift,2,1,2.0
adaptively shift the,3,1,3.0
shift the decision,3,1,3.0
decision boundary toward,4,1,4.0
boundary toward minori,1,1,1.0
toward minori ty,1,1,1.0
minori ty and,1,1,1.0
ty and majority,1,1,1.0
instances by using,1,1,1.0
by using a,1,1,1.0
using a hypothesis,1,1,1.0
a hypothesis assessment,1,1,1.0
hypothesis assessment procedure,1,1,1.0
assessment procedure simulation,1,1,1.0
procedure simulation analysis,1,1,1.0
simulation analysis on,2,1,2.0
analysis on datasets,1,1,1.0
on datasets assessed,1,1,1.0
datasets assessed over,1,1,1.0
assessed over various,1,1,1.0
over various overall,1,1,1.0
various overall accuracy,1,1,1.0
overall accuracy precision,1,1,1.0
accuracy precision recall,1,1,1.0
recall and receiver,1,1,1.0
and receiver operation,1,1,1.0
receiver operation characteristic,1,1,1.0
operation characteristic used,1,1,1.0
characteristic used to,1,1,1.0
used to illustrate,1,1,1.0
to illustrate the,2,1,2.0
illustrate the effectiveness,2,1,2.0
the effectiveness of,2,1,2.0
effectiveness of this,1,1,1.0
this method index,1,1,1.0
method index t,1,1,1.0
index t adaptive,1,1,1.0
t adaptive boosting,1,1,1.0
adaptive boosting data,1,1,1.0
boosting data mining,1,1,1.0
data mining ensemble,1,1,1.0
mining ensemble ing,1,1,1.0
ensemble ing imbalanced,1,1,1.0
ing imbalanced data,1,1,1.0
imbalanced data i,1,1,1.0
data i ntroduction,1,1,1.0
i ntroduction l,1,1,1.0
ntroduction l earning,1,1,1.0
l earning from,1,1,1.0
earning from imbalanced,1,1,1.0
from imbalanced da,1,1,1.0
imbalanced da ta,1,1,1.0
da ta imbalanced,1,1,1.0
ta imbalanced learning,1,1,1.0
imbalanced learning has,1,1,1.0
learning has become,1,1,1.0
become a criti,1,1,1.0
a criti cal,1,1,1.0
criti cal and,1,1,1.0
cal and signiﬁcant,1,1,1.0
and signiﬁcant research,1,1,1.0
signiﬁcant research issue,1,1,1.0
research issue in,1,1,1.0
issue in many,1,1,1.0
in many of,1,1,1.0
many of today,1,1,1.0
of today s,1,1,1.0
today s applications,1,1,1.0
s applications such,1,1,1.0
such as ﬁnancial,1,1,1.0
as ﬁnancial engineering,1,1,1.0
ﬁnancial engineering anom,1,1,1.0
engineering anom aly,1,1,1.0
anom aly detection,1,1,1.0
aly detection biomedical,1,1,1.0
detection biomedical data,1,1,1.0
biomedical data analysis,2,1,2.0
data analysis and,1,1,1.0
analysis and many,1,1,1.0
and many others,1,1,1.0
many others the,1,1,1.0
others the amount,1,1,1.0
the amount and,1,1,1.0
amount and complexity,1,1,1.0
and complexity of,1,1,1.0
complexity of raw,1,1,1.0
of raw data,1,1,1.0
raw data that,1,1,1.0
that is captured,1,1,1.0
is captured to,1,1,1.0
captured to monitor,1,1,1.0
to monitor analyze,1,1,1.0
monitor analyze and,1,1,1.0
analyze and support,1,1,1.0
and support making,1,1,1.0
support making processes,1,1,1.0
making processes continue,1,1,1.0
processes continue to,1,1,1.0
continue to grow,1,1,1.0
to grow at,1,1,1.0
grow at an,1,1,1.0
at an incredible,1,1,1.0
an incredible rate,1,1,1.0
incredible rate consequently,1,1,1.0
rate consequently this,1,1,1.0
consequently this enhances,1,1,1.0
this enhances the,1,1,1.0
enhances the capacity,1,1,1.0
the capacity for,1,1,1.0
capacity for computationally,1,1,1.0
for computationally intelligent,1,1,1.0
computationally intelligent methods,1,1,1.0
intelligent methods to,1,1,1.0
methods to play,1,1,1.0
to play an,1,1,1.0
play an essential,1,1,1.0
an essential role,1,1,1.0
essential role in,1,1,1.0
role in applications,1,1,1.0
in applications involving,1,1,1.0
applications involving large,1,1,1.0
involving large amounts,1,1,1.0
large amounts of,1,1,1.0
amounts of data,1,1,1.0
of data on,1,1,1.0
data on the,1,1,1.0
other hand these,1,1,1.0
hand these opportunities,1,1,1.0
these opportunities also,1,1,1.0
opportunities also raise,1,1,1.0
also raise many,1,1,1.0
raise many new,1,1,1.0
many new challenges,1,1,1.0
new challenges for,1,1,1.0
challenges for the,1,1,1.0
for the research,1,1,1.0
the research community,1,1,1.0
research community in,1,1,1.0
community in general,1,1,1.0
in general generally,1,1,1.0
general generally speaking,1,1,1.0
generally speaking any,1,1,1.0
speaking any dataset,1,1,1.0
any dataset that,1,1,1.0
dataset that exhibits,1,1,1.0
that exhibits an,1,1,1.0
exhibits an unequal,1,1,1.0
distribution between its,1,1,1.0
between its classes,1,1,1.0
its classes can,1,1,1.0
be considered imbalanced,1,1,1.0
considered imbalanced in,1,1,1.0
imbalanced in applications,1,1,1.0
in applications datasets,1,1,1.0
applications datasets exhibiting,1,1,1.0
datasets exhibiting severe,1,1,1.0
exhibiting severe ances,1,1,1.0
severe ances are,1,1,1.0
ances are of,1,1,1.0
are of great,1,1,1.0
of great interest,1,1,1.0
great interest since,1,1,1.0
interest since they,1,1,1.0
since they generally,1,1,1.0
they generally present,1,1,1.0
generally present icant,1,1,1.0
present icant difﬁculties,1,1,1.0
icant difﬁculties for,1,1,1.0
difﬁculties for learning,1,1,1.0
for learning mechanisms,1,1,1.0
learning mechanisms typical,1,1,1.0
mechanisms typical imbalance,1,1,1.0
typical imbalance manuscript,1,1,1.0
imbalance manuscript received,1,1,1.0
manuscript received february,1,1,1.0
received february revised,1,1,1.0
february revised november,1,1,1.0
revised november march,1,1,1.0
november march august,1,1,1.0
march august and,1,1,1.0
august and august,1,1,1.0
and august accepted,1,1,1.0
august accepted august,1,1,1.0
accepted august date,1,1,1.0
august date of,2,1,2.0
date of publication,1,1,1.0
of publication august,1,1,1.0
publication august date,1,1,1.0
date of current,1,1,1.0
of current version,1,1,1.0
current version october,1,1,1.0
version october chen,1,1,1.0
october chen and,1,1,1.0
chen and garcia,1,1,1.0
and garcia are,1,1,1.0
garcia are with,1,1,1.0
department of electrical,3,1,3.0
of electrical and,3,1,3.0
electrical and computer,3,1,3.0
and computer engineering,2,1,2.0
computer engineering stevens,2,1,2.0
engineering stevens institute,2,1,2.0
stevens institute of,5,1,5.0
institute of technology,5,1,5.0
of technology hoboken,4,1,4.0
technology hoboken nj,4,1,4.0
hoboken nj usa,1,1,1.0
nj usa egarcia,1,1,1.0
usa egarcia he,1,1,1.0
egarcia he is,1,1,1.0
he is with,1,1,1.0
of electrical computer,2,1,2.0
electrical computer and,2,1,2.0
computer and biomedical,2,1,2.0
and biomedical engineering,2,1,2.0
biomedical engineering university,2,1,2.0
engineering university of,2,1,2.0
university of rhode,2,1,2.0
of rhode island,2,1,2.0
rhode island kingston,2,1,2.0
island kingston ri,1,1,1.0
kingston ri usa,1,1,1.0
ri usa he,1,1,1.0
usa he color,1,1,1.0
he color versions,1,1,1.0
color versions of,1,1,1.0
versions of one,1,1,1.0
of one or,1,1,1.0
more of the,1,1,1.0
of the ﬁgures,1,1,1.0
the ﬁgures in,1,1,1.0
ﬁgures in this,1,1,1.0
are available online,1,1,1.0
available online at,1,1,1.0
online at http,1,1,1.0
at http digital,1,1,1.0
http digital object,1,1,1.0
digital object identiﬁer,1,1,1.0
object identiﬁer ratios,1,1,1.0
identiﬁer ratios can,1,1,1.0
ratios can range,1,1,1.0
can range from,1,1,1.0
range from in,1,1,1.0
from in fraud,1,1,1.0
fraud detection problems,1,1,1.0
detection problems to,1,1,1.0
problems to in,1,1,1.0
to in physics,1,1,1.0
in physics event,1,1,1.0
physics event classiﬁcation,2,1,2.0
event classiﬁcation however,1,1,1.0
classiﬁcation however imbalances,1,1,1.0
however imbalances of,1,1,1.0
imbalances of this,1,1,1.0
of this form,1,1,1.0
this form are,1,1,1.0
form are just,1,1,1.0
are just one,1,1,1.0
just one aspect,1,1,1.0
one aspect of,1,1,1.0
aspect of the,1,1,1.0
the imbalanced learning,5,1,5.0
imbalanced learning problem,2,1,2.0
problem the imbalance,1,1,1.0
the imbalance learning,1,1,1.0
imbalance learning problem,1,1,1.0
learning problem generally,1,1,1.0
problem generally manifests,1,1,1.0
generally manifests itself,1,1,1.0
manifests itself in,1,1,1.0
itself in two,1,1,1.0
in two forms,1,1,1.0
two forms relative,1,1,1.0
forms relative imbalances,1,1,1.0
relative imbalances and,1,1,1.0
imbalances and absolute,1,1,1.0
and absolute imbalances,2,1,2.0
absolute imbalances absolute,1,1,1.0
imbalances absolute imbalances,1,1,1.0
absolute imbalances arise,1,1,1.0
imbalances arise in,1,1,1.0
arise in datasets,1,1,1.0
in datasets where,1,1,1.0
datasets where minority,1,1,1.0
where minority exampl,1,1,1.0
minority exampl es,1,1,1.0
exampl es are,1,1,1.0
es are deﬁnitively,1,1,1.0
are deﬁnitively scarce,1,1,1.0
deﬁnitively scarce and,1,1,1.0
scarce and underrepresented,1,1,1.0
and underrepresented whereas,1,1,1.0
underrepresented whereas relative,1,1,1.0
whereas relative imbalances,1,1,1.0
relative imbalances are,1,1,1.0
imbalances are indicative,1,1,1.0
are indicative of,1,1,1.0
indicative of datasets,1,1,1.0
of datasets in,1,1,1.0
datasets in which,1,1,1.0
in which minority,1,1,1.0
which minority examples,1,1,1.0
minority examples are,1,1,1.0
examples are well,1,1,1.0
are well represented,1,1,1.0
well represented but,1,1,1.0
represented but remain,1,1,1.0
but remain severely,1,1,1.0
remain severely outnumbere,1,1,1.0
severely outnumbere d,1,1,1.0
outnumbere d by,1,1,1.0
d by majority,1,1,1.0
by majority class,1,1,1.0
class examples some,1,1,1.0
examples some studies,1,1,1.0
some studies have,1,1,1.0
studies have shown,1,1,1.0
shown that the,1,1,1.0
that the degradation,1,1,1.0
the degradation of,1,1,1.0
degradation of classiﬁcation,1,1,1.0
of classiﬁcation performance,1,1,1.0
classiﬁcation performance attributed,1,1,1.0
performance attributed to,1,1,1.0
attributed to imbalanced,1,1,1.0
to imbalanced data,1,1,1.0
imbalanced data is,2,1,2.0
data is not,1,1,1.0
is not necessarily,1,1,1.0
not necessarily the,1,1,1.0
necessarily the result,1,1,1.0
result of relative,1,1,1.0
of relative imbalances,1,1,1.0
relative imbalances but,1,1,1.0
but rather due,1,1,1.0
rather due to,1,1,1.0
to the lack,1,1,1.0
the lack of,1,1,1.0
lack of representative,1,1,1.0
of representative examples,1,1,1.0
representative examples absolute,1,1,1.0
examples absolute imbalances,1,1,1.0
absolute imbalances in,1,1,1.0
imbalances in particular,1,1,1.0
in particular for,1,1,1.0
particular for a,1,1,1.0
given dataset that,1,1,1.0
dataset that contains,1,1,1.0
that contains several,1,1,1.0
contains several the,1,1,1.0
several the distribution,1,1,1.0
distribution of minority,1,1,1.0
of minority examples,2,1,2.0
minority examples over,1,1,1.0
examples over the,1,1,1.0
minority class concepts,1,1,1.0
class concepts may,1,1,1.0
concepts may yield,1,1,1.0
may yield clusters,1,1,1.0
yield clusters with,1,1,1.0
clusters with insufﬁcient,1,1,1.0
with insufﬁcient representative,1,1,1.0
insufﬁcient representative exa,1,1,1.0
representative exa mples,1,1,1.0
exa mples to,1,1,1.0
mples to form,1,1,1.0
form a classiﬁcation,1,1,1.0
a classiﬁcation rule,1,1,1.0
classiﬁcation rule this,1,1,1.0
rule this problem,1,1,1.0
this problem of,1,1,1.0
problem of concept,1,1,1.0
of concept data,1,1,1.0
concept data representation,1,1,1.0
data representation within,1,1,1.0
representation within a,1,1,1.0
class is also,1,1,1.0
is also known,1,1,1.0
also known as,1,1,1.0
as the imbalance,1,1,1.0
problem and was,1,1,1.0
and was veriﬁed,1,1,1.0
was veriﬁed to,1,1,1.0
veriﬁed to be,1,1,1.0
be more difﬁcult,1,1,1.0
difﬁcult to handle,1,1,1.0
to handle than,1,1,1.0
handle than datasets,1,1,1.0
than datasets with,1,1,1.0
datasets with only,1,1,1.0
with only homogeneous,1,1,1.0
only homogeneous concepts,1,1,1.0
homogeneous concepts for,1,1,1.0
concepts for each,1,1,1.0
each class logically,1,1,1.0
class logically it,1,1,1.0
logically it would,1,1,1.0
it would follow,1,1,1.0
would follow that,1,1,1.0
follow that solutions,1,1,1.0
that solutions targeted,1,1,1.0
solutions targeted at,1,1,1.0
targeted at both,1,1,1.0
at both relative,1,1,1.0
both relative and,1,1,1.0
relative and absolute,1,1,1.0
absolute imbalances would,1,1,1.0
imbalances would be,1,1,1.0
would be more,1,1,1.0
be more adept,1,1,1.0
more adept to,1,1,1.0
adept to handling,1,1,1.0
to handling a,1,1,1.0
handling a wide,1,1,1.0
a wide spectrum,1,1,1.0
wide spectrum of,1,1,1.0
spectrum of imbalanced,1,1,1.0
of imbalanced learning,1,1,1.0
imbalanced learning problems,8,1,8.0
learning problems to,2,1,2.0
problems to this,1,1,1.0
to this end,3,1,3.0
this end this,1,1,1.0
end this paper,1,1,1.0
paper proposes ramoboost,1,1,1.0
proposes ramoboost which,1,1,1.0
ramo technique embedded,1,1,1.0
technique embedded with,1,1,1.0
embedded with a,1,1,1.0
with a boosting,1,1,1.0
a boosting procedure,1,1,1.0
boosting procedure to,1,1,1.0
procedure to facilitate,1,1,1.0
to facilitate learning,2,1,2.0
facilitate learning from,2,1,2.0
from imbalanced datase,1,1,1.0
imbalanced datase ts,1,1,1.0
datase ts based,1,1,1.0
ts based on,1,1,1.0
based on an,1,1,1.0
on an integration,1,1,1.0
an integration of,1,1,1.0
integration of oversampling,1,1,1.0
oversampling and ensemble,1,1,1.0
ensemble learning ramoboost,1,1,1.0
learning ramoboost tematically,1,1,1.0
ramoboost tematically generates,1,1,1.0
tematically generates synthetic,1,1,1.0
generates synthetic instances,2,1,2.0
synthetic instances by,2,1,2.0
instances by considering,1,1,1.0
considering the class,1,1,1.0
the class ratios,1,1,1.0
class ratios of,1,1,1.0
ratios of surrounding,1,1,1.0
of surrounding nearest,1,1,1.0
surrounding nearest neighbors,1,1,1.0
neighbors of each,6,1,6.0
of each minority,10,1,10.0
minority class example,1,1,1.0
class example in,1,1,1.0
in the underlying,1,1,1.0
the underlying training,1,1,1.0
underlying training data,1,1,1.0
training data distribution,1,1,1.0
data distribution unlike,1,1,1.0
distribution unlike many,1,1,1.0
unlike many existing,1,1,1.0
many existing approaches,1,1,1.0
existing approaches that,1,1,1.0
approaches that use,1,1,1.0
that use uniform,1,1,1.0
use uniform sampling,1,1,1.0
uniform sampling distributions,1,1,1.0
sampling distributions ramoboost,1,1,1.0
distributions ramoboost adaptively,1,1,1.0
ramoboost adaptively adjusts,1,1,1.0
adaptively adjusts the,1,1,1.0
adjusts the sampling,1,1,1.0
the sampling weights,1,1,1.0
sampling weights of,1,1,1.0
weights of minority,1,1,1.0
class examples according,1,1,1.0
examples according to,1,1,1.0
to their data,1,1,1.0
their data distributions,1,1,1.0
data distributions moreover,1,1,1.0
distributions moreover by,1,1,1.0
moreover by integrating,1,1,1.0
by integrating the,1,1,1.0
integrating the ensemble,1,1,1.0
the ensemble learning,1,1,1.0
ensemble learning methodology,1,1,1.0
learning methodology ramoboost,1,1,1.0
methodology ramoboost adopts,1,1,1.0
ramoboost adopts an,1,1,1.0
adopts an iterative,1,1,1.0
an iterative learning,1,1,1.0
iterative learning dure,1,1,1.0
learning dure that,1,1,1.0
dure that assesses,1,1,1.0
that assesses the,1,1,1.0
assesses the hypothesis,1,1,1.0
the hypothesis developed,1,1,1.0
hypothesis developed at,1,1,1.0
developed at each,1,1,1.0
at each boosting,2,1,2.0
each boosting iteration,3,1,3.0
boosting iteration to,1,1,1.0
iteration to adaptively,1,1,1.0
to adaptively shift,1,1,1.0
decision boundary to,2,1,2.0
boundary to focus,1,1,1.0
more on those,1,1,1.0
on those instances,1,1,1.0
those instances of,1,1,1.0
instances of both,1,1,1.0
of both the,1,1,1.0
both the majority,1,1,1.0
majority and the,1,1,1.0
minority classes we,1,1,1.0
classes we organize,1,1,1.0
we organize the,1,1,1.0
organize the remainder,1,1,1.0
this paper as,1,1,1.0
paper as follows,1,1,1.0
as follows in,1,1,1.0
follows in section,1,1,1.0
in section ii,2,1,2.0
section ii we,1,1,1.0
ii we present,1,1,1.0
present a brief,1,1,1.0
a brief review,1,1,1.0
brief review of,1,1,1.0
of the art,1,1,1.0
the art techniques,1,1,1.0
art techniques proposed,1,1,1.0
proposed in the,1,1,1.0
in the community,1,1,1.0
the community to,1,1,1.0
community to address,1,1,1.0
address the imbalanced,1,1,1.0
learning problem in,1,1,1.0
problem in section,1,1,1.0
section iii we,1,1,1.0
iii we discuss,1,1,1.0
we discuss the,1,1,1.0
discuss the motivation,1,1,1.0
the motivation behind,1,1,1.0
motivation behind the,1,1,1.0
behind the ramoboost,1,1,1.0
the ramoboost framework,2,1,2.0
ramoboost framework and,1,1,1.0
framework and present,1,1,1.0
and present ieeechen,1,1,1.0
present ieeechen et,1,1,1.0
ieeechen et al,1,1,1.0
et al ranked,9,1,9.0
al ranked minority,9,1,9.0
in boosting the,2,1,2.0
boosting the algorithm,1,1,1.0
the algorithm in,2,1,2.0
algorithm in detail,1,1,1.0
in detail the,1,1,1.0
detail the computational,1,1,1.0
the computational complexity,3,1,3.0
computational complexity analysis,2,1,2.0
complexity analysis of,1,1,1.0
the proposed ramoboost,3,1,3.0
proposed ramoboost algorithms,1,1,1.0
ramoboost algorithms is,1,1,1.0
algorithms is also,1,1,1.0
is also presented,1,1,1.0
also presented in,2,1,2.0
presented in this,5,1,5.0
this section in,1,1,1.0
section in section,1,1,1.0
in section iv,1,1,1.0
section iv simulation,1,1,1.0
iv simulation analysis,1,1,1.0
analysis on world,1,1,1.0
on world machine,1,1,1.0
world machine learning,1,1,1.0
machine learning datasets,1,1,1.0
learning datasets are,1,1,1.0
datasets are provided,1,1,1.0
are provided to,1,1,1.0
provided to illustrate,1,1,1.0
effectiveness of the,1,1,1.0
proposed method details,1,1,1.0
method details of,1,1,1.0
details of experimental,1,1,1.0
of experimental parameters,1,1,1.0
experimental parameters and,1,1,1.0
parameters and evaluation,1,1,1.0
and evaluation metrics,1,1,1.0
evaluation metrics are,1,1,1.0
metrics are also,1,1,1.0
are also presented,1,1,1.0
this section finally,1,1,1.0
section finally a,1,1,1.0
finally a conclusion,1,1,1.0
a conclusion and,1,1,1.0
conclusion and brief,1,1,1.0
and brief discussion,1,1,1.0
discussion on future,1,1,1.0
on future research,1,1,1.0
future research directions,2,1,2.0
research directions are,1,1,1.0
directions are presented,1,1,1.0
presented in section,1,1,1.0
section ii r,1,1,1.0
ii r elated,1,1,1.0
r elated works,1,1,1.0
elated works a,1,1,1.0
works a comprehensive,1,1,1.0
a comprehensive review,1,1,1.0
comprehensive review of,1,1,1.0
of the development,1,1,1.0
development of research,1,1,1.0
research in learning,1,1,1.0
imbalanced data including,1,1,1.0
data including the,1,1,1.0
including the nature,1,1,1.0
of the problem,1,1,1.0
problem the rt,1,1,1.0
the rt approaches,1,1,1.0
rt approaches the,1,1,1.0
approaches the assessment,1,1,1.0
the assessment metrics,1,1,1.0
assessment metrics and,1,1,1.0
metrics and the,1,1,1.0
and the major,1,1,1.0
the major opportunities,1,1,1.0
major opportunities and,1,1,1.0
opportunities and challenges,1,1,1.0
and challenges has,1,1,1.0
challenges has been,1,1,1.0
has been presented,1,1,1.0
been presented in,1,1,1.0
presented in interested,1,1,1.0
in interested readers,1,1,1.0
interested readers can,1,1,1.0
readers can refer,1,1,1.0
can refer to,2,1,2.0
refer to that,1,1,1.0
to that article,1,1,1.0
that article for,1,1,1.0
article for details,1,1,1.0
for details in,1,1,1.0
details in this,1,1,1.0
section we provide,1,1,1.0
provide a focused,1,1,1.0
a focused review,1,1,1.0
focused review of,1,1,1.0
review of four,1,1,1.0
of four major,1,1,1.0
four major categories,1,1,1.0
major categories of,1,1,1.0
categories of research,1,1,1.0
of research activity,1,1,1.0
research activity in,1,1,1.0
activity in imbalanced,1,1,1.0
in imbalanced learning,2,1,2.0
imbalanced learning sampling,1,1,1.0
learning sampling methods,1,1,1.0
sampling methods building,1,1,1.0
methods building on,1,1,1.0
building on the,1,1,1.0
on the foundation,1,1,1.0
the foundation of,1,1,1.0
foundation of the,1,1,1.0
of the random,3,1,3.0
the random simple,1,1,1.0
random simple pling,1,1,1.0
simple pling and,1,1,1.0
pling and undersampling,1,1,1.0
and undersampling techniques,1,1,1.0
undersampling techniques researchers,1,1,1.0
techniques researchers have,1,1,1.0
researchers have oped,1,1,1.0
have oped advanced,1,1,1.0
oped advanced sampling,1,1,1.0
sampling methods to,1,1,1.0
methods to address,1,1,1.0
address the shortcomings,1,1,1.0
the shortcomings of,1,1,1.0
shortcomings of these,1,1,1.0
of these basic,1,1,1.0
these basic techniques,1,1,1.0
basic techniques such,1,1,1.0
such as overﬁtting,1,1,1.0
as overﬁtting and,1,1,1.0
overﬁtting and information,1,1,1.0
and information loss,1,1,1.0
information loss for,1,1,1.0
loss for example,1,1,1.0
example the synthetic,1,1,1.0
minority oversampling nique,1,1,1.0
oversampling nique smote,1,1,1.0
nique smote algorithm,1,1,1.0
smote algorithm was,1,1,1.0
algorithm was proposed,1,1,1.0
proposed to search,1,1,1.0
to search for,1,1,1.0
neighbors of every,1,1,1.0
of every minority,1,1,1.0
minority instance and,1,1,1.0
instance and generates,1,1,1.0
and generates synthetic,1,1,1.0
synthetic minority data,1,1,1.0
minority data by,1,1,1.0
data by calculating,1,1,1.0
by calculating linear,1,1,1.0
calculating linear interpolations,1,1,1.0
linear interpolations between,1,1,1.0
interpolations between an,1,1,1.0
between an original,1,1,1.0
an original minority,1,1,1.0
original minority class,1,1,1.0
class instance and,1,1,1.0
and a randomly,1,1,1.0
a randomly selected,1,1,1.0
randomly selected neighbor,1,1,1.0
selected neighbor expanding,1,1,1.0
neighbor expanding on,1,1,1.0
expanding on the,1,1,1.0
on the smote,2,1,2.0
the smote framework,1,1,1.0
smote framework the,1,1,1.0
framework the algorithm,1,1,1.0
the algorithm locates,1,1,1.0
algorithm locates those,1,1,1.0
locates those minority,1,1,1.0
those minority class,1,1,1.0
examples that reside,1,1,1.0
that reside along,1,1,1.0
reside along the,1,1,1.0
along the borders,1,1,1.0
the borders between,1,1,1.0
borders between majority,1,1,1.0
minority classes other,1,1,1.0
classes other popular,1,1,1.0
other popular approaches,1,1,1.0
popular approaches include,1,1,1.0
approaches include the,1,1,1.0
include the imbalanced,1,1,1.0
imbalanced learning adaboost,1,1,1.0
learning adaboost in,1,1,1.0
adaboost in junction,1,1,1.0
in junction with,1,1,1.0
junction with and,1,1,1.0
with and jittering,1,1,1.0
and jittering of,1,1,1.0
jittering of the,1,1,1.0
the data approach,1,1,1.0
data approach and,1,1,1.0
approach and the,1,1,1.0
and the integration,1,1,1.0
the integration of,1,1,1.0
integration of smote,1,1,1.0
of smote with,1,1,1.0
smote with tomek,1,1,1.0
with tomek links,1,1,1.0
tomek links and,1,1,1.0
links and edited,1,1,1.0
and edited nearest,1,1,1.0
nearest neighbor since,1,1,1.0
neighbor since many,1,1,1.0
since many of,1,1,1.0
of the aforementioned,1,1,1.0
the aforementioned sampling,1,1,1.0
aforementioned sampling algorithms,1,1,1.0
sampling algorithms solely,1,1,1.0
algorithms solely focus,1,1,1.0
solely focus on,1,1,1.0
focus on relative,1,1,1.0
on relative imbalances,1,1,1.0
relative imbalances various,1,1,1.0
imbalances various approaches,1,1,1.0
various approaches were,1,1,1.0
approaches were proposed,1,1,1.0
were proposed to,1,1,1.0
proposed to explicitly,1,1,1.0
to explicitly address,1,1,1.0
explicitly address the,1,1,1.0
address the imbalance,1,1,1.0
the imbalance issue,1,1,1.0
imbalance issue some,1,1,1.0
issue some of,1,1,1.0
of the important,1,1,1.0
the important works,1,1,1.0
important works include,1,1,1.0
works include the,1,1,1.0
include the oversampling,1,1,1.0
the oversampling algorithm,1,1,1.0
oversampling algorithm support,1,1,1.0
algorithm support cluster,1,1,1.0
cluster machines and,1,1,1.0
machines and others,1,1,1.0
and others learning,2,1,2.0
others learning methods,2,1,2.0
learning methods learning,1,1,1.0
methods learning methods,1,1,1.0
learning methods typically,1,1,1.0
methods typically employ,1,1,1.0
typically employ the,1,1,1.0
employ the use,1,1,1.0
use of cost,1,1,1.0
of cost matrices,1,1,1.0
cost matrices to,1,1,1.0
matrices to estimate,1,1,1.0
to estimate the,1,1,1.0
estimate the costs,1,1,1.0
the costs of,1,1,1.0
costs of different,1,1,1.0
of different classiﬁcation,1,1,1.0
different classiﬁcation errors,1,1,1.0
classiﬁcation errors these,1,1,1.0
errors these techniques,1,1,1.0
these techniques have,1,1,1.0
techniques have shown,1,1,1.0
have shown great,1,1,1.0
shown great success,1,1,1.0
great success when,1,1,1.0
success when applied,1,1,1.0
when applied to,1,1,1.0
applied to imbalanced,1,1,1.0
to imbalanced learning,1,1,1.0
learning problems for,1,1,1.0
problems for example,1,1,1.0
example in an,1,1,1.0
in an method,1,1,1.0
an method was,1,1,1.0
method was presented,1,1,1.0
was presented to,1,1,1.0
presented to induce,1,1,1.0
to induce trees,2,1,2.0
induce trees in,1,1,1.0
trees in and,1,1,1.0
in and the,1,1,1.0
and the asymmetric,1,1,1.0
the asymmetric adaboost,1,1,1.0
asymmetric adaboost method,1,1,1.0
adaboost method is,1,1,1.0
method is proposed,1,1,1.0
is proposed to,1,1,1.0
to handle face,1,1,1.0
handle face detection,1,1,1.0
face detection problems,1,1,1.0
detection problems where,1,1,1.0
where the skewed,1,1,1.0
the skewed class,1,1,1.0
skewed class ratio,1,1,1.0
class ratio can,1,1,1.0
ratio can be,1,1,1.0
can be quite,1,1,1.0
be quite high,1,1,1.0
quite high the,1,1,1.0
high the adacost,1,1,1.0
the adacost method,1,1,1.0
adacost method proposed,1,1,1.0
method proposed in,1,1,1.0
proposed in combines,1,1,1.0
in combines learning,1,1,1.0
combines learning with,1,1,1.0
learning with ing,1,1,1.0
with ing by,1,1,1.0
ing by referring,1,1,1.0
by referring to,1,1,1.0
referring to the,1,1,1.0
the matrix adacost,1,1,1.0
matrix adacost assigns,1,1,1.0
adacost assigns different,1,1,1.0
assigns different cost,1,1,1.0
different cost values,1,1,1.0
cost values to,1,1,1.0
values to misclassiﬁed,1,1,1.0
to misclassiﬁed minority,1,1,1.0
misclassiﬁed minority and,1,1,1.0
and majority examples,3,1,3.0
majority examples by,1,1,1.0
examples by the,1,1,1.0
by the trained,1,1,1.0
the trained hypothesis,2,1,2.0
trained hypothesis at,1,1,1.0
hypothesis at each,2,1,2.0
at each iteration,3,1,3.0
each iteration loop,4,1,4.0
iteration loop other,1,1,1.0
loop other examples,1,1,1.0
other examples of,1,1,1.0
examples of learning,1,1,1.0
of learning include,1,1,1.0
learning include the,1,1,1.0
include the cost,1,1,1.0
the cost framework,1,1,1.0
cost framework neural,1,1,1.0
framework neural network,1,1,1.0
neural network sensitive,1,1,1.0
network sensitive support,1,1,1.0
sensitive support vector,1,1,1.0
vector machines svms,1,1,1.0
machines svms and,1,1,1.0
svms and others,1,1,1.0
learning methods methods,1,1,1.0
methods methods have,1,1,1.0
methods have recently,1,1,1.0
have recently become,1,1,1.0
recently become very,1,1,1.0
become very popular,1,1,1.0
very popular across,1,1,1.0
popular across various,1,1,1.0
across various ﬁelds,1,1,1.0
various ﬁelds including,1,1,1.0
ﬁelds including imbalanced,1,1,1.0
including imbalanced learning,1,1,1.0
learning in eral,1,1,1.0
in eral methods,1,1,1.0
eral methods fac,1,1,1.0
methods fac ilitate,1,1,1.0
fac ilitate learning,1,1,1.0
ilitate learning by,1,1,1.0
learning by maximizing,1,1,1.0
by maximizing the,1,1,1.0
maximizing the separation,1,1,1.0
the separation margin,1,1,1.0
separation margin between,1,1,1.0
margin between concepts,1,1,1.0
between concepts in,1,1,1.0
concepts in linearly,1,1,1.0
in linearly separable,1,1,1.0
linearly separable feature,1,1,1.0
separable feature spaces,1,1,1.0
spaces for instance,1,1,1.0
for instance the,1,1,1.0
instance the alignment,1,1,1.0
the alignment gorithm,1,1,1.0
alignment gorithm was,1,1,1.0
gorithm was proposed,1,1,1.0
proposed in and,2,1,2.0
in and in,1,1,1.0
and in which,1,1,1.0
in which imbalanced,1,1,1.0
which imbalanced data,1,1,1.0
data is used,1,1,1.0
is used as,4,1,4.0
used as information,1,1,1.0
as information prior,1,1,1.0
information prior to,1,1,1.0
prior to adjusting,1,1,1.0
to adjusting the,1,1,1.0
adjusting the kernel,1,1,1.0
matrix in order,1,1,1.0
order to facilitate,1,1,1.0
to facilitate svm,1,1,1.0
facilitate svm learning,1,1,1.0
svm learning for,1,1,1.0
learning for improved,1,1,1.0
for improved prediction,1,1,1.0
improved prediction accuracy,1,1,1.0
prediction accuracy another,1,1,1.0
accuracy another example,1,1,1.0
another example of,1,1,1.0
example of learning,1,1,1.0
of learning presents,1,1,1.0
learning presents a,1,1,1.0
presents a kernel,1,1,1.0
a kernel classiﬁer,1,1,1.0
kernel classiﬁer construction,1,1,1.0
classiﬁer construction algorithm,1,1,1.0
construction algorithm using,1,1,1.0
algorithm using orthogonal,1,1,1.0
using orthogonal ward,1,1,1.0
orthogonal ward selection,1,1,1.0
ward selection to,1,1,1.0
selection to optimize,1,1,1.0
to optimize the,1,1,1.0
optimize the generalization,1,1,1.0
the generalization model,1,1,1.0
generalization model for,1,1,1.0
model for class,1,1,1.0
for class imbalanced,1,1,1.0
class imbalanced learning,1,1,1.0
learning problems this,1,1,1.0
problems this is,1,1,1.0
this is accomplished,2,1,2.0
accomplished by using,1,1,1.0
using the regularized,1,1,1.0
the regularized orthogonal,1,1,1.0
regularized orthogonal weighted,1,1,1.0
orthogonal weighted least,1,1,1.0
weighted least squares,1,1,1.0
least squares method,1,1,1.0
squares method and,1,1,1.0
method and a,1,1,1.0
and a model,1,1,1.0
a model selection,1,1,1.0
model selection criterion,1,1,1.0
selection criterion of,1,1,1.0
criterion of maximal,1,1,1.0
of maximal area,1,1,1.0
maximal area under,1,1,1.0
curve auc of,1,1,1.0
auc of the,4,1,4.0
of the receiver,1,1,1.0
the receiver operating,1,1,1.0
operating characteristic roc,1,1,1.0
characteristic roc graph,1,1,1.0
roc graph active,1,1,1.0
graph active learning,1,1,1.0
active learning methods,2,1,2.0
learning methods active,1,1,1.0
methods active learning,1,1,1.0
learning methods were,1,1,1.0
methods were originally,1,1,1.0
were originally developed,1,1,1.0
originally developed for,1,1,1.0
developed for learning,1,1,1.0
learning from datasets,1,1,1.0
from datasets with,1,1,1.0
datasets with unl,1,1,1.0
with unl abeled,1,1,1.0
unl abeled instances,1,1,1.0
abeled instances recently,1,1,1.0
instances recently tive,1,1,1.0
recently tive learning,1,1,1.0
tive learning methods,1,1,1.0
methods have found,1,1,1.0
have found increased,1,1,1.0
found increased use,1,1,1.0
increased use in,1,1,1.0
use in imbalanced,1,1,1.0
imbalanced learning applications,1,1,1.0
learning applications fo,1,1,1.0
applications fo r,1,1,1.0
fo r example,1,1,1.0
r example an,1,1,1.0
example an active,1,1,1.0
an active learning,1,1,1.0
active learning approach,1,1,1.0
learning approach for,1,1,1.0
imbalanced datasets was,1,1,1.0
datasets was proposed,1,1,1.0
in and this,1,1,1.0
and this algorithm,1,1,1.0
this algorithm locates,1,1,1.0
algorithm locates the,1,1,1.0
locates the most,1,1,1.0
the most informative,2,1,2.0
most informative sample,1,1,1.0
informative sample by,1,1,1.0
sample by evaluating,1,1,1.0
by evaluating a,1,1,1.0
evaluating a small,1,1,1.0
a small ﬁxed,1,1,1.0
small ﬁxed number,1,1,1.0
ﬁxed number of,1,1,1.0
number of randomly,1,1,1.0
of randomly selected,1,1,1.0
randomly selected examples,1,1,1.0
selected examples instead,1,1,1.0
examples instead of,1,1,1.0
of the entire,1,1,1.0
the entire dataset,2,1,2.0
entire dataset in,1,1,1.0
in the stopping,1,1,1.0
the stopping condition,1,1,1.0
stopping condition for,1,1,1.0
condition for active,1,1,1.0
for active l,1,1,1.0
active l earning,1,1,1.0
l earning applications,1,1,1.0
earning applications in,1,1,1.0
applications in word,1,1,1.0
in word sense,1,1,1.0
word sense disambiguation,2,1,2.0
sense disambiguation wsd,1,1,1.0
disambiguation wsd domains,1,1,1.0
wsd domains was,1,1,1.0
domains was investigated,1,1,1.0
was investigated to,1,1,1.0
investigated to alleviate,1,1,1.0
to alleviate the,1,1,1.0
alleviate the complications,1,1,1.0
the complications introduced,1,1,1.0
complications introduced by,1,1,1.0
introduced by ances,1,1,1.0
by ances the,1,1,1.0
ances the authors,1,1,1.0
the authors proposed,1,1,1.0
authors proposed a,1,1,1.0
proposed a oversampling,1,1,1.0
a oversampling technique,1,1,1.0
oversampling technique to,1,1,1.0
technique to improve,1,1,1.0
to improve active,1,1,1.0
improve active learning,1,1,1.0
active learning performance,1,1,1.0
learning performance for,2,1,2.0
performance for anced,1,1,1.0
for anced wsd,1,1,1.0
anced wsd applications,1,1,1.0
wsd applications iii,1,1,1.0
applications iii ramob,1,1,1.0
iii ramob oost,1,1,1.0
ramob oost framework,1,1,1.0
oost framework i,1,1,1.0
framework i ntegration,1,1,1.0
i ntegration of,1,1,1.0
ntegration of data,1,1,1.0
of data generation,1,1,1.0
data generation and,1,1,1.0
generation and boosting,1,1,1.0
and boosting ensemble,1,1,1.0
boosting ensemble learning,1,1,1.0
ensemble learning preliminaries,1,1,1.0
learning preliminaries for,1,1,1.0
preliminaries for ramoboost,1,1,1.0
for ramoboost various,1,1,1.0
ramoboost various techniques,1,1,1.0
various techniques exist,1,1,1.0
techniques exist for,1,1,1.0
exist for quantizing,1,1,1.0
for quantizing the,1,1,1.0
quantizing the learning,1,1,1.0
the learning culty,1,1,1.0
learning culty level,1,1,1.0
culty level of,1,1,1.0
level of an,1,1,1.0
an example accord,1,1,1.0
example accord ing,1,1,1.0
accord ing to,2,1,2.0
to the distance,1,1,1.0
distance between minority,1,1,1.0
and majority cases,1,1,1.0
majority cases an,1,1,1.0
cases an intuitive,1,1,1.0
an intuitive method,1,1,1.0
intuitive method can,1,1,1.0
can be formulated,2,1,2.0
be formulated as,1,1,1.0
formulated as follows,1,1,1.0
as follows first,1,1,1.0
follows first l,1,1,1.0
first l ocate,1,1,1.0
l ocate the,1,1,1.0
ocate the center,1,1,1.0
the center mean,1,1,1.0
center mean of,1,1,1.0
the majority examples,1,1,1.0
majority examples in,2,1,2.0
in the featur,1,1,1.0
the featur e,1,1,1.0
featur e space,1,1,1.0
e space then,1,1,1.0
space then calculate,1,1,1.0
then calculate the,1,1,1.0
calculate the euclidean,1,1,1.0
the euclidean distance,7,1,7.0
euclidean distance between,2,1,2.0
distance between each,1,1,1.0
between each minority,1,1,1.0
each minority example,21,1,21.0
minority example and,4,1,4.0
example and the,1,1,1.0
and the center,1,1,1.0
the center the,1,1,1.0
center the set,1,1,1.0
set of minimal,1,1,1.0
of minimal distanced,1,1,1.0
minimal distanced minority,1,1,1.0
distanced minority examples,1,1,1.0
minority examples up,1,1,1.0
examples up to,1,1,1.0
up to some,1,1,1.0
to some threshold,1,1,1.0
some threshold then,1,1,1.0
threshold then would,1,1,1.0
then would represent,1,1,1.0
would represent the,1,1,1.0
represent the most,1,1,1.0
most informative examples,1,1,1.0
informative examples fig,1,1,1.0
examples fig a,2,1,2.0
fig a illustrates,1,1,1.0
a illustrates this,1,1,1.0
illustrates this idea,1,1,1.0
this idea in,2,1,2.0
idea in this,1,1,1.0
in this ﬁgure,1,1,1.0
this ﬁgure the,1,1,1.0
ﬁgure the stars,1,1,1.0
the stars and,1,1,1.0
stars and circles,1,1,1.0
and circles represent,1,1,1.0
circles represent the,1,1,1.0
represent the minority,1,1,1.0
majority classes respectively,1,1,1.0
classes respectively and,1,1,1.0
respectively and the,2,1,2.0
and the triangle,1,1,1.0
the triangle within,1,1,1.0
triangle within the,1,1,1.0
within the majority,2,1,2.0
majority data sents,1,1,1.0
data sents the,1,1,1.0
sents the approximate,1,1,1.0
the approximate center,1,1,1.0
approximate center the,1,1,1.0
center the euclidean,1,1,1.0
the euclidean distances,1,1,1.0
euclidean distances between,1,1,1.0
distances between minority,1,1,1.0
between minority examples,1,1,1.0
minority examples a,1,1,1.0
examples a n,1,1,1.0
n d and,1,1,1.0
d and the,1,1,1.0
majority class center,1,1,1.0
class center would,1,1,1.0
center would be,1,1,1.0
would be calculated,1,1,1.0
be calculated respectively,1,1,1.0
calculated respectively as,1,1,1.0
respectively as efﬁcient,1,1,1.0
as efﬁcient as,1,1,1.0
efﬁcient as it,1,1,1.0
as it appears,1,1,1.0
it appears this,1,1,1.0
appears this method,1,1,1.0
this method exhibits,1,1,1.0
method exhibits a,1,1,1.0
exhibits a critical,1,1,1.0
a critical ﬂaw,1,1,1.0
critical ﬂaw it,1,1,1.0
ﬂaw it assumes,1,1,1.0
assumes that there,1,1,1.0
are no disjuncts,1,1,1.0
no disjuncts within,1,1,1.0
disjuncts within the,1,1,1.0
majority class concept,1,1,1.0
class concept otherwise,1,1,1.0
concept otherwise there,1,1,1.0
otherwise there may,1,1,1.0
there may ex,1,1,1.0
may ex ist,1,1,1.0
ex ist several,1,1,1.0
ist several for,1,1,1.0
several for the,1,1,1.0
and the task,1,1,1.0
the task of,1,1,1.0
task of computing,1,1,1.0
computing the euclidean,1,1,1.0
euclidean distance against,1,1,1.0
distance against the,1,1,1.0
against the centers,1,1,1.0
the centers beco,1,1,1.0
centers beco mes,1,1,1.0
beco mes complicated,1,1,1.0
mes complicated and,1,1,1.0
complicated and requires,1,1,1.0
and requires careful,1,1,1.0
requires careful examination,1,1,1.0
careful examination we,1,1,1.0
examination we highlight,1,1,1.0
we highlight this,1,1,1.0
highlight this issue,1,1,1.0
this issue in,2,1,2.0
issue in fig,1,1,1.0
in fig b,1,1,1.0
fig b ieee,1,1,1.0
b ieee transactions,1,1,1.0
no october a,2,1,2.0
october a b,2,1,2.0
a b c,3,1,3.0
b c majority,1,1,1.0
c majority cluster,1,1,1.0
majority cluster majority,1,1,1.0
cluster majority cluster,1,1,1.0
majority cluster fig,1,1,1.0
cluster fig cluster,1,1,1.0
fig cluster and,1,1,1.0
cluster and distance,1,1,1.0
and distance calculation,1,1,1.0
distance calculation based,1,1,1.0
calculation based on,1,1,1.0
based on euclidean,1,1,1.0
on euclidean distance,1,1,1.0
distance a int,1,1,1.0
a int uitive,1,1,1.0
int uitive approach,1,1,1.0
uitive approach to,1,1,1.0
approach to decide,1,1,1.0
to decide the,2,1,2.0
decide the infor,1,1,1.0
the infor mative,1,1,1.0
infor mative data,1,1,1.0
mative data examples,1,1,1.0
data examples based,1,1,1.0
examples based on,1,1,1.0
based on euclid,1,1,1.0
on euclid ean,1,1,1.0
euclid ean distance,1,1,1.0
ean distance b,1,1,1.0
distance b potential,1,1,1.0
b potential dilemma,1,1,1.0
potential dilemma by,1,1,1.0
dilemma by applying,1,1,1.0
by applying this,1,1,1.0
applying this in,1,1,1.0
this in tuitive,1,1,1.0
in tuitive approach,1,1,1.0
tuitive approach c,1,1,1.0
approach c proposed,1,1,1.0
c proposed approach,1,1,1.0
proposed approach by,1,1,1.0
approach by using,1,1,1.0
by using k,1,1,1.0
using k nearest,1,1,1.0
example and using,1,1,1.0
and using this,1,1,1.0
using this value,1,1,1.0
this value to,1,1,1.0
value to decide,1,1,1.0
decide the weight,1,1,1.0
the weight to,1,1,1.0
weight to ﬁnd,1,1,1.0
ﬁnd the informative,1,1,1.0
the informative data,1,1,1.0
informative data examples,1,1,1.0
data examples generated,1,1,1.0
examples generated synthetic,1,1,1.0
synthetic instance xi,1,1,1.0
instance xi xiˆ,1,1,1.0
xi xiˆ fig,1,1,1.0
xiˆ fig synthetic,1,1,1.0
fig synthetic data,1,1,1.0
data generation based,1,1,1.0
generation based on,1,1,1.0
algorithm the majority,1,1,1.0
majority class contains,1,1,1.0
class contains two,1,1,1.0
contains two clusters,1,1,1.0
two clusters and,1,1,1.0
clusters and each,1,1,1.0
and each minority,1,1,1.0
minority example has,2,1,2.0
example has to,1,1,1.0
has to locate,1,1,1.0
to locate its,1,1,1.0
locate its respective,1,1,1.0
its respective closest,1,1,1.0
respective closest cluster,1,1,1.0
closest cluster center,1,1,1.0
cluster center before,1,1,1.0
center before calculating,1,1,1.0
before calculating the,1,1,1.0
calculating the dist,1,1,1.0
the dist ance,1,1,1.0
dist ance to,1,1,1.0
ance to the,1,1,1.0
the majority center,1,1,1.0
majority center ramoboost,1,1,1.0
center ramoboost targets,1,1,1.0
ramoboost targets this,1,1,1.0
targets this ﬂaw,2,1,2.0
this ﬂaw in,1,1,1.0
ﬂaw in the,1,1,1.0
the following way,1,1,1.0
following way stead,1,1,1.0
way stead of,1,1,1.0
stead of searching,1,1,1.0
of searching for,1,1,1.0
searching for the,1,1,1.0
for the center,1,1,1.0
the center s,1,1,1.0
center s of,1,1,1.0
s of the,1,1,1.0
the majority dataset,1,1,1.0
majority dataset ramoboost,1,1,1.0
dataset ramoboost determines,1,1,1.0
ramoboost determines the,1,1,1.0
determines the number,1,1,1.0
of majority examples,2,1,2.0
majority examples that,2,1,2.0
that are within,2,1,2.0
are within the,2,1,2.0
within the k,2,1,2.0
the k nearest,6,1,6.0
example and assigns,1,1,1.0
and assigns this,1,1,1.0
assigns this value,1,1,1.0
this value as,1,1,1.0
value as the,1,1,1.0
as the information,1,1,1.0
the information weight,1,1,1.0
information weight we,1,1,1.0
weight we illustrate,1,1,1.0
we illustrate this,1,1,1.0
illustrate this idea,1,1,1.0
idea in fig,1,1,1.0
in fig c,1,1,1.0
fig c here,1,1,1.0
c here the,1,1,1.0
here the highlighted,1,1,1.0
the highlighted areas,1,1,1.0
highlighted areas surrounded,1,1,1.0
areas surrounded by,1,1,1.0
surrounded by dashed,1,1,1.0
by dashed circles,1,1,1.0
dashed circles represents,1,1,1.0
circles represents the,1,1,1.0
represents the k,1,1,1.0
k nearest neighbor,2,1,2.0
neighbor search area,1,1,1.0
search area for,1,1,1.0
area for each,1,1,1.0
minority example a,1,1,1.0
example a n,1,1,1.0
n d k,1,1,1.0
d k n,1,1,1.0
k n this,1,1,1.0
n this case,1,1,1.0
this case accordingly,1,1,1.0
case accordingly ther,1,1,1.0
accordingly ther e,1,1,1.0
ther e are,1,1,1.0
e are and,1,1,1.0
are and majority,1,1,1.0
within the four,1,1,1.0
the four nearest,1,1,1.0
four nearest neighbors,1,1,1.0
neighbors of a,1,1,1.0
of a n,1,1,1.0
n d respectively,1,1,1.0
d respectively this,1,1,1.0
respectively this means,1,1,1.0
this means the,1,1,1.0
means the minority,1,1,1.0
the minority example,8,1,8.0
minority example is,1,1,1.0
example is more,1,1,1.0
is more likely,1,1,1.0
to be located,1,1,1.0
be located near,1,1,1.0
located near the,1,1,1.0
near the decision,2,1,2.0
boundary is a,1,1,1.0
is a example,1,1,1.0
a example and,1,1,1.0
example and therefore,1,1,1.0
and therefore more,1,1,1.0
therefore more synthetic,1,1,1.0
more synthetic samples,1,1,1.0
synthetic samples should,1,1,1.0
samples should be,1,1,1.0
should be generated,1,1,1.0
be generated for,4,1,4.0
generated for to,2,1,2.0
for to force,1,1,1.0
to force the,1,1,1.0
force the ﬁnal,1,1,1.0
the ﬁnal learning,2,1,2.0
ﬁnal learning hypothesis,1,1,1.0
learning hypothesis to,1,1,1.0
hypothesis to be,1,1,1.0
be more focused,3,1,3.0
more focused on,3,1,3.0
focused on the,2,1,2.0
on the difﬁcult,2,1,2.0
the difﬁcult learning,1,1,1.0
difﬁcult learning regions,1,1,1.0
learning regions our,1,1,1.0
regions our previous,1,1,1.0
our previous work,1,1,1.0
previous work adaptive,1,1,1.0
work adaptive synthetic,1,1,1.0
adaptive synthetic adasyn,1,1,1.0
synthetic adasyn also,1,1,1.0
adasyn also applies,1,1,1.0
also applies a,1,1,1.0
applies a similar,1,1,1.0
a similar idea,1,1,1.0
similar idea to,1,1,1.0
idea to assess,1,1,1.0
assess the difﬁculty,1,1,1.0
the difﬁculty level,1,1,1.0
difﬁculty level of,1,1,1.0
level of minority,1,1,1.0
minority data the,1,1,1.0
data the difference,1,1,1.0
the difference is,1,1,1.0
difference is that,1,1,1.0
is that instead,1,1,1.0
that instead of,1,1,1.0
instead of directly,1,1,1.0
of directly specifying,1,1,1.0
directly specifying the,1,1,1.0
specifying the number,1,1,1.0
of synthetic instances,10,1,10.0
instances generated for,1,1,1.0
generated for each,3,1,3.0
minority example as,1,1,1.0
example as in,1,1,1.0
in adasyn ramoboost,1,1,1.0
adasyn ramoboost adopts,1,1,1.0
ramoboost adopts this,1,1,1.0
adopts this mechanism,1,1,1.0
this mechanism to,1,1,1.0
mechanism to determi,1,1,1.0
to determi ne,1,1,1.0
determi ne the,1,1,1.0
ne the chance,1,1,1.0
the chance of,1,1,1.0
chance of each,1,1,1.0
minority example for,3,1,3.0
example for generating,2,1,2.0
for generating the,2,1,2.0
generating the synthetic,2,1,2.0
synthetic instances before,1,1,1.0
instances before presenting,1,1,1.0
before presenting the,1,1,1.0
presenting the details,1,1,1.0
the details of,1,1,1.0
details of the,1,1,1.0
of the ramoboost,3,1,3.0
the ramoboost algorithm,3,1,3.0
ramoboost algorithm we,2,1,2.0
algorithm we brieﬂy,1,1,1.0
we brieﬂy discuss,1,1,1.0
brieﬂy discuss the,1,1,1.0
discuss the data,1,1,1.0
the data generation,7,1,7.0
data generation mechanisms,2,1,2.0
generation mechanisms of,1,1,1.0
mechanisms of smote,1,1,1.0
of smote and,3,1,3.0
smote and adasyn,8,2,4.0
and adasyn which,1,1,1.0
adasyn which motivated,1,1,1.0
which motivated the,1,1,1.0
motivated the work,1,1,1.0
the work presented,1,1,1.0
work presented in,1,1,1.0
this paper and,2,1,2.0
paper and can,1,1,1.0
and can provide,1,1,1.0
can provide a,1,1,1.0
provide a better,2,1,2.0
a better understanding,1,1,1.0
better understanding of,1,1,1.0
understanding of ramoboost,1,1,1.0
of ramoboost in,1,1,1.0
ramoboost in the,1,1,1.0
in the smote,1,1,1.0
the smote method,1,1,1.0
smote method the,1,1,1.0
method the k,1,1,1.0
minority data where,1,1,1.0
data where k,1,1,1.0
k is a,1,1,1.0
is a parameter,1,1,1.0
a parameter are,1,1,1.0
parameter are adasyn,1,1,1.0
are adasyn smote,1,1,1.0
adasyn smote fig,1,1,1.0
smote fig representation,1,1,1.0
the synthetic instance,1,1,1.0
synthetic instance proportions,1,1,1.0
instance proportions for,1,1,1.0
proportions for smote,1,1,1.0
and adasyn identiﬁed,1,1,1.0
adasyn identiﬁed for,1,1,1.0
identiﬁed for each,1,1,1.0
minority example the,3,1,3.0
example the are,1,1,1.0
the are deﬁned,1,1,1.0
are deﬁned as,1,1,1.0
deﬁned as the,2,1,2.0
as the set,1,1,1.0
set of k,1,1,1.0
of k instances,1,1,1.0
k instances such,1,1,1.0
instances such that,1,1,1.0
that the euclidean,2,1,2.0
minority example under,7,1,7.0
example under consideration,9,1,9.0
under consideration and,3,1,3.0
consideration and the,1,1,1.0
and the examples,1,1,1.0
the minority set,1,1,1.0
minority set are,1,1,1.0
set are of,1,1,1.0
are of minimal,1,1,1.0
of minimal length,1,1,1.0
minimal length along,1,1,1.0
length along the,1,1,1.0
along the feature,1,1,1.0
feature space synthetic,1,1,1.0
space synthetic samples,1,1,1.0
synthetic samples are,1,1,1.0
samples are then,1,1,1.0
are then created,1,1,1.0
then created by,1,1,1.0
created by randomly,1,1,1.0
randomly selecting one,1,1,1.0
selecting one of,1,1,1.0
nearest neighbors and,1,1,1.0
neighbors and multiplying,1,1,1.0
and multiplying the,1,1,1.0
multiplying the corresponding,1,1,1.0
the corresponding feature,1,1,1.0
corresponding feature vector,1,1,1.0
feature vector difference,1,1,1.0
vector difference with,1,1,1.0
difference with a,1,1,1.0
with a random,1,1,1.0
number between xnew,1,1,1.0
between xnew xi,1,1,1.0
xnew xi ˆxi,1,1,1.0
xi ˆxi xi,1,1,1.0
ˆxi xi where,1,1,1.0
xi where xi,1,1,1.0
where xi is,1,1,1.0
xi is the,1,1,1.0
is the minority,1,1,1.0
under consideration ˆxi,1,1,1.0
consideration ˆxi is,1,1,1.0
ˆxi is one,1,1,1.0
of the neighbors,1,1,1.0
the neighbors minority,1,1,1.0
neighbors minority class,1,1,1.0
class for xi,1,1,1.0
for xi a,1,1,1.0
xi a n,1,1,1.0
a n is,1,1,1.0
n is a,1,1,1.0
random number therefore,1,1,1.0
number therefore the,1,1,1.0
therefore the resulting,1,1,1.0
the resulting synthetic,1,1,1.0
resulting synthetic instance,1,1,1.0
synthetic instance according,1,1,1.0
instance according to,1,1,1.0
according to is,1,1,1.0
to is a,1,1,1.0
is a point,1,1,1.0
a point along,1,1,1.0
point along the,1,1,1.0
along the line,1,1,1.0
the line segment,1,1,1.0
line segment joining,1,1,1.0
segment joining the,1,1,1.0
joining the example,1,1,1.0
the example under,2,1,2.0
under consideration xi,1,1,1.0
consideration xi and,1,1,1.0
xi and the,1,1,1.0
and the randomly,1,1,1.0
the randomly selected,3,1,3.0
randomly selected k,1,1,1.0
selected k nearest,1,1,1.0
nearest neighbor ˆxi,1,1,1.0
neighbor ˆxi fig,1,1,1.0
ˆxi fig illustrates,1,1,1.0
fig illustrates this,1,1,1.0
illustrates this procedure,1,1,1.0
this procedure in,1,1,1.0
procedure in this,1,1,1.0
this way smote,1,1,1.0
way smote generates,1,1,1.0
smote generates the,1,1,1.0
generates the same,1,1,1.0
synthetic instances for,6,1,6.0
instances for each,3,1,3.0
each minority exam,1,1,1.0
minority exam ple,1,1,1.0
exam ple uniform,1,1,1.0
ple uniform di,1,1,1.0
uniform di stribution,1,1,1.0
di stribution adasyn,1,1,1.0
stribution adasyn also,1,1,1.0
adasyn also uses,1,1,1.0
also uses feature,1,1,1.0
uses feature int,1,1,1.0
feature int erpolation,1,1,1.0
int erpolation to,1,1,1.0
erpolation to generate,1,1,1.0
to generate thetic,1,1,1.0
generate thetic instances,1,1,1.0
thetic instances the,1,1,1.0
instances the difference,1,1,1.0
the difference from,1,1,1.0
difference from smote,1,1,1.0
from smote is,1,1,1.0
smote is instead,1,1,1.0
is instead of,1,1,1.0
instead of applying,1,1,1.0
of applying a,1,1,1.0
applying a uniform,1,1,1.0
a uniform distribution,1,1,1.0
uniform distribution for,1,1,1.0
distribution for data,1,1,1.0
for data generation,1,1,1.0
data generation like,1,1,1.0
generation like smote,1,1,1.0
like smote adasyn,1,1,1.0
smote adasyn uses,1,1,1.0
adasyn uses a,2,1,2.0
uses a density,1,1,1.0
a density distribution,1,1,1.0
density distribution ˆri,1,1,1.0
distribution ˆri as,1,1,1.0
ˆri as a,1,1,1.0
as a rion,1,1,1.0
a rion to,1,1,1.0
rion to automatically,1,1,1.0
to automatically decide,1,1,1.0
automatically decide the,1,1,1.0
decide the number,1,1,1.0
of synthetic samples,1,1,1.0
synthetic samples that,1,1,1.0
samples that need,1,1,1.0
example the density,1,1,1.0
the density distribution,2,1,2.0
density distribution criteria,1,1,1.0
distribution criteria is,1,1,1.0
criteria is deﬁned,1,1,1.0
as the normalized,1,1,1.0
the normalized number,1,1,1.0
normalized number of,1,1,1.0
of majority cases,6,1,6.0
majority cases within,2,1,2.0
cases within the,3,1,3.0
within the neighbor,1,1,1.0
the neighbor of,3,1,3.0
neighbor of each,1,1,1.0
minority example fig,1,1,1.0
example fig shows,1,1,1.0
shows the proportion,1,1,1.0
the proportion of,1,1,1.0
proportion of synthetic,1,1,1.0
generated by smote,1,1,1.0
by smote and,1,1,1.0
and adasyn based,1,1,1.0
adasyn based on,1,1,1.0
based on neighbors,1,1,1.0
on neighbors for,1,1,1.0
neighbors for minority,1,1,1.0
for minority examples,5,1,5.0
minority examples through,1,1,1.0
examples through from,1,1,1.0
through from fig,1,1,1.0
from fig it,1,1,1.0
fig it is,1,1,1.0
clear that smote,1,1,1.0
that smote uniformly,1,1,1.0
smote uniformly assigns,1,1,1.0
uniformly assigns the,1,1,1.0
assigns the number,1,1,1.0
synthetic instances to,1,1,1.0
instances to be,1,1,1.0
for to and,1,1,1.0
to and adasyn,1,1,1.0
and adasyn uses,1,1,1.0
uses a differen,1,1,1.0
a differen t,1,1,1.0
differen t distribution,1,1,1.0
t distribution to,1,1,1.0
distribution to determinechen,1,1,1.0
to determinechen et,1,1,1.0
determinechen et al,1,1,1.0
boosting the number,1,1,1.0
instances for to,1,1,1.0
for to d,1,1,1.0
to d a,1,1,1.0
d a s,1,1,1.0
a s y,1,1,1.0
y n ﬁrst,1,1,1.0
n ﬁrst calculates,1,1,1.0
ﬁrst calculates the,1,1,1.0
calculates the number,1,1,1.0
number of m,1,1,1.0
of m ajority,1,1,1.0
m ajority cases,1,1,1.0
ajority cases within,1,1,1.0
within the nearest,1,1,1.0
neighbors of to,1,1,1.0
of to ﬁrst,1,1,1.0
to ﬁrst which,1,1,1.0
ﬁrst which is,1,1,1.0
which is in,1,1,1.0
is in this,1,1,1.0
in this example,1,1,1.0
this example and,1,1,1.0
example and normalizes,1,1,1.0
and normalizes this,1,1,1.0
normalizes this into,1,1,1.0
this into the,1,1,1.0
into the density,1,1,1.0
density distribution which,1,1,1.0
used to bias,1,1,1.0
to bias the,1,1,1.0
bias the data,1,1,1.0
data generation pro,1,1,1.0
generation pro cess,1,1,1.0
pro cess algorithm,1,1,1.0
cess algorithm adasyn,1,1,1.0
algorithm adasyn one,1,1,1.0
adasyn one issue,1,1,1.0
one issue worth,1,1,1.0
issue worth noting,1,1,1.0
worth noting is,1,1,1.0
noting is that,1,1,1.0
is that adasyn,1,1,1.0
that adasyn does,1,1,1.0
adasyn does not,1,1,1.0
does not generate,1,1,1.0
not generate instances,1,1,1.0
generate instances for,1,1,1.0
instances for minority,1,1,1.0
minority examples with,1,1,1.0
examples with no,1,1,1.0
with no majority,1,1,1.0
no majority cases,1,1,1.0
majority cases in,5,1,5.0
cases in their,1,1,1.0
in their neighbors,1,1,1.0
their neighbors like,1,1,1.0
neighbors like in,1,1,1.0
like in fig,1,1,1.0
in fig ramoboost,1,1,1.0
fig ramoboost learning,1,1,1.0
ramoboost learning algorithm,2,1,2.0
learning algorithm the,1,1,1.0
algorithm the objective,1,1,1.0
the objective of,1,1,1.0
objective of ramoboost,1,1,1.0
of ramoboost is,5,1,5.0
ramoboost is twofold,1,1,1.0
is twofold to,1,1,1.0
twofold to reduce,1,1,1.0
reduce the induction,1,1,1.0
the induction biases,1,1,1.0
induction biases introduced,1,1,1.0
biases introduced from,1,1,1.0
introduced from imbalanced,1,1,1.0
data and to,1,1,1.0
and to adaptively,1,1,1.0
to adaptively learn,1,1,1.0
adaptively learn information,1,1,1.0
learn information from,1,1,1.0
information from the,1,1,1.0
data distribution this,1,1,1.0
distribution this is,1,1,1.0
this is achieved,2,1,2.0
is achieved in,1,1,1.0
achieved in two,1,1,1.0
in two respects,1,1,1.0
two respects first,1,1,1.0
respects first an,1,1,1.0
first an adaptive,1,1,1.0
an adaptive weight,1,1,1.0
adaptive weight adjustment,1,1,1.0
weight adjustment procedure,1,1,1.0
adjustment procedure is,1,1,1.0
procedure is embedded,1,1,1.0
is embedded in,1,1,1.0
embedded in ramoboost,1,1,1.0
in ramoboost that,1,1,1.0
ramoboost that shifts,1,1,1.0
that shifts the,1,1,1.0
shifts the decision,1,1,1.0
boundary toward the,2,1,2.0
toward the examples,2,1,2.0
examples from both,1,1,1.0
from both the,1,1,1.0
both the minority,1,1,1.0
majority classes second,1,1,1.0
classes second a,1,1,1.0
second a ranked,1,1,1.0
a ranked sampling,1,1,1.0
ranked sampling probability,1,1,1.0
distribution is used,1,1,1.0
generate synthetic minority,2,1,2.0
instances to balance,1,1,1.0
balance the skewed,1,1,1.0
the skewed distribution,1,1,1.0
skewed distribution motivated,1,1,1.0
distribution motivated by,1,1,1.0
by the smote,1,1,1.0
the smote smoteboost,1,1,1.0
smote smoteboost and,1,1,1.0
smoteboost and adasyn,1,1,1.0
and adasyn rithms,1,1,1.0
adasyn rithms ramoboost,1,1,1.0
rithms ramoboost facilitates,1,1,1.0
ramoboost facilitates imbalanced,1,1,1.0
facilitates imbalanced learning,1,1,1.0
imbalanced learning by,1,1,1.0
learning by tively,1,1,1.0
by tively developing,1,1,1.0
tively developing an,1,1,1.0
developing an ensemble,1,1,1.0
an ensemble of,1,1,1.0
ensemble of hypotheses,1,1,1.0
of hypotheses however,1,1,1.0
hypotheses however unlike,1,1,1.0
however unlike smote,1,1,1.0
unlike smote which,1,1,1.0
smote which samples,1,1,1.0
which samples minority,1,1,1.0
samples minority examples,1,1,1.0
minority examples indiscriminately,1,1,1.0
examples indiscriminately and,1,1,1.0
indiscriminately and uniformly,1,1,1.0
and uniformly ramoboost,1,1,1.0
uniformly ramoboost evaluates,1,1,1.0
ramoboost evaluates the,1,1,1.0
evaluates the potential,1,1,1.0
the potential learning,1,1,1.0
potential learning contribution,1,1,1.0
learning contribution of,1,1,1.0
contribution of each,1,1,1.0
example and determines,1,1,1.0
and determines their,1,1,1.0
determines their sampling,1,1,1.0
their sampling weights,1,1,1.0
sampling weights accordingly,1,1,1.0
weights accordingly this,1,1,1.0
accordingly this is,1,1,1.0
achieved by calculating,1,1,1.0
by calculating the,1,1,1.0
calculating the distance,1,1,1.0
the distance of,1,1,1.0
distance of any,1,1,1.0
of any single,1,1,1.0
any single minority,1,1,1.0
single minority example,1,1,1.0
minority example from,1,1,1.0
example from the,1,1,1.0
set of nearest,1,1,1.0
neighbors to determine,1,1,1.0
to determine how,1,1,1.0
determine how greatly,1,1,1.0
how greatly it,1,1,1.0
greatly it will,1,1,1.0
it will beneﬁt,1,1,1.0
will beneﬁt the,1,1,1.0
beneﬁt the learning,1,1,1.0
learning process before,1,1,1.0
process before ofﬁcially,1,1,1.0
before ofﬁcially describing,1,1,1.0
ofﬁcially describing the,1,1,1.0
describing the ramoboost,1,1,1.0
algorithm we would,1,1,1.0
like to explain,1,1,1.0
to explain several,1,1,1.0
explain several terms,1,1,1.0
several terms for,1,1,1.0
terms for improved,1,1,1.0
for improved readability,1,1,1.0
improved readability in,1,1,1.0
readability in lieu,1,1,1.0
in lieu of,1,1,1.0
lieu of directly,1,1,1.0
of directly operating,1,1,1.0
directly operating on,1,1,1.0
operating on the,1,1,1.0
original training dataset,1,1,1.0
training dataset d,1,1,1.0
dataset d ramoboost,1,1,1.0
d ramoboost follows,1,1,1.0
ramoboost follows the,1,1,1.0
follows the classic,1,1,1.0
the classic procedure,1,1,1.0
classic procedure to,1,1,1.0
procedure to apply,1,1,1.0
to apply the,1,1,1.0
apply the boosting,1,1,1.0
the boosting process,2,1,2.0
boosting process on,1,1,1.0
process on the,1,1,1.0
on the misclassiﬁed,1,1,1.0
the misclassiﬁed dataset,1,1,1.0
misclassiﬁed dataset for,1,1,1.0
classiﬁcation problem b,1,1,1.0
problem b is,1,1,1.0
b is deﬁned,1,1,1.0
is deﬁned to,1,1,1.0
deﬁned to be,1,1,1.0
the set where,1,1,1.0
set where each,1,1,1.0
where each example,1,1,1.0
each example in,2,1,2.0
example in d,1,1,1.0
in d is,1,1,1.0
d is replicated,1,1,1.0
is replicated n,1,1,1.0
replicated n times,1,1,1.0
n times with,1,1,1.0
times with a,1,1,1.0
a different class,1,1,1.0
different class label,2,1,2.0
class label other,1,1,1.0
label other than,1,1,1.0
other than the,1,1,1.0
than the true,1,1,1.0
the true one,1,1,1.0
true one ˆri,1,1,1.0
one ˆri serves,1,1,1.0
ˆri serves as,1,1,1.0
serves as the,1,1,1.0
as the distribution,1,1,1.0
the distribution function,3,1,3.0
distribution function determining,1,1,1.0
function determining the,1,1,1.0
determining the probability,2,1,2.0
the probability that,1,1,1.0
probability that examples,1,1,1.0
that examples in,1,1,1.0
examples in b,1,1,1.0
in b are,1,1,1.0
b are chosen,1,1,1.0
are chosen for,1,1,1.0
chosen for generating,1,1,1.0
synthetic instances it,1,1,1.0
instances it is,1,1,1.0
it is calculated,1,1,1.0
is calculated by,1,1,1.0
calculated by mapping,1,1,1.0
by mapping the,1,1,1.0
mapping the number,1,1,1.0
in the nearest,1,1,1.0
of each example,1,1,1.0
example in b,1,1,1.0
in b into,1,1,1.0
b into the,1,1,1.0
into the range,1,1,1.0
the range the,1,1,1.0
range the ramoboost,1,1,1.0
the ramoboost learning,2,1,2.0
algorithm is presented,1,1,1.0
is presented presented,1,1,1.0
presented presented in,1,1,1.0
presented in algorithm,1,1,1.0
in algorithm according,1,1,1.0
algorithm according to,1,1,1.0
to this description,1,1,1.0
this description the,1,1,1.0
description the ramoboost,1,1,1.0
ramoboost algorithm includes,1,1,1.0
algorithm includes two,1,1,1.0
includes two mechanisms,1,1,1.0
two mechanisms to,1,1,1.0
mechanisms to facilitate,1,1,1.0
learning from anced,1,1,1.0
from anced data,1,1,1.0
anced data the,1,1,1.0
data the ﬁrst,1,1,1.0
the ﬁrst consists,1,1,1.0
ﬁrst consists of,1,1,1.0
consists of steps,1,1,1.0
of steps to,1,1,1.0
steps to where,1,1,1.0
to where instances,1,1,1.0
where instances are,1,1,1.0
instances are adaptively,1,1,1.0
are adaptively generated,1,1,1.0
adaptively generated accord,1,1,1.0
generated accord ing,1,1,1.0
ing to their,1,1,1.0
to their distributions,1,1,1.0
their distributions in,1,1,1.0
distributions in this,1,1,1.0
this way more,1,1,1.0
way more synthetic,1,1,1.0
more synthetic instances,4,1,4.0
synthetic instances are,5,1,5.0
instances are created,2,1,2.0
are created for,2,1,2.0
created for minority,1,1,1.0
minority examples th,1,1,1.0
examples th at,1,1,1.0
th at are,1,1,1.0
at are more,1,1,1.0
to be classiﬁed,1,1,1.0
be classiﬁed compared,1,1,1.0
classiﬁed compared to,1,1,1.0
to minority examples,1,1,1.0
minority examples this,1,1,1.0
examples this is,1,1,1.0
this is signiﬁcantly,1,1,1.0
is signiﬁcantly different,1,1,1.0
signiﬁcantly different from,1,1,1.0
different from the,1,1,1.0
from the smote,1,1,1.0
smote algorithm where,1,1,1.0
algorithm where each,1,1,1.0
where each minority,1,1,1.0
example has equal,1,1,1.0
has equal weight,1,1,1.0
equal weight and,1,1,1.0
weight and therefore,1,1,1.0
and therefore the,1,1,1.0
therefore the same,1,1,1.0
the same numbers,2,1,2.0
same numbers of,2,1,2.0
numbers of synthetic,1,1,1.0
example the second,1,1,1.0
the second mechanism,1,1,1.0
second mechanism steps,1,1,1.0
mechanism steps to,1,1,1.0
steps to use,1,1,1.0
to use the,1,1,1.0
use the of,1,1,1.0
the current hypothesis,1,1,1.0
current hypothesis ht,1,1,1.0
hypothesis ht to,1,1,1.0
ht to update,1,1,1.0
to update the,1,1,1.0
update the sampling,1,1,1.0
the sampling distribution,1,1,1.0
sampling distribution dt,1,1,1.0
distribution dt which,1,1,1.0
dt which is,1,1,1.0
used to sample,1,1,1.0
to sample the,1,1,1.0
sample the training,1,1,1.0
training dataset in,2,1,2.0
next iteration as,1,1,1.0
iteration as shown,2,1,2.0
shown in step,2,1,2.0
in step similar,1,1,1.0
step similar to,1,1,1.0
similar to the,1,1,1.0
the algorithm ram,1,1,1.0
algorithm ram o,1,1,1.0
ram o boost,1,1,1.0
o boost n,1,1,1.0
boost n t,1,1,1.0
n t α,1,1,1.0
t α input,1,1,1.0
α input training,1,1,1.0
input training dataset,1,1,1.0
training dataset with,2,1,2.0
dataset with m,1,1,1.0
with m class,1,1,1.0
m class examples,1,1,1.0
class examples xm,1,1,1.0
examples xm ym,1,1,1.0
xm ym w,1,1,1.0
ym w h,1,1,1.0
w h e,1,1,1.0
h e r,1,1,1.0
r e xi,1,1,1.0
e xi i,1,1,1.0
i m is,1,1,1.0
m is an,1,1,1.0
is an instance,1,1,1.0
an instance of,2,1,2.0
instance of the,1,1,1.0
of the n,1,1,1.0
the n dimensional,1,1,1.0
n dimensional feature,1,1,1.0
dimensional feature space,1,1,1.0
feature space x,1,1,1.0
space x and,1,1,1.0
x and yi,1,1,1.0
and yi y,1,1,1.0
yi y major,1,1,1.0
y major minor,1,1,1.0
major minor is,1,1,1.0
minor is the,1,1,1.0
is the class,1,1,1.0
the class identity,1,1,1.0
class identity label,1,1,1.0
identity label associated,1,1,1.0
label associated with,1,1,1.0
associated with instance,1,1,1.0
with instance xi,1,1,1.0
instance xi n,1,1,1.0
xi n number,1,1,1.0
n number of,1,1,1.0
synthetic data samples,3,1,3.0
data samples to,1,1,1.0
be generated at,1,1,1.0
generated at each,2,1,2.0
each iteration t,1,1,1.0
iteration t number,1,1,1.0
t number of,2,1,2.0
number of iterations,1,1,1.0
of iterations the,1,1,1.0
iterations the number,1,1,1.0
number of base,1,1,1.0
of base classiﬁers,1,1,1.0
base classiﬁers number,1,1,1.0
classiﬁers number of,1,1,1.0
nearest neighbors in,3,1,3.0
neighbors in adjusting,1,1,1.0
in adjusting the,1,1,1.0
adjusting the sampling,1,1,1.0
the sampling probability,1,1,1.0
sampling probability of,1,1,1.0
probability of the,1,1,1.0
minority examples number,1,1,1.0
examples number of,1,1,1.0
to generate the,1,1,1.0
generate the synthetic,1,1,1.0
synthetic data instances,2,1,2.0
data instances α,1,1,1.0
instances α the,1,1,1.0
α the scaling,1,1,1.0
the scaling coefﬁcient,2,1,2.0
scaling coefﬁcient let,1,1,1.0
coefﬁcient let b,1,1,1.0
let b i,1,1,1.0
b i y,1,1,1.0
i y i,1,1,1.0
i m y,1,1,1.0
m y initialize,1,1,1.0
y initialize i,1,1,1.0
initialize i y,1,1,1.0
i y for,1,1,1.0
y for i,1,1,1.0
for i y,1,1,1.0
i y b,2,1,2.0
y b for,1,1,1.0
b for two,1,1,1.0
for two class,1,1,1.0
two class problems,1,1,1.0
class problems m,1,1,1.0
problems m do,1,1,1.0
m do for,1,1,1.0
do for t,1,1,1.0
for t t,1,1,1.0
t t sample,1,1,1.0
t sample the,1,1,1.0
sample the mislabeled,1,1,1.0
the mislabeled training,1,1,1.0
mislabeled training data,1,1,1.0
data with dt,1,1,1.0
with dt and,2,1,2.0
dt and get,2,1,2.0
and get back,2,1,2.0
get back the,1,1,1.0
back the sampled,1,1,1.0
the sampled dataset,1,1,1.0
sampled dataset se,1,1,1.0
dataset se of,1,1,1.0
se of identical,1,1,1.0
of identical size,1,1,1.0
identical size slice,1,1,1.0
size slice se,1,1,1.0
slice se into,1,1,1.0
se into the,1,1,1.0
the majority subset,1,1,1.0
majority subset and,1,1,1.0
subset and the,1,1,1.0
the minority subset,1,1,1.0
minority subset of,1,1,1.0
subset of size,1,1,1.0
of size mlt,1,1,1.0
size mlt and,1,1,1.0
mlt and mst,1,1,1.0
and mst respectively,1,1,1.0
mst respectively for,1,1,1.0
respectively for each,1,1,1.0
each example xi,2,1,2.0
example xi ﬁnd,1,1,1.0
xi ﬁnd its,1,1,1.0
ﬁnd its nearest,2,1,2.0
neighbors in the,1,1,1.0
the dataset se,1,1,1.0
dataset se according,1,1,1.0
se according to,1,1,1.0
euclidean distance in,2,1,2.0
distance in space,1,1,1.0
in space and,1,1,1.0
space and calculate,1,1,1.0
and calculate ri,1,1,1.0
calculate ri deﬁned,1,1,1.0
ri deﬁned as,1,1,1.0
deﬁned as ri,1,1,1.0
as ri exp,1,1,1.0
ri exp α,1,1,1.0
exp α δi,1,1,1.0
α δi i,1,1,1.0
δi i mst,1,1,1.0
i mst where,1,1,1.0
mst where δi,1,1,1.0
where δi is,1,1,1.0
δi is the,1,1,1.0
cases in examples,1,1,1.0
in examples normalize,1,1,1.0
examples normalize ri,1,1,1.0
normalize ri according,1,1,1.0
ri according to,1,1,1.0
according to ˆri,1,1,1.0
to ˆri ri,1,1,1.0
ˆri ri ri,1,1,1.0
ri ri such,1,1,1.0
ri such that,1,1,1.0
such that ˆri,1,1,1.0
that ˆri is,1,1,1.0
ˆri is a,1,1,1.0
is a distribution,1,1,1.0
a distribution function,2,1,2.0
distribution function mst,1,1,1.0
function mst ri,1,1,1.0
mst ri deﬁne,1,1,1.0
ri deﬁne dt,1,1,1.0
deﬁne dt ˆri,1,1,1.0
dt ˆri sample,1,1,1.0
ˆri sample with,1,1,1.0
sample with dt,1,1,1.0
get back a,2,1,2.0
back a sampling,1,1,1.0
a sampling minority,1,1,1.0
sampling minority dataset,1,1,1.0
minority dataset gt,1,1,1.0
dataset gt o,1,1,1.0
gt o fs,1,1,1.0
o fs i,1,1,1.0
fs i z,1,1,1.0
i z emst,1,1,1.0
z emst for,1,1,1.0
emst for each,1,1,1.0
example xi gt,1,1,1.0
xi gt ﬁnd,1,1,1.0
gt ﬁnd its,1,1,1.0
neighbors in according,1,1,1.0
in according to,1,1,1.0
distance in n,1,1,1.0
in n dimensional,1,1,1.0
n dimensional space,1,1,1.0
dimensional space and,1,1,1.0
space and use,1,1,1.0
and use linear,1,1,1.0
use linear inter,1,1,1.0
linear inter polation,1,1,1.0
inter polation to,1,1,1.0
polation to generate,1,1,1.0
to generate n,1,1,1.0
generate n synthetic,1,1,1.0
n synthetic data,2,1,2.0
data samples provide,1,1,1.0
samples provide the,1,1,1.0
provide the base,1,1,1.0
the base classiﬁer,2,1,2.0
base classiﬁer with,1,1,1.0
classiﬁer with sampling,1,1,1.0
with sampling dataset,1,1,1.0
sampling dataset se,1,1,1.0
dataset se and,1,1,1.0
se and the,1,1,1.0
and the n,1,1,1.0
the n synthetic,1,1,1.0
data samples get,1,1,1.0
samples get back,1,1,1.0
back a hypothesis,1,1,1.0
a hypothesis ht,1,1,1.0
hypothesis ht x,1,1,1.0
ht x y,2,1,2.0
x y calculate,1,1,1.0
y calculate the,1,1,1.0
calculate the of,1,1,1.0
the of ht,1,1,1.0
of ht εt,1,1,1.0
ht εt i,1,1,1.0
εt i y,1,1,1.0
y b dt,1,1,1.0
b dt i,1,1,1.0
dt i y,3,1,3.0
i y ht,1,1,1.0
y ht xi,1,1,1.0
ht xi yi,1,1,1.0
xi yi ht,2,1,2.0
yi ht xi,2,1,2.0
ht xi y,2,1,2.0
xi y set,1,1,1.0
y set βt,1,1,1.0
set βt εt,1,1,1.0
βt εt εt,1,1,1.0
εt εt update,1,1,1.0
εt update dt,1,1,1.0
update dt i,1,1,1.0
i y dt,1,1,1.0
y dt i,1,1,1.0
i y zt,1,1,1.0
y zt β,1,1,1.0
zt β xi,1,1,1.0
β xi yi,1,1,1.0
xi y t,1,1,1.0
y t where,1,1,1.0
t where zt,1,1,1.0
where zt is,1,1,1.0
zt is a,1,1,1.0
is a normalization,1,1,1.0
a normalization constant,1,1,1.0
normalization constant end,1,1,1.0
constant end ieee,1,1,1.0
end ieee transactions,1,1,1.0
no october output,1,1,1.0
october output the,1,1,1.0
output the output,1,1,1.0
the output hypothesis,1,1,1.0
output hypothesis h,1,1,1.0
hypothesis h f,1,1,1.0
h f inal,2,1,2.0
f inal x,2,1,2.0
inal x is,1,1,1.0
x is calculated,1,1,1.0
is calculated as,1,1,1.0
calculated as follows,1,1,1.0
as follows h,1,1,1.0
follows h f,1,1,1.0
inal x arg,1,1,1.0
x arg max,1,1,1.0
arg max log,1,1,1.0
max log βt,1,1,1.0
log βt ht,1,1,1.0
βt ht x,1,1,1.0
x y algorithm,1,1,1.0
y algorithm the,1,1,1.0
algorithm the anism,1,1,1.0
the anism can,1,1,1.0
anism can adaptively,1,1,1.0
shift the ﬁnal,1,1,1.0
the ﬁnal hypothesis,4,1,4.0
ﬁnal hypothesis toward,1,1,1.0
hypothesis toward the,1,1,1.0
toward the decision,1,1,1.0
boundary to facilitate,1,1,1.0
to facilitate the,1,1,1.0
facilitate the learning,1,1,1.0
learning process the,1,1,1.0
process the key,1,1,1.0
the key components,1,1,1.0
key components of,1,1,1.0
components of the,1,1,1.0
the two ramoboost,1,1,1.0
two ramoboost mechanisms,1,1,1.0
ramoboost mechanisms are,1,1,1.0
mechanisms are steps,1,1,1.0
are steps and,1,1,1.0
steps and in,2,1,2.0
the by sampling,1,1,1.0
by sampling the,2,1,2.0
sampling the training,1,1,1.0
dataset with the,2,1,2.0
with the updated,1,1,1.0
the updated weight,1,1,1.0
updated weight distribution,1,1,1.0
weight distribution function,1,1,1.0
distribution function dt,1,1,1.0
function dt at,1,1,1.0
dt at the,1,1,1.0
at the tth,1,1,1.0
the tth iteration,1,1,1.0
tth iteration as,1,1,1.0
in step ramoboost,2,1,2.0
step ramoboost can,1,1,1.0
ramoboost can sively,1,1,1.0
can sively shift,1,1,1.0
sively shift the,1,1,1.0
the examples meanwhile,1,1,1.0
examples meanwhile in,1,1,1.0
meanwhile in step,1,1,1.0
step ramoboost generates,1,1,1.0
ramoboost generates more,1,1,1.0
generates more synthetic,1,1,1.0
instances for those,1,1,1.0
for those minority,1,1,1.0
those minority ples,1,1,1.0
minority ples by,1,1,1.0
ples by sampling,1,1,1.0
sampling the minority,1,1,1.0
minority dataset with,1,1,1.0
with the distribution,1,1,1.0
distribution function obtained,1,1,1.0
function obtained from,1,1,1.0
obtained from manipulating,1,1,1.0
from manipulating the,1,1,1.0
manipulating the number,2,1,2.0
in the k,1,1,1.0
minority example through,1,1,1.0
example through and,1,1,1.0
through and analysis,1,1,1.0
and analysis of,1,1,1.0
ramoboost learning methodology,1,1,1.0
learning methodology data,1,1,1.0
methodology data generation,1,1,1.0
data generation mechanism,5,1,5.0
generation mechanism the,1,1,1.0
mechanism the proposed,1,1,1.0
the proposed boost,1,1,1.0
proposed boost algorithm,1,1,1.0
boost algorithm shares,1,1,1.0
algorithm shares some,1,1,1.0
shares some data,1,1,1.0
some data generation,1,1,1.0
data generation aspects,1,1,1.0
generation aspects with,1,1,1.0
aspects with smote,1,1,1.0
with smote a,1,1,1.0
smote a synthetic,1,1,1.0
synthetic instance is,1,1,1.0
instance is generated,1,1,1.0
generated by adding,1,1,1.0
by adding the,1,1,1.0
adding the feature,1,1,1.0
the feature vector,2,1,2.0
feature vector of,2,1,2.0
vector of the,2,1,2.0
consideration and a,1,1,1.0
and a randomized,1,1,1.0
a randomized real,1,1,1.0
randomized real number,1,1,1.0
real number in,1,1,1.0
number in multiplied,1,1,1.0
in multiplied by,1,1,1.0
multiplied by the,1,1,1.0
by the difference,1,1,1.0
difference between the,2,1,2.0
the minority ple,1,1,1.0
minority ple under,1,1,1.0
ple under consideration,1,1,1.0
consideration and one,1,1,1.0
and one randomly,1,1,1.0
randomly chosen example,1,1,1.0
chosen example within,1,1,1.0
example within its,1,1,1.0
within its k,1,1,1.0
its k nearest,1,1,1.0
the minority sampling,1,1,1.0
minority sampling dataset,1,1,1.0
sampling dataset step,1,1,1.0
dataset step in,1,1,1.0
step in the,1,1,1.0
in the which,1,1,1.0
the which can,1,1,1.0
be formulated in,1,1,1.0
the same way,1,1,1.0
same way as,1,1,1.0
way as the,1,1,1.0
as the difference,1,1,1.0
the two however,1,1,1.0
two however is,1,1,1.0
however is that,1,1,1.0
is that ramoboost,1,1,1.0
that ramoboost employs,1,1,1.0
ramoboost employs a,1,1,1.0
employs a systematic,1,1,1.0
a systematic method,1,1,1.0
systematic method to,1,1,1.0
method to adaptively,1,1,1.0
to adaptively determine,1,1,1.0
adaptively determine the,1,1,1.0
determine the number,1,1,1.0
synthetic instances created,1,1,1.0
instances created for,1,1,1.0
minority example in,2,1,2.0
the sampling dataset,1,1,1.0
sampling dataset according,1,1,1.0
dataset according to,1,1,1.0
to their learnability,1,1,1.0
their learnability more,1,1,1.0
learnability more synthetic,1,1,1.0
synthetic instances will,1,1,1.0
instances will be,1,1,1.0
generated for those,1,1,1.0
for those examples,1,1,1.0
those examples therefore,1,1,1.0
examples therefore the,2,1,2.0
therefore the ﬁnal,1,1,1.0
ﬁnal hypothesis will,1,1,1.0
hypothesis will be,1,1,1.0
focused on those,1,1,1.0
on those difﬁcult,1,1,1.0
those difﬁcult decision,1,1,1.0
difﬁcult decision regions,1,1,1.0
decision regions concretely,1,1,1.0
regions concretely ramoboost,1,1,1.0
concretely ramoboost generates,1,1,1.0
ramoboost generates synthetic,1,1,1.0
instances by manipulating,1,1,1.0
by manipulating the,2,1,2.0
the neighbors across,1,1,1.0
neighbors across the,1,1,1.0
across the whole,1,1,1.0
the whole sampling,1,1,1.0
whole sampling dataset,1,1,1.0
sampling dataset steps,1,1,1.0
dataset steps and,1,1,1.0
in the while,1,1,1.0
the while smote,1,1,1.0
while smote universally,1,1,1.0
smote universally generates,1,1,1.0
universally generates identical,1,1,1.0
generates identical number,1,1,1.0
identical number of,1,1,1.0
minority example similar,1,1,1.0
example similar to,1,1,1.0
similar to ramoboost,1,1,1.0
to ramoboost adasyn,1,1,1.0
ramoboost adasyn also,1,1,1.0
adasyn also aims,1,1,1.0
also aims to,1,1,1.0
aims to cally,1,1,1.0
to cally generate,1,1,1.0
cally generate synthetic,1,1,1.0
minority instances according,1,1,1.0
to the derlying,1,1,1.0
the derlying data,1,1,1.0
derlying data distribution,1,1,1.0
data distribution instead,1,1,1.0
distribution instead of,1,1,1.0
instead of using,1,1,1.0
of using a,2,1,2.0
using a uniform,1,1,1.0
a uniform sampling,1,1,1.0
uniform sampling distribution,1,1,1.0
sampling distribution consequently,1,1,1.0
distribution consequently adasyn,1,1,1.0
consequently adasyn also,1,1,1.0
adasyn also has,1,1,1.0
also has the,1,1,1.0
has the ability,1,1,1.0
ability to push,1,1,1.0
to push the,1,1,1.0
push the learning,1,1,1.0
the difﬁcult regions,1,1,1.0
difﬁcult regions of,1,1,1.0
decision boundary however,1,1,1.0
boundary however adasyn,1,1,1.0
however adasyn does,1,1,1.0
adasyn does this,1,1,1.0
does this in,1,1,1.0
this in an,1,1,1.0
in an aggressive,1,1,1.0
an aggressive manner,1,1,1.0
aggressive manner almost,1,1,1.0
manner almost all,1,1,1.0
almost all of,2,1,2.0
of the generated,1,1,1.0
generated synthetic minority,1,1,1.0
minority instances are,1,1,1.0
instances are very,1,1,1.0
are very close,1,1,1.0
very close to,3,1,3.0
decision boundary this,2,1,2.0
boundary this is,1,1,1.0
this is because,3,1,3.0
is because the,1,1,1.0
because the data,1,1,1.0
generation mechanism of,4,1,4.0
mechanism of adasyn,1,1,1.0
of adasyn approves,1,1,1.0
adasyn approves generation,1,1,1.0
approves generation of,1,1,1.0
synthetic data only,1,1,1.0
data only when,1,1,1.0
only when there,1,1,1.0
when there exists,1,1,1.0
there exists at,1,1,1.0
exists at least,1,1,1.0
least one majority,1,1,1.0
one majority case,1,1,1.0
majority case in,1,1,1.0
case in the,1,1,1.0
in the neighbor,2,1,2.0
neighbor of the,2,1,2.0
under consideration speciﬁcally,1,1,1.0
consideration speciﬁcally the,1,1,1.0
speciﬁcally the number,1,1,1.0
under consideration directly,1,1,1.0
consideration directly dictates,1,1,1.0
directly dictates the,1,1,1.0
dictates the number,1,1,1.0
instances generated around,1,1,1.0
generated around it,1,1,1.0
around it in,1,1,1.0
it in the,1,1,1.0
in the extreme,1,1,1.0
the extreme case,1,1,1.0
extreme case noisy,1,1,1.0
case noisy examples,1,1,1.0
noisy examples in,1,1,1.0
class can have,1,1,1.0
can have multiple,1,1,1.0
have multiple synthetic,1,1,1.0
multiple synthetic instances,1,1,1.0
instances generated while,1,1,1.0
generated while examples,1,1,1.0
while examples that,1,1,1.0
that are relatively,1,1,1.0
are relatively far,1,1,1.0
relatively far away,1,1,1.0
far away from,1,1,1.0
class boundary but,1,1,1.0
boundary but are,1,1,1.0
but are tive,1,1,1.0
are tive of,1,1,1.0
tive of the,1,1,1.0
the target concept,1,1,1.0
target concept of,1,1,1.0
concept of the,1,1,1.0
class are not,1,1,1.0
are not selected,1,1,1.0
not selected for,1,1,1.0
selected for synthetic,1,1,1.0
for synthetic data,2,1,2.0
generation in contrast,1,1,1.0
in contrast ramoboost,1,1,1.0
contrast ramoboost ploys,1,1,1.0
ramoboost ploys a,1,1,1.0
ploys a logistic,1,1,1.0
a logistic function,1,1,1.0
logistic function to,1,1,1.0
function to ﬁrstly,1,1,1.0
to ﬁrstly map,1,1,1.0
ﬁrstly map δ,1,1,1.0
map δ the,1,1,1.0
δ the number,1,1,1.0
under consideration to,2,1,2.0
consideration to a,1,1,1.0
to a real,1,1,1.0
a real number,1,1,1.0
real number r,1,1,1.0
number r between,1,1,1.0
r between and,1,1,1.0
between and which,1,1,1.0
and which is,1,1,1.0
which is then,1,1,1.0
is then normalized,1,1,1.0
then normalized to,1,1,1.0
normalized to a,1,1,1.0
to a distribution,1,1,1.0
distribution function for,1,1,1.0
function for determining,1,1,1.0
for determining the,2,1,2.0
probability of each,2,1,2.0
example in this,1,1,1.0
this way ramoboost,1,1,1.0
way ramoboost considers,1,1,1.0
ramoboost considers all,1,1,1.0
considers all minority,1,1,1.0
all minority examples,1,1,1.0
minority examples for,1,1,1.0
examples for synthetic,1,1,1.0
for synthetic generation,1,1,1.0
synthetic generation albeit,1,1,1.0
generation albeit at,1,1,1.0
albeit at varied,1,1,1.0
at varied levels,1,1,1.0
varied levels adasyn,1,1,1.0
levels adasyn additionally,1,1,1.0
adasyn additionally introduces,1,1,1.0
additionally introduces complications,1,1,1.0
introduces complications at,1,1,1.0
complications at the,1,1,1.0
at the decision,1,1,1.0
decision boundary since,1,1,1.0
boundary since almost,1,1,1.0
since almost all,1,1,1.0
instances are located,1,1,1.0
are located in,1,1,1.0
located in the,1,1,1.0
in the decision,1,1,1.0
decision boundary region,2,1,2.0
boundary region which,1,1,1.0
region which means,1,1,1.0
means that excessively,1,1,1.0
that excessively more,1,1,1.0
excessively more synthetic,1,1,1.0
instances are probably,1,1,1.0
are probably generated,1,1,1.0
probably generated for,1,1,1.0
generated for noisy,1,1,1.0
for noisy examples,1,1,1.0
noisy examples with,1,1,1.0
with the minority,1,1,1.0
minority class label,1,1,1.0
class label phasizing,1,1,1.0
label phasizing the,1,1,1.0
phasizing the decision,1,1,1.0
boundary region may,1,1,1.0
region may magnify,1,1,1.0
may magnify the,1,1,1.0
magnify the inﬂuence,1,1,1.0
inﬂuence of noise,1,1,1.0
of noise within,1,1,1.0
noise within the,1,1,1.0
within the training,1,1,1.0
training dataset thereby,1,1,1.0
dataset thereby leading,1,1,1.0
thereby leading to,1,1,1.0
leading to performance,1,1,1.0
to performance depreciation,1,1,1.0
performance depreciation ramoboost,1,1,1.0
depreciation ramoboost on,1,1,1.0
ramoboost on the,1,1,1.0
other hand assigns,1,1,1.0
hand assigns high,1,1,1.0
assigns high probabilities,1,1,1.0
high probabilities for,1,1,1.0
probabilities for minority,1,1,1.0
minority examples close,1,1,1.0
examples close to,1,1,1.0
decision boundary which,1,1,1.0
boundary which means,1,1,1.0
means that synthetic,1,1,1.0
that synthetic instances,1,1,1.0
instances are generated,2,1,2.0
are generated near,1,1,1.0
generated near the,1,1,1.0
decision boundary on,1,1,1.0
boundary on a,1,1,1.0
on a relative,1,1,1.0
a relative basis,1,1,1.0
relative basis as,1,1,1.0
basis as opposed,1,1,1.0
opposed to an,1,1,1.0
to an absolute,1,1,1.0
an absolute basis,1,1,1.0
absolute basis as,1,1,1.0
basis as a,1,1,1.0
a result the,2,1,2.0
result the negative,1,1,1.0
the negative impact,1,1,1.0
negative impact of,1,1,1.0
impact of noise,1,1,1.0
of noise is,1,1,1.0
noise is attenuated,1,1,1.0
is attenuated in,1,1,1.0
attenuated in ramoboost,1,1,1.0
in ramoboost as,1,1,1.0
ramoboost as compared,1,1,1.0
as compared to,1,1,1.0
compared to adasyn,1,1,1.0
to adasyn in,1,1,1.0
adasyn in order,1,1,1.0
order to compare,1,1,1.0
compare the data,1,1,1.0
mechanism of ramoboost,3,1,3.0
of ramoboost with,1,1,1.0
ramoboost with that,1,1,1.0
with that of,2,1,2.0
that of smote,2,1,2.0
and adasyn we,1,1,1.0
adasyn we provide,1,1,1.0
we provide an,1,1,1.0
provide an example,1,1,1.0
of a dataset,1,1,1.0
dataset with majority,1,1,1.0
with majority examples,1,1,1.0
majority examples and,3,1,3.0
examples and minority,1,1,1.0
and minority examples,1,1,1.0
minority examples fig,1,1,1.0
fig a shows,1,1,1.0
a shows the,1,1,1.0
shows the original,1,1,1.0
original imbalanced data,2,1,2.0
distribution and fig,1,1,1.0
and fig b,1,1,1.0
fig b d,1,1,1.0
b d shows,1,1,1.0
d shows the,1,1,1.0
shows the data,1,1,1.0
data distribution data,1,1,1.0
distribution data distribution,1,1,1.0
distribution and the,1,1,1.0
and the data,1,1,1.0
data distribution respectively,1,1,1.0
distribution respectively in,1,1,1.0
respectively in all,1,1,1.0
in all of,1,1,1.0
all of these,1,1,1.0
of these ﬁgures,1,1,1.0
these ﬁgures the,1,1,1.0
ﬁgures the plus,1,1,1.0
the plus and,1,1,1.0
plus and point,1,1,1.0
and point shapes,1,1,1.0
point shapes represent,1,1,1.0
shapes represent the,1,1,1.0
represent the inal,1,1,1.0
the inal majority,1,1,1.0
inal majority data,1,1,1.0
majority data original,1,1,1.0
data original minority,1,1,1.0
original minority data,1,1,1.0
minority data and,1,1,1.0
and the generated,1,1,1.0
generated synthetic data,1,1,1.0
synthetic data respectively,1,1,1.0
data respectively furthermore,1,1,1.0
respectively furthermore for,1,1,1.0
furthermore for each,1,1,1.0
for each ﬁgure,1,1,1.0
each ﬁgure we,2,1,2.0
ﬁgure we also,1,1,1.0
we also illustrate,1,1,1.0
also illustrate the,1,1,1.0
illustrate the classiﬁcation,1,1,1.0
the classiﬁcation confusion,1,1,1.0
classiﬁcation confusion matrix,1,1,1.0
confusion matrix in,1,1,1.0
matrix in terms,1,1,1.0
terms of instant,1,1,1.0
of instant counts,1,1,1.0
instant counts for,1,1,1.0
counts for performance,1,1,1.0
for performance assessment,1,1,1.0
performance assessment here,1,1,1.0
assessment here we,1,1,1.0
here we follow,1,1,1.0
we follow the,1,1,1.0
follow the suggestions,1,1,1.0
the suggestions of,1,1,1.0
suggestions of and,1,1,1.0
of and and,1,1,1.0
and and use,1,1,1.0
use the minority,1,1,1.0
minority class as,1,1,1.0
positive class and,1,1,1.0
negative class the,1,1,1.0
class the classiﬁer,1,1,1.0
the classiﬁer used,1,1,1.0
classiﬁer used to,1,1,1.0
used to make,1,1,1.0
to make predictions,1,1,1.0
make predictions on,1,1,1.0
predictions on all,1,1,1.0
on all datasets,3,1,3.0
all datasets shown,1,1,1.0
datasets shown in,1,1,1.0
in fig is,1,1,1.0
fig is classiﬁcation,1,1,1.0
is classiﬁcation and,1,1,1.0
classiﬁcation and regression,1,1,1.0
and regression tree,1,1,1.0
regression tree comparing,1,1,1.0
tree comparing the,1,1,1.0
comparing the confusion,1,1,1.0
confusion matrix of,1,1,1.0
matrix of each,1,1,1.0
of each ﬁgure,1,1,1.0
ﬁgure we see,1,1,1.0
that the proposed,1,1,1.0
proposed ramoboost method,2,1,2.0
ramoboost method can,1,1,1.0
method can improve,1,1,1.0
can improve classiﬁcation,1,1,1.0
improve classiﬁcation performance,1,1,1.0
classiﬁcation performance speciﬁcally,1,1,1.0
performance speciﬁcally the,1,1,1.0
speciﬁcally the improvement,1,1,1.0
the improvement of,1,1,1.0
improvement of true,1,1,1.0
of true negative,1,1,1.0
negative tn counts,1,1,1.0
tn counts for,1,1,1.0
counts for smote,1,1,1.0
for smote with,1,1,1.0
smote with respect,1,1,1.0
original dataset changes,1,1,1.0
dataset changes from,1,1,1.0
changes from to,1,1,1.0
from to while,1,1,1.0
to while for,1,1,1.0
while for ramoboost,1,1,1.0
for ramoboost it,1,1,1.0
ramoboost it increases,1,1,1.0
it increases from,1,1,1.0
increases from to,1,1,1.0
from to this,1,1,1.0
to this is,1,1,1.0
is because in,1,1,1.0
because in smote,1,1,1.0
smote the same,1,1,1.0
numbers of instances,1,1,1.0
of instances are,1,1,1.0
are generated for,1,1,1.0
minority example while,1,1,1.0
example while in,1,1,1.0
while in ramoboost,1,1,1.0
in ramoboost the,2,1,2.0
ramoboost the data,1,1,1.0
data generation process,1,1,1.0
generation process is,1,1,1.0
process is adaptive,1,1,1.0
is adaptive according,1,1,1.0
adaptive according to,1,1,1.0
data distribution from,1,1,1.0
distribution from fig,1,1,1.0
from fig c,2,1,2.0
fig c we,1,1,1.0
c we also,1,1,1.0
we also see,1,1,1.0
see that adasyn,2,1,2.0
that adasyn is,1,1,1.0
adasyn is very,1,1,1.0
is very aggressive,1,1,1.0
very aggressive in,1,1,1.0
aggressive in learning,1,1,1.0
learning from the,1,1,1.0
from the boundary,2,1,2.0
the boundary since,2,1,2.0
boundary since it,2,1,2.0
since it generates,2,1,2.0
generates synthetic data,1,1,1.0
data instances very,2,1,2.0
instances very close,2,1,2.0
boundary this may,1,1,1.0
this may have,1,1,1.0
may have two,1,1,1.0
have two effects,1,1,1.0
two effects on,1,1,1.0
effects on the,2,1,2.0
on the learning,1,1,1.0
the learning performance,2,1,2.0
learning performance it,1,1,1.0
performance it may,1,1,1.0
it may increasechen,1,1,1.0
may increasechen et,1,1,1.0
increasechen et al,1,1,1.0
in boosting a,1,1,1.0
boosting a b,1,1,1.0
b c d,2,1,2.0
c d hypothesis,1,1,1.0
d hypothesis outputy,1,1,1.0
hypothesis outputy p,4,1,4.0
outputy p true,4,1,4.0
p true class,4,1,4.0
true class n,4,1,4.0
class n n,4,1,4.0
n n hypothesis,3,1,3.0
n hypothesis outputy,3,1,3.0
n n fig,1,1,1.0
n fig comparison,1,1,1.0
fig comparison of,1,1,1.0
comparison of different,1,1,1.0
of different synthetic,1,1,1.0
different synthetic data,1,1,1.0
generation mechanisms a,1,1,1.0
mechanisms a original,1,1,1.0
a original imbalanced,1,1,1.0
imbalanced data distribu,1,1,1.0
data distribu tion,2,1,2.0
distribu tion majority,1,1,1.0
tion majority examples,1,1,1.0
examples and mi,1,1,1.0
and mi nority,1,1,1.0
mi nority examples,1,1,1.0
nority examples b,1,1,1.0
examples b data,1,1,1.0
b data distribution,1,1,1.0
data distribution after,1,1,1.0
distribution after smote,1,1,1.0
after smote method,1,1,1.0
smote method c,1,1,1.0
method c data,1,1,1.0
c data distribu,1,1,1.0
distribu tion after,1,1,1.0
tion after adasyn,1,1,1.0
after adasyn method,1,1,1.0
adasyn method d,1,1,1.0
method d data,1,1,1.0
d data dis,1,1,1.0
data dis tribution,1,1,1.0
dis tribution after,1,1,1.0
tribution after ramoboost,1,1,1.0
after ramoboost method,1,1,1.0
ramoboost method the,1,1,1.0
method the classiﬁcation,1,1,1.0
the classiﬁcation accuracy,2,1,2.0
classiﬁcation accuracy of,1,1,1.0
minority data as,1,1,1.0
data as it,1,1,1.0
as it provides,1,1,1.0
it provides a,1,1,1.0
provides a good,1,1,1.0
a good representation,1,1,1.0
good representation of,1,1,1.0
minority data distribution,1,1,1.0
data distribution close,1,1,1.0
distribution close to,1,1,1.0
the boundary thereby,1,1,1.0
boundary thereby improving,1,1,1.0
thereby improving the,1,1,1.0
improving the recall,1,1,1.0
the recall performance,1,1,1.0
recall performance which,1,1,1.0
performance which will,1,1,1.0
will be discussed,1,1,1.0
be discussed in,1,1,1.0
discussed in detail,1,1,1.0
in detail in,1,1,1.0
detail in section,1,1,1.0
in section however,1,1,1.0
section however it,1,1,1.0
may also decrease,1,1,1.0
also decrease the,1,1,1.0
decrease the classiﬁcation,1,1,1.0
the classiﬁcation performance,2,1,2.0
classiﬁcation performance of,1,1,1.0
of the jority,1,1,1.0
the jority class,1,1,1.0
jority class which,1,1,1.0
class which in,1,1,1.0
in turn deteriorates,1,1,1.0
turn deteriorates the,1,1,1.0
deteriorates the overall,1,1,1.0
the overall classiﬁcation,1,1,1.0
overall classiﬁcation performance,1,1,1.0
classiﬁcation performance one,1,1,1.0
performance one can,1,1,1.0
one can observe,1,1,1.0
can observe from,1,1,1.0
observe from fig,1,1,1.0
fig c that,1,1,1.0
c that although,1,1,1.0
that although the,1,1,1.0
although the classiﬁcation,1,1,1.0
classiﬁcation accuracy for,1,1,1.0
accuracy for minority,1,1,1.0
minority examples under,1,1,1.0
examples under the,1,1,1.0
under the adasyn,1,1,1.0
the adasyn technique,1,1,1.0
adasyn technique is,1,1,1.0
the best among,1,1,1.0
best among all,1,1,1.0
among all these,1,1,1.0
all these methods,1,1,1.0
these methods true,1,1,1.0
methods true positive,1,1,1.0
positive tp therefore,1,1,1.0
tp therefore recall,1,1,1.0
therefore recall the,1,1,1.0
recall the tn,1,1,1.0
the tn counts,1,1,1.0
tn counts of,1,1,1.0
counts of adasyn,1,1,1.0
of adasyn also,1,1,1.0
adasyn also decreases,1,1,1.0
also decreases signiﬁcantly,1,1,1.0
decreases signiﬁcantly the,1,1,1.0
signiﬁcantly the lowest,1,1,1.0
the lowest of,1,1,1.0
lowest of all,1,1,1.0
of all in,1,1,1.0
all in this,1,1,1.0
this case with,1,1,1.0
case with tn,1,1,1.0
with tn to,1,1,1.0
tn to this,1,1,1.0
this end ramoboost,1,1,1.0
end ramoboost can,1,1,1.0
ramoboost can be,2,1,2.0
considered to take,1,1,1.0
to take advantage,1,1,1.0
advantage of both,1,1,1.0
of both smote,1,1,1.0
and adasyn to,1,1,1.0
adasyn to improve,1,1,1.0
improve the overall,1,1,1.0
the overall learning,1,1,1.0
overall learning performance,1,1,1.0
learning performance our,1,1,1.0
performance our simulation,1,1,1.0
our simulation analyses,1,1,1.0
simulation analyses which,1,1,1.0
analyses which are,1,1,1.0
which are based,1,1,1.0
based on various,1,1,1.0
on various datasets,1,1,1.0
various datasets and,1,1,1.0
datasets and various,1,1,1.0
and various assessment,1,1,1.0
various assessment metrics,2,1,2.0
assessment metrics in,1,1,1.0
metrics in section,1,1,1.0
in section also,1,1,1.0
section also conﬁrm,1,1,1.0
also conﬁrm this,1,1,1.0
conﬁrm this the,1,1,1.0
this the boosting,1,1,1.0
the boosting procedure,2,1,2.0
boosting procedure boosting,1,1,1.0
procedure boosting has,1,1,1.0
boosting has attracted,1,1,1.0
has attracted cantly,1,1,1.0
attracted cantly increased,1,1,1.0
cantly increased attention,1,1,1.0
increased attention recently,1,1,1.0
attention recently in,1,1,1.0
recently in the,1,1,1.0
in the computational,2,1,2.0
the computational gence,1,1,1.0
computational gence community,1,1,1.0
gence community in,1,1,1.0
community in our,1,1,1.0
in our proposed,1,1,1.0
our proposed ramoboost,1,1,1.0
proposed ramoboost approach,1,1,1.0
ramoboost approach the,1,1,1.0
approach the boosting,1,1,1.0
the boosting algorithm,2,1,2.0
boosting algorithm is,1,1,1.0
algorithm is essentially,1,1,1.0
is essentially the,1,1,1.0
essentially the same,1,1,1.0
same as the,1,1,1.0
as the classic,1,1,1.0
the classic rather,1,1,1.0
classic rather than,1,1,1.0
rather than reducing,1,1,1.0
than reducing the,1,1,1.0
reducing the prediction,1,1,1.0
the prediction error,1,1,1.0
prediction error on,1,1,1.0
error on the,1,1,1.0
dataset in each,1,1,1.0
in each iteration,2,1,2.0
iteration loop in,1,1,1.0
loop in a,1,1,1.0
in a stepwise,1,1,1.0
a stepwise manner,1,1,1.0
stepwise manner the,1,1,1.0
manner the boosting,1,1,1.0
boosting algorithm of,1,1,1.0
algorithm of can,1,1,1.0
of can focus,1,1,1.0
can focus the,1,1,1.0
focus the weak,1,1,1.0
weak learner on,1,1,1.0
learner on the,1,1,1.0
on the labels,1,1,1.0
the labels that,1,1,1.0
labels that are,1,1,1.0
that are hardest,1,1,1.0
are hardest to,1,1,1.0
hardest to discriminate,1,1,1.0
to discriminate by,1,1,1.0
discriminate by manipulating,1,1,1.0
manipulating the as,1,1,1.0
the as deﬁned,1,1,1.0
as deﬁned in,1,1,1.0
deﬁned in in,1,1,1.0
in in other,1,1,1.0
other words the,1,1,1.0
words the emphasis,1,1,1.0
the emphasis of,1,1,1.0
emphasis of is,1,1,1.0
of is to,1,1,1.0
is to make,1,1,1.0
to make the,1,1,1.0
make the incorrect,1,1,1.0
the incorrect class,1,1,1.0
incorrect class label,1,1,1.0
class label as,1,1,1.0
label as distinguishable,1,1,1.0
as distinguishable as,1,1,1.0
distinguishable as possible,1,1,1.0
as possible from,1,1,1.0
possible from the,1,1,1.0
from the correct,1,1,1.0
the correct class,1,1,1.0
correct class label,1,1,1.0
class label belonging,1,1,1.0
label belonging to,1,1,1.0
to the example,1,1,1.0
under consideration this,1,1,1.0
consideration this is,1,1,1.0
why the mislabeled,1,1,1.0
the mislabeled dataset,1,1,1.0
mislabeled dataset is,1,1,1.0
dataset is sampled,1,1,1.0
is sampled in,1,1,1.0
sampled in each,1,1,1.0
iteration loop instead,1,1,1.0
loop instead of,1,1,1.0
of the correct,1,1,1.0
the correct one,1,1,1.0
correct one compared,1,1,1.0
one compared to,1,1,1.0
compared to enables,1,1,1.0
to enables the,1,1,1.0
enables the weak,1,1,1.0
weak learner to,2,1,2.0
learner to make,1,1,1.0
to make useful,1,1,1.0
make useful contributions,1,1,1.0
useful contributions to,1,1,1.0
contributions to the,1,1,1.0
to the accuracy,1,1,1.0
of the ﬁnal,1,1,1.0
ﬁnal hypothesis even,1,1,1.0
hypothesis even when,1,1,1.0
even when the,1,1,1.0
when the weak,1,1,1.0
the weak hypothesis,1,1,1.0
weak hypothesis does,1,1,1.0
hypothesis does not,1,1,1.0
does not predict,1,1,1.0
not predict the,1,1,1.0
predict the correct,1,1,1.0
the correct label,1,1,1.0
correct label with,1,1,1.0
label with a,1,1,1.0
with a probability,1,1,1.0
a probability greater,1,1,1.0
probability greater than,1,1,1.0
greater than in,1,1,1.0
this way the,1,1,1.0
way the iteration,1,1,1.0
the iteration loop,1,1,1.0
iteration loop is,1,1,1.0
loop is not,1,1,1.0
is not broken,1,1,1.0
not broken regardless,1,1,1.0
broken regardless of,1,1,1.0
of the trained,1,1,1.0
trained hypothesis which,1,1,1.0
hypothesis which is,1,1,1.0
which is not,1,1,1.0
case for other,1,1,1.0
for other boosting,1,1,1.0
other boosting algorithms,1,1,1.0
boosting algorithms including,1,1,1.0
algorithms including given,1,1,1.0
including given that,1,1,1.0
given that it,1,1,1.0
it is generally,1,1,1.0
is generally very,1,1,1.0
generally very hard,1,1,1.0
very hard for,1,1,1.0
hard for a,1,1,1.0
for a single,1,1,1.0
a single weak,1,1,1.0
single weak learner,1,1,1.0
learner to extract,1,1,1.0
to extract sufﬁcient,1,1,1.0
extract sufﬁcient knowledge,1,1,1.0
sufﬁcient knowledge from,1,1,1.0
from the imbalanced,1,1,1.0
the imbalanced dataset,1,1,1.0
imbalanced dataset at,1,1,1.0
dataset at one,1,1,1.0
at one instance,1,1,1.0
one instance the,1,1,1.0
instance the performance,1,1,1.0
performance of ramoboost,3,1,3.0
of ramoboost and,2,1,2.0
ramoboost and most,1,1,1.0
and most algorithms,1,1,1.0
most algorithms may,1,1,1.0
algorithms may suffer,1,1,1.0
suffer from an,1,1,1.0
from an insufﬁcient,1,1,1.0
an insufﬁcient number,1,1,1.0
insufﬁcient number of,1,1,1.0
number of boosting,2,1,2.0
of boosting iterations,1,1,1.0
boosting iterations thus,1,1,1.0
iterations thus for,1,1,1.0
thus for an,1,1,1.0
for an imbalanced,1,1,1.0
an imbalanced ieee,1,1,1.0
imbalanced ieee transactions,1,1,1.0
no october of,1,1,1.0
october of any,1,1,1.0
of any size,1,1,1.0
any size whose,1,1,1.0
size whose target,1,1,1.0
whose target concept,1,1,1.0
target concept we,1,1,1.0
concept we assume,1,1,1.0
we assume is,1,1,1.0
assume is difﬁcult,1,1,1.0
is difﬁcult to,1,1,1.0
difﬁcult to learn,1,1,1.0
to learn it,1,1,1.0
learn it is,1,1,1.0
it is our,1,1,1.0
is our belief,1,1,1.0
our belief that,1,1,1.0
belief that the,1,1,1.0
of ramoboost can,1,1,1.0
can be guaranteed,1,1,1.0
be guaranteed satisfactory,1,1,1.0
guaranteed satisfactory if,1,1,1.0
satisfactory if it,1,1,1.0
if it can,1,1,1.0
it can iterate,1,1,1.0
can iterate for,1,1,1.0
iterate for enough,1,1,1.0
for enough epochs,1,1,1.0
enough epochs this,1,1,1.0
epochs this is,1,1,1.0
this is our,1,1,1.0
is our motivation,1,1,1.0
our motivation for,1,1,1.0
motivation for employing,1,1,1.0
for employing the,1,1,1.0
employing the algorithm,1,1,1.0
algorithm in ramoboost,1,1,1.0
in ramoboost computational,1,1,1.0
ramoboost computational complexity,1,1,1.0
complexity analysis in,1,1,1.0
analysis in the,1,1,1.0
the training stage,1,1,1.0
training stage the,1,1,1.0
stage the computational,2,1,2.0
computational complexity of,2,1,2.0
complexity of ramoboost,1,1,1.0
of ramoboost arises,1,1,1.0
ramoboost arises from,1,1,1.0
arises from the,1,1,1.0
from the construction,1,1,1.0
the construction of,1,1,1.0
construction of the,1,1,1.0
the hypothesis at,1,1,1.0
iteration loop as,1,1,1.0
loop as well,1,1,1.0
as the boosting,1,1,1.0
boosting procedure we,1,1,1.0
procedure we assume,1,1,1.0
we assume the,1,1,1.0
assume the following,1,1,1.0
the following in,1,1,1.0
following in our,1,1,1.0
in our analysis,1,1,1.0
our analysis dimension,1,1,1.0
analysis dimension of,1,1,1.0
feature space number,1,1,1.0
space number of,1,1,1.0
number of training,1,1,1.0
of training examples,1,1,1.0
training examples ratio,1,1,1.0
examples ratio of,1,1,1.0
ratio of minority,1,1,1.0
minority examples in,2,1,2.0
training dataset p,1,1,1.0
dataset p t,1,1,1.0
p t number,1,1,1.0
of boosting epochs,1,1,1.0
boosting epochs the,1,1,1.0
epochs the procedure,1,1,1.0
the procedure of,2,1,2.0
procedure of generatin,1,1,1.0
of generatin g,1,1,1.0
generatin g synthetic,1,1,1.0
g synthetic instances,1,1,1.0
instances for the,1,1,1.0
for the training,1,1,1.0
training dataset is,1,1,1.0
dataset is initialized,1,1,1.0
is initialized with,1,1,1.0
initialized with the,1,1,1.0
with the calculation,1,1,1.0
the calculation of,1,1,1.0
calculation of the,1,1,1.0
of the probability,1,1,1.0
generating synthetic instances,1,1,1.0
synthetic instances the,1,1,1.0
instances the time,1,1,1.0
the time complexity,7,1,7.0
time complexity can,1,1,1.0
complexity can be,1,1,1.0
can be decomposed,1,1,1.0
be decomposed into,1,1,1.0
decomposed into three,1,1,1.0
into three steps,1,1,1.0
three steps calculating,1,1,1.0
steps calculating the,1,1,1.0
calculating the euc,1,1,1.0
the euc lidean,1,1,1.0
euc lidean distance,1,1,1.0
lidean distance from,1,1,1.0
consideration to all,1,1,1.0
to all the,1,1,1.0
the other examples,1,1,1.0
other examples in,1,1,1.0
the training o,1,1,1.0
training o mn,1,1,1.0
o mn sorting,1,1,1.0
mn sorting all,1,1,1.0
sorting all current,1,1,1.0
all current euclidean,1,1,1.0
current euclidean distance,1,1,1.0
euclidean distance calculations,1,1,1.0
distance calculations in,1,1,1.0
calculations in ascending,1,1,1.0
in ascending o,1,1,1.0
ascending o n,1,1,1.0
o n log,3,1,3.0
n log n,4,1,4.0
log n r,1,1,1.0
n r e,1,1,1.0
r e t,1,1,1.0
e t r,1,1,1.0
t r i,1,1,1.0
r i e,1,1,1.0
i e v,1,1,1.0
e v i,1,1,1.0
v i n,1,1,1.0
i n gt,1,1,1.0
n gt h,1,1,1.0
gt h eﬁ,1,1,1.0
h eﬁ r,1,1,1.0
eﬁ r s,1,1,1.0
r s examples,1,1,1.0
s examples corresponding,1,1,1.0
examples corresponding to,1,1,1.0
corresponding to the,2,1,2.0
to the ﬁrst,1,1,1.0
the ﬁrst items,1,1,1.0
ﬁrst items in,1,1,1.0
items in the,1,1,1.0
in the sorted,1,1,1.0
the sorted euclidean,1,1,1.0
sorted euclidean distance,1,1,1.0
euclidean distance complexity,1,1,1.0
distance complexity o,1,1,1.0
complexity o thus,1,1,1.0
o thus the,1,1,1.0
thus the time,1,1,1.0
time complexity for,3,1,3.0
complexity for this,1,1,1.0
for this step,1,1,1.0
this step should,1,1,1.0
step should be,1,1,1.0
should be o,2,1,2.0
be o mn,1,1,1.0
o mn n,1,1,1.0
mn n log,1,1,1.0
log n in,2,1,2.0
n in typical,1,1,1.0
in typical situations,1,1,1.0
typical situations and,1,1,1.0
situations and m,1,1,1.0
and m are,1,1,1.0
m are both,1,1,1.0
are both signiﬁcantly,1,1,1.0
both signiﬁcantly smaller,1,1,1.0
signiﬁcantly smaller than,1,1,1.0
smaller than n,1,1,1.0
than n which,1,1,1.0
n which simpliﬁes,1,1,1.0
simpliﬁes the time,1,1,1.0
time complexity to,1,1,1.0
complexity to approximately,1,1,1.0
to approximately o,1,1,1.0
approximately o n,1,1,1.0
log n the,1,1,1.0
n the next,1,1,1.0
next step is,1,1,1.0
step is to,2,1,2.0
is to ﬁnd,1,1,1.0
ﬁnd the minority,1,1,1.0
the minority neighbors,1,1,1.0
minority neighbors of,1,1,1.0
example for synthetic,1,1,1.0
generation the time,1,1,1.0
time complexity of,2,1,2.0
complexity of this,1,1,1.0
of this calculation,1,1,1.0
this calculation is,1,1,1.0
calculation is the,1,1,1.0
is the same,2,1,2.0
the same with,1,1,1.0
same with the,1,1,1.0
with the ﬁrst,1,1,1.0
ﬁrst step except,1,1,1.0
step except that,1,1,1.0
except that the,1,1,1.0
euclidean distance calcula,1,1,1.0
distance calcula tion,1,1,1.0
calcula tion is,1,1,1.0
tion is between,1,1,1.0
is between the,1,1,1.0
minority examples therefore,1,1,1.0
therefore the time,1,1,1.0
time complexity is,1,1,1.0
complexity is no,1,1,1.0
is no greater,1,1,1.0
no greater than,1,1,1.0
greater than o,1,1,1.0
than o n,1,1,1.0
log n since,1,1,1.0
n since there,1,1,1.0
since there are,3,1,3.0
there are altogether,1,1,1.0
are altogether np,1,1,1.0
altogether np minority,1,1,1.0
np minority examples,1,1,1.0
training dataset the,1,1,1.0
dataset the total,1,1,1.0
the total time,1,1,1.0
total time complexity,1,1,1.0
time complexity should,1,1,1.0
complexity should be,1,1,1.0
be o log,1,1,1.0
o log n,4,1,4.0
log n which,1,1,1.0
n which can,1,1,1.0
can be simpliﬁed,1,1,1.0
be simpliﬁed to,1,1,1.0
simpliﬁed to o,1,1,1.0
to o log,1,1,1.0
log n lastly,1,1,1.0
n lastly since,1,1,1.0
lastly since data,1,1,1.0
since data generation,1,1,1.0
data generation is,2,1,2.0
generation is applied,1,1,1.0
is applied in,1,1,1.0
applied in each,1,1,1.0
in each boosting,1,1,1.0
boosting iteration the,1,1,1.0
iteration the time,1,1,1.0
complexity of synthetic,1,1,1.0
data generation for,1,1,1.0
generation for the,1,1,1.0
for the learning,1,1,1.0
learning process of,1,1,1.0
process of ramoboost,1,1,1.0
ramoboost is o,1,1,1.0
is o log,2,1,2.0
log n for,1,1,1.0
n for a,1,1,1.0
for a neural,1,1,1.0
neural network with,2,1,2.0
network with multilayer,1,1,1.0
with multilayer perceptron,1,1,1.0
perceptron mlp the,1,1,1.0
mlp the time,1,1,1.0
complexity for the,1,1,1.0
for the boosting,1,1,1.0
boosting process excluding,1,1,1.0
process excluding thetic,1,1,1.0
excluding thetic data,1,1,1.0
thetic data generation,1,1,1.0
generation is at,1,1,1.0
is at worse,1,1,1.0
at worse o,1,1,1.0
worse o therefore,1,1,1.0
o therefore we,1,1,1.0
therefore we summarize,1,1,1.0
we summarize that,1,1,1.0
summarize that the,1,1,1.0
that the training,2,1,2.0
the training process,1,1,1.0
training process time,1,1,1.0
process time complexity,1,1,1.0
complexity for ramoboost,1,1,1.0
for ramoboost with,1,1,1.0
ramoboost with mlp,1,1,1.0
with mlp as,1,1,1.0
mlp as a,1,1,1.0
as a base,1,1,1.0
a base classiﬁer,1,1,1.0
base classiﬁer is,1,1,1.0
classiﬁer is o,1,1,1.0
in the testing,1,1,1.0
the testing stage,1,1,1.0
testing stage the,1,1,1.0
the computational operation,1,1,1.0
computational operation in,1,1,1.0
operation in each,1,1,1.0
in each hypothesis,1,1,1.0
each hypothesis is,1,1,1.0
hypothesis is just,1,1,1.0
is just a,1,1,1.0
just a comparison,1,1,1.0
a comparison operation,1,1,1.0
comparison operation the,1,1,1.0
operation the time,1,1,1.0
the time tion,1,1,1.0
time tion for,1,1,1.0
tion for each,1,1,1.0
each of them,1,1,1.0
of them is,1,1,1.0
them is very,1,1,1.0
is very small,1,1,1.0
very small since,1,1,1.0
small since the,1,1,1.0
since the ﬁnal,1,1,1.0
ﬁnal hypothesis is,1,1,1.0
hypothesis is a,1,1,1.0
is a weighted,1,1,1.0
a weighted combination,1,1,1.0
weighted combination of,1,1,1.0
combination of all,1,1,1.0
of all trained,1,1,1.0
all trained hypothesis,1,1,1.0
trained hypothesis as,1,1,1.0
hypothesis as shown,1,1,1.0
shown in the,1,1,1.0
complexity of predicting,1,1,1.0
of predicting the,1,1,1.0
predicting the class,1,1,1.0
label of an,1,1,1.0
of an instance,1,1,1.0
an instance can,1,1,1.0
instance can be,1,1,1.0
be estimated as,1,1,1.0
estimated as o,1,1,1.0
as o t,1,1,1.0
o t iv,1,1,1.0
t iv simulation,1,1,1.0
iv simulation and,1,1,1.0
simulation and discussion,1,1,1.0
and discussion in,1,1,1.0
discussion in this,1,1,1.0
section we conduct,1,1,1.0
we conduct various,1,1,1.0
conduct various simulations,1,1,1.0
various simulations of,1,1,1.0
simulations of the,1,1,1.0
ramoboost method and,1,1,1.0
method and compare,1,1,1.0
and compare its,1,1,1.0
compare its performance,1,1,1.0
its performance table,1,1,1.0
performance table i,2,1,2.0
table i summary,1,1,1.0
i summary of,1,1,1.0
summary of the,1,1,1.0
of the datas,1,1,1.0
the datas et,1,1,1.0
datas et characteristics,1,1,1.0
et characteristics sorted,1,1,1.0
characteristics sorted in,1,1,1.0
sorted in the,1,1,1.0
in the degree,1,1,1.0
degree of class,1,1,1.0
of class skew,1,1,1.0
class skew dataset,1,1,1.0
skew dataset feature,1,1,1.0
dataset feature data,1,1,1.0
feature data minority,1,1,1.0
data minority majority,1,1,1.0
minority majority imbalanced,2,1,2.0
majority imbalanced instances,1,1,1.0
imbalanced instances instances,1,1,1.0
instances instances ratio,1,1,1.0
instances ratio sonar,1,1,1.0
ratio sonar spambase,1,1,1.0
sonar spambase ionosphere,5,1,5.0
spambase ionosphere pid,5,1,5.0
ionosphere pid wine,5,1,5.0
pid wine german,5,1,5.0
wine german phoneme,5,1,5.0
german phoneme vehicle,5,1,5.0
phoneme vehicle texture,5,1,5.0
vehicle texture segment,5,1,5.0
texture segment satimage,6,1,6.0
segment satimage vow,6,1,6.0
satimage vow e,6,1,6.0
vow e l,7,1,7.0
e l abalone,6,1,6.0
l abalone glass,6,1,6.0
abalone glass yeast,6,1,6.0
glass yeast letter,6,1,6.0
yeast letter shuttle,5,1,5.0
letter shuttle with,1,1,1.0
shuttle with smoteboost,1,1,1.0
with smoteboost smote,1,1,1.0
smoteboost smote adasyn,41,1,41.0
smote adasyn adacost,39,1,39.0
adasyn adacost linesmote,1,1,1.0
adacost linesmote and,1,1,1.0
linesmote and across,1,1,1.0
and across different,1,1,1.0
across different datasets,1,1,1.0
different datasets the,1,1,1.0
datasets the neural,1,1,1.0
the neural network,2,1,2.0
network with mlp,1,1,1.0
with mlp is,1,1,1.0
mlp is employed,1,1,1.0
is employed as,2,1,2.0
employed as the,2,1,2.0
as the base,2,1,2.0
the base learner,2,1,2.0
base learner the,2,1,2.0
learner the mlp,1,1,1.0
the mlp is,1,1,1.0
mlp is conﬁgured,1,1,1.0
is conﬁgured as,1,1,1.0
conﬁgured as follows,1,1,1.0
number of hidden,3,1,3.0
of hidden layer,3,1,3.0
hidden layer neurons,3,1,3.0
layer neurons is,1,1,1.0
neurons is set,2,1,2.0
is set to,8,1,8.0
set to be,4,1,4.0
to be four,1,1,1.0
be four and,1,1,1.0
four and the,1,1,1.0
number of input,1,1,1.0
of input neurons,1,1,1.0
input neurons is,1,1,1.0
neurons is equal,1,1,1.0
is equal to,3,1,3.0
of features for,1,1,1.0
features for each,1,1,1.0
each dataset similar,1,1,1.0
dataset similar to,1,1,1.0
similar to most,1,1,1.0
to most of,1,1,1.0
of the existing,3,1,3.0
the existing imbalanced,2,1,2.0
existing imbalanced learning,2,1,2.0
imbalanced learning methods,1,1,1.0
learning methods in,1,1,1.0
methods in literature,1,1,1.0
in literature we,1,1,1.0
literature we also,1,1,1.0
we also consider,1,1,1.0
also consider only,1,1,1.0
consider only imbalanced,1,1,1.0
only imbalanced problems,1,1,1.0
imbalanced problems in,1,1,1.0
problems in our,2,1,2.0
in our current,1,1,1.0
our current study,2,1,2.0
current study therefore,1,1,1.0
study therefore the,1,1,1.0
number of output,1,1,1.0
of output neurons,1,1,1.0
output neurons is,1,1,1.0
set to two,1,1,1.0
to two for,1,1,1.0
two for all,1,1,1.0
for all simulations,2,1,2.0
all simulations the,1,1,1.0
simulations the sigmoid,1,1,1.0
the sigmoid function,1,1,1.0
sigmoid function is,1,1,1.0
function is used,1,1,1.0
as the activation,1,1,1.0
the activation function,1,1,1.0
activation function and,1,1,1.0
function and the,1,1,1.0
and the inner,1,1,1.0
the inner training,1,1,1.0
inner training epochs,1,1,1.0
training epochs is,1,1,1.0
epochs is set,1,1,1.0
to be with,1,1,1.0
be with a,1,1,1.0
with a learning,1,1,1.0
a learning rate,1,1,1.0
learning rate of,1,1,1.0
rate of due,1,1,1.0
of due to,1,1,1.0
to the concern,1,1,1.0
the concern that,1,1,1.0
concern that the,1,1,1.0
that the scattered,1,1,1.0
the scattered feature,1,1,1.0
scattered feature distribution,1,1,1.0
feature distribution of,1,1,1.0
distribution of some,1,1,1.0
of some datasets,1,1,1.0
some datasets may,1,1,1.0
datasets may hinder,1,1,1.0
may hinder the,1,1,1.0
hinder the neural,1,1,1.0
neural network from,1,1,1.0
network from converging,1,1,1.0
from converging fast,1,1,1.0
converging fast enough,1,1,1.0
fast enough for,1,1,1.0
enough for the,1,1,1.0
the parameter acceleration,1,1,1.0
parameter acceleration process,1,1,1.0
acceleration process before,1,1,1.0
process before all,1,1,1.0
before all datasets,1,1,1.0
all datasets are,1,1,1.0
datasets are presented,1,1,1.0
are presented to,1,1,1.0
presented to t,1,1,1.0
to t he,1,1,1.0
t he comparative,1,1,1.0
he comparative algorithms,1,1,1.0
comparative algorithms for,3,1,3.0
for learning we,1,1,1.0
learning we ﬁrst,1,1,1.0
we ﬁrst use,1,1,1.0
ﬁrst use the,1,1,1.0
use the nonlinear,1,1,1.0
the nonlinear normalization,1,1,1.0
nonlinear normalization approach,1,1,1.0
normalization approach to,1,1,1.0
approach to normalize,1,1,1.0
to normalize the,1,1,1.0
normalize the features,1,1,1.0
the features of,2,2,1.0
features of the,1,1,1.0
the datasets to,1,1,1.0
datasets to reside,1,1,1.0
to reside in,1,1,1.0
reside in the,1,1,1.0
in the interval,1,1,1.0
the interval dataset,1,1,1.0
interval dataset description,1,1,1.0
dataset description the,1,1,1.0
description the performance,1,1,1.0
ramoboost is evaluated,1,1,1.0
is evaluated on,1,1,1.0
evaluated on datasets,1,1,1.0
on datasets from,1,1,1.0
the uci machine,1,1,1.0
learning repository and,1,1,1.0
repository and elena,1,1,1.0
and elena project,1,1,1.0
elena project these,1,1,1.0
project these datasets,1,1,1.0
these datasets vary,1,1,1.0
datasets vary in,1,1,1.0
vary in size,1,1,1.0
in size and,1,1,1.0
size and class,1,1,1.0
class distributions to,1,1,1.0
distributions to ensure,1,1,1.0
to ensure a,1,1,1.0
ensure a thorough,1,1,1.0
a thorough assessment,1,1,1.0
thorough assessment of,1,1,1.0
assessment of performance,1,1,1.0
of performance table,1,1,1.0
table i summarizes,1,1,1.0
i summarizes the,1,1,1.0
summarizes the characteristics,1,1,1.0
in our simulation,2,1,2.0
our simulation since,1,1,1.0
simulation since several,1,1,1.0
since several of,1,1,1.0
several of the,1,1,1.0
the original datasets,1,1,1.0
original datasets are,1,1,1.0
datasets are multiclass,1,1,1.0
are multiclass data,1,1,1.0
multiclass data we,1,1,1.0
data we modiﬁed,1,1,1.0
we modiﬁed those,1,1,1.0
modiﬁed those datasets,1,1,1.0
those datasets following,1,1,1.0
datasets following suggestions,1,1,1.0
following suggestions in,1,1,1.0
suggestions in literature,1,1,1.0
in literature to,1,1,1.0
literature to make,1,1,1.0
to make them,1,1,1.0
make them into,1,1,1.0
them into datasets,1,1,1.0
into datasets table,1,1,1.0
datasets table ii,1,1,1.0
table ii shows,1,1,1.0
ii shows the,1,1,1.0
shows the modiﬁcations,1,1,1.0
the modiﬁcations that,1,1,1.0
modiﬁcations that we,1,1,1.0
that we used,1,1,1.0
we used in,1,1,1.0
used in this,2,1,2.0
this paper to,1,1,1.0
paper to create,1,1,1.0
create the minority,1,1,1.0
majority classes assessment,1,1,1.0
classes assessment metrics,1,1,1.0
assessment metrics under,1,1,1.0
metrics under the,1,1,1.0
under the imbalanced,1,1,1.0
imbalanced learning scenario,1,1,1.0
learning scenario the,1,1,1.0
scenario the conventional,1,1,1.0
the conventional assessment,1,1,1.0
conventional assessment method,1,1,1.0
assessment method of,1,1,1.0
method of using,1,1,1.0
using a single,1,1,1.0
a single criterion,1,1,1.0
single criterion such,1,1,1.0
criterion such as,1,1,1.0
such as overallchen,1,1,1.0
as overallchen et,1,1,1.0
overallchen et al,1,1,1.0
in boosting table,4,1,4.0
boosting table ii,1,1,1.0
table ii description,1,1,1.0
ii description of,1,1,1.0
description of imbalanced,1,1,1.0
of imbalanced datasets,1,1,1.0
imbalanced datasets dataset,1,1,1.0
datasets dataset minority,1,1,1.0
dataset minority class,1,1,1.0
minority class majority,1,1,1.0
class majority class,1,1,1.0
majority class sonar,1,1,1.0
class sonar class,1,1,1.0
sonar class r,1,1,1.0
class r rock,1,1,1.0
r rock instances,1,1,1.0
rock instances class,1,1,1.0
instances class m,1,1,1.0
class m metal,1,1,1.0
m metal cylinder,1,1,1.0
metal cylinder instances,1,1,1.0
cylinder instances spambase,1,1,1.0
instances spambase spam,1,1,1.0
spambase spam email,1,1,1.0
spam email legitimate,1,1,1.0
email legitimate email,1,1,1.0
legitimate email ionosphere,1,1,1.0
email ionosphere bad,1,1,1.0
ionosphere bad radar,1,1,1.0
bad radar class,1,1,1.0
radar class good,1,1,1.0
class good radar,1,1,1.0
good radar class,1,1,1.0
radar class pid,1,1,1.0
class pid positive,1,1,1.0
pid positive class,1,1,1.0
positive class negative,1,1,1.0
class negative class,1,1,1.0
negative class wine,1,1,1.0
class wine class,1,1,1.0
wine class classes,1,1,1.0
class classes and,1,1,1.0
classes and german,1,1,1.0
and german customers,1,1,1.0
german customers with,1,1,1.0
customers with bad,1,1,1.0
with bad credit,1,1,1.0
bad credit customers,1,1,1.0
credit customers with,1,1,1.0
customers with good,1,1,1.0
with good credit,1,1,1.0
good credit phoneme,1,1,1.0
credit phoneme class,1,1,1.0
phoneme class of,1,1,1.0
class of oral,1,1,1.0
of oral sounds,1,1,1.0
oral sounds class,1,1,1.0
sounds class class,1,1,1.0
class class of,1,1,1.0
class of nasal,1,1,1.0
of nasal sounds,1,1,1.0
nasal sounds class,1,1,1.0
sounds class vehicle,1,1,1.0
class vehicle class,1,1,1.0
vehicle class of,1,1,1.0
class of van,1,1,1.0
of van classes,1,1,1.0
van classes of,1,1,1.0
classes of opel,1,1,1.0
of opel saas,1,1,1.0
opel saas and,1,1,1.0
saas and bus,1,1,1.0
and bus texture,1,1,1.0
bus texture classes,1,1,1.0
texture classes of,1,1,1.0
classes of and,2,1,2.0
of and classes,1,1,1.0
and classes of,1,1,1.0
of and segment,1,1,1.0
and segment class,1,1,1.0
segment class of,1,1,1.0
class of brickface,1,1,1.0
of brickface classes,1,1,1.0
brickface classes of,1,1,1.0
classes of sky,1,1,1.0
of sky foliage,1,1,1.0
sky foliage cement,1,1,1.0
foliage cement window,1,1,1.0
cement window path,1,1,1.0
window path and,1,1,1.0
path and grass,1,1,1.0
and grass classes,1,1,1.0
grass classes of,1,1,1.0
classes of horizontal,1,1,1.0
of horizontal line,1,1,1.0
horizontal line graphic,1,1,1.0
line graphic class,1,1,1.0
graphic class of,1,1,1.0
class of text,1,1,1.0
of text vertical,1,1,1.0
text vertical line,1,1,1.0
vertical line and,1,1,1.0
line and picture,1,1,1.0
and picture satimage,1,1,1.0
picture satimage class,1,1,1.0
satimage class of,1,1,1.0
class of damp,1,1,1.0
of damp grey,1,1,1.0
damp grey soil,2,1,2.0
grey soil classes,1,1,1.0
soil classes of,1,1,1.0
classes of red,1,1,1.0
of red soil,1,1,1.0
red soil cotton,1,1,1.0
soil cotton crop,1,1,1.0
cotton crop grey,1,1,1.0
crop grey soil,1,1,1.0
grey soil soil,1,1,1.0
soil soil with,1,1,1.0
soil with vegetation,1,1,1.0
with vegetation stubble,1,1,1.0
vegetation stubble and,1,1,1.0
stubble and very,1,1,1.0
and very damp,1,1,1.0
very damp grey,1,1,1.0
grey soil class,1,1,1.0
soil class of,1,1,1.0
class of digit,1,1,1.0
of digit classes,1,1,1.0
digit classes of,1,1,1.0
classes of digits,1,1,1.0
of digits and,1,1,1.0
digits and vow,1,1,1.0
and vow e,1,1,1.0
e l class,1,1,1.0
l class classes,1,1,1.0
class classes of,1,1,1.0
classes of to,1,1,1.0
of to abalone,1,1,1.0
to abalone class,1,1,1.0
abalone class of,1,1,1.0
class of class,1,1,1.0
of class of,1,1,1.0
class of glass,1,1,1.0
of glass class,1,1,1.0
glass class tableware,1,1,1.0
class tableware all,1,1,1.0
tableware all other,1,1,1.0
other classes yeast,1,1,1.0
classes yeast class,1,1,1.0
yeast class of,1,1,1.0
class of pox,1,1,1.0
of pox class,1,1,1.0
pox class of,1,1,1.0
class of cyt,1,1,1.0
of cyt letter,1,1,1.0
cyt letter class,1,1,1.0
letter class of,1,1,1.0
class of letter,1,1,1.0
of letter z,1,1,1.0
letter z classes,1,1,1.0
z classes of,1,1,1.0
classes of letters,1,1,1.0
of letters a,1,1,1.0
letters a y,1,1,1.0
a y shuttle,1,1,1.0
y shuttle class,1,1,1.0
shuttle class of,1,1,1.0
class of fpv,1,1,1.0
of fpv close,1,1,1.0
fpv close classes,1,1,1.0
close classes of,1,1,1.0
classes of rad,1,1,1.0
of rad flow,1,1,1.0
rad flow fpv,1,1,1.0
flow fpv open,1,1,1.0
fpv open high,1,1,1.0
open high bypass,1,1,1.0
high bypass bpv,1,1,1.0
bypass bpv close,1,1,1.0
bpv close and,1,1,1.0
close and bpv,1,1,1.0
and bpv open,1,1,1.0
bpv open accuracy,1,1,1.0
open accuracy oa,1,1,1.0
accuracy oa may,1,1,1.0
oa may not,1,1,1.0
not be able,1,1,1.0
be able to,1,1,1.0
able to provide,1,1,1.0
provide a comprehensive,1,1,1.0
a comprehensive assessment,1,1,1.0
comprehensive assessment of,1,1,1.0
assessment of the,1,1,1.0
learning algorithm considering,1,1,1.0
algorithm considering a,1,1,1.0
considering a simple,1,1,1.0
a simple case,1,1,1.0
simple case of,1,1,1.0
case of a,1,1,1.0
given dataset with,1,1,1.0
dataset with minority,1,1,1.0
with minority class,1,1,1.0
class examples and,2,1,2.0
examples and majority,1,1,1.0
class examples a,1,1,1.0
examples a naïve,1,1,1.0
a naïve approach,1,1,1.0
naïve approach of,1,1,1.0
approach of classifying,1,1,1.0
of classifying every,1,1,1.0
classifying every example,1,1,1.0
every example to,1,1,1.0
example to be,1,1,1.0
be the majority,1,1,1.0
class can at,1,1,1.0
can at best,1,1,1.0
at best provide,1,1,1.0
best provide an,1,1,1.0
provide an oa,1,1,1.0
an oa of,1,1,1.0
oa of over,1,1,1.0
of over the,1,1,1.0
over the entire,1,1,1.0
entire dataset however,1,1,1.0
dataset however in,1,1,1.0
many applications such,1,1,1.0
such as biomedical,1,1,1.0
as biomedical data,1,1,1.0
data analysis suc,1,1,1.0
analysis suc h,1,1,1.0
suc h a,1,1,1.0
h a classiﬁcation,1,1,1.0
a classiﬁcation performance,1,1,1.0
classiﬁcation performance would,1,1,1.0
performance would be,1,1,1.0
would be unacceptable,1,1,1.0
be unacceptable as,1,1,1.0
unacceptable as it,1,1,1.0
as it misclassiﬁes,1,1,1.0
it misclassiﬁes all,1,1,1.0
misclassiﬁes all the,1,1,1.0
all the minority,1,1,1.0
the minority cases,1,1,1.0
minority cases which,1,1,1.0
cases which generally,1,1,1.0
which generally are,1,1,1.0
generally are more,1,1,1.0
more important in,1,1,1.0
important in such,1,1,1.0
in such situations,1,1,1.0
such situations as,1,1,1.0
situations as a,1,1,1.0
result the oa,1,1,1.0
the oa by,1,1,1.0
oa by itself,1,1,1.0
by itself may,1,1,1.0
itself may not,1,1,1.0
not be sufﬁcient,1,1,1.0
be sufﬁcient in,1,1,1.0
sufﬁcient in evaluating,1,1,1.0
in evaluating the,1,1,1.0
evaluating the classiﬁcation,1,1,1.0
classiﬁcation performance fo,1,1,1.0
performance fo r,1,1,1.0
fo r imbalanced,1,1,1.0
r imbalanced learning,1,1,1.0
learning problems in,1,1,1.0
in our simulations,1,1,1.0
our simulations we,1,1,1.0
simulations we adopt,1,1,1.0
we adopt ﬁve,1,1,1.0
adopt ﬁve major,1,1,1.0
ﬁve major assessment,1,1,1.0
major assessment metrics,1,1,1.0
assessment metrics related,1,1,1.0
metrics related to,1,1,1.0
to the confusion,1,1,1.0
matrix for analysis,1,1,1.0
for analysis oa,1,1,1.0
analysis oa precision,1,1,1.0
oa precision recall,4,1,4.0
recall and the,1,1,1.0
and the detailed,1,1,1.0
the detailed discussions,1,1,1.0
detailed discussions on,1,1,1.0
discussions on these,1,1,1.0
on these metrics,1,1,1.0
these metrics and,1,1,1.0
metrics and their,1,1,1.0
and their appli,1,1,1.0
their appli cations,1,1,1.0
appli cations for,1,1,1.0
cations for imbalanced,1,1,1.0
imbalanced learning can,1,1,1.0
learning can be,1,1,1.0
found in in,1,1,1.0
in in addition,1,1,1.0
in addition to,1,1,1.0
addition to these,1,1,1.0
to these singular,1,1,1.0
these singular assessment,1,1,1.0
singular assessment metrics,1,1,1.0
assessment metrics we,1,1,1.0
metrics we also,1,1,1.0
we also adopted,1,1,1.0
also adopted the,1,1,1.0
adopted the roc,1,1,1.0
the roc graph,1,1,1.0
roc graph for,1,1,1.0
graph for evaluation,1,1,1.0
for evaluation in,1,1,1.0
evaluation in this,1,1,1.0
this paper brieﬂy,1,1,1.0
paper brieﬂy speaking,1,1,1.0
brieﬂy speaking the,1,1,1.0
speaking the roc,1,1,1.0
roc space is,1,1,1.0
is established by,1,1,1.0
established by plotting,1,1,1.0
by plotting the,1,1,1.0
plotting the tps,1,1,1.0
the tps rate,1,1,1.0
tps rate tp,1,1,1.0
rate tp over,1,1,1.0
tp over false,1,1,1.0
over false positives,1,1,1.0
false positives rate,1,1,1.0
positives rate fp,1,1,1.0
rate fp h,1,1,1.0
fp h e,1,1,1.0
h e roc,1,1,1.0
e roc curves,1,1,1.0
roc curves are,1,1,1.0
curves are normally,1,1,1.0
are normally formulated,1,1,1.0
normally formulated by,1,1,1.0
formulated by adjusting,1,1,1.0
adjusting the decision,1,1,1.0
decision threshold to,1,1,1.0
threshold to generate,1,1,1.0
generate a seri,1,1,1.0
a seri es,1,1,1.0
seri es of,1,1,1.0
es of points,1,1,1.0
in the roc,2,1,2.0
roc space in,1,1,1.0
order to assess,1,1,1.0
assess different classiﬁers,1,1,1.0
different classiﬁers performance,1,1,1.0
classiﬁers performance in,1,1,1.0
this case one,2,1,2.0
case one generally,1,1,1.0
one generally uses,1,1,1.0
generally uses the,1,1,1.0
uses the auc,1,1,1.0
the auc as,1,1,1.0
auc as an,1,1,1.0
as an evaluation,1,1,1.0
an evaluation criterion,1,1,1.0
evaluation criterion a,1,1,1.0
criterion a detailed,1,1,1.0
a detailed discussion,1,1,1.0
detailed discussion of,1,1,1.0
discussion of roc,1,1,1.0
analysis and its,1,1,1.0
and its assessment,1,1,1.0
its assessment for,1,1,1.0
assessment for classiﬁer,1,1,1.0
for classiﬁer performances,1,1,1.0
classiﬁer performances can,1,1,1.0
performances can be,1,1,1.0
found in and,1,1,1.0
in and we,1,1,1.0
and we would,1,1,1.0
we would also,1,1,1.0
like to note,2,1,2.0
note that there,1,1,1.0
there are other,2,1,2.0
are other metrics,1,1,1.0
other metrics that,1,1,1.0
metrics that can,1,1,1.0
that can potentially,1,1,1.0
can potentially be,1,1,1.0
potentially be used,1,1,1.0
assess the imbalanced,1,1,1.0
imbalanced learning performance,1,1,1.0
performance for instance,1,1,1.0
instance it was,1,1,1.0
it was recently,1,1,1.0
was recently presented,1,1,1.0
recently presented in,1,1,1.0
presented in that,1,1,1.0
in that h,1,1,1.0
that h could,1,1,1.0
h could be,1,1,1.0
could be a,2,1,2.0
be a qualiﬁed,1,1,1.0
a qualiﬁed alternative,1,1,1.0
qualiﬁed alternative metric,1,1,1.0
alternative metric of,1,1,1.0
metric of auc,1,1,1.0
of auc the,1,1,1.0
auc the major,1,1,1.0
the major motivation,1,1,1.0
major motivation of,1,1,1.0
motivation of h,1,1,1.0
of h is,1,1,1.0
h is based,1,1,1.0
on the fact,1,1,1.0
fact that auc,1,1,1.0
that auc is,1,1,1.0
auc is equivalent,1,1,1.0
is equivalent to,1,1,1.0
equivalent to averaging,1,1,1.0
to averaging the,1,1,1.0
averaging the misclassiﬁcation,1,1,1.0
the misclassiﬁcation loss,1,1,1.0
misclassiﬁcation loss over,1,1,1.0
loss over a,1,1,1.0
over a cost,1,1,1.0
a cost ratio,1,1,1.0
cost ratio distribution,1,1,1.0
ratio distribution dependent,1,1,1.0
distribution dependent on,1,1,1.0
dependent on the,1,1,1.0
the score distributions,1,1,1.0
score distributions which,1,1,1.0
distributions which are,1,1,1.0
which are decided,1,1,1.0
are decided by,1,1,1.0
decided by the,1,1,1.0
by the classiﬁer,1,1,1.0
the classiﬁer itself,1,1,1.0
classiﬁer itself rather,1,1,1.0
itself rather than,1,1,1.0
than the roc,1,1,1.0
roc curve averaged,1,1,1.0
curve averaged roc,1,1,1.0
averaged roc curve,4,1,4.0
roc curve roc,1,1,1.0
curve roc curve,1,1,1.0
roc curve point,2,1,2.0
curve point on,2,1,2.0
point on roc,2,1,2.0
on roc curve,2,1,2.0
roc curve l,1,1,1.0
curve l point,1,1,1.0
l point on,1,1,1.0
point on averaged,1,1,1.0
on averaged roc,1,1,1.0
roc curve x,1,1,1.0
curve x fig,1,1,1.0
x fig vertical,1,1,1.0
fig vertical averaging,1,1,1.0
vertical averaging approach,2,1,2.0
averaging approach of,1,1,1.0
approach of roc,1,1,1.0
roc curves target,1,1,1.0
curves target dataset,1,1,1.0
target dataset this,1,1,1.0
dataset this may,1,1,1.0
this may introduce,1,1,1.0
may introduce undesired,1,1,1.0
introduce undesired subjectivity,1,1,1.0
undesired subjectivity into,1,1,1.0
subjectivity into performance,1,1,1.0
into performance evaluation,1,1,1.0
performance evaluation h,1,1,1.0
evaluation h targets,1,1,1.0
h targets this,1,1,1.0
this ﬂaw by,1,1,1.0
ﬂaw by decoupling,1,1,1.0
by decoupling the,1,1,1.0
decoupling the weight,1,1,1.0
the weight function,2,1,2.0
weight function for,1,1,1.0
function for loss,1,1,1.0
for loss calculation,1,1,1.0
loss calculation from,1,1,1.0
calculation from score,1,1,1.0
from score distributions,1,1,1.0
score distributions for,1,1,1.0
distributions for instance,1,1,1.0
instance it can,1,1,1.0
it can apply,1,1,1.0
can apply a,1,1,1.0
apply a beta,1,1,1.0
a beta distribution,1,1,1.0
beta distribution to,1,1,1.0
distribution to simulate,1,1,1.0
to simulate the,1,1,1.0
simulate the weight,1,1,1.0
weight function which,1,1,1.0
function which ensures,1,1,1.0
which ensures objectivity,1,1,1.0
ensures objectivity across,1,1,1.0
objectivity across all,1,1,1.0
across all algorithms,1,1,1.0
all algorithms under,1,1,1.0
algorithms under comparison,1,1,1.0
under comparison the,1,1,1.0
comparison the interested,1,1,1.0
the interested reader,1,1,1.0
interested reader can,1,1,1.0
reader can refer,1,1,1.0
refer to for,1,1,1.0
to for further,1,1,1.0
for further details,1,1,1.0
further details on,1,1,1.0
details on h,1,1,1.0
on h and,1,1,1.0
h and for,1,1,1.0
and for a,1,1,1.0
for a ical,1,1,1.0
a ical review,1,1,1.0
ical review of,1,1,1.0
of the assessment,1,1,1.0
the assessment met,1,1,1.0
assessment met rics,1,1,1.0
met rics for,1,1,1.0
rics for imbalanced,1,1,1.0
learning in order,1,1,1.0
order to reﬂect,1,1,1.0
to reﬂect the,1,1,1.0
reﬂect the roc,1,1,1.0
roc curve characteristics,1,1,1.0
curve characteristics for,1,1,1.0
characteristics for all,1,1,1.0
the random runs,2,1,2.0
random runs we,1,1,1.0
runs we adopt,1,1,1.0
we adopt the,1,1,1.0
adopt the vertical,1,1,1.0
the vertical averaging,2,1,2.0
averaging approach in,1,1,1.0
approach in to,1,1,1.0
in to plot,1,1,1.0
to plot the,1,1,1.0
plot the averaged,1,1,1.0
the averaged roc,5,1,5.0
averaged roc curves,8,1,8.0
roc curves our,1,1,1.0
curves our implementation,1,1,1.0
of the vertical,1,1,1.0
vertical averaging method,1,1,1.0
averaging method is,1,1,1.0
method is illustrated,1,1,1.0
is illustrated in,1,1,1.0
illustrated in fig,1,1,1.0
in fig assume,1,1,1.0
fig assume one,1,1,1.0
assume one would,1,1,1.0
one would like,1,1,1.0
like to average,1,1,1.0
to average two,1,1,1.0
average two roc,1,1,1.0
two roc curves,1,1,1.0
roc curves and,1,1,1.0
curves and each,1,1,1.0
and each of,12,1,12.0
each of which,1,1,1.0
of which is,1,1,1.0
which is formed,1,1,1.0
is formed by,1,1,1.0
formed by a,1,1,1.0
by a series,1,1,1.0
a series of,1,1,1.0
series of points,1,1,1.0
roc space the,1,1,1.0
space the ﬁrst,1,1,1.0
ﬁrst step is,1,1,1.0
is to evenly,1,1,1.0
to evenly divide,1,1,1.0
evenly divide the,1,1,1.0
divide the range,1,1,1.0
the range of,1,1,1.0
range of fp,1,1,1.0
of fp into,1,1,1.0
fp into a,1,1,1.0
into a set,1,1,1.0
set of intervals,1,1,1.0
of intervals then,1,1,1.0
intervals then at,1,1,1.0
then at each,1,1,1.0
at each interval,1,1,1.0
each interval ﬁnd,1,1,1.0
interval ﬁnd the,1,1,1.0
ﬁnd the ieee,1,1,1.0
the ieee transactions,1,1,1.0
no october table,3,1,3.0
october table iii,1,1,1.0
table iii eva,1,1,1.0
iii eva l,1,1,1.0
eva l uat,1,1,1.0
l uat i,1,1,1.0
uat i o,1,1,1.0
i o nmetrics,1,1,1.0
o nmetrics and,1,1,1.0
nmetrics and performance,1,1,1.0
and performance comparison,1,1,1.0
performance comparison dataset,1,1,1.0
comparison dataset methods,1,1,1.0
dataset methods oa,2,1,2.0
methods oa precision,2,1,2.0
recall auc sonar,1,1,1.0
auc sonar ramoboost,1,1,1.0
sonar ramoboost smoteboost,1,1,1.0
ramoboost smoteboost smote,34,1,34.0
adasyn adacost borderlinesmote,36,1,36.0
adacost borderlinesmote spambase,1,1,1.0
borderlinesmote spambase ramoboost,1,1,1.0
spambase ramoboost smoteboost,1,1,1.0
adacost borderlinesmote ionosphere,1,1,1.0
borderlinesmote ionosphere ramoboost,1,1,1.0
ionosphere ramoboost smoteboost,1,1,1.0
adacost borderlinesmote pid,1,1,1.0
borderlinesmote pid ramoboost,1,1,1.0
pid ramoboost smoteboost,1,1,1.0
adacost borderlinesmote wine,1,1,1.0
borderlinesmote wine ramoboost,1,1,1.0
wine ramoboost smoteboost,1,1,1.0
adacost borderlinesmote german,1,1,1.0
borderlinesmote german ramoboost,1,1,1.0
german ramoboost smoteboost,1,1,1.0
adacost borderlinesmote phoneme,1,1,1.0
borderlinesmote phoneme ramoboost,1,1,1.0
phoneme ramoboost smoteboost,1,1,1.0
adacost borderlinesmote vehicle,1,1,1.0
borderlinesmote vehicle ramoboost,1,1,1.0
vehicle ramoboost smoteboost,1,1,1.0
adacost borderlinesmote texture,2,1,2.0
borderlinesmote texture ramoboost,1,1,1.0
texture ramoboost smoteboost,1,1,1.0
adacost borderlinesmote segment,1,1,1.0
borderlinesmote segment ramoboost,1,1,1.0
segment ramoboost smoteboost,1,1,1.0
adacost borderlinesmote et,1,1,1.0
borderlinesmote et al,1,1,1.0
in boosting dataset,1,1,1.0
boosting dataset methods,1,1,1.0
recall auc ramoboost,1,1,1.0
auc ramoboost smoteboost,1,1,1.0
adacost borderlinesmote satimage,1,1,1.0
borderlinesmote satimage ramoboost,1,1,1.0
satimage ramoboost smoteboost,1,1,1.0
adacost borderlinesmote ramoboost,1,1,1.0
borderlinesmote ramoboost smoteboost,1,1,1.0
adacost borderlinesmote vowel,1,1,1.0
borderlinesmote vowel ramoboost,1,1,1.0
vowel ramoboost smoteboost,1,1,1.0
adacost borderlinesmote abalone,1,1,1.0
borderlinesmote abalone ramoboost,1,1,1.0
abalone ramoboost smoteboost,1,1,1.0
adacost borderlinesmote glass,1,1,1.0
borderlinesmote glass ramoboost,1,1,1.0
glass ramoboost smoteboost,1,1,1.0
adacost borderlinesmote yeast,1,1,1.0
borderlinesmote yeast ramoboost,1,1,1.0
yeast ramoboost smoteboost,1,1,1.0
adacost borderlinesmote letter,1,1,1.0
borderlinesmote letter ramoboost,1,1,1.0
letter ramoboost smoteboost,1,1,1.0
adacost borderlinesmote shuttle,1,1,1.0
borderlinesmote shuttle ramoboost,1,1,1.0
shuttle ramoboost smoteboost,1,1,1.0
adacost borderlinesmote tp,1,1,1.0
borderlinesmote tp values,1,1,1.0
tp values of,1,1,1.0
values of each,1,1,1.0
of each roc,1,1,1.0
each roc curve,1,1,1.0
roc curve and,1,1,1.0
curve and average,1,1,1.0
and average them,1,1,1.0
average them in,1,1,1.0
them in fig,1,1,1.0
in fig and,1,1,1.0
fig and are,1,1,1.0
and are the,1,1,1.0
are the points,1,1,1.0
the points from,1,1,1.0
points from and,1,1,1.0
from and corresponding,1,1,1.0
and corresponding to,1,1,1.0
to the interval,1,1,1.0
the interval fp,1,1,1.0
interval fp by,1,1,1.0
fp by averaging,1,1,1.0
by averaging their,1,1,1.0
averaging their tp,1,1,1.0
their tp values,1,1,1.0
tp values the,1,1,1.0
values the corresponding,1,1,1.0
the corresponding roc,1,1,1.0
corresponding roc point,1,1,1.0
roc point on,1,1,1.0
point on the,2,1,2.0
on the averaged,6,1,6.0
curve is obtained,1,1,1.0
is obtained however,1,1,1.0
obtained however there,1,1,1.0
however there exist,1,1,1.0
exist some roc,1,1,1.0
some roc curves,1,1,1.0
roc curves that,1,1,1.0
curves that do,1,1,1.0
do not have,1,1,1.0
not have corresponding,1,1,1.0
have corresponding points,1,1,1.0
corresponding points on,1,1,1.0
points on certain,1,1,1.0
on certain intervals,1,1,1.0
certain intervals in,1,1,1.0
intervals in this,1,1,1.0
case one can,1,1,1.0
one can use,1,1,1.0
use the linear,1,1,1.0
the linear interpolation,2,1,2.0
linear interpolation method,1,1,1.0
interpolation method to,1,1,1.0
method to obtain,1,1,1.0
obtain the averaged,1,1,1.0
averaged roc points,1,1,1.0
roc points for,1,1,1.0
points for instance,1,1,1.0
instance in fig,1,1,1.0
in fig the,1,1,1.0
fig the point,1,1,1.0
the point corresponding,1,1,1.0
point corresponding to,1,1,1.0
corresponding to fp,1,1,1.0
to fp is,1,1,1.0
fp is calculated,1,1,1.0
is calculated based,1,1,1.0
calculated based on,1,1,1.0
on the linear,1,1,1.0
linear interpolation of,1,1,1.0
interpolation of the,1,1,1.0
the two neighboring,1,1,1.0
two neighboring points,1,1,1.0
neighboring points and,1,1,1.0
points and n,1,1,1.0
and n c,1,1,1.0
n c is,1,1,1.0
c is obtained,1,1,1.0
is obtained it,1,1,1.0
can be averaged,1,1,1.0
be averaged with,1,1,1.0
averaged with to,1,1,1.0
with to get,1,1,1.0
to get the,1,1,1.0
get the corresponding,1,1,1.0
the corresponding point,1,1,1.0
corresponding point on,1,1,1.0
roc curve our,1,1,1.0
curve our auc,1,1,1.0
our auc results,1,1,1.0
auc results presented,1,1,1.0
results presented in,2,1,2.0
this section are,1,1,1.0
section are based,1,1,1.0
on the average,2,1,2.0
the average of,2,1,2.0
average of all,1,1,1.0
of all random,1,1,1.0
all random runs,1,1,1.0
random runs according,1,1,1.0
runs according to,1,1,1.0
to the vertical,1,1,1.0
the vertical aver,1,1,1.0
vertical aver aging,1,1,1.0
aver aging ieee,1,1,1.0
aging ieee transactions,1,1,1.0
c d e,1,1,1.0
d e f,1,1,1.0
e f ramoboost,1,1,1.0
f ramoboost smoteboost,1,1,1.0
adacost borderlinesmote smote,2,1,2.0
borderlinesmote smote adasyn,1,1,1.0
adasyn adacost borerlinesmote,1,1,1.0
adacost borerlinesmote ramoboost,1,1,1.0
borerlinesmote ramoboost smoteboost,1,1,1.0
ramoboost smoteboost smoteboost,1,1,1.0
smoteboost smoteboost smote,1,1,1.0
smote adasyn borderlinesmote,7,1,7.0
adasyn borderlinesmote ramoboost,2,1,2.0
borderlinesmote ramoboost adacost,1,1,1.0
ramoboost adacost ramoboost,1,1,1.0
adacost ramoboost smoteboost,1,1,1.0
borderlinesmote smote borderlinesmote,1,1,1.0
smote borderlinesmote adasyn,1,1,1.0
borderlinesmote adasyn adacost,1,1,1.0
adasyn adacost smoteboost,1,1,1.0
adacost smoteboost ramoboost,1,1,1.0
smoteboost ramoboost smoteboost,1,1,1.0
ramoboost smoteboost adacostsmote,1,1,1.0
smoteboost adacostsmote adasyn,1,1,1.0
adacostsmote adasyn borderlinesmote,1,1,1.0
borderlinesmote ramoboost fig,1,1,1.0
ramoboost fig averaged,1,1,1.0
fig averaged roc,1,1,1.0
roc curves for,7,1,7.0
curves for ramoboost,1,1,1.0
for ramoboost smoteboost,2,1,2.0
smoteboost smote adas,1,1,1.0
smote adas yn,1,1,1.0
adas yn adacost,1,1,1.0
yn adacost borderlinesmote,1,1,1.0
adacost borderlinesmote and,1,1,1.0
borderlinesmote and methods,2,1,2.0
and methods a,1,1,1.0
methods a averaged,1,1,1.0
a averaged roc,1,1,1.0
curves for german,1,1,1.0
for german dataset,1,1,1.0
german dataset b,1,1,1.0
dataset b averaged,1,1,1.0
b averaged roc,1,1,1.0
curves for ionosphere,1,1,1.0
for ionosphere dataset,1,1,1.0
ionosphere dataset c,1,1,1.0
dataset c averaged,1,1,1.0
c averaged roc,1,1,1.0
curves for dataset,1,1,1.0
for dataset d,1,1,1.0
dataset d averaged,1,1,1.0
d averaged roc,1,1,1.0
curves for phoneme,1,1,1.0
for phoneme dataset,1,1,1.0
phoneme dataset e,1,1,1.0
dataset e averaged,1,1,1.0
e averaged roc,1,1,1.0
curves for satim,1,1,1.0
for satim age,1,1,1.0
satim age dataset,1,1,1.0
age dataset f,1,1,1.0
dataset f averaged,1,1,1.0
f averaged roc,1,1,1.0
curves for abalone,1,1,1.0
for abalone dataset,1,1,1.0
abalone dataset table,1,1,1.0
dataset table iv,1,1,1.0
table iv simula,1,1,1.0
iv simula tion,1,1,1.0
simula tion s,5,1,5.0
tion s ignificance,5,1,5.0
s ignificance test,12,1,12.0
ignificance test of,12,1,12.0
test of averaged,8,1,8.0
of averaged auc,10,1,10.0
averaged auc b,8,1,8.0
auc b etween,8,1,8.0
b etween ramob,8,1,8.0
etween ramob oost,8,1,8.0
ramob oost and,12,1,12.0
oost and smoteb,1,1,1.0
and smoteb oost,1,1,1.0
smoteb oost dataset,1,1,1.0
oost dataset ramoboost,1,1,1.0
dataset ramoboost smoteboost,3,1,3.0
ramoboost smoteboost difference,1,1,1.0
smoteboost difference rank,1,1,1.0
difference rank sonar,2,1,2.0
rank sonar spambase,2,1,2.0
letter shuttle n,1,1,1.0
shuttle n d,1,1,1.0
n d in,1,1,1.0
d in order,1,1,1.0
order to evaluate,1,1,1.0
evaluate the signiﬁcance,1,1,1.0
the signiﬁcance of,1,1,1.0
signiﬁcance of the,1,1,1.0
of the simulation,1,1,1.0
the simulation results,6,1,6.0
simulation results of,4,1,4.0
of the comparative,5,1,5.0
the comparative algorithms,6,1,6.0
comparative algorithms wilcoxon,1,1,1.0
algorithms wilcoxon test,1,1,1.0
wilcoxon test is,3,1,3.0
test is used,1,1,1.0
this paper wilcoxon,1,1,1.0
paper wilcoxon test,1,1,1.0
test is a,1,1,1.0
is a metric,1,1,1.0
a metric statistical,1,1,1.0
metric statistical procedure,1,1,1.0
statistical procedure for,1,1,1.0
procedure for comparing,1,1,1.0
for comparing two,1,1,1.0
comparing two samples,1,1,1.0
two samples that,1,1,1.0
samples that are,1,1,1.0
that are paired,1,1,1.0
are paired or,1,1,1.0
paired or related,1,1,1.0
or related it,1,1,1.0
related it assumes,1,1,1.0
it assumes commensurability,1,1,1.0
assumes commensurability of,1,1,1.0
commensurability of ferences,1,1,1.0
of ferences but,1,1,1.0
ferences but only,1,1,1.0
but only qualitatively,1,1,1.0
only qualitatively greater,1,1,1.0
qualitatively greater differences,1,1,1.0
greater differences still,1,1,1.0
differences still count,1,1,1.0
still count more,1,1,1.0
count more which,1,1,1.0
more which is,1,1,1.0
which is probably,1,1,1.0
is probably desired,1,1,1.0
probably desired but,1,1,1.0
desired but the,1,1,1.0
but the absolute,1,1,1.0
the absolute magnitudes,1,1,1.0
absolute magnitudes are,1,1,1.0
magnitudes are ignored,1,1,1.0
are ignored from,1,1,1.0
ignored from a,1,1,1.0
from a statistical,2,1,2.0
a statistical point,2,1,2.0
statistical point of,2,1,2.0
point of view,2,1,2.0
of view the,1,1,1.0
view the test,1,1,1.0
the test is,1,1,1.0
test is safer,1,1,1.0
is safer since,1,1,1.0
safer since it,1,1,1.0
since it does,1,1,1.0
does not assume,1,1,1.0
not assume normal,1,1,1.0
assume normal distributions,1,1,1.0
normal distributions also,1,1,1.0
distributions also outliers,1,1,1.0
also outliers exceptionally,1,1,1.0
outliers exceptionally performances,1,1,1.0
exceptionally performances on,1,1,1.0
performances on a,1,1,1.0
on a few,1,1,1.0
a few datasets,1,1,1.0
few datasets have,1,1,1.0
datasets have less,1,1,1.0
have less effect,1,1,1.0
less effect on,1,1,1.0
on the wilcoxon,1,1,1.0
the wilcoxon than,1,1,1.0
wilcoxon than on,1,1,1.0
than on the,1,1,1.0
on the suppose,1,1,1.0
the suppose there,1,1,1.0
suppose there are,1,1,1.0
there are n,1,1,1.0
are n objects,1,1,1.0
n objects to,1,1,1.0
objects to be,2,1,2.0
to be observed,1,1,1.0
be observed by,1,1,1.0
observed by two,1,1,1.0
by two gorithms,1,1,1.0
two gorithms let,1,1,1.0
gorithms let us,1,1,1.0
let us denote,1,1,1.0
us denote the,1,1,1.0
denote the difference,1,1,1.0
the difference value,2,1,2.0
difference value of,2,1,2.0
the two table,1,1,1.0
two table v,1,1,1.0
table v simula,1,1,1.0
v simula tion,1,1,1.0
oost and adacost,1,1,1.0
and adacost dataset,1,1,1.0
adacost dataset ramoboost,1,1,1.0
dataset ramoboost adacost,1,1,1.0
ramoboost adacost difference,1,1,1.0
adacost difference rank,1,1,1.0
letter shuttle and,1,1,1.0
shuttle and algorithms,1,1,1.0
and algorithms observation,1,1,1.0
algorithms observation on,1,1,1.0
observation on the,1,1,1.0
on the ith,1,1,1.0
the ith objects,1,1,1.0
ith objects to,1,1,1.0
to be di,1,1,1.0
be di i,1,1,1.0
di i the,1,1,1.0
i the differences,1,1,1.0
differences are ranked,1,1,1.0
are ranked according,1,1,1.0
ranked according to,1,1,1.0
to their solute,1,1,1.0
their solute values,1,1,1.0
solute values ranks,1,1,1.0
values ranks of,1,1,1.0
ranks of the,2,1,2.0
of the tied,1,1,1.0
the tied values,1,1,1.0
tied values are,1,1,1.0
values are averaged,1,1,1.0
are averaged let,1,1,1.0
averaged let stand,1,1,1.0
let stand for,1,1,1.0
stand for the,1,1,1.0
for the sum,1,1,1.0
the sum of,2,1,2.0
sum of the,2,1,2.0
of the ranks,1,1,1.0
the ranks of,1,1,1.0
of the objects,1,1,1.0
the objects on,1,1,1.0
objects on which,1,1,1.0
on which the,1,1,1.0
which the difference,1,1,1.0
the two algorithms,1,1,1.0
two algorithms observations,1,1,1.0
algorithms observations are,1,1,1.0
observations are greater,1,1,1.0
are greater than,1,1,1.0
greater than zero,1,1,1.0
than zero and,1,1,1.0
zero and denote,1,1,1.0
and denote the,1,1,1.0
denote the sum,1,1,1.0
of the opposite,1,1,1.0
the opposite ranks,1,1,1.0
opposite ranks of,1,1,1.0
ranks of di,1,1,1.0
of di are,1,1,1.0
di are evenly,1,1,1.0
are evenly split,1,1,1.0
evenly split between,1,1,1.0
split between and,1,1,1.0
between and equations,1,1,1.0
and equations and,1,1,1.0
equations and conclude,1,1,1.0
and conclude the,1,1,1.0
conclude the calculations,1,1,1.0
the calculations of,1,1,1.0
calculations of and,1,1,1.0
of and di,1,1,1.0
and di rank,1,1,1.0
di rank di,3,1,3.0
rank di rank,3,1,3.0
di rank chen,1,1,1.0
rank chen et,1,1,1.0
chen et al,1,1,1.0
boosting table vi,1,1,1.0
table vi simula,1,1,1.0
vi simula tion,1,1,1.0
oost and each,10,1,10.0
each of smoteb,10,1,10.0
of smoteb oost,10,1,10.0
smoteb oost smote,10,1,10.0
oost smote adasyn,10,1,10.0
smote adasyn a,9,1,9.0
adasyn a dacost,9,1,9.0
a dacost b,9,1,9.0
dacost b orderline,9,1,9.0
b orderline smote,10,1,10.0
orderline smote and,10,1,10.0
smote and omek,10,1,10.0
and omek ramoboost,5,1,5.0
omek ramoboost smoteboost,5,1,5.0
adacost borderlinesmote t,5,1,5.0
borderlinesmote t table,4,1,4.0
t table vii,1,1,1.0
table vii simula,1,1,1.0
vii simula tion,1,1,1.0
simula tion auc,2,1,2.0
tion auc p,2,1,2.0
auc p erformance,2,1,2.0
p erformance characteristics,2,1,2.0
erformance characteristics dataset,2,1,2.0
characteristics dataset ramo,1,1,1.0
dataset ramo smote,1,1,1.0
ramo smote adasyn,2,1,2.0
adasyn borderlinesmote sonar,1,1,1.0
borderlinesmote sonar spambase,2,1,2.0
letter shuttle if,1,1,1.0
shuttle if we,1,1,1.0
if we set,1,1,1.0
we set t,1,1,1.0
set t min,1,1,1.0
t min with,1,1,1.0
min with a,1,1,1.0
with a signiﬁcance,1,1,1.0
of α and,1,1,1.0
α and the,1,1,1.0
number of observed,1,1,1.0
of observed objects,1,1,1.0
observed objects being,1,1,1.0
objects being n,1,1,1.0
being n the,1,1,1.0
n the signiﬁcance,1,1,1.0
the signiﬁcance value,2,1,2.0
signiﬁcance value n,2,1,2.0
value n that,1,1,1.0
n that t,1,1,1.0
that t should,1,1,1.0
t should be,3,1,3.0
be equal or,1,1,1.0
equal or less,1,1,1.0
or less than,1,1,1.0
than for rejection,1,1,1.0
for rejection of,1,1,1.0
rejection of a,1,1,1.0
of a null,1,1,1.0
a null hypothesis,2,1,2.0
hypothesis can be,1,1,1.0
can be retrieved,1,1,1.0
be retrieved by,1,1,1.0
retrieved by querying,1,1,1.0
by querying the,1,1,1.0
querying the critical,1,1,1.0
the critical value,2,1,2.0
critical value table,3,1,3.0
value table which,1,1,1.0
table which can,1,1,1.0
can be accessed,1,1,1.0
be accessed in,1,1,1.0
accessed in in,1,1,1.0
in in the,1,1,1.0
in the rest,2,1,2.0
rest of this,2,1,2.0
this section wilcoxon,1,1,1.0
section wilcoxon test,1,1,1.0
test is conducted,1,1,1.0
is conducted between,1,1,1.0
conducted between ramoboost,1,1,1.0
between ramoboost and,3,1,3.0
ramoboost and each,1,1,1.0
each of other,1,1,1.0
of other comparative,1,1,1.0
other comparative algorithms,11,1,11.0
comparative algorithms ramoboost,1,1,1.0
algorithms ramoboost smoteboost,1,1,1.0
ramoboost smoteboost ramoboost,1,1,1.0
smoteboost ramoboost adacost,1,1,1.0
ramoboost adacost ramo,1,1,1.0
adacost ramo smote,1,1,1.0
ramo smote etc,1,1,1.0
smote etc in,1,1,1.0
etc in all,1,1,1.0
in all tables,1,1,1.0
all tables presenting,1,1,1.0
tables presenting the,1,1,1.0
presenting the results,1,1,1.0
results of signiﬁcance,1,1,1.0
of signiﬁcance test,1,1,1.0
signiﬁcance test the,1,1,1.0
test the symbol,1,1,1.0
the symbol signiﬁes,1,1,1.0
symbol signiﬁes that,1,1,1.0
signiﬁes that ramoboost,1,1,1.0
that ramoboost is,1,1,1.0
ramoboost is quantitatively,1,1,1.0
is quantitatively better,1,1,1.0
quantitatively better than,1,1,1.0
than the comparative,1,1,1.0
the comparative algorithm,2,1,2.0
comparative algorithm under,1,1,1.0
algorithm under consideration,1,1,1.0
under consideration in,1,1,1.0
consideration in terms,1,1,1.0
of the speciﬁed,1,1,1.0
the speciﬁed assessment,1,1,1.0
speciﬁed assessment metric,1,1,1.0
assessment metric and,1,1,1.0
metric and denotes,1,1,1.0
and denotes the,1,1,1.0
denotes the opposite,1,1,1.0
the opposite whenever,1,1,1.0
opposite whenever there,1,1,1.0
whenever there is,1,1,1.0
is a signiﬁcance,1,1,1.0
a signiﬁcance existing,1,1,1.0
signiﬁcance existing we,1,1,1.0
existing we highlight,1,1,1.0
we highlight the,1,1,1.0
highlight the corresponding,1,1,1.0
the corresponding result,1,1,1.0
corresponding result by,1,1,1.0
result by underscoring,1,1,1.0
by underscoring it,1,1,1.0
underscoring it simulation,1,1,1.0
it simulation results,1,1,1.0
simulation results in,1,1,1.0
results in our,1,1,1.0
our simulation we,1,1,1.0
simulation we use,1,1,1.0
we use boosting,1,1,1.0
use boosting iterations,1,1,1.0
boosting iterations t,1,1,1.0
iterations t in,1,1,1.0
t in the,1,1,1.0
in the algorithm,1,1,1.0
the algorithm as,1,1,1.0
algorithm as suggested,1,1,1.0
as suggested in,2,1,2.0
suggested in for,1,1,1.0
in for ensemble,1,1,1.0
for ensemble learning,1,1,1.0
ensemble learning the,1,1,1.0
learning the number,1,1,1.0
synthetic data generated,1,1,1.0
data generated at,1,1,1.0
boosting iteration is,1,1,1.0
iteration is set,1,1,1.0
set to of,1,1,1.0
to of the,1,1,1.0
number of the,1,1,1.0
minority instances the,1,1,1.0
instances the parameters,1,1,1.0
the parameters and,1,1,1.0
parameters and are,1,1,1.0
and are set,1,1,1.0
are set to,1,1,1.0
be and respectively,1,1,1.0
and respectively the,1,1,1.0
respectively the scaling,1,1,1.0
scaling coefﬁcient α,1,1,1.0
coefﬁcient α is,1,1,1.0
α is set,1,1,1.0
set to which,1,1,1.0
to which was,1,1,1.0
which was sen,1,1,1.0
was sen using,1,1,1.0
sen using techniques,1,1,1.0
using techniques for,1,1,1.0
techniques for optimizing,1,1,1.0
for optimizing boost,1,1,1.0
optimizing boost s,1,1,1.0
boost s performance,1,1,1.0
s performance for,1,1,1.0
performance for smoteboost,1,1,1.0
for smoteboost smote,1,1,1.0
adasyn borderlinesmote and,5,1,5.0
borderlinesmote and the,1,1,1.0
number of est,1,1,1.0
of est neighbors,1,1,1.0
est neighbors is,1,1,1.0
neighbors is set,1,1,1.0
set to ﬁve,1,1,1.0
to ﬁve the,1,1,1.0
ﬁve the cost,1,1,1.0
the cost factor,1,1,1.0
cost factor c,1,1,1.0
factor c for,1,1,1.0
c for adacost,1,1,1.0
for adacost is,1,1,1.0
adacost is set,1,1,1.0
set to three,1,1,1.0
to three according,1,1,1.0
three according to,1,1,1.0
to the suggestion,1,1,1.0
the suggestion of,2,1,2.0
suggestion of c,1,1,1.0
of c should,1,1,1.0
c should be,1,1,1.0
should be an,1,1,1.0
be an integer,1,1,1.0
an integer between,1,1,1.0
integer between and,1,1,1.0
between and following,1,1,1.0
following the suggestion,1,1,1.0
suggestion of the,1,1,1.0
of the signiﬁcance,1,1,1.0
the signiﬁcance test,8,1,8.0
signiﬁcance test are,1,1,1.0
test are conducted,1,1,1.0
are conducted on,1,1,1.0
conducted on the,1,1,1.0
the averaged auc,7,1,7.0
averaged auc of,3,1,3.0
auc of all,1,1,1.0
of all algorithms,1,1,1.0
all algorithms in,1,1,1.0
algorithms in a,1,1,1.0
in a pairwise,1,1,1.0
a pairwise table,1,1,1.0
pairwise table viii,1,1,1.0
table viii simula,1,1,1.0
viii simula tion,1,1,1.0
adasyn adacost b,1,1,1.0
adacost b orderline,1,1,1.0
and omek ramo,1,1,1.0
omek ramo smote,1,1,1.0
adasyn borderlinesmote t,1,1,1.0
borderlinesmote t manner,1,1,1.0
t manner for,1,1,1.0
manner for all,1,1,1.0
all simulations introduced,1,1,1.0
simulations introduced in,1,1,1.0
this section simulation,1,1,1.0
section simulation in,1,1,1.0
simulation in this,1,1,1.0
in this simulation,1,1,1.0
this simulation we,1,1,1.0
simulation we apply,1,1,1.0
we apply all,1,1,1.0
apply all ative,1,1,1.0
all ative algorithms,1,1,1.0
ative algorithms to,1,1,1.0
algorithms to the,1,1,1.0
to the datasets,1,1,1.0
the datasets described,2,1,2.0
datasets described in,2,1,2.0
described in table,2,1,2.0
in table the,1,1,1.0
table the simulation,1,1,1.0
simulation results are,2,1,2.0
results are based,1,1,1.0
average of ten,1,1,1.0
of ten runs,1,1,1.0
ten runs at,1,1,1.0
runs at each,1,1,1.0
at each run,1,1,1.0
each run we,1,1,1.0
run we randomly,1,1,1.0
we randomly select,1,1,1.0
randomly select half,1,1,1.0
select half of,1,1,1.0
dataset as training,1,1,1.0
as training data,1,1,1.0
use the remaining,1,1,1.0
the remaining half,1,1,1.0
remaining half as,1,1,1.0
half as testing,1,1,1.0
as testing data,1,1,1.0
testing data fig,1,1,1.0
data fig gives,1,1,1.0
fig gives several,1,1,1.0
gives several snapshots,1,1,1.0
several snapshots of,1,1,1.0
snapshots of the,1,1,1.0
of the averaged,1,1,1.0
averaged roc graphs,1,1,1.0
roc graphs of,1,1,1.0
graphs of the,1,1,1.0
the ramoboost smoteboost,1,1,1.0
smote adasyn cost,1,1,1.0
adasyn cost borderlinesmote,1,1,1.0
cost borderlinesmote and,1,1,1.0
and methods here,1,1,1.0
methods here fig,1,1,1.0
here fig a,1,1,1.0
fig a f,1,1,1.0
a f represents,1,1,1.0
f represents the,1,1,1.0
represents the results,1,1,1.0
for the german,1,1,1.0
the german ionosphere,1,1,1.0
german ionosphere phoneme,1,1,1.0
ionosphere phoneme satimage,1,1,1.0
phoneme satimage and,1,1,1.0
satimage and abalone,1,1,1.0
and abalone datasets,1,1,1.0
abalone datasets respectively,1,1,1.0
datasets respectively this,1,1,1.0
respectively this ﬁgure,1,1,1.0
this ﬁgure indicates,1,1,1.0
ﬁgure indicates that,1,1,1.0
indicates that the,1,1,1.0
that the ramoboost,1,1,1.0
the ramoboost method,2,1,2.0
ramoboost method is,1,1,1.0
method is competitive,1,1,1.0
is competitive when,1,1,1.0
competitive when compared,1,1,1.0
methods in roc,1,1,1.0
in roc space,1,1,1.0
roc space table,1,1,1.0
space table iii,1,1,1.0
table iii summarizes,1,1,1.0
iii summarizes the,1,1,1.0
comparative algorithms in,5,1,5.0
algorithms in which,1,1,1.0
which the best,3,1,3.0
the best performance,3,1,3.0
best performance of,1,1,1.0
of each algorithm,1,1,1.0
each algorithm across,1,1,1.0
algorithm across each,1,1,1.0
across each evaluation,1,1,1.0
each evaluation criteria,1,1,1.0
evaluation criteria i,1,1,1.0
criteria i s,1,1,1.0
i s highlighted,1,1,1.0
s highlighted from,1,1,1.0
highlighted from table,1,1,1.0
from table iii,1,1,1.0
table iii we,1,1,1.0
iii we ﬁnd,1,1,1.0
we ﬁnd that,1,1,1.0
ﬁnd that ramoboost,1,1,1.0
that ramoboost can,4,1,4.0
ramoboost can provide,1,1,1.0
can provide competitive,1,1,1.0
provide competitive simulation,1,1,1.0
competitive simulation results,1,1,1.0
simulation results on,4,1,4.0
results on most,1,1,1.0
on most of,3,1,3.0
the datasets when,1,1,1.0
datasets when compared,1,1,1.0
to other comparative,1,1,1.0
comparative algorithms except,1,1,1.0
algorithms except for,1,1,1.0
except for recall,1,1,1.0
for recall performance,1,1,1.0
recall performance ieee,1,1,1.0
performance ieee transactions,1,1,1.0
october table ix,1,1,1.0
table ix simula,1,1,1.0
ix simula tion,1,1,1.0
characteristics dataset ramoboost,1,1,1.0
borderlinesmote texture segment,1,1,1.0
yeast letter table,1,1,1.0
letter table x,1,1,1.0
table x simula,1,1,1.0
x simula tion,1,1,1.0
t table xi,1,1,1.0
table xi simula,1,1,1.0
xi simula tion,1,1,1.0
simula tion time,1,1,1.0
tion time in,1,1,1.0
time in seconds,2,1,2.0
in seconds of,2,1,2.0
seconds of comparative,1,1,1.0
of comparative algorithms,1,1,1.0
comparative algorithms across,1,1,1.0
algorithms across all,1,1,1.0
across all datasets,1,1,1.0
all datasets dataset,1,1,1.0
datasets dataset ramoboost,1,1,1.0
adacost borderlinesmote sonar,1,1,1.0
letter shuttle table,1,1,1.0
shuttle table xii,1,1,1.0
table xii simula,1,1,1.0
xii simula tion,1,1,1.0
simula tion of,12,1,12.0
tion of tuning,12,1,12.0
of tuning the,12,1,12.0
tuning the oversampling,2,1,2.0
the oversampling rati,2,1,2.0
oversampling rati o,2,1,2.0
rati o a,2,1,2.0
o a u,2,1,2.0
a u cp,4,1,4.0
u cp erformance,4,1,4.0
cp erformance charateristics,1,1,1.0
erformance charateristics oversampling,1,1,1.0
charateristics oversampling ratio,1,1,1.0
oversampling ratio ramoboost,1,1,1.0
ratio ramoboost smoteboost,2,1,2.0
adacost borderlinesmote see,1,1,1.0
borderlinesmote see that,1,1,1.0
that adasyn seems,1,1,1.0
adasyn seems to,1,1,1.0
seems to provide,1,1,1.0
a better recall,1,1,1.0
better recall rate,1,1,1.0
recall rate on,1,1,1.0
rate on most,1,1,1.0
most of these,1,1,1.0
these datasets this,1,1,1.0
datasets this is,1,1,1.0
is because adasyn,1,1,1.0
because adasyn can,1,1,1.0
adasyn can learn,1,1,1.0
can learn very,1,1,1.0
learn very aggressively,1,1,1.0
very aggressively from,1,1,1.0
aggressively from the,1,1,1.0
it generates thetic,1,1,1.0
generates thetic data,1,1,1.0
thetic data instances,1,1,1.0
decision boundary see,1,1,1.0
boundary see fig,1,1,1.0
see fig c,1,1,1.0
fig c this,1,1,1.0
c this means,1,1,1.0
means that adasyn,1,1,1.0
that adasyn may,1,1,1.0
adasyn may push,1,1,1.0
may push the,1,1,1.0
push the algorithm,1,1,1.0
algorithm to focus,1,1,1.0
the minority positive,1,1,1.0
minority positive class,1,1,1.0
positive class data,1,1,1.0
class data to,1,1,1.0
data to improve,1,1,1.0
improve the recall,1,1,1.0
the recall criteria,1,1,1.0
recall criteria while,1,1,1.0
criteria while the,1,1,1.0
while the overall,1,1,1.0
overall performance may,1,1,1.0
performance may not,1,1,1.0
may not prove,1,1,1.0
not prove signiﬁcantly,1,1,1.0
prove signiﬁcantly in,1,1,1.0
signiﬁcantly in other,1,1,1.0
other words if,1,1,1.0
words if one,1,1,1.0
if one algorithm,1,1,1.0
one algorithm classiﬁes,1,1,1.0
algorithm classiﬁes all,1,1,1.0
classiﬁes all testing,1,1,1.0
all testing data,1,1,1.0
testing data as,1,1,1.0
data as positive,1,1,1.0
as positive minority,1,1,1.0
class its recall,1,1,1.0
its recall r,1,1,1.0
recall r a,1,1,1.0
r a t,2,1,2.0
a t e,1,1,1.0
t e will,1,1,1.0
e will be,1,1,1.0
will be maximized,1,1,1.0
be maximized even,1,1,1.0
maximized even if,1,1,1.0
even if the,1,1,1.0
if the overall,1,1,1.0
overall performance is,1,1,1.0
performance is low,1,1,1.0
is low the,1,1,1.0
low the results,1,1,1.0
results in table,4,1,4.0
in table iii,1,1,1.0
table iii shows,1,1,1.0
iii shows that,1,1,1.0
shows that adasyn,1,1,1.0
that adasyn performs,1,1,1.0
adasyn performs better,1,1,1.0
better than other,1,1,1.0
than other comparative,2,1,2.0
algorithms in terms,1,1,1.0
terms of recall,1,1,1.0
of recall w,1,1,1.0
recall w h,1,1,1.0
w h i,1,1,1.0
h i c,1,1,1.0
i c h,1,1,1.0
c h only,1,1,1.0
h only stands,1,1,1.0
only stands for,1,1,1.0
stands for the,1,1,1.0
number of correctly,1,1,1.0
correctly classiﬁed minority,1,1,1.0
classiﬁed minority instances,1,1,1.0
minority instances but,1,1,1.0
instances but performs,1,1,1.0
but performs worse,1,1,1.0
performs worse in,1,1,1.0
worse in all,1,1,1.0
in all other,1,1,1.0
all other assessment,1,1,1.0
other assessment metrics,1,1,1.0
assessment metrics such,1,1,1.0
as and which,1,1,1.0
and which represent,1,1,1.0
which represent the,1,1,1.0
represent the rithm,1,1,1.0
the rithm s,1,1,1.0
rithm s overall,1,1,1.0
s overall performance,1,1,1.0
overall performance on,1,1,1.0
performance on most,1,1,1.0
the datasets these,1,1,1.0
datasets these results,1,1,1.0
these results conﬁrm,1,1,1.0
results conﬁrm our,1,1,1.0
conﬁrm our discussions,1,1,1.0
our discussions in,1,1,1.0
discussions in section,1,1,1.0
in section regarding,1,1,1.0
section regarding the,1,1,1.0
regarding the different,1,1,1.0
the different characteristics,1,1,1.0
different characteristics of,1,1,1.0
of these algorithms,1,1,1.0
these algorithms the,1,1,1.0
algorithms the signiﬁcance,1,1,1.0
signiﬁcance test is,2,1,2.0
test is applied,3,1,3.0
is applied on,1,1,1.0
applied on the,1,1,1.0
on the simulation,2,1,2.0
simulation results to,1,1,1.0
results to evaluate,1,1,1.0
to evaluate whether,3,1,3.0
evaluate whether ramoboost,2,1,2.0
whether ramoboost can,2,1,2.0
ramoboost can statistically,3,1,3.0
can statistically outperformchen,1,1,1.0
statistically outperformchen et,1,1,1.0
outperformchen et al,1,1,1.0
boosting table xiii,1,1,1.0
table xiii simula,1,1,1.0
xiii simula tion,1,1,1.0
rati o s,3,1,3.0
o s ignificance,3,1,3.0
test of auc,4,1,4.0
of auc based,3,1,3.0
auc based on,3,1,3.0
on random runs,5,1,5.0
random runs between,4,1,4.0
runs between ramob,4,1,4.0
between ramob oost,4,1,4.0
and omek oversampling,1,1,1.0
omek oversampling ramoboost,1,1,1.0
oversampling ramoboost ratio,1,1,1.0
ramoboost ratio smoteboost,2,1,2.0
ratio smoteboost smote,2,1,2.0
adacost borderlinesmote table,5,1,5.0
borderlinesmote table xiv,1,1,1.0
table xiv simula,1,1,1.0
xiv simula tion,1,1,1.0
tuning the imbalanced,5,1,5.0
the imbalanced rati,4,1,4.0
imbalanced rati o,4,1,4.0
rati o p,1,1,1.0
o p olicy,1,1,1.0
p olicy of,1,1,1.0
olicy of combination,1,1,1.0
combination of classes,1,1,1.0
of classes in,1,1,1.0
classes in a,1,1,1.0
in a balone,1,1,1.0
a balone d,1,1,1.0
balone d atas,1,1,1.0
d atas et,1,1,1.0
atas et index,1,1,1.0
et index minority,1,1,1.0
index minority combination,1,1,1.0
minority combination majority,1,1,1.0
combination majority combination,1,1,1.0
majority combination minority,1,1,1.0
combination minority majority,1,1,1.0
majority imbalanced ratio,1,1,1.0
imbalanced ratio i,1,1,1.0
ratio i ii,1,1,1.0
i ii iii,1,1,1.0
ii iii iv,1,1,1.0
iii iv v,1,1,1.0
iv v vi,1,1,1.0
v vi vii,1,1,1.0
vi vii viii,1,1,1.0
vii viii ix,1,1,1.0
viii ix x,1,1,1.0
ix x table,1,1,1.0
x table xv,1,1,1.0
table xv simula,1,1,1.0
xv simula tion,1,1,1.0
cp erformance characteristics,3,1,3.0
erformance characteristics imbalanced,1,1,1.0
characteristics imbalanced ratio,1,1,1.0
imbalanced ratio ramoboost,1,1,1.0
borderlinesmote table xvi,1,1,1.0
table xvi simula,1,1,1.0
xvi simula tion,1,1,1.0
borderlinesmote t other,1,1,1.0
t other comparative,1,1,1.0
comparative algorithms since,1,1,1.0
algorithms since there,1,1,1.0
there are datasets,1,1,1.0
are datasets t,1,1,1.0
datasets t should,2,1,2.0
should be less,2,1,2.0
be less than,2,1,2.0
less than or,2,1,2.0
than or equal,2,1,2.0
or equal to,2,1,2.0
equal to to,1,1,1.0
to to reject,1,1,1.0
to reject a,2,1,2.0
reject a null,2,1,2.0
a null pothesis,1,1,1.0
null pothesis in,1,1,1.0
pothesis in the,1,1,1.0
in the signiﬁcance,1,1,1.0
the signiﬁcance level,1,1,1.0
level of according,1,1,1.0
of according to,1,1,1.0
to the critical,1,1,1.0
value table table,1,1,1.0
table table iv,1,1,1.0
table iv shows,1,1,1.0
iv shows the,1,1,1.0
shows the signiﬁcance,1,1,1.0
signiﬁcance test result,1,1,1.0
test result of,1,1,1.0
result of averaged,1,1,1.0
averaged auc for,1,1,1.0
auc for ramoboost,1,1,1.0
ramoboost smoteboost one,1,1,1.0
smoteboost one can,1,1,1.0
one can conclude,1,1,1.0
can conclude that,1,1,1.0
conclude that ramoboost,1,1,1.0
can statistically outperform,4,1,4.0
statistically outperform smoteboost,2,1,2.0
outperform smoteboost t,1,1,1.0
smoteboost t min,1,1,1.0
t min it,1,1,1.0
min it proves,1,1,1.0
it proves that,1,1,1.0
proves that although,1,1,1.0
that although ramoboost,1,1,1.0
although ramoboost shares,1,1,1.0
ramoboost shares the,1,1,1.0
shares the same,1,1,1.0
the same boosting,1,1,1.0
same boosting procedure,1,1,1.0
boosting procedure and,1,1,1.0
procedure and data,1,1,1.0
data generation technique,1,1,1.0
generation technique with,1,1,1.0
technique with smoteboost,1,1,1.0
with smoteboost the,1,1,1.0
smoteboost the adaptive,1,1,1.0
the adaptive ranking,1,1,1.0
adaptive ranking mechanism,1,1,1.0
ranking mechanism for,1,1,1.0
mechanism for determining,1,1,1.0
determining the number,1,1,1.0
minority example makes,1,1,1.0
example makes ramoboost,1,1,1.0
makes ramoboost perform,1,1,1.0
ramoboost perform better,1,1,1.0
better than smoteboost,1,1,1.0
than smoteboost table,1,1,1.0
smoteboost table v,1,1,1.0
table v shows,1,1,1.0
shows the similar,1,1,1.0
the similar result,1,1,1.0
similar result with,1,1,1.0
result with that,1,1,1.0
that of table,1,1,1.0
of table iv,1,1,1.0
table iv for,1,1,1.0
iv for boost,1,1,1.0
for boost adacost,1,1,1.0
boost adacost from,1,1,1.0
adacost from which,1,1,1.0
from which however,1,1,1.0
which however one,1,1,1.0
however one can,1,1,1.0
see that ramoboost,3,1,3.0
ramoboost can not,1,1,1.0
can not statistically,1,1,1.0
not statistically outperform,1,1,1.0
statistically outperform adacost,1,1,1.0
outperform adacost t,1,1,1.0
adacost t min,1,1,1.0
t min for,1,1,1.0
min for space,1,1,1.0
for space consideration,2,1,2.0
space consideration the,1,1,1.0
consideration the detailed,1,1,1.0
the detailed statistical,1,1,1.0
detailed statistical analysis,1,1,1.0
statistical analysis for,1,1,1.0
analysis for ramoboost,1,1,1.0
for ramoboost against,1,1,1.0
ramoboost against the,1,1,1.0
against the remaining,1,1,1.0
the remaining comparative,1,1,1.0
remaining comparative algorithms,1,1,1.0
comparative algorithms is,1,1,1.0
algorithms is omitted,1,1,1.0
is omitted instead,1,1,1.0
omitted instead we,1,1,1.0
instead we provide,1,1,1.0
we provide in,1,1,1.0
provide in table,1,1,1.0
in table vi,1,1,1.0
table vi the,1,1,1.0
vi the t,1,1,1.0
the t of,1,1,1.0
t of ramoboost,1,1,1.0
of ramoboost against,4,1,4.0
ramoboost against all,1,1,1.0
against all comparative,1,1,1.0
all comparative algorithms,3,1,3.0
comparative algorithms one,2,1,2.0
algorithms one can,2,1,2.0
that ramoboost also,1,1,1.0
ramoboost also ieee,1,1,1.0
also ieee transactions,1,1,1.0
no october outperforms,1,1,1.0
october outperforms smote,1,1,1.0
outperforms smote adasyn,1,1,1.0
borderlinesmote and we,1,1,1.0
and we have,1,1,1.0
we have also,1,1,1.0
have also conducted,1,1,1.0
also conducted the,2,1,2.0
conducted the simulations,1,1,1.0
the simulations of,1,1,1.0
simulations of ramoboost,1,1,1.0
of ramoboost on,2,1,2.0
ramoboost on all,2,1,2.0
all datasets when,1,1,1.0
datasets when the,1,1,1.0
layer neurons for,1,1,1.0
neurons for the,1,1,1.0
for the base,1,1,1.0
base classiﬁer mlp,1,1,1.0
classiﬁer mlp is,1,1,1.0
mlp is set,1,1,1.0
to be ten,2,1,2.0
be ten our,1,1,1.0
ten our simulation,1,1,1.0
our simulation results,1,1,1.0
simulation results indicate,1,1,1.0
indicate that increasing,1,1,1.0
that increasing the,1,1,1.0
increasing the number,1,1,1.0
layer neurons does,1,1,1.0
neurons does not,1,1,1.0
does not necessarily,1,1,1.0
not necessarily improve,1,1,1.0
necessarily improve the,1,1,1.0
improve the learning,1,1,1.0
learning performance in,1,1,1.0
case we feel,1,1,1.0
we feel there,1,1,1.0
feel there might,1,1,1.0
might be several,1,1,1.0
be several reasons,1,1,1.0
several reasons for,1,1,1.0
reasons for this,1,1,1.0
for this such,1,1,1.0
this such as,1,1,1.0
as the potential,1,1,1.0
the potential overﬁtting,1,1,1.0
potential overﬁtting issue,1,1,1.0
overﬁtting issue furthermore,1,1,1.0
issue furthermore as,1,1,1.0
furthermore as suggested,1,1,1.0
suggested in using,1,1,1.0
in using a,1,1,1.0
using a strong,1,1,1.0
a strong base,1,1,1.0
strong base classiﬁer,1,1,1.0
base classiﬁer in,1,1,1.0
classiﬁer in the,2,1,2.0
in the ensemble,1,1,1.0
the ensemble approach,1,1,1.0
ensemble approach may,1,1,1.0
approach may not,1,1,1.0
may not beneﬁt,1,1,1.0
not beneﬁt the,1,1,1.0
beneﬁt the ﬁnal,1,1,1.0
ﬁnal learning performance,1,1,1.0
learning performance due,1,1,1.0
due to increased,1,1,1.0
to increased bias,1,1,1.0
increased bias of,1,1,1.0
bias of such,1,1,1.0
of such classiﬁers,1,1,1.0
such classiﬁers due,1,1,1.0
classiﬁers due to,1,1,1.0
due to space,1,1,1.0
to space consideration,1,1,1.0
space consideration we,2,1,2.0
consideration we refrain,1,1,1.0
we refrain from,1,1,1.0
refrain from providing,1,1,1.0
from providing the,1,1,1.0
providing the detailed,1,1,1.0
the detailed results,1,1,1.0
detailed results for,1,1,1.0
for all these,1,1,1.0
all these experiments,1,1,1.0
these experiments simulation,1,1,1.0
experiments simulation in,1,1,1.0
simulation in section,1,1,1.0
in section we,1,1,1.0
section we investigated,1,1,1.0
we investigated the,1,1,1.0
investigated the data,1,1,1.0
of ramoboost compared,1,1,1.0
ramoboost compared to,1,1,1.0
compared to that,1,1,1.0
to that of,2,1,2.0
and adasyn on,1,1,1.0
adasyn on a,1,1,1.0
on a synthetic,1,1,1.0
a synthetic dataset,1,1,1.0
synthetic dataset shown,1,1,1.0
dataset shown in,1,1,1.0
in fig one,1,1,1.0
fig one interesting,1,1,1.0
one interesting question,1,1,1.0
interesting question that,1,1,1.0
question that arises,1,1,1.0
that arises is,1,1,1.0
arises is if,1,1,1.0
ramoboost is extracted,1,1,1.0
is extracted and,1,1,1.0
extracted and wrapped,1,1,1.0
and wrapped up,1,1,1.0
wrapped up with,1,1,1.0
up with other,1,1,1.0
with other classiﬁer,1,1,1.0
other classiﬁer in,1,1,1.0
in the way,1,1,1.0
the way that,1,1,1.0
way that smote,1,1,1.0
that smote and,1,1,1.0
and adasyn is,1,1,1.0
adasyn is used,1,1,1.0
is used which,1,1,1.0
used which can,1,1,1.0
can be named,1,1,1.0
be named as,1,1,1.0
named as ramo,1,1,1.0
as ramo how,1,1,1.0
ramo how will,1,1,1.0
how will the,1,1,1.0
will the learning,1,1,1.0
the learning formance,1,1,1.0
learning formance of,1,1,1.0
formance of ramo,1,1,1.0
of ramo be,1,1,1.0
ramo be when,1,1,1.0
be when it,1,1,1.0
when it is,1,1,1.0
it is compared,1,1,1.0
is compared to,1,1,1.0
to other sampling,1,1,1.0
other sampling approaches,1,1,1.0
sampling approaches to,1,1,1.0
approaches to this,1,1,1.0
this end we,1,1,1.0
end we have,1,1,1.0
we have conducted,1,1,1.0
have conducted simulations,1,1,1.0
conducted simulations for,1,1,1.0
simulations for ramo,1,1,1.0
for ramo against,1,1,1.0
ramo against smote,1,1,1.0
against smote adasyn,1,1,1.0
borderlinesmote and on,1,1,1.0
and on the,1,1,1.0
in table for,1,1,1.0
table for space,1,1,1.0
for space considerations,2,1,2.0
space considerations we,1,1,1.0
considerations we only,1,1,1.0
we only provide,1,1,1.0
only provide simulation,1,1,1.0
provide simulation results,1,1,1.0
results of averaged,1,1,1.0
algorithms in table,1,1,1.0
in table vii,2,1,2.0
table vii the,1,1,1.0
vii the auc,1,1,1.0
the auc value,2,1,2.0
auc value of,2,1,2.0
the corresponding winning,1,1,1.0
corresponding winning approach,1,1,1.0
winning approach for,1,1,1.0
approach for each,1,1,1.0
dataset is highlighted,1,1,1.0
is highlighted based,2,1,2.0
highlighted based on,3,1,3.0
table vii signiﬁcance,1,1,1.0
vii signiﬁcance test,1,1,1.0
applied to evaluate,2,1,2.0
evaluate whether ramo,1,1,1.0
whether ramo can,1,1,1.0
ramo can statistically,2,1,2.0
statistically outperform other,2,1,2.0
outperform other existing,2,1,2.0
other existing approaches,2,1,2.0
existing approaches since,1,1,1.0
approaches since the,1,1,1.0
since the number,2,1,2.0
of datasets used,1,1,1.0
used in simulation,1,1,1.0
in simulation is,1,1,1.0
simulation is the,1,1,1.0
same as in,1,1,1.0
as in simulation,1,1,1.0
in simulation the,1,1,1.0
simulation the signiﬁcance,1,1,1.0
value n is,1,1,1.0
n is also,1,1,1.0
is also table,1,1,1.0
also table viii,1,1,1.0
table viii demonstrates,1,1,1.0
viii demonstrates the,1,1,1.0
demonstrates the signiﬁcance,1,1,1.0
signiﬁcance test results,2,1,2.0
test results the,1,1,1.0
results the t,1,1,1.0
the t value,3,1,3.0
t value of,2,1,2.0
value of each,1,1,1.0
of each comparative,1,1,1.0
each comparative algorithms,1,1,1.0
see that ramo,1,1,1.0
that ramo can,1,1,1.0
statistically outperform adasyn,1,1,1.0
outperform adasyn and,1,1,1.0
adasyn and but,1,1,1.0
and but it,1,1,1.0
but it can,1,1,1.0
it can not,1,1,1.0
can not outperform,2,1,2.0
not outperform smote,2,1,2.0
outperform smote in,1,1,1.0
smote in this,1,1,1.0
this case simulation,1,1,1.0
case simulation another,1,1,1.0
simulation another interesting,1,1,1.0
another interesting simulation,1,1,1.0
interesting simulation we,1,1,1.0
simulation we conducted,1,1,1.0
we conducted is,1,1,1.0
conducted is to,1,1,1.0
is to compare,1,1,1.0
to compare ramoboost,1,1,1.0
compare ramoboost with,1,1,1.0
ramoboost with other,1,1,1.0
with other comparative,1,1,1.0
comparative algorithms when,1,1,1.0
algorithms when both,1,1,1.0
when both and,2,1,2.0
both and are,2,1,2.0
and are ten,1,1,1.0
are ten for,1,1,1.0
ten for ramoboost,1,1,1.0
for ramoboost we,2,1,2.0
ramoboost we also,1,1,1.0
we also conﬁgured,1,1,1.0
also conﬁgured the,1,1,1.0
conﬁgured the k,1,1,1.0
the k value,1,1,1.0
k value of,1,1,1.0
value of smoteboost,1,1,1.0
of smoteboost smote,1,1,1.0
borderlinesmote and to,1,1,1.0
and to be,1,1,1.0
be ten to,1,1,1.0
ten to provide,1,1,1.0
provide a fair,1,1,1.0
a fair comparison,1,1,1.0
fair comparison all,1,1,1.0
comparison all other,1,1,1.0
all other parameters,1,1,1.0
other parameters remained,1,1,1.0
parameters remained the,1,1,1.0
remained the same,1,1,1.0
the same we,1,1,1.0
same we compared,1,1,1.0
we compared the,1,1,1.0
compared the algorithms,1,1,1.0
the algorithms against,1,1,1.0
algorithms against the,1,1,1.0
against the ten,1,1,1.0
the ten datasets,1,1,1.0
ten datasets with,1,1,1.0
the largest skew,1,1,1.0
largest skew ratio,1,1,1.0
skew ratio since,1,1,1.0
ratio since we,1,1,1.0
we are more,1,1,1.0
are more interested,1,1,1.0
more interested in,1,1,1.0
interested in investigating,1,1,1.0
in investigating how,1,1,1.0
investigating how ramoboost,1,1,1.0
how ramoboost performs,1,1,1.0
ramoboost performs with,1,1,1.0
performs with highly,1,1,1.0
with highly imbalanced,1,1,1.0
highly imbalanced datasets,1,1,1.0
datasets in order,1,1,1.0
order to retain,1,1,1.0
to retain these,1,1,1.0
retain these severely,1,1,1.0
these severely anced,1,1,1.0
severely anced ratios,1,1,1.0
anced ratios we,1,1,1.0
ratios we adopted,1,1,1.0
we adopted a,1,1,1.0
adopted a different,1,1,1.0
a different way,1,1,1.0
different way of,1,1,1.0
way of generating,1,1,1.0
of generating the,1,1,1.0
generating the training,1,1,1.0
the training and,2,1,2.0
and testing datasets,2,1,2.0
testing datasets sp,1,1,1.0
datasets sp eciﬁcally,1,1,1.0
sp eciﬁcally the,1,1,1.0
eciﬁcally the training,1,1,1.0
training dataset was,1,1,1.0
dataset was created,1,1,1.0
was created by,1,1,1.0
created by consolidating,1,1,1.0
by consolidating half,1,1,1.0
consolidating half of,1,1,1.0
of the randomly,2,1,2.0
randomly selected majority,1,1,1.0
selected majority class,1,1,1.0
examples and half,1,1,1.0
and half of,1,1,1.0
randomly selected minority,1,1,1.0
selected minority class,1,1,1.0
class examples the,1,1,1.0
examples the r,1,1,1.0
the r emaining,1,1,1.0
r emaining examples,1,1,1.0
emaining examples were,1,1,1.0
examples were used,1,1,1.0
used as testing,1,1,1.0
as testing dataset,1,1,1.0
testing dataset one,1,1,1.0
dataset one can,1,1,1.0
one can easily,1,1,1.0
can easily verify,1,1,1.0
easily verify that,1,1,1.0
verify that the,1,1,1.0
testing datasets generated,1,1,1.0
datasets generated th,1,1,1.0
generated th is,1,1,1.0
th is way,1,1,1.0
is way bear,1,1,1.0
way bear the,1,1,1.0
bear the same,1,1,1.0
the same imbalance,1,1,1.0
same imbalance ratio,1,1,1.0
imbalance ratio as,1,1,1.0
ratio as the,1,1,1.0
as the original,1,1,1.0
original dataset for,1,1,1.0
dataset for space,1,1,1.0
space considerations only,1,1,1.0
considerations only the,1,1,1.0
only the simulation,1,1,1.0
results of auc,1,1,1.0
of auc values,1,1,1.0
auc values are,1,1,1.0
values are provided,1,1,1.0
are provided in,1,1,1.0
provided in table,1,1,1.0
in table ix,2,1,2.0
table ix with,1,1,1.0
ix with the,1,1,1.0
with the winning,1,1,1.0
the winning value,1,1,1.0
winning value across,1,1,1.0
value across all,1,1,1.0
across all comparative,1,1,1.0
algorithms for each,1,1,1.0
each dataset highlighted,1,1,1.0
dataset highlighted based,1,1,1.0
table ix niﬁcance,1,1,1.0
ix niﬁcance test,1,1,1.0
niﬁcance test is,1,1,1.0
existing approaches when,1,1,1.0
approaches when both,1,1,1.0
and are equal,1,1,1.0
are equal to,1,1,1.0
equal to ten,2,1,2.0
to ten since,1,1,1.0
ten since there,1,1,1.0
there are just,1,1,1.0
are just ten,1,1,1.0
just ten datasets,1,1,1.0
ten datasets t,1,1,1.0
equal to eight,2,1,2.0
to eight to,1,1,1.0
eight to reject,1,1,1.0
null hypothesis between,1,1,1.0
hypothesis between two,1,1,1.0
between two comparative,1,1,1.0
two comparative algorithms,1,1,1.0
comparative algorithms table,1,1,1.0
algorithms table x,1,1,1.0
table x presents,1,1,1.0
x presents the,1,1,1.0
presents the t,1,1,1.0
value of ramoboost,2,1,2.0
ramoboost against other,3,1,3.0
against other comparative,3,1,3.0
comparative algorithms from,2,1,2.0
algorithms from which,1,1,1.0
from which one,1,1,1.0
which one can,1,1,1.0
ramoboost can also,1,1,1.0
can also statistically,1,1,1.0
also statistically outperform,1,1,1.0
outperform smoteboost adasyn,1,1,1.0
smoteboost adasyn borderlinesmote,1,1,1.0
borderlinesmote and but,1,1,1.0
and but can,1,1,1.0
but can not,1,1,1.0
outperform smote and,1,1,1.0
smote and adacost,1,1,1.0
and adacost in,1,1,1.0
adacost in this,1,1,1.0
this case computational,1,1,1.0
case computational time,1,1,1.0
computational time for,1,1,1.0
time for simulation,1,1,1.0
for simulation table,1,1,1.0
simulation table xi,1,1,1.0
table xi shows,1,1,1.0
xi shows the,1,1,1.0
shows the computational,1,1,1.0
computational time in,1,1,1.0
seconds of ramoboost,1,1,1.0
all datasets based,1,1,1.0
datasets based on,1,1,1.0
the simulation ronment,1,1,1.0
simulation ronment of,1,1,1.0
ronment of intel,1,1,1.0
of intel core,1,1,1.0
intel core duo,1,1,1.0
core duo cpu,1,1,1.0
duo cpu ghz,1,1,1.0
cpu ghz gb,1,1,1.0
ghz gb ram,1,1,1.0
gb ram and,1,1,1.0
ram and matlab,1,1,1.0
and matlab version,1,1,1.0
matlab version from,1,1,1.0
version from table,1,1,1.0
from table xi,1,1,1.0
table xi one,1,1,1.0
xi one can,1,1,1.0
computational time cost,1,1,1.0
time cost of,1,1,1.0
cost of ramoboost,1,1,1.0
ramoboost is similar,1,1,1.0
is similar to,1,1,1.0
similar to that,1,1,1.0
the existing approaches,1,1,1.0
existing approaches the,1,1,1.0
approaches the runtime,1,1,1.0
the runtime cost,1,1,1.0
runtime cost for,1,1,1.0
cost for is,1,1,1.0
for is generally,1,1,1.0
is generally higher,1,1,1.0
generally higher than,1,1,1.0
higher than other,1,1,1.0
comparative algorithms especially,1,1,1.0
algorithms especially when,1,1,1.0
especially when the,1,1,1.0
when the size,1,1,1.0
is very large,1,1,1.0
very large letter,1,1,1.0
large letter and,1,1,1.0
letter and this,1,1,1.0
and this is,1,1,1.0
this is probably,1,1,1.0
is probably because,1,1,1.0
probably because tomek,1,1,1.0
because tomek iterates,1,1,1.0
tomek iterates across,1,1,1.0
iterates across the,1,1,1.0
across the entire,1,1,1.0
the entire d,1,1,1.0
entire d ata,1,1,1.0
d ata space,1,1,1.0
ata space repeatedly,1,1,1.0
space repeatedly until,1,1,1.0
repeatedly until all,1,1,1.0
until all tomek,1,1,1.0
all tomek links,1,1,1.0
tomek links have,1,1,1.0
links have been,1,1,1.0
have been cleared,1,1,1.0
been cleared simulation,1,1,1.0
cleared simulation results,1,1,1.0
results on tuning,1,1,1.0
on tuning parameters,1,1,1.0
tuning parameters to,1,1,1.0
parameters to evaluate,1,1,1.0
evaluate the robustness,1,1,1.0
the robustness of,3,1,3.0
robustness of ramoboost,2,1,2.0
algorithms in diff,1,1,1.0
in diff erent,1,1,1.0
diff erent parameter,1,1,1.0
erent parameter conﬁgurations,1,1,1.0
parameter conﬁgurations and,1,1,1.0
conﬁgurations and scenarios,1,1,1.0
and scenarios simulations,1,1,1.0
scenarios simulations on,1,1,1.0
simulations on tuning,1,1,1.0
on tuning the,2,1,2.0
tuning the minority,1,1,1.0
the minority pling,1,1,1.0
minority pling ratios,1,1,1.0
pling ratios the,1,1,1.0
ratios the imbalanced,1,1,1.0
imbalanced ratio and,1,1,1.0
ratio and the,1,1,1.0
class label noise,9,1,9.0
label noise and,2,1,2.0
noise and the,1,1,1.0
and the attribute,1,1,1.0
the attribute noise,2,1,2.0
attribute noise of,2,1,2.0
noise of the,1,1,1.0
the datasets have,1,1,1.0
datasets have been,1,1,1.0
have been conducted,1,1,1.0
been conducted for,1,1,1.0
conducted for space,1,1,1.0
consideration we only,1,1,1.0
we only present,1,1,1.0
only present the,1,1,1.0
present the results,1,1,1.0
on the abalone,1,1,1.0
the abalone dataset,4,1,4.0
abalone dataset again,1,1,1.0
dataset again mlp,1,1,1.0
again mlp with,1,1,1.0
with the conﬁguration,1,1,1.0
the conﬁguration scribed,1,1,1.0
conﬁguration scribed at,1,1,1.0
scribed at the,1,1,1.0
beginning of section,1,1,1.0
of section is,1,1,1.0
section is used,1,1,1.0
learner the simulation,1,1,1.0
results are also,1,1,1.0
are also based,1,1,1.0
also based on,1,1,1.0
based on ten,1,1,1.0
on ten random,2,1,2.0
ten random runs,7,1,7.0
random runs in,2,1,2.0
runs in each,1,1,1.0
of these random,1,1,1.0
these random runs,1,1,1.0
random runs half,1,1,1.0
runs half of,1,1,1.0
the original minority,1,1,1.0
original minority and,1,1,1.0
and majority datasets,1,1,1.0
majority datasets are,1,1,1.0
datasets are randomly,1,1,1.0
are randomly chosen,1,1,1.0
randomly chosen and,1,1,1.0
chosen and merged,1,1,1.0
and merged to,1,1,1.0
merged to be,1,1,1.0
be the training,1,1,1.0
training dataset and,1,1,1.0
and the remaining,1,1,1.0
the remaining part,1,1,1.0
remaining part is,1,1,1.0
part is used,1,1,1.0
as the testing,1,1,1.0
the testing dataset,1,1,1.0
testing dataset this,1,1,1.0
dataset this experiment,1,1,1.0
experiment is motivated,1,1,1.0
is motivated by,1,1,1.0
motivated by which,1,1,1.0
by which suggested,1,1,1.0
which suggested that,1,1,1.0
that the oversampling,1,1,1.0
the oversampling ratio,2,1,2.0
oversampling ratio could,1,1,1.0
ratio could play,1,1,1.0
could play a,1,1,1.0
play a critical,1,1,1.0
a critical role,1,1,1.0
critical role for,1,1,1.0
role for imbalanced,1,1,1.0
learning problems here,1,1,1.0
problems here we,1,1,1.0
here we use,1,1,1.0
use the abalone,1,1,1.0
abalone dataset described,1,1,1.0
dataset described in,1,1,1.0
described in section,2,1,2.0
in section as,1,1,1.0
section as an,1,1,1.0
as an example,1,1,1.0
an example to,1,1,1.0
example to show,1,1,1.0
to show the,1,1,1.0
show the performance,1,1,1.0
the performance by,1,1,1.0
performance by tuning,1,1,1.0
by tuning the,1,1,1.0
tuning the overs,1,1,1.0
the overs ampling,1,1,1.0
overs ampling ratio,1,1,1.0
ampling ratio speciﬁcally,1,1,1.0
ratio speciﬁcally the,1,1,1.0
speciﬁcally the oversampling,1,1,1.0
oversampling ratio for,1,1,1.0
ratio for t,1,1,1.0
for t he,1,1,1.0
t he minority,1,1,1.0
he minority class,1,1,1.0
class is increased,1,1,1.0
is increased progressively,1,1,1.0
increased progressively from,1,1,1.0
progressively from to,1,1,1.0
from to with,1,1,1.0
to with an,1,1,1.0
with an interval,1,1,1.0
an interval of,1,1,1.0
interval of table,1,1,1.0
of table xii,1,1,1.0
table xii displays,1,1,1.0
xii displays the,1,1,1.0
displays the simulation,1,1,1.0
results on ten,1,1,1.0
random runs using,1,1,1.0
runs using the,1,1,1.0
using the averaged,1,1,1.0
algorithms for this,1,1,1.0
for this dataset,1,1,1.0
this dataset in,1,1,1.0
dataset in which,1,1,1.0
best performance is,2,1,2.0
performance is highlighted,2,1,2.0
is highlighted for,1,1,1.0
highlighted for the,1,1,1.0
for the signiﬁcance,1,1,1.0
signiﬁcance test if,1,1,1.0
test if we,1,1,1.0
if we consider,1,1,1.0
we consider only,1,1,1.0
consider only the,1,1,1.0
only the averaged,1,1,1.0
averaged auc since,1,1,1.0
auc since t,1,1,1.0
since t ramoboost,1,1,1.0
t ramoboost is,1,1,1.0
ramoboost is undoubtedly,1,1,1.0
is undoubtedly signiﬁcantly,1,1,1.0
undoubtedly signiﬁcantly better,1,1,1.0
signiﬁcantly better than,1,1,1.0
than all comparative,1,1,1.0
algorithms in all,1,1,1.0
in all simulation,1,1,1.0
all simulation scenarios,1,1,1.0
simulation scenarios we,1,1,1.0
scenarios we also,1,1,1.0
we also conducted,1,1,1.0
conducted the signiﬁcance,1,1,1.0
signiﬁcance test based,2,1,2.0
test based on,2,1,2.0
on the auc,2,1,2.0
the auc of,2,1,2.0
runs in this,1,1,1.0
this case n,1,1,1.0
case n is,1,1,1.0
n is equal,1,1,1.0
to eight since,1,1,1.0
eight since the,1,1,1.0
number of random,1,1,1.0
of random runs,1,1,1.0
random runs is,1,1,1.0
runs is equal,1,1,1.0
to ten table,1,1,1.0
ten table xiii,1,1,1.0
table xiii shows,1,1,1.0
xiii shows the,1,1,1.0
shows the t,1,1,1.0
t value for,1,1,1.0
for the comparison,1,1,1.0
the comparison between,1,1,1.0
comparison between ramoboost,1,1,1.0
ramoboost and other,2,1,2.0
and other comparative,2,1,2.0
comparative algorithms based,1,1,1.0
algorithms based on,1,1,1.0
random runs the,2,1,2.0
runs the original,1,1,1.0
the original abalone,2,1,2.0
original abalone dataset,2,1,2.0
abalone dataset has,1,1,1.0
dataset has classes,1,1,1.0
has classes and,1,1,1.0
classes and examples,1,1,1.0
and examples in,1,1,1.0
examples in which,1,1,1.0
in which we,1,1,1.0
which we employed,1,1,1.0
we employed only,1,1,1.0
employed only two,1,1,1.0
only two classes,1,1,1.0
two classes to,1,1,1.0
classes to evaluatechen,1,1,1.0
to evaluatechen et,1,1,1.0
evaluatechen et al,1,1,1.0
boosting table xvii,1,1,1.0
table xvii simula,1,1,1.0
xvii simula tion,1,1,1.0
and omek imbalanced,1,1,1.0
omek imbalanced ramoboost,1,1,1.0
imbalanced ramoboost ratio,1,1,1.0
borderlinesmote table xviii,1,1,1.0
table xviii simula,1,1,1.0
xviii simula tion,1,1,1.0
tuning the class,5,1,5.0
label noise level,5,1,5.0
noise level a,2,1,2.0
level a u,2,1,2.0
erformance characteristics noise,2,1,2.0
characteristics noise level,2,1,2.0
noise level ramoboost,2,1,2.0
level ramoboost smoteboost,2,1,2.0
borderlinesmote table xix,1,1,1.0
table xix simula,1,1,1.0
xix simula tion,1,1,1.0
noise level s,4,1,4.0
level s ignificance,4,1,4.0
t table xx,1,1,1.0
table xx simula,1,1,1.0
xx simula tion,1,1,1.0
of auc b,1,1,1.0
auc b ased,1,1,1.0
b ased on,1,1,1.0
ased on random,1,1,1.0
and omek noise,2,1,2.0
omek noise ramoboost,2,1,2.0
noise ramoboost level,2,1,2.0
ramoboost level smoteboost,2,1,2.0
level smoteboost smote,2,1,2.0
adacost borderlinesmote the,1,1,1.0
borderlinesmote the comparative,1,1,1.0
comparative algorithms on,1,1,1.0
algorithms on versatile,1,1,1.0
on versatile datasets,1,1,1.0
versatile datasets as,1,1,1.0
datasets as described,1,1,1.0
as described in,1,1,1.0
in section in,1,1,1.0
section in order,1,1,1.0
to obtain versatile,1,1,1.0
obtain versatile imbalanced,1,1,1.0
versatile imbalanced ratio,1,1,1.0
imbalanced ratio we,1,1,1.0
ratio we manipulate,1,1,1.0
we manipulate the,1,1,1.0
manipulate the classes,1,1,1.0
the classes combination,1,1,1.0
classes combination of,1,1,1.0
abalone dataset to,1,1,1.0
dataset to form,1,1,1.0
to form minority,1,1,1.0
form minority class,1,1,1.0
class table xiv,1,1,1.0
table xiv concludes,1,1,1.0
xiv concludes the,1,1,1.0
concludes the details,1,1,1.0
the details for,1,1,1.0
details for such,1,1,1.0
for such combination,1,1,1.0
such combination and,1,1,1.0
combination and the,1,1,1.0
the corresponding imbalanced,1,1,1.0
corresponding imbalanced ratio,1,1,1.0
imbalanced ratio table,1,1,1.0
ratio table xv,1,1,1.0
table xv presents,1,1,1.0
xv presents simulation,1,1,1.0
presents simulation results,1,1,1.0
results of ten,2,1,2.0
of ten random,5,1,5.0
random runs of,1,1,1.0
runs of experiments,1,1,1.0
of experiments on,1,1,1.0
experiments on tuning,1,1,1.0
ratio in which,1,1,1.0
averaged auc results,2,1,2.0
auc results in,1,1,1.0
in table xv,1,1,1.0
table xv wilcoxon,1,1,1.0
xv wilcoxon test,1,1,1.0
wilcoxon test tells,1,1,1.0
test tells us,1,1,1.0
tells us whether,1,1,1.0
us whether any,1,1,1.0
whether any signiﬁcance,1,1,1.0
any signiﬁcance exists,1,1,1.0
signiﬁcance exists between,1,1,1.0
exists between ramoboost,1,1,1.0
ramoboost and any,1,1,1.0
and any of,1,1,1.0
comparative algorithms which,1,1,1.0
algorithms which is,1,1,1.0
which is shown,1,1,1.0
in table xvi,1,1,1.0
table xvi using,1,1,1.0
xvi using the,1,1,1.0
using the abalone,1,1,1.0
abalone dataset we,1,1,1.0
dataset we conducted,1,1,1.0
we conducted signiﬁcance,1,1,1.0
conducted signiﬁcance tests,1,1,1.0
signiﬁcance tests on,1,1,1.0
tests on the,1,1,1.0
the auc values,1,1,1.0
auc values of,2,1,2.0
values of ten,2,1,2.0
runs the results,1,1,1.0
results are given,1,1,1.0
are given in,1,1,1.0
given in table,1,1,1.0
in table xvii,1,1,1.0
table xvii noise,1,1,1.0
xvii noise in,1,1,1.0
noise in imbalanced,2,1,2.0
in imbalanced datasets,2,1,2.0
imbalanced datasets may,1,1,1.0
datasets may exhibit,1,1,1.0
may exhibit unpredictably,1,1,1.0
exhibit unpredictably negative,1,1,1.0
unpredictably negative effects,1,1,1.0
learning algorithms in,1,1,1.0
algorithms in order,1,1,1.0
order to systematically,1,1,1.0
to systematically investigate,1,1,1.0
systematically investigate the,1,1,1.0
investigate the robustness,1,1,1.0
robustness of boost,1,1,1.0
of boost we,1,1,1.0
boost we manually,1,1,1.0
we manually introduce,1,1,1.0
manually introduce class,1,1,1.0
introduce class label,1,1,1.0
noise and attribute,1,1,1.0
and attribute noise,1,1,1.0
noise of different,1,1,1.0
of different levels,1,1,1.0
different levels into,1,1,1.0
levels into the,1,1,1.0
into the abalone,1,1,1.0
abalone dataset and,1,1,1.0
dataset and let,1,1,1.0
and let ramoboost,1,1,1.0
let ramoboost as,1,1,1.0
ramoboost as well,1,1,1.0
well as other,1,1,1.0
as other comparative,1,1,1.0
comparative algorithms ieee,1,1,1.0
algorithms ieee transactions,1,1,1.0
october table xxi,1,1,1.0
table xxi simula,1,1,1.0
xxi simula tion,1,1,1.0
tuning the attribute,1,1,1.0
attribute noise level,1,1,1.0
borderlinesmote table xxii,1,1,1.0
table xxii simula,1,1,1.0
xxii simula tion,1,1,1.0
t table xxiii,1,1,1.0
table xxiii simula,1,1,1.0
xxiii simula tion,1,1,1.0
adacost borderlinesmote from,1,1,1.0
borderlinesmote from it,1,1,1.0
from it for,1,1,1.0
it for adding,1,1,1.0
for adding class,1,1,1.0
adding class label,1,1,1.0
label noise we,1,1,1.0
noise we adopted,1,1,1.0
we adopted the,1,1,1.0
adopted the procedure,1,1,1.0
procedure of speciﬁcally,1,1,1.0
of speciﬁcally given,1,1,1.0
speciﬁcally given a,1,1,1.0
given a pair,1,1,1.0
of classes x,1,1,1.0
classes x y,1,1,1.0
x y and,1,1,1.0
y and a,1,1,1.0
and a noise,1,1,1.0
a noise level,2,1,2.0
noise level x,2,1,2.0
level x an,1,1,1.0
x an instance,1,1,1.0
an instance with,1,1,1.0
instance with its,1,1,1.0
with its label,1,1,1.0
its label x,1,1,1.0
label x has,1,1,1.0
x has an,1,1,1.0
has an x,1,1,1.0
an x chance,1,1,1.0
x chance to,1,1,1.0
chance to be,1,1,1.0
to be corrupted,1,1,1.0
be corrupted and,1,1,1.0
corrupted and mislabeled,1,1,1.0
and mislabeled as,1,1,1.0
mislabeled as y,1,1,1.0
as y and,1,1,1.0
y and so,1,1,1.0
and so does,1,1,1.0
so does an,1,1,1.0
does an instance,1,1,1.0
instance of class,1,1,1.0
of class y,1,1,1.0
class y table,1,1,1.0
y table xviii,1,1,1.0
table xviii shows,1,1,1.0
xviii shows the,1,1,1.0
shows the auc,1,1,1.0
other comparative learning,1,1,1.0
comparative learning algorithms,1,1,1.0
learning algorithms under,1,1,1.0
algorithms under different,1,1,1.0
under different class,1,1,1.0
label noise levels,1,1,1.0
noise levels tables,1,1,1.0
levels tables xix,1,1,1.0
tables xix and,1,1,1.0
xix and xx,1,1,1.0
and xx show,1,1,1.0
xx show the,1,1,1.0
show the signiﬁcance,1,1,1.0
averaged auc and,2,1,2.0
auc and auc,1,1,1.0
and auc values,1,1,1.0
random runs attribute,1,1,1.0
runs attribute noise,1,1,1.0
attribute noise was,1,1,1.0
noise was manually,1,1,1.0
was manually added,1,1,1.0
manually added in,1,1,1.0
added in accordance,1,1,1.0
in accordance with,1,1,1.0
accordance with the,1,1,1.0
with the procedure,1,1,1.0
the procedure in,1,1,1.0
procedure in to,1,1,1.0
in to corrupt,1,1,1.0
to corrupt each,1,1,1.0
corrupt each attribute,1,1,1.0
each attribute ai,1,1,1.0
attribute ai with,1,1,1.0
ai with a,1,1,1.0
with a noise,1,1,1.0
level x the,1,1,1.0
x the value,1,1,1.0
value of ai,1,1,1.0
of ai is,1,1,1.0
ai is assigned,1,1,1.0
is assigned a,1,1,1.0
assigned a random,1,1,1.0
a random value,1,1,1.0
random value approximately,1,1,1.0
value approximately x,1,1,1.0
approximately x of,1,1,1.0
x of the,1,1,1.0
of the time,1,1,1.0
the time with,1,1,1.0
time with each,1,1,1.0
with each tive,1,1,1.0
each tive value,1,1,1.0
tive value being,1,1,1.0
value being approximately,1,1,1.0
being approximately equally,1,1,1.0
approximately equally likely,1,1,1.0
to be selected,1,1,1.0
be selected table,1,1,1.0
selected table xxi,1,1,1.0
table xxi shows,1,1,1.0
xxi shows the,1,1,1.0
shows the averaged,1,1,1.0
auc results of,1,1,1.0
random runs tables,1,1,1.0
runs tables xxii,1,1,1.0
tables xxii and,1,1,1.0
xxii and xxiii,1,1,1.0
and xxiii present,1,1,1.0
xxiii present the,1,1,1.0
present the signiﬁcance,1,1,1.0
test results based,1,1,1.0
results based on,1,1,1.0
auc and the,1,1,1.0
auc of ten,1,1,1.0
random runs all,1,1,1.0
runs all simulation,1,1,1.0
all simulation results,1,1,1.0
simulation results presented,1,1,1.0
this section illustrate,1,1,1.0
section illustrate the,1,1,1.0
illustrate the robustness,1,1,1.0
of ramoboost when,1,1,1.0
ramoboost when exposed,1,1,1.0
when exposed to,1,1,1.0
exposed to different,1,1,1.0
to different internal,1,1,1.0
different internal is,1,1,1.0
internal is signiﬁcantly,1,1,1.0
is signiﬁcantly better,1,1,1.0
signiﬁcantly better tha,1,1,1.0
better tha n,2,2,1.0
tha n the,1,1,1.0
n the comparative,1,1,1.0
comparative algorithm only,1,1,1.0
algorithm only if,1,1,1.0
only if the,1,1,1.0
if the corresponding,1,1,1.0
the corresponding table,1,1,1.0
corresponding table cell,1,1,1.0
table cell is,1,1,1.0
cell is hi,1,1,1.0
is hi ghlighted,1,1,1.0
hi ghlighted and,1,1,1.0
ghlighted and underscored,1,1,1.0
and underscored with,1,1,1.0
underscored with symbol,1,1,1.0
with symbol symbol,1,1,1.0
symbol symbol represents,1,1,1.0
symbol represents the,1,1,1.0
represents the opposite,1,1,1.0
the opposite oversampling,1,1,1.0
opposite oversampling ratio,1,1,1.0
oversampling ratio and,1,1,1.0
ratio and exte,1,1,1.0
and exte rnal,1,1,1.0
exte rnal imbalanced,1,1,1.0
rnal imbalanced class,1,1,1.0
imbalanced class ratio,1,1,1.0
class ratio noises,1,1,1.0
ratio noises conﬁgurations,1,1,1.0
noises conﬁgurations the,1,1,1.0
conﬁgurations the signiﬁcance,1,1,1.0
the signiﬁcance tests,1,1,1.0
signiﬁcance tests also,1,1,1.0
tests also demonstrate,1,1,1.0
also demonstrate the,1,1,1.0
demonstrate the competitiveness,1,1,1.0
the competitiveness of,1,1,1.0
competitiveness of ramoboost,1,1,1.0
algorithms from a,1,1,1.0
of view c,1,1,1.0
view c onclusion,1,1,1.0
c onclusion in,1,1,1.0
onclusion in this,1,1,1.0
paper we presented,1,1,1.0
we presented the,1,1,1.0
presented the ramoboost,1,1,1.0
ramoboost method for,1,1,1.0
method for balanced,1,1,1.0
for balanced data,1,1,1.0
balanced data classiﬁcation,1,1,1.0
data classiﬁcation problem,1,1,1.0
classiﬁcation problem the,1,1,1.0
problem the key,1,1,1.0
the key characteristics,1,1,1.0
key characteristics of,1,1,1.0
characteristics of ramoboost,1,1,1.0
of ramoboost are,1,1,1.0
ramoboost are adaptive,1,1,1.0
are adaptive learning,1,1,1.0
adaptive learning and,1,1,1.0
learning and reduction,1,1,1.0
and reduction of,1,1,1.0
reduction of bias,1,1,1.0
of bias this,1,1,1.0
bias this is,1,1,1.0
accomplished by adaptively,1,1,1.0
by adaptively shifting,1,1,1.0
adaptively shifting the,1,1,1.0
shifting the decision,1,1,1.0
boundary toward those,1,1,1.0
toward those difﬁcult,1,1,1.0
those difﬁcult examples,1,1,1.0
difﬁcult examples in,1,1,1.0
examples in both,1,1,1.0
in both minority,1,1,1.0
both minority and,1,1,1.0
examples and systematically,1,1,1.0
and systematically creating,1,1,1.0
systematically creating minority,1,1,1.0
creating minority synthetic,1,1,1.0
minority synthetic stances,1,1,1.0
synthetic stances based,1,1,1.0
stances based on,1,1,1.0
on the distribution,1,1,1.0
distribution function simulation,1,1,1.0
function simulation results,1,1,1.0
results on datasets,1,1,1.0
on datasets across,1,1,1.0
datasets across various,1,1,1.0
across various assessment,1,1,1.0
assessment metrics including,1,1,1.0
metrics including oa,1,1,1.0
including oa precision,1,1,1.0
precision recall roc,1,1,1.0
recall roc graphs,1,1,1.0
roc graphs and,1,1,1.0
graphs and auc,1,1,1.0
and auc demonstrate,1,1,1.0
auc demonstrate the,1,1,1.0
demonstrate the effectiveness,1,1,1.0
the effectiveness and,1,1,1.0
effectiveness and robustness,1,1,1.0
and robustness of,1,1,1.0
robustness of the,1,1,1.0
proposed method as,1,1,1.0
method as a,1,1,1.0
new method for,1,1,1.0
method for imbalanced,1,1,1.0
learning problems there,1,1,1.0
problems there are,1,1,1.0
are several interesting,1,1,1.0
several interesting future,1,1,1.0
interesting future research,1,1,1.0
research directions for,1,1,1.0
directions for boost,1,1,1.0
for boost for,1,1,1.0
boost for instance,1,1,1.0
for instance our,1,1,1.0
instance our current,1,1,1.0
current study is,1,1,1.0
study is focused,1,1,1.0
focused on handling,1,1,1.0
on handling datasets,1,1,1.0
handling datasets with,1,1,1.0
datasets with continuous,1,1,1.0
with continuous features,1,1,1.0
continuous features it,1,1,1.0
features it is,1,1,1.0
it is possible,1,1,1.0
is possible to,1,1,1.0
possible to extend,1,1,1.0
to extend ramoboost,1,1,1.0
extend ramoboost to,1,1,1.0
ramoboost to handle,1,1,1.0
to handle datasets,1,1,1.0
handle datasets with,1,1,1.0
datasets with nominal,1,1,1.0
with nominal features,1,1,1.0
nominal features by,1,1,1.0
features by adopting,1,1,1.0
by adopting the,1,1,1.0
adopting the techniques,1,1,1.0
the techniques used,1,1,1.0
techniques used in,1,1,1.0
the method et,1,1,1.0
method et al,1,1,1.0
in boosting second,1,1,1.0
boosting second in,1,1,1.0
second in this,1,1,1.0
this paper ramoboost,1,1,1.0
paper ramoboost is,1,1,1.0
ramoboost is only,1,1,1.0
is only evaluated,1,1,1.0
only evaluated on,1,1,1.0
evaluated on class,1,1,1.0
on class imbalanced,1,1,1.0
class imbalanced problems,1,1,1.0
imbalanced problems it,1,1,1.0
problems it can,1,1,1.0
can be generalized,1,1,1.0
be generalized to,1,1,1.0
generalized to handle,1,1,1.0
to handle multiclass,1,1,1.0
handle multiclass imbalanced,1,1,1.0
multiclass imbalanced learning,1,1,1.0
problems to improve,1,1,1.0
to improve its,1,1,1.0
improve its plicability,1,1,1.0
its plicability in,1,1,1.0
plicability in practice,1,1,1.0
in practice third,1,1,1.0
practice third in,1,1,1.0
third in ramoboost,1,1,1.0
ramoboost the euclidean,1,1,1.0
distance is employed,1,1,1.0
as the distance,1,1,1.0
the distance measure,1,1,1.0
distance measure however,1,1,1.0
measure however there,1,1,1.0
are other alternatives,1,1,1.0
other alternatives that,1,1,1.0
alternatives that are,1,1,1.0
that are also,1,1,1.0
are also eligible,1,1,1.0
also eligible and,1,1,1.0
eligible and worthy,1,1,1.0
and worthy of,1,1,1.0
worthy of trying,1,1,1.0
of trying and,1,1,1.0
trying and may,1,1,1.0
and may show,1,1,1.0
may show improved,1,1,1.0
show improved performance,1,1,1.0
improved performance for,1,1,1.0
performance for the,1,1,1.0
for the ramoboost,1,1,1.0
ramoboost framework finally,1,1,1.0
framework finally similar,1,1,1.0
finally similar to,1,1,1.0
similar to many,1,1,1.0
to many of,1,1,1.0
imbalanced learning algorithms,1,1,1.0
learning algorithms there,1,1,1.0
algorithms there are,1,1,1.0
are several parameters,1,1,1.0
several parameters that,1,1,1.0
parameters that need,1,1,1.0
to be determined,1,1,1.0
be determined for,1,1,1.0
determined for ramoboost,1,1,1.0
ramoboost we have,1,1,1.0
we have shown,1,1,1.0
have shown some,1,1,1.0
shown some empirical,1,1,1.0
some empirical results,1,1,1.0
empirical results regarding,1,1,1.0
results regarding this,1,1,1.0
regarding this issue,1,1,1.0
issue in this,1,1,1.0
paper and we,1,1,1.0
and we also,1,1,1.0
we also would,1,1,1.0
also would like,1,1,1.0
note that a,1,1,1.0
that a systematic,1,1,1.0
a systematic and,1,1,1.0
systematic and adaptive,1,1,1.0
and adaptive way,1,1,1.0
adaptive way to,1,1,1.0
way to adjust,1,1,1.0
to adjust those,1,1,1.0
adjust those parameters,1,1,1.0
those parameters could,1,1,1.0
parameters could be,1,1,1.0
be a challenging,1,1,1.0
a challenging but,1,1,1.0
challenging but important,1,1,1.0
but important issue,1,1,1.0
important issue for,1,1,1.0
for this method,1,1,1.0
this method to,1,1,1.0
method to be,1,1,1.0
be applied across,1,1,1.0
applied across different,1,1,1.0
across different application,1,1,1.0
different application domains,1,1,1.0
application domains our,1,1,1.0
domains our group,1,1,1.0
our group is,1,1,1.0
group is currently,1,1,1.0
is currently investigating,1,1,1.0
currently investigating all,1,1,1.0
investigating all these,1,1,1.0
all these issues,1,1,1.0
these issues motivated,1,1,1.0
issues motivated by,1,1,1.0
motivated by our,1,1,1.0
by our initial,1,1,1.0
our initial results,1,1,1.0
initial results in,1,1,1.0
paper we believe,1,1,1.0
believe that ramoboost,1,1,1.0
that ramoboost may,1,1,1.0
ramoboost may provide,1,1,1.0
may provide new,1,1,1.0
provide new insights,1,1,1.0
new insights for,1,1,1.0
insights for imbalanced,1,1,1.0
learning problems and,1,1,1.0
problems and have,1,1,1.0
and have the,1,1,1.0
have the potential,1,1,1.0
the potential to,1,1,1.0
potential to be,1,1,1.0
be a powerful,1,1,1.0
a powerful tool,1,1,1.0
powerful tool in,1,1,1.0
tool in many,1,1,1.0
application domains references,1,1,1.0
domains references he,1,1,1.0
references he and,1,1,1.0
data eng vol,3,1,3.0
eng vol no,3,1,3.0
no pp provost,1,1,1.0
pp provost learning,1,1,1.0
provost learning with,1,1,1.0
sets in learning,1,1,1.0
data sets japkowicz,1,1,1.0
sets japkowicz ed,1,1,1.0
japkowicz ed menlo,1,1,1.0
ed menlo park,1,1,1.0
aaai press v,1,1,1.0
press v chawla,1,1,1.0
v chawla japkowicz,2,1,2.0
chawla japkowicz and,2,1,2.0
japkowicz and kołcz,2,1,2.0
and kołcz editorial,1,1,1.0
kołcz editorial special,1,1,1.0
editorial special issue,1,1,1.0
issue on learning,1,1,1.0
data sets acm,1,1,1.0
sets acm sigkdd,1,1,1.0
sigkdd explorations vol,1,1,1.0
explorations vol no,1,1,1.0
pp jun v,2,1,2.0
jun v chawla,2,1,2.0
and kołcz uncertainty,1,1,1.0
kołcz uncertainty sampling,1,1,1.0
uncertainty sampling for,1,1,1.0
sampling for classiﬁers,1,1,1.0
for classiﬁers in,1,1,1.0
classiﬁers in proc,1,1,1.0
in proc int,12,1,12.0
proc int conf,11,1,11.0
int conf mach,8,1,8.0
conf mach workshop,4,1,4.0
mach workshop learn,4,1,4.0
workshop learn imbalanced,4,1,4.0
learn imbalanced data,5,1,5.0
ii washington pp,2,1,2.0
washington pp japkowicz,1,1,1.0
pp japkowicz learning,1,1,1.0
various strategies in,1,1,1.0
strategies in proc,1,1,1.0
in proc learn,1,1,1.0
proc learn imbalanced,1,1,1.0
data sets papers,1,1,1.0
sets papers aaai,1,1,1.0
papers aaai workshop,1,1,1.0
aaai workshop menlo,1,1,1.0
workshop menlo park,1,1,1.0
park ca pp,1,1,1.0
ca pp provost,1,1,1.0
fawcett robust classiﬁcation,1,1,1.0
robust classiﬁcation for,1,1,1.0
classiﬁcation for imprecise,1,1,1.0
for imprecise ments,1,1,1.0
imprecise ments mach,1,1,1.0
ments mach vol,1,1,1.0
mach vol no,1,1,1.0
no pp mar,1,1,1.0
pp mar clearwater,1,1,1.0
mar clearwater and,1,1,1.0
clearwater and stern,1,1,1.0
and stern a,1,1,1.0
stern a program,1,1,1.0
a program in,1,1,1.0
program in high,1,1,1.0
in high energy,1,1,1.0
high energy physics,1,1,1.0
energy physics event,1,1,1.0
event classiﬁcation comput,1,1,1.0
classiﬁcation comput phys,1,1,1.0
comput phys commun,1,1,1.0
phys commun vol,1,1,1.0
commun vol no,1,1,1.0
no pp weiss,1,1,1.0
unifying framework acm,1,1,1.0
framework acm sigkdd,1,1,1.0
sigkdd explorations newslett,4,1,4.0
explorations newslett vol,4,1,4.0
newslett vol no,4,1,4.0
pp jun g,1,1,1.0
jun g a,1,1,1.0
g a t,1,1,1.0
a t i,2,1,2.0
t i s,1,1,1.0
i s t,1,1,1.0
s t a,1,1,1.0
t a r,1,1,1.0
a r r,1,1,1.0
r r a,1,1,1.0
i a n,1,1,1.0
a n dm,1,1,1.0
n dm o,1,1,1.0
dm o n,1,1,1.0
o n a,1,1,1.0
n a r,1,1,1.0
a r d,1,1,1.0
r d as,1,1,1.0
d as t,1,1,1.0
as t u,1,1,1.0
t u d,1,1,1.0
u d yo,1,1,1.0
d yo f,1,1,1.0
yo f the,1,1,1.0
f the behavior,1,1,1.0
methods for b,1,1,1.0
for b alancing,1,1,1.0
b alancing machine,1,1,1.0
alancing machine learning,1,1,1.0
training data acm,1,1,1.0
chawla and imbalanced,1,1,1.0
and imbalanced datasets,1,1,1.0
imbalanced datasets investigating,1,1,1.0
datasets investigating the,1,1,1.0
investigating the effect,1,1,1.0
effect of sampling,1,1,1.0
of sampling method,1,1,1.0
sampling method probabilistic,1,1,1.0
method probabilistic estimate,1,1,1.0
probabilistic estimate and,1,1,1.0
estimate and decision,1,1,1.0
and decision tree,1,1,1.0
decision tree structure,1,1,1.0
tree structure in,1,1,1.0
structure in proc,1,1,1.0
ii pp jo,1,1,1.0
pp jo and,1,1,1.0
class imbalances small,1,1,1.0
imbalances small disjuncts,1,1,1.0
small disjuncts acm,1,1,1.0
disjuncts acm sigkdd,1,1,1.0
pp jun weiss,1,1,1.0
jun weiss and,1,1,1.0
tree induction artiﬁcial,1,1,1.0
induction artiﬁcial intell,1,1,1.0
artiﬁcial intell vol,2,1,2.0
intell vol no,2,1,2.0
no pp jul,1,1,1.0
pp jul japkowicz,1,1,1.0
jul japkowicz class,1,1,1.0
right issue in,1,1,1.0
issue in proc,1,1,1.0
ii pp prati,1,1,1.0
class imbalances class,1,1,1.0
imbalances class overlapping,1,1,1.0
behavior in proc,1,1,1.0
in proc mexican,1,1,1.0
proc mexican int,1,1,1.0
mexican int conf,1,1,1.0
int conf artiﬁcial,2,1,2.0
conf artiﬁcial adv,1,1,1.0
artiﬁcial adv artiﬁcial,1,1,1.0
adv artiﬁcial intell,1,1,1.0
artiﬁcial intell pp,2,1,2.0
intell pp v,1,1,1.0
v chawla bowyer,1,1,1.0
hall and kegelmeyer,1,1,1.0
minority technique artiﬁcial,1,1,1.0
technique artiﬁcial intell,1,1,1.0
no pp han,1,1,1.0
and mao a,1,1,1.0
a new sampling,1,1,1.0
new sampling method,1,1,1.0
learning in proc,2,1,2.0
conf intell adv,1,1,1.0
intell adv intell,1,1,1.0
adv intell comput,1,1,1.0
intell comput pp,1,1,1.0
comput pp guo,1,1,1.0
pp guo and,1,1,1.0
guo and viktor,1,1,1.0
and viktor learning,1,1,1.0
viktor learning from,1,1,1.0
generation the approach,1,1,1.0
the approach acm,1,1,1.0
approach acm sigkdd,1,1,1.0
pp jun mease,1,1,1.0
jun mease wyner,1,1,1.0
mease wyner and,1,1,1.0
wyner and buja,1,1,1.0
and buja boosted,1,1,1.0
buja boosted classiﬁcation,1,1,1.0
boosted classiﬁcation trees,1,1,1.0
classiﬁcation trees and,1,1,1.0
trees and class,1,1,1.0
and class estimation,1,1,1.0
class estimation mach,1,1,1.0
estimation mach learn,1,1,1.0
mach learn res,2,1,2.0
learn res vol,2,1,2.0
res vol pp,2,1,2.0
vol pp may,1,1,1.0
pp may yuan,1,1,1.0
may yuan li,1,1,1.0
yuan li and,1,1,1.0
machines in proc,1,1,1.0
in proc annu,2,1,2.0
proc annu acm,1,1,1.0
annu acm int,1,1,1.0
acm int conf,1,1,1.0
int conf multimedia,1,1,1.0
conf multimedia santa,1,1,1.0
multimedia santa barbara,1,1,1.0
santa barbara ca,1,1,1.0
barbara ca pp,1,1,1.0
ca pp ting,1,1,1.0
pp ting an,1,1,1.0
ting an method,1,1,1.0
an method to,1,1,1.0
method to induce,1,1,1.0
induce trees ieee,1,1,1.0
trees ieee trans,1,1,1.0
no pp and,1,1,1.0
pp and vasconcelos,1,1,1.0
and vasconcelos asymmetric,1,1,1.0
vasconcelos asymmetric boosting,1,1,1.0
asymmetric boosting in,1,1,1.0
boosting in proc,3,1,3.0
conf mach learn,4,1,4.0
mach learn corvallis,1,1,1.0
learn corvallis or,1,1,1.0
corvallis or pp,1,1,1.0
or pp viola,1,1,1.0
pp viola and,1,1,1.0
viola and jones,1,1,1.0
and jones fast,1,1,1.0
jones fast and,1,1,1.0
fast and robust,1,1,1.0
and robust classiﬁcation,1,1,1.0
robust classiﬁcation using,1,1,1.0
classiﬁcation using asymmetric,1,1,1.0
using asymmetric adaboost,1,1,1.0
asymmetric adaboost and,1,1,1.0
adaboost and a,1,1,1.0
and a detector,1,1,1.0
a detector cascade,1,1,1.0
detector cascade in,1,1,1.0
cascade in advances,1,1,1.0
information processing system,1,1,1.0
processing system cambridge,1,1,1.0
system cambridge ma,1,1,1.0
cambridge ma mit,2,1,2.0
ma mit press,2,1,2.0
mit press pp,2,1,2.0
press pp fan,1,1,1.0
pp fan stolfo,1,1,1.0
fan stolfo zhang,1,1,1.0
stolfo zhang an,1,1,1.0
zhang an d,1,1,1.0
an d chan,1,1,1.0
d chan adacost,1,1,1.0
chan adacost cation,1,1,1.0
adacost cation boosting,1,1,1.0
cation boosting in,1,1,1.0
mach learn pp,2,1,2.0
learn pp domingos,1,1,1.0
pp domingos metacost,1,1,1.0
metacost a genera,1,1,1.0
a genera l,1,1,1.0
genera l method,1,1,1.0
l method for,1,1,1.0
for making classiﬁers,1,1,1.0
making classiﬁers sensitive,1,1,1.0
classiﬁers sensitive in,1,1,1.0
sensitive in proc,1,1,1.0
in proc acm,2,1,2.0
proc acm sigkdd,1,1,1.0
acm sigkdd int,1,1,1.0
sigkdd int conf,1,1,1.0
int conf knowl,2,1,2.0
conf knowl discovery,2,1,2.0
knowl discovery data,2,1,2.0
discovery data mining,2,1,2.0
mining pp liu,1,1,1.0
pp liu and,1,1,1.0
liu and zhou,1,1,1.0
and zhou training,1,1,1.0
zhou training neural,1,1,1.0
training neural networks,1,1,1.0
neural networks with,1,1,1.0
networks with methods,1,1,1.0
with methods addressing,1,1,1.0
methods addressing the,1,1,1.0
addressing the class,2,1,2.0
imbalance problem ieee,1,1,1.0
problem ieee trans,1,1,1.0
knowl data vol,1,1,1.0
data vol no,1,1,1.0
y liu and,1,1,1.0
liu and y,1,1,1.0
and y chen,1,1,1.0
y chen face,1,1,1.0
chen face recognition,1,1,1.0
face recognition using,1,1,1.0
recognition using total,1,1,1.0
using total adaptive,1,1,1.0
total adaptive fuzzy,1,1,1.0
adaptive fuzzy support,1,1,1.0
fuzzy support vector,1,1,1.0
vector machines ieee,1,1,1.0
machines ieee trans,2,1,2.0
ieee trans neural,7,1,7.0
trans neural v,1,1,1.0
neural v o,1,1,1.0
v o l,1,1,1.0
o l no,1,1,1.0
l no pp,1,1,1.0
no pp wu,1,1,1.0
boundary alignment considering,1,1,1.0
alignment considering imbalanced,1,1,1.0
considering imbalanced data,1,1,1.0
distribution ieee trans,1,1,1.0
pp jun wu,1,1,1.0
jun wu and,1,1,1.0
y chang aligning,1,1,1.0
chang aligning boundary,1,1,1.0
aligning boundary in,1,1,1.0
boundary in kernel,1,1,1.0
in kernel space,1,1,1.0
kernel space for,1,1,1.0
space for learning,1,1,1.0
for learning imbalanced,1,1,1.0
learning imbalanced dataset,1,1,1.0
imbalanced dataset in,1,1,1.0
dataset in proc,1,1,1.0
in proc ieee,1,1,1.0
proc ieee int,1,1,1.0
ieee int conf,1,1,1.0
int conf data,1,1,1.0
conf data mining,1,1,1.0
data mining brighton,1,1,1.0
mining brighton pp,1,1,1.0
brighton pp hong,1,1,1.0
sets ieee trans,1,1,1.0
trans neural vol,4,1,4.0
neural vol no,4,1,4.0
no pp ertekin,1,1,1.0
pp ertekin huang,2,1,2.0
ertekin huang bottou,1,1,1.0
huang bottou and,1,1,1.0
bottou and giles,1,1,1.0
and giles learning,1,1,1.0
giles learning on,1,1,1.0
learning on the,1,1,1.0
the border active,1,1,1.0
border active learning,1,1,1.0
active learning in,1,1,1.0
learning in imbalanced,1,1,1.0
data classiﬁcation in,2,1,2.0
classiﬁcation in proc,2,1,2.0
proc acm conf,1,1,1.0
acm conf inform,1,1,1.0
conf inform knowl,1,1,1.0
inform knowl manage,1,1,1.0
knowl manage lisbon,1,1,1.0
manage lisbon portugal,1,1,1.0
lisbon portugal pp,1,1,1.0
portugal pp ertekin,1,1,1.0
ertekin huang and,1,1,1.0
huang and giles,1,1,1.0
and giles active,1,1,1.0
giles active learning,1,1,1.0
active learning for,2,1,2.0
learning for class,1,1,1.0
for class imbalance,1,1,1.0
problem in proc,2,1,2.0
proc annu int,1,1,1.0
annu int acm,1,1,1.0
int acm sigir,1,1,1.0
acm sigir conf,1,1,1.0
sigir conf res,1,1,1.0
conf res develop,1,1,1.0
res develop inform,1,1,1.0
develop inform retrieval,1,1,1.0
inform retrieval amsterdam,1,1,1.0
retrieval amsterdam the,1,1,1.0
amsterdam the netherlands,1,1,1.0
the netherlands pp,1,1,1.0
netherlands pp zhu,1,1,1.0
pp zhu and,1,1,1.0
zhu and hovy,1,1,1.0
and hovy active,1,1,1.0
hovy active learning,1,1,1.0
learning for word,1,1,1.0
for word sense,1,1,1.0
sense disambiguation with,1,1,1.0
disambiguation with methods,1,1,1.0
with methods for,1,1,1.0
methods for addressing,1,1,1.0
for addressing the,1,1,1.0
in proc joint,1,1,1.0
proc joint conf,1,1,1.0
joint conf empirical,1,1,1.0
conf empirical methods,1,1,1.0
empirical methods natural,1,1,1.0
methods natural l,1,1,1.0
natural l ang,1,1,1.0
l ang process,1,1,1.0
ang process computat,1,1,1.0
process computat natural,1,1,1.0
computat natural lang,1,1,1.0
natural lang prague,1,1,1.0
lang prague czech,1,1,1.0
prague czech republic,1,1,1.0
czech republic pp,1,1,1.0
republic pp he,1,1,1.0
proc int joint,1,1,1.0
int joint conf,1,1,1.0
joint conf neural,1,1,1.0
conf neural jun,1,1,1.0
neural jun pp,1,1,1.0
jun pp v,1,1,1.0
v chawla lazarevic,1,1,1.0
and bowyer boost,1,1,1.0
bowyer boost improving,1,1,1.0
boost improving prediction,1,1,1.0
in proc principles,1,1,1.0
proc principles knowl,1,1,1.0
principles knowl discovery,1,1,1.0
knowl discovery databases,1,1,1.0
discovery databases croatia,1,1,1.0
databases croatia pp,1,1,1.0
croatia pp y,1,1,1.0
pp y freund,2,1,2.0
y freund and,2,1,2.0
and schapire experiments,1,1,1.0
schapire experiments with,1,1,1.0
experiments with a,1,1,1.0
with a new,1,1,1.0
a new boosting,1,1,1.0
new boosting algorithm,1,1,1.0
boosting algorithm in,1,1,1.0
algorithm in proc,1,1,1.0
learn pp y,1,1,1.0
to boosting comput,1,1,1.0
boosting comput syst,1,1,1.0
comput syst sci,1,1,1.0
syst sci vol,1,1,1.0
sci vol no,1,1,1.0
no pp kubat,1,1,1.0
and matwin addressi,1,1,1.0
matwin addressi ng,1,1,1.0
addressi ng the,1,1,1.0
ng the curse,1,1,1.0
selection in proc,1,1,1.0
mach learn nashville,1,1,1.0
learn nashville tn,1,1,1.0
nashville tn pp,1,1,1.0
tn pp caballero,1,1,1.0
pp caballero martinze,1,1,1.0
caballero martinze hervas,1,1,1.0
martinze hervas and,1,1,1.0
hervas and gutierrez,1,1,1.0
and gutierrez sensitivity,1,1,1.0
gutierrez sensitivity accuracy,1,1,1.0
sensitivity accuracy in,1,1,1.0
accuracy in multiclass,1,1,1.0
in multiclass problems,1,1,1.0
multiclass problems using,1,1,1.0
problems using memetic,1,1,1.0
using memetic pareto,1,1,1.0
memetic pareto evolutionary,1,1,1.0
pareto evolutionary neural,1,1,1.0
evolutionary neural networks,1,1,1.0
neural networks ieee,1,1,1.0
networks ieee trans,1,1,1.0
trans neural netw,2,1,2.0
neural netw vol,2,1,2.0
netw vol no,2,1,2.0
no pp may,1,1,1.0
pp may constructing,1,1,1.0
may constructing ensembles,1,1,1.0
constructing ensembles of,1,1,1.0
ensembles of classiﬁers,1,1,1.0
of classiﬁers by,1,1,1.0
classiﬁers by means,1,1,1.0
means of weighted,1,1,1.0
of weighted instance,1,1,1.0
weighted instance selection,1,1,1.0
instance selection ieee,1,1,1.0
selection ieee trans,1,1,1.0
no pp muhlbaier,1,1,1.0
pp muhlbaier topalis,1,1,1.0
muhlbaier topalis and,1,1,1.0
topalis and polikar,1,1,1.0
and polikar learn,1,1,1.0
polikar learn combining,1,1,1.0
learn combining ensemble,1,1,1.0
combining ensemble of,1,1,1.0
ensemble of classiﬁers,1,1,1.0
of classiﬁers with,1,1,1.0
classiﬁers with dynamically,1,1,1.0
with dynamically weighted,1,1,1.0
dynamically weighted for,1,1,1.0
weighted for efﬁcient,1,1,1.0
for efﬁcient incremental,1,1,1.0
efﬁcient incremental learning,1,1,1.0
incremental learning of,1,1,1.0
learning of new,1,1,1.0
of new classes,1,1,1.0
new classes ieee,1,1,1.0
classes ieee trans,1,1,1.0
no pp ieee,1,1,1.0
pp ieee transactions,1,1,1.0
no october shen,1,1,1.0
october shen and,1,1,1.0
shen and li,1,1,1.0
and li boosting,1,1,1.0
li boosting through,1,1,1.0
boosting through optimization,1,1,1.0
through optimization of,1,1,1.0
optimization of margin,1,1,1.0
of margin utions,1,1,1.0
margin utions ieee,1,1,1.0
utions ieee trans,1,1,1.0
no pp apr,2,1,2.0
pp apr sun,1,1,1.0
apr sun and,1,1,1.0
sun and yao,1,1,1.0
and yao sparse,1,1,1.0
yao sparse approximation,1,1,1.0
sparse approximation through,1,1,1.0
approximation through boosting,1,1,1.0
through boosting for,1,1,1.0
boosting for learning,1,1,1.0
for learning large,1,1,1.0
learning large scale,1,1,1.0
large scale kernel,1,1,1.0
scale kernel machines,1,1,1.0
kernel machines ieee,1,1,1.0
pp jun hu,1,1,1.0
jun hu hu,1,1,1.0
hu hu and,1,1,1.0
hu and maybank,1,1,1.0
and maybank algorithm,1,1,1.0
maybank algorithm for,1,1,1.0
algorithm for network,1,1,1.0
for network intrusion,1,1,1.0
intrusion detection ieee,1,1,1.0
detection ieee trans,1,1,1.0
ieee trans man,1,1,1.0
trans man part,1,1,1.0
man part b,1,1,1.0
part b vol,1,1,1.0
b vol no,1,1,1.0
pp apr he,1,1,1.0
apr he and,1,1,1.0
he and shen,1,1,1.0
and shen a,1,1,1.0
shen a ranked,1,1,1.0
a ranked subspace,1,1,1.0
ranked subspace learning,1,1,1.0
subspace learning method,1,1,1.0
learning method for,1,1,1.0
method for gene,1,1,1.0
for gene expression,1,1,1.0
gene expression data,1,1,1.0
expression data classiﬁcation,1,1,1.0
conf artiﬁcial intell,1,1,1.0
intell pp asuncion,1,1,1.0
pp asuncion and,1,1,1.0
asuncion and newman,1,1,1.0
and newman uci,1,1,1.0
repository online available,1,1,1.0
online available http,2,1,2.0
available http elena,1,1,1.0
http elena project,1,1,1.0
elena project online,1,1,1.0
project online available,1,1,1.0
online available ftp,1,1,1.0
available ftp fawcett,1,1,1.0
ftp fawcett roc,1,1,1.0
fawcett roc graphs,1,1,1.0
roc graphs notes,1,1,1.0
graphs notes and,1,1,1.0
notes and practical,1,1,1.0
and practical considerations,1,1,1.0
practical considerations for,1,1,1.0
considerations for data,1,1,1.0
for data mining,1,1,1.0
data mining researchers,1,1,1.0
mining researchers hp,1,1,1.0
researchers hp palo,1,1,1.0
hp palo alto,1,1,1.0
palo alto ca,1,1,1.0
alto ca tech,1,1,1.0
ca tech kubat,1,1,1.0
tech kubat holte,1,1,1.0
and matwin machine,1,1,1.0
radar images mach,1,1,1.0
images mach vol,1,1,1.0
mach vol nos,1,1,1.0
vol nos pp,1,1,1.0
nos pp maloof,1,1,1.0
unknown in proc,1,1,1.0
washington pp provost,1,1,1.0
and fawcett analysis,1,1,1.0
fawcett analysis and,1,1,1.0
analysis and visualization,1,1,1.0
and visualization of,1,1,1.0
visualization of classiﬁer,1,1,1.0
of classiﬁer performance,1,1,1.0
classiﬁer performance comparison,1,1,1.0
performance comparison under,1,1,1.0
comparison under imprecise,1,1,1.0
under imprecise class,1,1,1.0
imprecise class and,1,1,1.0
cost distributions in,1,1,1.0
distributions in proc,1,1,1.0
data mining newport,1,1,1.0
mining newport beach,1,1,1.0
newport beach ca,1,1,1.0
beach ca pp,1,1,1.0
ca pp hand,1,1,1.0
pp hand measuring,1,1,1.0
hand measuring classiﬁer,1,1,1.0
measuring classiﬁer performance,1,1,1.0
classiﬁer performance a,1,1,1.0
performance a coherent,1,1,1.0
a coherent alternative,1,1,1.0
coherent alternative to,1,1,1.0
alternative to the,1,1,1.0
to the area,1,1,1.0
roc curve mach,1,1,1.0
curve mach learn,1,1,1.0
mach learn vol,1,1,1.0
learn vol no,1,1,1.0
no pp corder,1,1,1.0
pp corder and,1,1,1.0
corder and foreman,1,1,1.0
and foreman nonparametric,1,1,1.0
foreman nonparametric statistics,1,1,1.0
nonparametric statistics for,1,1,1.0
statistics for a,1,1,1.0
for a approach,1,1,1.0
a approach new,1,1,1.0
approach new york,1,1,1.0
new york wiley,1,1,1.0
york wiley demšar,1,1,1.0
wiley demšar statistical,1,1,1.0
demšar statistical comparisons,1,1,1.0
data sets mach,1,1,1.0
sets mach learn,1,1,1.0
res vol no,1,1,1.0
no pp critical,1,1,1.0
pp critical value,1,1,1.0
value table of,1,1,1.0
table of wilcoxon,1,1,1.0
of wilcoxon test,1,1,1.0
wilcoxon test online,1,1,1.0
test online available,1,1,1.0
available http opitz,1,1,1.0
http opitz and,1,1,1.0
opitz and maclin,1,1,1.0
and maclin popular,1,1,1.0
maclin popular ensemble,1,1,1.0
popular ensemble methods,1,1,1.0
ensemble methods an,1,1,1.0
methods an empirical,1,1,1.0
empirical study artiﬁcial,1,1,1.0
study artiﬁcial intell,1,1,1.0
artiﬁcial intell res,1,1,1.0
intell res vol,1,1,1.0
vol pp breiman,1,1,1.0
pp breiman arcing,1,1,1.0
breiman arcing classiﬁers,1,1,1.0
arcing classiﬁers ann,1,1,1.0
classiﬁers ann vol,1,1,1.0
ann vol no,1,1,1.0
v chawla cieslak,1,1,1.0
chawla cieslak hall,1,1,1.0
cieslak hall and,1,1,1.0
hall and joshi,1,1,1.0
and joshi ically,1,1,1.0
joshi ically countering,1,1,1.0
ically countering imbalance,1,1,1.0
countering imbalance and,1,1,1.0
imbalance and its,1,1,1.0
and its empirical,1,1,1.0
its empirical relationship,1,1,1.0
empirical relationship to,1,1,1.0
relationship to cost,1,1,1.0
to cost data,1,1,1.0
cost data mining,1,1,1.0
data mining knowl,1,1,1.0
mining knowl discovery,1,1,1.0
knowl discovery vol,1,1,1.0
discovery vol no,1,1,1.0
no pp anyfantis,1,1,1.0
pp anyfantis karagiannopoulos,1,1,1.0
anyfantis karagiannopoulos kotsiantis,1,1,1.0
karagiannopoulos kotsiantis and,1,1,1.0
and pintelas robustness,1,1,1.0
pintelas robustness of,1,1,1.0
robustness of learning,1,1,1.0
of learning techniques,1,1,1.0
learning techniques in,1,1,1.0
techniques in handling,1,1,1.0
in handling class,1,1,1.0
handling class noise,1,1,1.0
class noise in,1,1,1.0
datasets in proc,1,1,1.0
in proc ifip,1,1,1.0
proc ifip int,1,1,1.0
ifip int federation,1,1,1.0
int federation inform,1,1,1.0
federation inform vol,1,1,1.0
inform vol pp,1,1,1.0
vol pp zhu,1,1,1.0
pp zhu wu,1,1,1.0
zhu wu and,1,1,1.0
and y yang,1,1,1.0
y yang error,1,1,1.0
yang error detection,1,1,1.0
error detection and,1,1,1.0
detection and sensitive,1,1,1.0
and sensitive instance,1,1,1.0
sensitive instance ranking,1,1,1.0
instance ranking in,1,1,1.0
ranking in noisy,1,1,1.0
in noisy datasets,1,1,1.0
noisy datasets in,1,1,1.0
datasets in american,1,1,1.0
in american tion,1,1,1.0
american tion for,1,1,1.0
tion for artiﬁcial,1,1,1.0
for artiﬁcial intelligence,1,1,1.0
artiﬁcial intelligence cambridge,1,1,1.0
intelligence cambridge ma,1,1,1.0
press pp sheng,1,1,1.0
pp sheng chen,1,1,1.0
sheng chen s,1,1,1.0
chen s received,1,1,1.0
s received the,1,1,1.0
received the and,2,1,2.0
the and degrees,1,1,1.0
and degrees in,1,1,1.0
degrees in control,1,1,1.0
in control science,1,1,1.0
control science and,1,1,1.0
and engineering from,1,1,1.0
engineering from huazhong,2,1,2.0
from huazhong university,1,1,1.0
huazhong university of,1,1,1.0
university of science,1,1,1.0
and technology wuhan,2,1,2.0
technology wuhan china,2,1,2.0
wuhan china in,2,1,2.0
china in and,2,1,2.0
in and respectively,2,1,2.0
and respectively he,1,1,1.0
respectively he is,1,1,1.0
he is currently,3,1,3.0
is currently pursuing,1,1,1.0
currently pursuing the,1,1,1.0
pursuing the degree,1,1,1.0
the degree in,4,1,4.0
degree in the,1,1,1.0
in the partment,1,1,1.0
the partment of,2,1,2.0
partment of electrical,2,1,2.0
hoboken nj his,2,1,2.0
nj his current,2,1,2.0
his current research,3,1,3.0
current research interests,3,1,3.0
research interests include,3,1,3.0
interests include machine,2,1,2.0
include machine learning,2,1,2.0
machine learning data,2,1,2.0
learning data mining,2,1,2.0
data mining and,1,1,1.0
mining and computational,1,1,1.0
and computational intelligent,1,1,1.0
computational intelligent systems,1,1,1.0
intelligent systems haibo,1,1,1.0
systems haibo he,1,1,1.0
haibo he m,1,1,1.0
he m received,1,1,1.0
m received the,1,1,1.0
the and grees,1,1,1.0
and grees in,1,1,1.0
grees in electrical,1,1,1.0
in electrical engineering,2,1,2.0
electrical engineering from,2,1,2.0
from huazhong versity,1,1,1.0
huazhong versity of,1,1,1.0
versity of science,1,1,1.0
and respectively and,1,1,1.0
and the degree,2,1,2.0
degree in electrical,1,1,1.0
engineering from ohio,1,1,1.0
from ohio university,1,1,1.0
ohio university athens,1,1,1.0
university athens in,1,1,1.0
athens in he,1,1,1.0
in he is,2,1,2.0
is currently an,1,1,1.0
currently an assistant,1,1,1.0
an assistant professor,2,1,2.0
assistant professor at,2,1,2.0
professor at the,2,1,2.0
at the partment,1,1,1.0
island kingston from,1,1,1.0
kingston from to,1,1,1.0
from to he,1,1,1.0
to he was,1,1,1.0
he was an,1,1,1.0
was an assistant,1,1,1.0
at the department,1,1,1.0
and computer neering,1,1,1.0
computer neering stevens,1,1,1.0
neering stevens institute,1,1,1.0
interests include sys,1,1,1.0
include sys tems,1,1,1.0
sys tems machine,1,1,1.0
tems machine learning,1,1,1.0
data mining putational,1,1,1.0
mining putational intelligence,1,1,1.0
putational intelligence and,1,1,1.0
intelligence and applications,1,1,1.0
and applications in,1,1,1.0
applications in critical,1,1,1.0
in critical engineering,1,1,1.0
critical engineering ﬁelds,1,1,1.0
engineering ﬁelds such,1,1,1.0
ﬁelds such as,1,1,1.0
such as smart,1,1,1.0
as smart grid,1,1,1.0
smart grid and,1,1,1.0
grid and sensor,1,1,1.0
and sensor networks,1,1,1.0
sensor networks very,1,1,1.0
networks very large,1,1,1.0
very large scale,1,1,1.0
large scale integration,1,1,1.0
scale integration and,1,1,1.0
integration and programmable,1,1,1.0
and programmable gate,1,1,1.0
programmable gate array,1,1,1.0
gate array design,1,1,1.0
array design he,1,1,1.0
design he has,1,1,1.0
he has served,1,1,1.0
has served regularly,1,1,1.0
served regularly on,1,1,1.0
regularly on the,1,1,1.0
on the organizing,1,1,1.0
the organizing committees,1,1,1.0
organizing committees of,1,1,1.0
committees of numerous,1,1,1.0
of numerous international,1,1,1.0
numerous international conferences,1,1,1.0
international conferences and,1,1,1.0
conferences and also,1,1,1.0
and also served,1,1,1.0
also served as,1,1,1.0
served as a,1,1,1.0
as a guest,1,1,1.0
a guest editor,1,1,1.0
guest editor for,1,1,1.0
editor for several,1,1,1.0
for several journals,1,1,1.0
several journals including,1,1,1.0
journals including applied,1,1,1.0
including applied mathematics,1,1,1.0
applied mathematics and,1,1,1.0
mathematics and computation,1,1,1.0
and computation soft,1,1,1.0
computation soft computing,1,1,1.0
soft computing and,1,1,1.0
computing and journal,1,1,1.0
and journal of,1,1,1.0
journal of experimental,1,1,1.0
of experimental theoretical,1,1,1.0
experimental theoretical artiﬁcial,1,1,1.0
theoretical artiﬁcial intelligence,1,1,1.0
artiﬁcial intelligence ei,1,1,1.0
intelligence ei s,1,1,1.0
ei s currently,1,1,1.0
s currently the,1,1,1.0
currently the editor,1,1,1.0
the editor of,1,1,1.0
editor of the,1,1,1.0
the ieee computational,1,1,1.0
ieee computational intelligence,1,1,1.0
computational intelligence society,1,1,1.0
intelligence society electronic,1,1,1.0
society electronic letter,1,1,1.0
electronic letter an,1,1,1.0
letter an editorial,1,1,1.0
an editorial board,1,1,1.0
editorial board member,1,1,1.0
board member of,1,1,1.0
member of cognitive,1,1,1.0
of cognitive computation,1,1,1.0
cognitive computation and,1,1,1.0
computation and an,1,1,1.0
and an ciate,1,1,1.0
an ciate editor,1,1,1.0
ciate editor of,1,1,1.0
editor of ieee,1,1,1.0
of ieee t,1,1,1.0
ieee t ransactions,1,1,1.0
t ransactions on,1,1,1.0
ransactions on neural,1,1,1.0
networks and ieee,1,1,1.0
and ieee transactions,1,1,1.0
transactions on smart,1,1,1.0
on smart grid,1,1,1.0
smart grid edwardo,1,1,1.0
grid edwardo garcia,1,1,1.0
edwardo garcia received,1,1,1.0
garcia received the,1,1,1.0
received the degree,1,1,1.0
degree in mathematics,1,1,1.0
in mathematics from,1,1,1.0
mathematics from new,1,1,1.0
from new york,1,1,1.0
new york university,1,1,1.0
york university new,1,1,1.0
university new york,1,1,1.0
new york and,1,1,1.0
york and the,1,1,1.0
degree in computer,1,1,1.0
in computer engineering,1,1,1.0
computer engineering from,1,1,1.0
engineering from stevens,1,1,1.0
from stevens institute,1,1,1.0
hoboken nj both,1,1,1.0
nj both in,1,1,1.0
both in he,1,1,1.0
is currently with,1,1,1.0
currently with the,1,1,1.0
with the stevens,1,1,1.0
the stevens institute,1,1,1.0
of technology his,1,1,1.0
technology his current,1,1,1.0
machine learning biologically,1,1,1.0
learning biologically inspired,1,1,1.0
biologically inspired intelligence,1,1,1.0
inspired intelligence cognitive,1,1,1.0
intelligence cognitive neuroscience,1,1,1.0
cognitive neuroscience data,1,1,1.0
neuroscience data mining,1,1,1.0
mining for medical,1,1,1.0
for medical agnostics,1,1,1.0
medical agnostics and,1,1,1.0
agnostics and mathematical,1,1,1.0
and mathematical methods,1,1,1.0
mathematical methods for,1,1,1.0
methods for magnetic,1,1,1.0
for magnetic resonance,1,1,1.0
magnetic resonance imaging,1,1,1.0
these methods erate,1,1,1.0
methods erate synthetic,1,1,1.0
erate synthetic instances,1,1,1.0
adaptively combines isting,1,1,1.0
combines isting instances,1,1,1.0
isting instances from,1,1,1.0
class the experi,1,1,1.0
the experi ments,1,1,1.0
experi ments also,1,1,1.0
ments also show,1,1,1.0
tasks when m,1,1,1.0
when m ost,1,1,1.0
m ost of,1,1,1.0
ost of data,1,1,1.0
as medical dia,1,1,1.0
medical dia gnosis,1,1,1.0
dia gnosis fraud,1,1,1.0
gnosis fraud detection,1,1,1.0
the class imbal,2,1,2.0
class imbal ance,1,1,1.0
imbal ance problem,1,1,1.0
ance problem have,1,1,1.0
the original d,1,1,1.0
original d ata,1,1,1.0
d ata typically,1,1,1.0
ata typically these,1,1,1.0
for solving ance,1,1,1.0
solving ance issues,1,1,1.0
ance issues in,1,1,1.0
the data augm,1,1,1.0
data augm entation,1,1,1.0
augm entation problem,1,1,1.0
entation problem in,1,1,1.0
imbalance and dat,1,1,1.0
and dat a,1,1,1.0
dat a augmentation,1,1,1.0
a augmentation problems,1,1,1.0
developed to sol,1,1,1.0
to sol ve,1,1,1.0
sol ve problems,1,1,1.0
ve problems in,1,1,1.0
to existing stances,1,1,1.0
existing stances thus,1,1,1.0
stances thus populating,1,1,1.0
adaptations of e,1,1,1.0
of e xisting,1,1,1.0
e xisting data,1,1,1.0
xisting data if,1,1,1.0
by this counterfac,1,1,1.0
this counterfac tual,1,1,1.0
counterfac tual method,1,1,1.0
tual method should,1,1,1.0
synthetic minority sampling,1,1,1.0
minority sampling technique,1,1,1.0
sampling technique smote,1,1,1.0
have some backs,1,1,1.0
some backs since,1,1,1.0
backs since ros,1,1,1.0
to the taset,1,1,1.0
the taset and,1,1,1.0
taset and hence,1,1,1.0
rus randomly re,1,1,1.0
randomly re moves,1,1,1.0
re moves examples,1,1,1.0
moves examples from,1,1,1.0
from the ity,1,1,1.0
the ity class,1,1,1.0
ity class as,1,1,1.0
data augmentation lems,1,1,1.0
augmentation lems see,1,1,1.0
lems see section,1,1,1.0
class is c,1,1,1.0
is c reated,1,1,1.0
c reated by,1,1,1.0
reated by lating,1,1,1.0
by lating between,1,1,1.0
lating between several,1,1,1.0
problem and creat,1,1,1.0
and creat es,1,1,1.0
creat es new,1,1,1.0
es new synthetic,1,1,1.0
and then determine,1,1,1.0
then determine s,1,1,1.0
determine s 𝑚,1,1,1.0
s 𝑚 as,1,1,1.0
where 𝑚 nally,1,1,1.0
𝑚 nally smote,1,1,1.0
nally smote creates,1,1,1.0
𝑝 𝑚 𝑝,1,1,1.0
𝑚 𝑝 𝛿,1,1,1.0
𝑝 𝛿 𝛿,1,1,1.0
𝛿 𝛿 where,1,1,1.0
𝛿 where 𝛿,1,1,1.0
indeed any consi,1,1,1.0
any consi deration,1,1,1.0
consi deration that,1,1,1.0
deration that some,1,1,1.0
do not exi,1,1,1.0
not exi st,1,1,1.0
exi st in,1,1,1.0
st in the,1,1,1.0
hinge on fying,1,1,1.0
on fying regions,1,1,1.0
fying regions in,1,1,1.0
emphasise the importanc,1,1,1.0
the importanc e,1,1,1.0
importanc e of,1,1,1.0
e of focusing,1,1,1.0
and sometimes ana,1,1,1.0
sometimes ana lyze,1,1,1.0
ana lyze the,1,1,1.0
lyze the majority,1,1,1.0
be more portant,1,1,1.0
more portant or,1,1,1.0
portant or safer,1,1,1.0
for instance ns,1,1,1.0
instance ns smote,1,1,1.0
ns smote clusters,1,1,1.0
and then oversample,1,1,1.0
then oversample s,1,1,1.0
oversample s from,1,1,1.0
s from clusters,1,1,1.0
the most stances,1,1,1.0
most stances the,1,1,1.0
stances the assumption,1,1,1.0
points within ters,1,1,1.0
within ters to,1,1,1.0
ters to guide,1,1,1.0
into a spac,1,1,1.0
a spac e,1,1,1.0
spac e and,1,1,1.0
e and uses,1,1,1.0
as smote and,1,1,1.0
to their butions,1,1,1.0
their butions generating,1,1,1.0
butions generating more,1,1,1.0
to the c,1,1,1.0
the c lass,1,1,1.0
c lass boundary,1,1,1.0
lass boundary are,1,1,1.0
to be dled,1,1,1.0
be dled differently,1,1,1.0
dled differently owes,1,1,1.0
to successful classificati,1,1,1.0
successful classificati on,1,1,1.0
classificati on so,1,1,1.0
on so generating,1,1,1.0
generating minority stances,1,1,1.0
minority stances in,1,1,1.0
stances in this,1,1,1.0
the minority c,1,1,1.0
minority c lass,1,1,1.0
c lass is,1,1,1.0
lass is 𝑃,1,1,1.0
as 𝑚 𝑚,1,1,1.0
𝑚 𝑚 𝑚,1,1,1.0
𝑚 𝑚 in,1,1,1.0
majority instances 𝑚,1,1,1.0
instances 𝑚 𝑚,1,1,1.0
𝑚 𝑚 𝑝,2,1,2.0
𝑚 𝑝 is,1,1,1.0
ones 𝑚 𝑚,1,1,1.0
be easily sified,1,1,1.0
easily sified and,1,1,1.0
sified and put,1,1,1.0
set if 𝑚,1,1,1.0
if 𝑚 then,1,1,1.0
𝑚 then 𝑝,1,1,1.0
to generate synthet,1,1,1.0
generate synthet ic,1,1,1.0
synthet ic instances,1,1,1.0
ic instances in,1,1,1.0
the decision ary,1,1,1.0
decision ary and,1,1,1.0
ary and then,1,1,1.0
in a s,1,1,1.0
a s imilar,1,1,1.0
s imilar vein,1,1,1.0
imilar vein smote,1,1,1.0
vein smote divides,1,1,1.0
smote divides minority,1,1,1.0
of majority stances,1,1,1.0
majority stances see,1,1,1.0
stances see also,1,1,1.0
account other ods,1,1,1.0
other ods explore,1,1,1.0
ods explore the,1,1,1.0
class for insta,1,1,1.0
for insta nce,1,1,1.0
insta nce tomek,1,1,1.0
nce tomek finds,1,1,1.0
tomek finds pairs,1,1,1.0
generation smote does,1,1,1.0
smote does this,1,1,1.0
its neighbours smote,1,1,1.0
neighbours smote s,1,1,1.0
smote s finer,1,1,1.0
shown to prove,1,1,1.0
to prove performance,1,1,1.0
prove performance over,1,1,1.0
adopt a differe,1,1,1.0
a differe nt,1,1,1.0
differe nt approach,1,1,1.0
nt approach leveraging,1,1,1.0
the mahalanobis distance,1,1,1.0
mahalanobis distance s,1,1,1.0
distance s requiring,1,1,1.0
s requiring generated,1,1,1.0
see and smote,1,1,1.0
and smote for,1,1,1.0
smote for related,1,1,1.0
for related proaches,1,1,1.0
related proaches finally,1,1,1.0
proaches finally is,1,1,1.0
to majority stances,1,1,1.0
majority stances into,1,1,1.0
stances into account,1,1,1.0
class after sm,1,1,1.0
after sm ote,1,1,1.0
sm ote has,1,1,1.0
ote has been,1,1,1.0
step to move,1,1,1.0
to move generated,1,1,1.0
move generated instances,1,1,1.0
classic counterfactual planation,1,1,1.0
counterfactual planation is,1,1,1.0
planation is one,1,1,1.0
if you quested,1,1,1.0
you quested a,1,1,1.0
quested a loan,1,1,1.0
under which out,1,1,1.0
which out come,1,1,1.0
out come would,1,1,1.0
come would change,1,1,1.0
under diverse nam,1,1,1.0
diverse nam es,1,1,1.0
nam es for,1,1,1.0
es for stance,1,1,1.0
for stance in,1,1,1.0
stance in the,1,1,1.0
in xai bec,1,1,1.0
xai bec ause,1,1,1.0
bec ause they,1,1,1.0
ause they appear,1,1,1.0
encoding the inal,1,1,1.0
the inal loan,1,1,1.0
inal loan refusal,1,1,1.0
a sometimes randoml,1,1,1.0
sometimes randoml y,1,1,1.0
randoml y generated,1,1,1.0
y generated space,1,1,1.0
function that bala,1,1,1.0
that bala nces,1,1,1.0
bala nces proximity,1,1,1.0
nces proximity to,1,1,1.0
best counterfactual instanc,1,1,1.0
counterfactual instanc e,1,1,1.0
instanc e for,1,1,1.0
e for a,1,1,1.0
set of verse,1,1,1.0
of verse counterfactual,1,1,1.0
verse counterfactual candidates,1,1,1.0
these optimization m,1,1,1.0
optimization m ethods,1,1,1.0
m ethods is,1,1,1.0
ethods is that,1,1,1.0
in the ance,1,1,1.0
the ance problem,1,1,1.0
ance problem as,1,1,1.0
minority class w,1,1,1.0
class w ith,1,1,1.0
w ith noise,1,1,1.0
ith noise with,1,1,1.0
noise with quential,1,1,1.0
with quential negative,1,1,1.0
quential negative effects,1,1,1.0
a counterfactual tion,1,1,1.0
counterfactual tion between,1,1,1.0
tion between existing,1,1,1.0
most two fea,1,1,1.0
two fea for,1,1,1.0
fea for example,1,1,1.0
old male accountant,1,1,1.0
male accountant earning,1,1,1.0
the loan sion,1,1,1.0
loan sion is,1,1,1.0
sion is likely,1,1,1.0
arise in zation,1,1,1.0
in zation techniques,1,1,1.0
zation techniques see,1,1,1.0
in data tion,1,1,1.0
data tion to,1,1,1.0
tion to solve,1,1,1.0
generated synthetic counterfact,1,1,1.0
synthetic counterfact ual,1,1,1.0
counterfact ual cases,1,1,1.0
ual cases could,1,1,1.0
cases could prove,1,1,1.0
could prove the,1,1,1.0
prove the predictive,1,1,1.0
papers on counte,1,1,1.0
on counte factuals,1,1,1.0
counte factuals in,1,1,1.0
factuals in xai,1,1,1.0
calling it ity,1,1,1.0
it ity that,1,1,1.0
ity that is,1,1,1.0
original dataset howe,1,1,1.0
dataset howe ver,1,1,1.0
howe ver mothilal,1,1,1.0
ver mothilal et,1,1,1.0
based on generat,1,1,1.0
on generat ed,1,1,1.0
generat ed factuals,1,1,1.0
ed factuals could,1,1,1.0
factuals could act,1,1,1.0
model was t,1,1,1.0
was t rained,1,1,1.0
t rained and,1,1,1.0
rained and tested,1,1,1.0
examples for tual,1,1,1.0
for tual data,1,1,1.0
tual data and,1,1,1.0
et al propos,1,1,1.0
al propos ed,1,1,1.0
propos ed counterfactual,1,1,1.0
ed counterfactual data,1,1,1.0
by stitching gether,1,1,1.0
stitching gether subsamples,1,1,1.0
gether subsamples from,1,1,1.0
bespoke counterfactual ods,1,1,1.0
counterfactual ods developed,1,1,1.0
ods developed for,1,1,1.0
and tested technique,1,1,1.0
tested technique s,1,1,1.0
technique s from,1,1,1.0
s from the,1,1,1.0
problem domain invol,1,1,1.0
domain invol ved,1,1,1.0
invol ved a,1,1,1.0
ved a model,1,1,1.0
from the narios,1,1,1.0
the narios recorded,1,1,1.0
narios recorded in,1,1,1.0
key weather variabl,1,1,1.0
weather variabl es,1,1,1.0
variabl es like,1,1,1.0
es like solar,1,1,1.0
like solar diation,1,1,1.0
solar diation or,1,1,1.0
diation or soil,1,1,1.0
burn grass cordingly,1,1,1.0
grass cordingly the,1,1,1.0
cordingly the model,1,1,1.0
for these disrupted,1,1,1.0
these disrupted months,1,1,1.0
disrupted months of,1,1,1.0
defined a based,1,1,1.0
a based class,1,1,1.0
based class boundary,1,1,1.0
from a cation,1,1,1.0
a cation perspective,1,1,1.0
cation perspective these,1,1,1.0
the counterfactual m,1,1,1.0
counterfactual m ethod,1,1,1.0
m ethod to,1,1,1.0
ethod to create,1,1,1.0
using these mi,1,1,1.0
these mi nority,1,1,1.0
mi nority interestingly,1,1,1.0
nority interestingly temraz,1,1,1.0
that the ods,1,1,1.0
the ods did,1,1,1.0
ods did better,1,1,1.0
domain specifically t,1,1,1.0
specifically t he,1,1,1.0
t he dice,1,1,1.0
he dice method,1,1,1.0
to other probl,1,1,1.0
other probl em,1,1,1.0
probl em domains,1,1,1.0
em domains classifiers,1,1,1.0
solution to ulating,1,1,1.0
to ulating the,1,1,1.0
ulating the minority,1,1,1.0
problems the cation,1,1,1.0
the cation of,1,1,1.0
cation of this,1,1,1.0
by the observati,1,1,1.0
the observati on,1,1,1.0
observati on that,1,1,1.0
on that it,1,1,1.0
furthermore the eva,1,1,1.0
the eva luation,1,1,1.0
eva luation metrics,1,1,1.0
luation metrics in,1,1,1.0
using a d,1,1,1.0
a d reasoning,1,1,1.0
d reasoning proach,1,1,1.0
reasoning proach to,1,1,1.0
proach to generating,1,1,1.0
to binary fication,1,1,1.0
binary fication problems,1,1,1.0
fication problems consider,1,1,1.0
age and history,1,1,1.0
and history that,1,1,1.0
history that is,1,1,1.0
a different ory,1,1,1.0
different ory they,1,1,1.0
ory they have,1,1,1.0
this dataset usi,1,1,1.0
dataset usi ng,1,1,1.0
usi ng our,1,1,1.0
ng our counterfactual,1,1,1.0
instance by thi,1,1,1.0
by thi s,1,1,1.0
thi s known,1,1,1.0
s known counterfactual,1,1,1.0
synthetic minority instanc,1,1,1.0
minority instanc e,1,1,1.0
instanc e using,1,1,1.0
e using the,1,1,1.0
using the features,1,1,1.0
features of and,1,1,1.0
and the ature,1,1,1.0
the ature from,1,1,1.0
ature from for,1,1,1.0
that is counterfac,1,1,1.0
is counterfac tually,1,1,1.0
counterfac tually related,1,1,1.0
tually related to,1,1,1.0
do this i,1,1,1.0
this i teratively,1,1,1.0
i teratively for,1,1,1.0
teratively for all,1,1,1.0
augmentation cfa a,1,1,1.0
cfa a n,1,1,1.0
a n unpaired,1,1,1.0
n unpaired instance,1,1,1.0
the 𝒑 low,1,1,1.0
𝒑 low box,1,1,1.0
low box to,1,1,1.0
synthetic counterfactual 𝒑,1,1,1.0
counterfactual 𝒑 green,1,1,1.0
class 𝑐𝑙𝑎𝑠𝑠e 𝑋,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑋 𝑥,1,1,1.0
𝑋 𝑥 𝑥,1,1,1.0
𝑐𝑙𝑎𝑠𝑠efg 𝑥 𝑥,1,1,1.0
𝑥 𝑥 𝑥j,1,1,1.0
where 𝑥 𝑐𝑓,2,1,2.0
𝑥 𝑐𝑓 𝑥,2,1,2.0
𝑥 𝑝 𝑃,1,1,1.0
𝑝 𝑃 𝑝,1,1,1.0
𝑃 𝑝 𝑝,1,1,1.0
in 𝑐𝑙𝑎𝑠𝑠e 𝑝,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑝 𝑝,1,1,1.0
𝑝 𝑝 𝑝j,1,1,1.0
𝑝 𝑝j 𝑝k,2,1,2.0
where 𝑝 𝑐𝑓,1,1,1.0
𝑝 𝑐𝑓 𝑥,1,1,1.0
𝑡𝑎𝑟𝑔𝑒𝑡 𝑥 𝑡𝑎𝑟𝑔𝑒𝑡,1,1,1.0
𝑥 𝑡𝑎𝑟𝑔𝑒𝑡 𝑝,1,1,1.0
𝑡𝑎𝑟𝑔𝑒𝑡 𝑝 neighbors,1,1,1.0
class 𝑃 𝑥,1,1,1.0
𝑃 𝑥 𝑥j,1,1,1.0
𝑥l 𝑥 𝑝,1,1,1.0
𝑥 𝑝 𝑝j,1,1,1.0
in a taset,1,1,1.0
a taset 𝑇,1,1,1.0
taset 𝑇 these,1,1,1.0
called native cause,1,1,1.0
native cause in,1,1,1.0
cause in one,1,1,1.0
for each paired,1,1,1.0
each paired instance,1,1,1.0
a paired stance,1,1,1.0
paired stance involved,1,1,1.0
stance involved in,1,1,1.0
pairs notably t,1,1,1.0
notably t his,1,1,1.0
t his means,1,1,1.0
his means that,1,1,1.0
distance ed de,1,1,1.0
ed de 𝑝,1,1,1.0
de 𝑝 𝑞,1,1,1.0
𝑝 𝑞 k,1,1,1.0
𝑞 k e,1,1,1.0
k e temraz,1,1,1.0
e temraz keane,1,1,1.0
identified a didate,1,1,1.0
a didate native,1,1,1.0
didate native counterfactual,1,1,1.0
a synthetic tual,1,1,1.0
synthetic tual instance,1,1,1.0
tual instance in,1,1,1.0
in finding matc,1,1,1.0
finding matc and,1,1,1.0
matc and between,1,1,1.0
counterfactual cfa c,1,1,1.0
cfa c omputes,1,1,1.0
c omputes a,1,1,1.0
omputes a ance,1,1,1.0
a ance by,1,1,1.0
ance by finding,1,1,1.0
from the mea,1,1,1.0
the mea n,1,1,1.0
mea n all,1,1,1.0
n all the,2,1,2.0
of the al,1,1,1.0
the al gorithm,1,1,1.0
al gorithm from,1,1,1.0
gorithm from its,1,1,1.0
a good factual,1,1,1.0
good factual pairing,1,1,1.0
factual pairing the,1,1,1.0
keane and sm,1,1,1.0
and sm yth,1,1,1.0
sm yth defined,1,1,1.0
yth defined a,1,1,1.0
are more temraz,1,1,1.0
more temraz keane,1,1,1.0
generated synthetic us,1,1,1.0
synthetic us ing,1,1,1.0
us ing these,1,1,1.0
ing these sparse,1,1,1.0
a critical ence,1,1,1.0
critical ence between,1,1,1.0
ence between the,1,1,1.0
in native counterfactual,1,1,1.0
native counterfactual s,1,1,1.0
counterfactual s this,1,1,1.0
s this is,1,1,1.0
the class literature,1,1,1.0
class literature first,1,1,1.0
on native terfactuals,1,1,1.0
native terfactuals in,2,1,2.0
terfactuals in the,3,1,3.0
and minority i,1,1,1.0
minority i nstances,1,1,1.0
i nstances and,1,1,1.0
nstances and as,1,1,1.0
of those invol,1,1,1.0
those invol ved,1,1,1.0
invol ved in,1,1,1.0
ved in known,1,1,1.0
this counterfactual me,1,1,1.0
counterfactual me thod,1,1,1.0
me thod is,1,1,1.0
thod is quite,1,1,1.0
use interpolation be,1,1,1.0
interpolation be tween,1,1,1.0
be tween nority,1,1,1.0
tween nority instances,1,1,1.0
nority instances but,1,1,1.0
using six oversam,1,1,1.0
six oversam pling,1,1,1.0
oversam pling methods,1,1,1.0
pling methods smote,1,1,1.0
adasyn smote rsb,1,1,1.0
smote rsb these,1,1,1.0
rsb these specific,1,1,1.0
their as tions,1,1,1.0
as tions the,1,1,1.0
tions the six,1,1,1.0
a representative selecti,1,1,1.0
representative selecti on,1,1,1.0
selecti on of,1,1,1.0
on of keel,1,1,1.0
used because ferent,1,1,1.0
because ferent models,1,1,1.0
ferent models find,1,1,1.0
assess the mance,1,1,1.0
the mance of,1,1,1.0
mance of the,1,1,1.0
of these dataset,1,1,1.0
these dataset s,1,1,1.0
dataset s are,1,1,1.0
s are they,1,1,1.0
or and rest,1,1,1.0
and rest ovr,1,1,1.0
rest ovr or,1,1,1.0
predicts it a,1,1,1.0
it a gainst,1,1,1.0
a gainst all,1,1,1.0
gainst all other,1,1,1.0
were modified usi,1,1,1.0
modified usi ng,1,1,1.0
usi ng both,1,1,1.0
ng both methods,1,1,1.0
the chemical ysis,1,1,1.0
chemical ysis consisting,1,1,1.0
ysis consisting of,1,1,1.0
dataset with classe,1,1,1.0
with classe the,1,1,1.0
classe the problem,1,1,1.0
of features tracted,1,1,1.0
features tracted from,1,1,1.0
tracted from the,1,1,1.0
where each subs,1,1,1.0
each subs et,1,1,1.0
subs et included,1,1,1.0
et included mately,1,1,1.0
included mately equal,1,1,1.0
mately equal size,1,1,1.0
split each data,1,1,1.0
each data set,1,1,1.0
data set into,1,1,1.0
set into training,1,1,1.0
the native terfactuals,1,1,1.0
these generated dataset,1,1,1.0
generated dataset s,1,1,1.0
dataset s from,1,1,1.0
s from cfa,1,1,1.0
and the datase,1,1,1.0
the datase ts,1,1,1.0
datase ts generated,1,1,1.0
ts generated by,1,1,1.0
for our ments,1,1,1.0
our ments we,1,1,1.0
ments we oversample,1,1,1.0
the data augmentati,1,1,1.0
data augmentati on,2,1,2.0
augmentati on methods,1,1,1.0
on methods until,1,1,1.0
dataset was als,1,1,1.0
was als o,1,1,1.0
als o run,1,1,1.0
o run as,1,1,1.0
for each tion,1,1,1.0
each tion method,1,1,1.0
tion method rf,1,1,1.0
forest rf 𝑛𝑡𝑟𝑒𝑒,1,1,1.0
rf 𝑛𝑡𝑟𝑒𝑒 𝑚𝑎𝑥r,1,1,1.0
𝑛𝑡𝑟𝑒𝑒 𝑚𝑎𝑥r stu,1,1,1.0
𝑚𝑎𝑥r stu neighbors,1,1,1.0
stu neighbors 𝑛,1,1,1.0
𝑚𝑎𝑥 t 𝐶,1,1,1.0
t 𝐶 𝑠𝑜𝑙𝑣𝑒𝑟,1,1,1.0
𝐶 𝑠𝑜𝑙𝑣𝑒𝑟 multilayer,1,1,1.0
𝑠𝑜𝑙𝑣𝑒𝑟 multilayer perceptron,1,1,1.0
perceptron mlp 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛,1,1,1.0
mlp 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 𝑎𝑙𝑝ℎ𝑎,1,1,1.0
𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 𝑎𝑙𝑝ℎ𝑎 𝑠𝑜𝑙𝑣𝑒𝑟,1,1,1.0
𝑎𝑙𝑝ℎ𝑎 𝑠𝑜𝑙𝑣𝑒𝑟 oversampling,1,1,1.0
𝑠𝑜𝑙𝑣𝑒𝑟 oversampling methods,1,1,1.0
positive or negati,1,1,1.0
or negati ve,1,1,1.0
negati ve so,1,1,1.0
ve so the,1,1,1.0
a given fication,1,1,1.0
given fication a,1,1,1.0
fication a true,1,1,1.0
negative fn curacy,1,1,1.0
fn curacy was,1,1,1.0
curacy was not,1,1,1.0
earlier it ca,1,1,1.0
it ca n,1,1,1.0
ca n be,1,1,1.0
n be spuriously,1,1,1.0
high for i,1,1,1.0
for i balanced,1,1,1.0
i balanced datasets,1,1,1.0
balanced datasets it,1,1,1.0
in our experime,1,1,1.0
our experime nts,1,1,1.0
experime nts were,1,1,1.0
nts were converted,1,1,1.0
as follows 𝑅𝑒𝑐𝑎𝑙𝑙,1,1,1.0
follows 𝑅𝑒𝑐𝑎𝑙𝑙 𝑇𝑃,1,1,1.0
𝑅𝑒𝑐𝑎𝑙𝑙 𝑇𝑃 𝑇𝑃,1,1,1.0
𝑇𝑃 𝑇𝑃 𝐹𝑁,1,1,1.0
𝑇𝑃 𝐹𝑁 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝐹𝑁 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑃,1,1,1.0
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑃 𝑇𝑃,1,1,1.0
𝑇𝑃 𝑇𝑃 𝐹𝑃,1,1,1.0
𝑇𝑃 𝐹𝑃 𝐹j,1,1,1.0
𝐹𝑃 𝐹j 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝐹j 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙,1,1,1.0
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙,1,1,1.0
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝐴𝑈𝐶,1,1,1.0
𝑟𝑒𝑐𝑎𝑙𝑙 𝐴𝑈𝐶 𝑇𝑃,1,1,1.0
𝐴𝑈𝐶 𝑇𝑃 𝑇𝑁,1,1,1.0
𝑇𝑃 𝑇𝑁 table,1,1,1.0
𝑇𝑁 table confusion,1,1,1.0
the classifiers test,1,1,1.0
classifiers test ed,1,1,1.0
test ed see,1,1,1.0
ed see bles,1,1,1.0
see bles recall,1,1,1.0
bles recall the,1,1,1.0
smote adasyn mote,1,1,1.0
adasyn mote smote,1,1,1.0
mote smote and,1,1,1.0
smote and cfa,1,1,1.0
metric for e,1,1,1.0
for e ach,1,1,1.0
e ach sifier,1,1,1.0
ach sifier on,1,1,1.0
sifier on the,1,1,1.0
of with smote,1,1,1.0
with smote being,1,1,1.0
smote being the,1,1,1.0
a greater provement,1,1,1.0
greater provement in,1,1,1.0
provement in the,1,1,1.0
does better tha,1,1,1.0
tha n all,1,1,1.0
being the ne,1,1,1.0
the ne xt,1,1,1.0
ne xt best,1,1,1.0
xt best with,1,1,1.0
the highest au,1,1,1.0
highest au in,1,1,1.0
au in dataset,1,1,1.0
with the rsb,1,1,1.0
the rsb being,1,1,1.0
rsb being the,1,1,1.0
datasets with e,1,1,1.0
with e being,1,1,1.0
e being the,1,1,1.0
had the highes,1,1,1.0
the highes t,1,1,1.0
highes t in,1,1,1.0
t in temraz,1,1,1.0
a contribution i,1,1,1.0
contribution i t,1,1,1.0
i t seems,1,1,1.0
t seems to,1,1,1.0
baseline smote smote,4,1,4.0
smote smote adasyn,4,1,4.0
when the sion,1,1,1.0
the sion score,1,1,1.0
sion score is,1,1,1.0
see table i,1,1,1.0
table i n,1,1,1.0
i n of,1,1,1.0
n of cases,1,1,1.0
for for line,1,1,1.0
for line for,1,1,1.0
line for for,1,1,1.0
for smote smote,1,1,1.0
smote smote and,1,1,1.0
cases it ha,1,1,1.0
it ha s,1,1,1.0
ha s the,1,1,1.0
s the highest,1,1,1.0
augmentation methods smot,1,1,1.0
methods smot e,1,1,1.0
smot e smote,1,1,1.0
e smote adasyn,1,1,1.0
and these rithms,1,1,1.0
these rithms achieved,1,1,1.0
rithms achieved lower,1,1,1.0
the most i,1,1,1.0
most i nteresting,1,1,1.0
i nteresting result,1,1,1.0
nteresting result is,1,1,1.0
no data tion,1,1,1.0
data tion and,1,1,1.0
tion and smote,1,1,1.0
the best mance,1,1,1.0
best mance in,1,1,1.0
mance in out,1,1,1.0
different data s,1,1,1.0
data s ets,1,1,1.0
s ets finally,1,1,1.0
ets finally in,1,1,1.0
sensitivity and s,1,1,1.0
and s pecificity,1,1,1.0
s pecificity are,1,1,1.0
pecificity are presented,1,1,1.0
where cfa formed,1,1,1.0
cfa formed methods,1,1,1.0
formed methods obtained,1,1,1.0
classifiers on ent,1,1,1.0
on ent data,1,1,1.0
ent data sets,1,1,1.0
other data augmenta,1,1,1.0
data augmenta tion,2,1,2.0
augmenta tion methods,1,1,1.0
tion methods smote,1,1,1.0
figure these re,1,1,1.0
these re sults,1,1,1.0
re sults support,1,1,1.0
sults support our,1,1,1.0
other hand fi,1,1,1.0
hand fi gure,1,1,1.0
fi gure shows,1,1,1.0
gure shows several,1,1,1.0
outperformed methods tained,1,1,1.0
methods tained for,1,1,1.0
tained for the,1,1,1.0
datapoints for planatory,1,1,1.0
for planatory purposes,1,1,1.0
planatory purposes indeed,1,1,1.0
these explanatory tuals,1,1,1.0
explanatory tuals are,1,1,1.0
tuals are generally,1,1,1.0
the data tion,1,1,1.0
data tion as,1,1,1.0
tion as we,1,1,1.0
prediction problem s,1,1,1.0
problem s howed,1,1,1.0
s howed that,1,1,1.0
howed that generated,1,1,1.0
the minority cla,1,1,1.0
minority cla ss,1,1,1.0
cla ss improved,1,1,1.0
ss improved performance,1,1,1.0
because it ates,1,1,1.0
it ates minority,1,1,1.0
ates minority instances,1,1,1.0
from known minority,1,1,1.0
known minority but,1,1,1.0
minority but this,1,1,1.0
solved by ing,1,1,1.0
by ing similar,1,1,1.0
ing similar cases,1,1,1.0
with a apa,1,1,1.0
a apa rtment,1,1,1.0
apa rtment with,1,1,1.0
rtment with bathrooms,1,1,1.0
with bathrooms and,1,1,1.0
bathrooms and the,1,1,1.0
apartment with hrooms,1,1,1.0
with hrooms the,1,1,1.0
hrooms the system,1,1,1.0
the historical c,1,1,1.0
historical c ase,1,1,1.0
c ase and,1,1,1.0
ase and the,1,1,1.0
learned from number,1,1,1.0
from number differences,1,1,1.0
number differences found,1,1,1.0
capture the ke,1,1,1.0
the ke y,1,1,1.0
ke y that,1,1,1.0
y that lead,1,1,1.0
synthetic minority insta,1,1,1.0
minority insta nces,1,1,1.0
insta nces they,1,1,1.0
nces they stand,1,1,1.0
transformations though t,1,1,1.0
though t hey,1,1,1.0
t hey lack,1,1,1.0
hey lack generality,1,1,1.0
the same rences,1,1,1.0
same rences as,1,1,1.0
rences as is,1,1,1.0
and local member,1,1,1.0
local member cfa,1,1,1.0
member cfa only,1,1,1.0
so the ship,1,1,1.0
the ship is,1,1,1.0
ship is highly,1,1,1.0
are already ve,1,1,1.0
already ve ry,1,1,1.0
ve ry similar,1,1,1.0
ry similar all,1,1,1.0
to fail howe,1,1,1.0
fail howe ver,1,1,1.0
howe ver there,1,1,1.0
ver there are,1,1,1.0
to i t,1,1,1.0
i t he,1,1,1.0
t he quality,1,1,1.0
he quality of,1,1,1.0
quality of datas,1,1,1.0
of datas et,1,1,1.0
datas et differences,1,1,1.0
et differences ii,1,1,1.0
generate synthetic dat,1,1,1.0
synthetic dat apoints,1,1,1.0
dat apoints will,1,1,1.0
apoints will be,1,1,1.0
will be verely,1,1,1.0
be verely hampered,1,1,1.0
verely hampered current,1,1,1.0
not systematically tes,1,1,1.0
systematically tes ted,1,1,1.0
tes ted how,1,1,1.0
ted how changes,1,1,1.0
to data augmenta,1,1,1.0
augmenta tion in,1,1,1.0
tion in data,1,1,1.0
produces very minim,1,1,1.0
very minim ferent,1,1,1.0
minim ferent counterfactual,1,1,1.0
ferent counterfactual pairs,1,1,1.0
they explored us,1,1,1.0
explored us ing,1,1,1.0
us ing and,1,1,1.0
ing and counterfactuals,1,1,1.0
significantly improve tive,1,1,1.0
improve tive importance,1,1,1.0
tive importance that,1,1,1.0
generate useful minori,1,1,1.0
useful minori ty,1,1,1.0
minori ty instances,1,1,1.0
ty instances we,1,1,1.0
numbers of differences,1,1,1.0
of differences were,1,1,1.0
differences were used,1,1,1.0
for that fe,1,1,1.0
that fe ature,1,1,1.0
fe ature this,1,1,1.0
ature this ance,1,1,1.0
this ance was,1,1,1.0
ance was applied,1,1,1.0
a more cated,1,1,1.0
more cated tolerance,1,1,1.0
cated tolerance scheme,1,1,1.0
tolerance fewer tuals,1,1,1.0
fewer tuals would,1,1,1.0
tuals would be,1,1,1.0
boundary so clearl,1,1,1.0
so clearl y,1,1,1.0
clearl y the,1,1,1.0
y the definition,1,1,1.0
is likely t,1,1,1.0
likely t o,1,1,1.0
t o disimprove,1,1,1.0
o disimprove in,1,1,1.0
uses a soning,1,1,1.0
a soning approach,1,1,1.0
soning approach to,1,1,1.0
the m l,1,1,1.0
m l models,1,1,1.0
l models ii,1,1,1.0
by leveraging know,1,1,1.0
leveraging know n,1,1,1.0
know n terfactuals,1,1,1.0
n terfactuals in,1,1,1.0
benchmark smote ants,1,1,1.0
smote ants on,1,1,1.0
ants on a,1,1,1.0
integration of rithms,1,1,1.0
of rithms and,1,1,1.0
rithms and experimental,1,1,1.0
knowledge and formation,1,1,1.0
and formation systems,1,1,1.0
formation systems bishop,1,1,1.0
sinapiromsaran lursinsap smote,1,1,1.0
lursinsap smote minority,1,1,1.0
smote minority technique,1,1,1.0
class imbal anced,1,1,1.0
imbal anced problem,1,1,1.0
anced problem in,1,1,1.0
lursinsap dbsmote based,1,1,1.0
dbsmote based synthetic,1,1,1.0
based synthetic minority,1,1,1.0
bischl b counterfactua,1,1,1.0
b counterfactua l,1,1,1.0
counterfactua l explanations,1,1,1.0
l explanations in,1,1,1.0
the twentieth national,1,1,1.0
twentieth national joint,1,1,1.0
for optimal neare,1,1,1.0
optimal neare st,1,1,1.0
neare st neighbor,1,1,1.0
st neighbor decision,1,1,1.0
keane counterfactual tions,1,1,1.0
counterfactual tions for,1,1,1.0
tions for time,1,1,1.0
somo for anced,1,1,1.0
anced data set,1,1,1.0
through a tic,1,1,1.0
a tic oversampling,1,1,1.0
tic oversampling method,1,1,1.0
a in ceedings,1,1,1.0
in ceedings of,1,1,1.0
ceedings of the,1,1,1.0
adaptive synthetic sampl,1,1,1.0
synthetic sampl ing,1,1,1.0
sampl ing approach,1,1,1.0
ing approach for,1,1,1.0
vector chines ieee,1,1,1.0
chines ieee transactions,1,1,1.0
improving classification mance,1,1,1.0
classification mance when,1,1,1.0
mance when training,1,1,1.0
imbalance data classific,1,1,1.0
data classific ation,1,1,1.0
classific ation based,1,1,1.0
ation based on,1,1,1.0
oversampling and evolutional,1,1,1.0
and evolutional ly,1,1,1.0
evolutional ly underdamping,1,1,1.0
ly underdamping soft,1,1,1.0
for feature lection,1,1,1.0
feature lection and,1,1,1.0
lection and parameter,1,1,1.0
borderline for anced,1,1,1.0
anced data classification,1,1,1.0
partially guided brid,1,1,1.0
guided brid sampling,1,1,1.0
brid sampling for,1,1,1.0
in data augmentati,1,1,1.0
augmentati on to,1,1,1.0
on to predict,1,1,1.0
conference on based,1,1,1.0
on based reasoning,1,1,1.0
based reasoning springer,1,1,1.0
costs in ings,1,1,1.0

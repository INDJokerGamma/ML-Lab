,gram_count,document_count,Average_freq
temraz keane counterfactual data,82,2,41.0
keane counterfactual data augmentation,94,2,47.0
counterfactual data augmentation abstract,2,2,1.0
data augmentation abstract learning,2,2,1.0
augmentation abstract learning from,2,2,1.0
abstract learning from class,2,2,1.0
learning from class imbalanced,2,2,1.0
from class imbalanced datasets,2,2,1.0
class imbalanced datasets poses,2,2,1.0
imbalanced datasets poses challenges,2,2,1.0
datasets poses challenges for,2,2,1.0
poses challenges for many,2,2,1.0
challenges for many machine,2,2,1.0
for many machine learning,2,2,1.0
many machine learning algorithms,2,2,1.0
machine learning algorithms many,2,2,1.0
learning algorithms many domains,2,2,1.0
algorithms many domains are,2,2,1.0
many domains are by,2,2,1.0
domains are by definition,2,2,1.0
are by definition class,2,2,1.0
by definition class imbalanced,2,2,1.0
definition class imbalanced by,2,2,1.0
class imbalanced by virtue,2,2,1.0
imbalanced by virtue of,2,2,1.0
by virtue of having,2,2,1.0
virtue of having a,2,2,1.0
of having a majority,2,2,1.0
having a majority class,2,2,1.0
a majority class that,2,2,1.0
majority class that naturally,2,2,1.0
class that naturally has,2,2,1.0
that naturally has many,2,2,1.0
naturally has many more,2,2,1.0
has many more instances,2,2,1.0
many more instances than,4,2,2.0
more instances than its,2,2,1.0
instances than its minority,2,2,1.0
than its minority class,2,2,1.0
its minority class genuine,2,2,1.0
minority class genuine bank,2,2,1.0
class genuine bank transactions,2,2,1.0
genuine bank transactions occur,2,2,1.0
bank transactions occur much,2,2,1.0
transactions occur much more,2,2,1.0
occur much more often,2,2,1.0
much more often than,2,2,1.0
more often than fraudulent,2,2,1.0
often than fraudulent ones,2,2,1.0
than fraudulent ones many,2,2,1.0
fraudulent ones many methods,2,2,1.0
ones many methods have,2,2,1.0
many methods have been,2,2,1.0
methods have been proposed,2,2,1.0
have been proposed to,2,2,1.0
been proposed to solve,2,2,1.0
proposed to solve the,4,2,2.0
to solve the class,4,2,2.0
solve the class imbalance,3,2,1.5
the class imbalance problem,39,6,6.5
class imbalance problem among,2,2,1.0
imbalance problem among the,2,2,1.0
problem among the most,2,2,1.0
among the most popular,2,2,1.0
the most popular being,2,2,1.0
most popular being oversampling,2,2,1.0
popular being oversampling techniques,2,2,1.0
being oversampling techniques such,2,2,1.0
oversampling techniques such as,2,2,1.0
techniques such as smote,2,2,1.0
such as smote these,2,2,1.0
as smote these methods,2,2,1.0
smote these methods synthetic,1,1,1.0
these methods synthetic instances,1,1,1.0
methods synthetic instances in,1,1,1.0
synthetic instances in the,3,2,1.5
instances in the minority,12,2,6.0
in the minority class,50,4,12.5
the minority class to,6,4,1.5
minority class to balance,2,2,1.0
class to balance the,2,2,1.0
to balance the dataset,2,2,1.0
balance the dataset performing,2,2,1.0
the dataset performing data,2,2,1.0
dataset performing data augmentations,2,2,1.0
performing data augmentations that,2,2,1.0
data augmentations that improve,2,2,1.0
augmentations that improve the,2,2,1.0
that improve the performance,4,2,2.0
improve the performance of,4,2,2.0
the performance of predictive,2,2,1.0
performance of predictive machine,2,2,1.0
of predictive machine learning,2,2,1.0
predictive machine learning ml,2,2,1.0
machine learning ml models,2,2,1.0
learning ml models in,2,2,1.0
ml models in this,2,2,1.0
models in this paper,2,2,1.0
in this paper we,11,5,2.2
this paper we advance,2,2,1.0
paper we advance a,2,2,1.0
we advance a novel,2,2,1.0
advance a novel data,2,2,1.0
a novel data augmentation,2,2,1.0
novel data augmentation method,2,2,1.0
data augmentation method adapted,2,2,1.0
augmentation method adapted from,2,2,1.0
method adapted from explainable,2,2,1.0
adapted from explainable ai,2,2,1.0
from explainable ai that,2,2,1.0
explainable ai that generates,2,2,1.0
ai that generates synthetic,2,2,1.0
that generates synthetic counterfactual,2,2,1.0
generates synthetic counterfactual instances,6,2,3.0
synthetic counterfactual instances in,6,2,3.0
counterfactual instances in the,4,2,2.0
the minority class unlike,2,2,1.0
minority class unlike other,2,2,1.0
class unlike other oversampling,2,2,1.0
unlike other oversampling techniques,2,2,1.0
other oversampling techniques this,2,2,1.0
oversampling techniques this method,2,2,1.0
techniques this method adaptively,2,2,1.0
this method adaptively combines,2,2,1.0
method adaptively combines instances,1,1,1.0
adaptively combines instances from,1,1,1.0
combines instances from the,1,1,1.0
instances from the dataset,2,2,1.0
from the dataset using,2,2,1.0
the dataset using actual,2,2,1.0
dataset using actual rather,2,2,1.0
using actual rather than,2,2,1.0
actual rather than interpolating,2,2,1.0
rather than interpolating values,2,2,1.0
than interpolating values between,2,2,1.0
interpolating values between instances,2,2,1.0
values between instances several,2,2,1.0
between instances several experiments,2,2,1.0
instances several experiments using,2,2,1.0
several experiments using four,2,2,1.0
experiments using four different,2,2,1.0
using four different classifiers,2,2,1.0
four different classifiers and,2,2,1.0
different classifiers and solving,2,2,1.0
classifiers and solving the,2,2,1.0
and solving the class,2,2,1.0
solving the class imbalance,2,2,1.0
class imbalance problem using,2,2,1.0
imbalance problem using a,2,2,1.0
problem using a counterfactual,2,2,1.0
using a counterfactual method,2,2,1.0
a counterfactual method for,2,2,1.0
counterfactual method for data,4,2,2.0
method for data augmentation,6,2,3.0
for data augmentation mohammed,2,2,1.0
data augmentation mohammed mark,2,2,1.0
augmentation mohammed mark school,2,2,1.0
mohammed mark school of,2,2,1.0
mark school of computer,2,2,1.0
school of computer science,5,4,1.25
of computer science university,4,2,2.0
computer science university college,2,2,1.0
science university college dublin,2,2,1.0
university college dublin belfield,6,2,3.0
college dublin belfield dublin,6,2,3.0
dublin belfield dublin ireland,6,2,3.0
belfield dublin ireland insight,2,2,1.0
dublin ireland insight centre,2,2,1.0
ireland insight centre for,2,2,1.0
insight centre for data,4,2,2.0
centre for data analytics,4,2,2.0
for data analytics university,2,2,1.0
data analytics university college,2,2,1.0
analytics university college dublin,2,2,1.0
belfield dublin ireland vistamilk,2,2,1.0
dublin ireland vistamilk sfi,2,2,1.0
ireland vistamilk sfi research,2,2,1.0
vistamilk sfi research centre,4,2,2.0
sfi research centre university,2,2,1.0
research centre university college,2,2,1.0
centre university college dublin,2,2,1.0
belfield dublin ireland temraz,2,2,1.0
dublin ireland temraz keane,2,2,1.0
ireland temraz keane counterfactual,2,2,1.0
counterfactual data augmentation datasets,4,2,2.0
data augmentation datasets are,2,2,1.0
augmentation datasets are reported,2,2,1.0
datasets are reported which,2,2,1.0
are reported which show,2,2,1.0
reported which show that,2,2,1.0
which show that this,2,2,1.0
show that this counterfactual,2,2,1.0
that this counterfactual augmentation,2,2,1.0
this counterfactual augmentation method,2,2,1.0
counterfactual augmentation method cfa,2,2,1.0
augmentation method cfa generates,2,2,1.0
method cfa generates useful,2,2,1.0
cfa generates useful synthetic,2,2,1.0
generates useful synthetic datapoints,2,2,1.0
useful synthetic datapoints in,2,2,1.0
synthetic datapoints in the,2,2,1.0
datapoints in the minority,2,2,1.0
the minority class the,7,4,1.75
minority class the experiments,1,1,1.0
class the experiments also,1,1,1.0
the experiments also show,1,1,1.0
experiments also show that,1,1,1.0
also show that cfa,2,2,1.0
show that cfa is,2,2,1.0
that cfa is competitive,2,2,1.0
cfa is competitive with,2,2,1.0
is competitive with many,2,2,1.0
competitive with many other,2,2,1.0
with many other oversampling,2,2,1.0
many other oversampling methods,2,2,1.0
other oversampling methods many,2,2,1.0
oversampling methods many of,2,2,1.0
methods many of which,2,2,1.0
many of which are,2,2,1.0
of which are variants,2,2,1.0
which are variants of,2,2,1.0
are variants of smote,2,2,1.0
variants of smote the,2,2,1.0
of smote the basis,2,2,1.0
smote the basis for,2,2,1.0
the basis for cfa,2,2,1.0
basis for cfa s,2,2,1.0
for cfa s performance,2,2,1.0
cfa s performance is,2,2,1.0
s performance is discussed,2,2,1.0
performance is discussed along,2,2,1.0
is discussed along with,2,2,1.0
discussed along with the,2,2,1.0
along with the conditions,2,2,1.0
with the conditions under,2,2,1.0
the conditions under which,4,2,2.0
conditions under which it,2,2,1.0
under which it is,2,2,1.0
which it is likely,2,2,1.0
it is likely to,2,2,1.0
is likely to perform,2,2,1.0
likely to perform better,2,2,1.0
to perform better or,2,2,1.0
perform better or worse,2,2,1.0
better or worse in,2,2,1.0
or worse in future,2,2,1.0
worse in future tests,2,2,1.0
in future tests keywords,2,2,1.0
future tests keywords counterfactual,2,2,1.0
tests keywords counterfactual class,2,2,1.0
keywords counterfactual class imbalance,2,2,1.0
counterfactual class imbalance problem,2,2,1.0
class imbalance problem reasoning,2,2,1.0
imbalance problem reasoning data,2,2,1.0
problem reasoning data augmentation,2,2,1.0
reasoning data augmentation explainable,2,2,1.0
data augmentation explainable ai,2,2,1.0
augmentation explainable ai introduction,2,2,1.0
explainable ai introduction imbalanced,2,2,1.0
ai introduction imbalanced datasets,2,2,1.0
introduction imbalanced datasets create,2,2,1.0
imbalanced datasets create significant,2,2,1.0
datasets create significant problems,2,2,1.0
create significant problems for,2,2,1.0
significant problems for machine,2,2,1.0
problems for machine learning,2,2,1.0
for machine learning ml,2,2,1.0
machine learning ml in,2,2,1.0
learning ml in classification,2,2,1.0
ml in classification tasks,2,2,1.0
in classification tasks classically,2,2,1.0
classification tasks classically this,2,2,1.0
tasks classically this problem,2,2,1.0
classically this problem arises,2,2,1.0
this problem arises in,2,2,1.0
problem arises in binary,2,2,1.0
arises in binary classification,2,2,1.0
in binary classification tasks,2,2,1.0
binary classification tasks when,2,2,1.0
classification tasks when most,1,1,1.0
tasks when most of,1,1,1.0
when most of data,1,1,1.0
most of data comes,1,1,1.0
of data comes from,2,2,1.0
data comes from one,2,2,1.0
comes from one class,2,2,1.0
from one class the,2,2,1.0
one class the majority,2,2,1.0
class the majority class,2,2,1.0
the majority class and,18,4,4.5
majority class and less,2,2,1.0
class and less comes,2,2,1.0
and less comes from,2,2,1.0
less comes from the,2,2,1.0
comes from the other,2,2,1.0
from the other class,2,2,1.0
the other class the,2,2,1.0
other class the minority,2,2,1.0
class the minority class,2,2,1.0
the minority class for,6,3,2.0
minority class for instance,3,2,1.5
class for instance in,2,2,1.0
for instance in datasets,2,2,1.0
instance in datasets always,2,2,1.0
in datasets always have,2,2,1.0
datasets always have many,2,2,1.0
always have many more,2,2,1.0
have many more instances,2,2,1.0
more instances than fraudulent,2,2,1.0
instances than fraudulent ones,2,2,1.0
than fraudulent ones simply,2,2,1.0
fraudulent ones simply because,2,2,1.0
ones simply because the,2,2,1.0
simply because the latter,2,2,1.0
because the latter are,2,2,1.0
the latter are rarer,2,2,1.0
latter are rarer than,2,2,1.0
are rarer than the,2,2,1.0
rarer than the former,2,2,1.0
than the former when,2,2,1.0
the former when a,2,2,1.0
former when a given,2,2,1.0
when a given class,2,2,1.0
a given class is,2,2,1.0
given class is in,2,2,1.0
class is in the,2,2,1.0
is in the dataset,2,2,1.0
in the dataset in,2,2,1.0
the dataset in this,3,3,1.0
dataset in this way,2,2,1.0
in this way a,2,2,1.0
this way a classifier,2,2,1.0
way a classifier s,2,2,1.0
a classifier s performance,4,2,2.0
classifier s performance can,2,2,1.0
s performance can be,2,2,1.0
performance can be compromised,2,2,1.0
can be compromised in,2,2,1.0
be compromised in several,2,2,1.0
compromised in several ways,2,2,1.0
in several ways for,2,2,1.0
several ways for instance,2,2,1.0
ways for instance it,2,2,1.0
for instance it may,2,2,1.0
instance it may show,2,2,1.0
it may show poor,2,2,1.0
may show poor accuracy,2,2,1.0
show poor accuracy in,2,2,1.0
poor accuracy in predicting,2,2,1.0
accuracy in predicting the,2,2,1.0
in predicting the minority,2,2,1.0
predicting the minority class,2,2,1.0
the minority class or,5,3,1.6666666666666667
minority class or spuriously,2,2,1.0
class or spuriously high,2,2,1.0
or spuriously high accuracy,2,2,1.0
spuriously high accuracy for,2,2,1.0
high accuracy for the,2,2,1.0
accuracy for the classifier,2,2,1.0
for the classifier as,2,2,1.0
the classifier as a,2,2,1.0
classifier as a whole,2,2,1.0
as a whole based,2,2,1.0
a whole based only,2,2,1.0
whole based only on,2,2,1.0
based only on its,2,2,1.0
only on its success,2,2,1.0
on its success with,2,2,1.0
its success with the,2,2,1.0
success with the majority,2,2,1.0
with the majority class,6,2,3.0
the majority class it,2,2,1.0
majority class it can,2,2,1.0
class it can result,2,2,1.0
it can result in,2,2,1.0
can result in poor,2,2,1.0
result in poor rule,2,2,1.0
in poor rule induction,2,2,1.0
poor rule induction for,2,2,1.0
rule induction for decision,2,2,1.0
induction for decision trees,2,2,1.0
for decision trees this,2,2,1.0
decision trees this problem,2,2,1.0
trees this problem has,2,2,1.0
this problem has been,2,2,1.0
problem has been recognized,4,3,1.3333333333333333
has been recognized in,3,3,1.0
been recognized in many,3,3,1.0
recognized in many application,2,2,1.0
in many application domains,3,3,1.0
many application domains such,2,2,1.0
application domains such as,3,3,1.0
domains such as medical,2,2,1.0
such as medical diagnosis,2,2,1.0
as medical diagnosis fraud,1,1,1.0
medical diagnosis fraud detection,1,1,1.0
diagnosis fraud detection text,1,1,1.0
fraud detection text classification,2,2,1.0
detection text classification and,2,2,1.0
text classification and detection,2,2,1.0
classification and detection of,2,2,1.0
and detection of oil,2,2,1.0
detection of oil spills,6,4,1.5
of oil spills in,6,4,1.5
oil spills in satellite,6,4,1.5
spills in satellite radar,6,4,1.5
in satellite radar images,6,4,1.5
satellite radar images notably,2,2,1.0
radar images notably recently,2,2,1.0
images notably recently some,2,2,1.0
notably recently some of,2,2,1.0
recently some of the,2,2,1.0
some of the techniques,2,2,1.0
of the techniques proposed,2,2,1.0
the techniques proposed to,2,2,1.0
techniques proposed to solve,2,2,1.0
class imbalance problem have,1,1,1.0
imbalance problem have also,1,1,1.0
problem have also proved,2,2,1.0
have also proved useful,2,2,1.0
also proved useful in,2,2,1.0
proved useful in data,2,2,1.0
useful in data augmentation,2,2,1.0
in data augmentation for,2,2,1.0
data augmentation for deep,6,2,3.0
augmentation for deep learning,6,2,3.0
for deep learning models,2,2,1.0
deep learning models when,2,2,1.0
learning models when new,2,2,1.0
models when new synthetic,2,2,1.0
when new synthetic keane,2,2,1.0
new synthetic keane counterfactual,2,2,1.0
synthetic keane counterfactual data,2,2,1.0
counterfactual data augmentation points,4,2,2.0
data augmentation points need,2,2,1.0
augmentation points need to,2,2,1.0
points need to be,2,2,1.0
need to be generated,3,3,1.0
to be generated to,2,2,1.0
be generated to create,2,2,1.0
generated to create the,2,2,1.0
to create the large,2,2,1.0
create the large labelled,2,2,1.0
the large labelled datasets,2,2,1.0
large labelled datasets required,2,2,1.0
labelled datasets required for,2,2,1.0
datasets required for better,2,2,1.0
required for better performance,2,2,1.0
for better performance in,2,2,1.0
better performance in this,2,2,1.0
performance in this literature,2,2,1.0
in this literature several,2,2,1.0
this literature several important,2,2,1.0
literature several important approaches,2,2,1.0
several important approaches have,2,2,1.0
important approaches have emerged,2,2,1.0
approaches have emerged to,2,2,1.0
have emerged to deal,2,2,1.0
emerged to deal with,2,2,1.0
to deal with this,2,2,1.0
deal with this problem,2,2,1.0
with this problem based,2,2,1.0
this problem based on,2,2,1.0
problem based on solutions,2,2,1.0
based on solutions at,2,2,1.0
on solutions at the,2,2,1.0
solutions at the data,2,2,1.0
at the data or,2,2,1.0
the data or algorithm,2,2,1.0
data or algorithm levels,2,2,1.0
or algorithm levels data,2,2,1.0
algorithm levels data level,2,2,1.0
levels data level solutions,2,2,1.0
data level solutions attempt,2,2,1.0
level solutions attempt to,2,2,1.0
solutions attempt to change,2,2,1.0
attempt to change the,2,2,1.0
to change the distribution,2,2,1.0
change the distribution of,2,2,1.0
the distribution of the,2,2,1.0
distribution of the imbalanced,2,2,1.0
of the imbalanced data,2,2,1.0
the imbalanced data by,2,2,1.0
imbalanced data by the,2,2,1.0
data by the original,2,2,1.0
by the original data,1,1,1.0
the original data typically,1,1,1.0
original data typically these,1,1,1.0
data typically these techniques,1,1,1.0
typically these techniques either,2,2,1.0
these techniques either oversample,2,2,1.0
techniques either oversample the,2,2,1.0
either oversample the minority,2,2,1.0
oversample the minority class,6,2,3.0
minority class or undersample,2,2,1.0
class or undersample the,2,2,1.0
or undersample the majority,2,2,1.0
undersample the majority class,4,2,2.0
the majority class or,4,2,2.0
majority class or sample,2,2,1.0
class or sample using,2,2,1.0
or sample using some,2,2,1.0
sample using some combination,2,2,1.0
using some combination of,2,2,1.0
some combination of both,2,2,1.0
combination of both of,2,2,1.0
of both of these,2,2,1.0
both of these methods,2,2,1.0
of these methods specifically,2,2,1.0
these methods specifically the,2,2,1.0
methods specifically the synthetic,2,2,1.0
specifically the synthetic minority,2,2,1.0
the synthetic minority technique,3,3,1.0
synthetic minority technique smote,5,3,1.6666666666666667
minority technique smote has,2,2,1.0
technique smote has become,2,2,1.0
smote has become a,2,2,1.0
has become a very,2,2,1.0
become a very popular,2,2,1.0
a very popular method,2,2,1.0
very popular method for,2,2,1.0
popular method for solving,2,2,1.0
method for solving issues,1,1,1.0
for solving issues in,1,1,1.0
solving issues in traditional,1,1,1.0
issues in traditional ml,2,2,1.0
in traditional ml and,2,2,1.0
traditional ml and has,2,2,1.0
ml and has also,2,2,1.0
and has also been,2,2,1.0
has also been applied,2,2,1.0
also been applied to,2,2,1.0
been applied to the,2,2,1.0
applied to the data,2,2,1.0
to the data augmentation,1,1,1.0
the data augmentation problem,1,1,1.0
data augmentation problem in,1,1,1.0
augmentation problem in deep,1,1,1.0
problem in deep learners,2,2,1.0
in deep learners algorithm,2,2,1.0
deep learners algorithm level,2,2,1.0
learners algorithm level solutions,2,2,1.0
algorithm level solutions aim,2,2,1.0
level solutions aim to,2,2,1.0
solutions aim to modify,2,2,1.0
aim to modify the,2,2,1.0
to modify the machine,2,2,1.0
modify the machine learning,2,2,1.0
the machine learning algorithms,2,2,1.0
machine learning algorithms used,2,2,1.0
learning algorithms used to,2,2,1.0
algorithms used to mitigate,2,2,1.0
used to mitigate their,2,2,1.0
to mitigate their bias,2,2,1.0
mitigate their bias towards,2,2,1.0
their bias towards majority,2,2,1.0
bias towards majority groups,2,2,1.0
towards majority groups typically,2,2,1.0
majority groups typically these,2,2,1.0
groups typically these techniques,2,2,1.0
typically these techniques involve,2,2,1.0
these techniques involve the,2,2,1.0
techniques involve the use,2,2,1.0
involve the use of,2,2,1.0
the use of and,2,2,1.0
use of and ensemble,2,2,1.0
of and ensemble methods,2,2,1.0
and ensemble methods in,2,2,1.0
ensemble methods in this,2,2,1.0
methods in this paper,2,2,1.0
this paper we explore,2,2,1.0
paper we explore a,2,2,1.0
we explore a novel,2,2,1.0
explore a novel approach,2,2,1.0
a novel approach to,2,2,1.0
novel approach to both,2,2,1.0
approach to both the,2,2,1.0
to both the class,2,2,1.0
both the class imbalance,2,2,1.0
the class imbalance and,3,3,1.0
class imbalance and data,1,1,1.0
imbalance and data augmentation,1,1,1.0
and data augmentation problems,1,1,1.0
data augmentation problems using,1,1,1.0
augmentation problems using an,2,2,1.0
problems using an counterfactual,2,2,1.0
using an counterfactual method,2,2,1.0
an counterfactual method that,2,2,1.0
counterfactual method that generates,2,2,1.0
method that generates synthetic,2,2,1.0
that generates synthetic in,2,2,1.0
generates synthetic in the,2,2,1.0
synthetic in the minority,4,2,2.0
the minority class interestingly,2,2,1.0
minority class interestingly this,2,2,1.0
class interestingly this method,2,2,1.0
interestingly this method was,2,2,1.0
this method was previously,2,2,1.0
method was previously developed,2,2,1.0
was previously developed to,2,2,1.0
previously developed to solve,1,1,1.0
developed to solve problems,1,1,1.0
to solve problems in,1,1,1.0
solve problems in explainable,1,1,1.0
problems in explainable ai,2,2,1.0
in explainable ai xai,2,2,1.0
explainable ai xai for,2,2,1.0
ai xai for reviews,2,2,1.0
xai for reviews see,2,2,1.0
for reviews see in,2,2,1.0
reviews see in logic,2,2,1.0
see in logic lewis,2,2,1.0
in logic lewis proposed,2,2,1.0
logic lewis proposed that,2,2,1.0
lewis proposed that counterfactuals,2,2,1.0
proposed that counterfactuals are,2,2,1.0
that counterfactuals are the,2,2,1.0
counterfactuals are the closest,2,2,1.0
are the closest possible,2,2,1.0
the closest possible world,2,2,1.0
closest possible world to,2,2,1.0
possible world to the,2,2,1.0
world to the current,2,2,1.0
to the current world,2,2,1.0
the current world in,2,2,1.0
current world in which,2,2,1.0
world in which the,4,2,2.0
in which the outcome,4,2,2.0
which the outcome is,2,2,1.0
the outcome is different,2,2,1.0
outcome is different hence,2,2,1.0
is different hence the,2,2,1.0
different hence the intuition,2,2,1.0
hence the intuition behind,2,2,1.0
the intuition behind the,2,2,1.0
intuition behind the current,2,2,1.0
behind the current technique,2,2,1.0
the current technique is,2,2,1.0
current technique is that,2,2,1.0
technique is that it,2,2,1.0
is that it generates,2,2,1.0
that it generates synthetic,2,2,1.0
it generates synthetic counterfactual,2,2,1.0
synthetic counterfactual instances using,2,2,1.0
counterfactual instances using the,2,2,1.0
instances using the actual,2,2,1.0
using the actual of,2,2,1.0
the actual of instances,2,2,1.0
actual of instances not,2,2,1.0
of instances not interpolated,2,2,1.0
instances not interpolated values,2,2,1.0
not interpolated values that,2,2,1.0
interpolated values that are,2,2,1.0
values that are the,2,2,1.0
that are the close,2,2,1.0
are the close to,2,2,1.0
the close to existing,2,2,1.0
close to existing thus,1,1,1.0
to existing thus populating,1,1,1.0
existing thus populating the,1,1,1.0
thus populating the minority,2,2,1.0
populating the minority class,2,2,1.0
the minority class with,6,3,2.0
minority class with plausible,2,2,1.0
class with plausible adaptations,2,2,1.0
with plausible adaptations of,2,2,1.0
plausible adaptations of existing,1,1,1.0
adaptations of existing data,1,1,1.0
of existing data if,1,1,1.0
existing data if this,1,1,1.0
data if this intuition,2,2,1.0
if this intuition is,2,2,1.0
this intuition is correct,2,2,1.0
intuition is correct then,2,2,1.0
is correct then the,2,2,1.0
correct then the synthetic,2,2,1.0
then the synthetic instances,2,2,1.0
the synthetic instances generated,2,2,1.0
synthetic instances generated by,3,3,1.0
instances generated by this,2,2,1.0
generated by this counterfactual,1,1,1.0
by this counterfactual method,1,1,1.0
this counterfactual method should,1,1,1.0
counterfactual method should improve,1,1,1.0
method should improve ml,2,2,1.0
should improve ml performance,2,2,1.0
improve ml performance perhaps,2,2,1.0
ml performance perhaps to,2,2,1.0
performance perhaps to a,2,2,1.0
perhaps to a level,2,2,1.0
to a level that,2,2,1.0
a level that advances,2,2,1.0
level that advances current,2,2,1.0
that advances current techniques,2,2,1.0
advances current techniques temraz,2,2,1.0
current techniques temraz keane,2,2,1.0
techniques temraz keane counterfactual,2,2,1.0
counterfactual data augmentation related,2,2,1.0
data augmentation related work,2,2,1.0
augmentation related work the,2,2,1.0
related work the related,2,2,1.0
work the related work,2,2,1.0
the related work to,2,2,1.0
related work to the,2,2,1.0
work to the present,2,2,1.0
to the present research,2,2,1.0
the present research comes,2,2,1.0
present research comes from,2,2,1.0
research comes from two,2,2,1.0
comes from two different,2,2,1.0
from two different strands,2,2,1.0
two different strands of,2,2,1.0
different strands of ai,2,2,1.0
strands of ai research,2,2,1.0
of ai research from,2,2,1.0
ai research from i,2,2,1.0
research from i sampling,2,2,1.0
from i sampling techniques,2,2,1.0
i sampling techniques for,2,2,1.0
sampling techniques for the,2,2,1.0
techniques for the class,2,2,1.0
for the class imbalance,10,4,2.5
class imbalance problem and,8,4,2.0
imbalance problem and ii,2,2,1.0
problem and ii counterfactual,2,2,1.0
and ii counterfactual methods,2,2,1.0
ii counterfactual methods for,2,2,1.0
counterfactual methods for explainable,2,2,1.0
methods for explainable ai,2,2,1.0
for explainable ai xai,2,2,1.0
explainable ai xai data,2,2,1.0
ai xai data level,2,2,1.0
xai data level solutions,2,2,1.0
data level solutions to,2,2,1.0
level solutions to the,2,2,1.0
solutions to the class,2,2,1.0
to the class imbalance,6,3,2.0
class imbalance problem are,2,2,1.0
imbalance problem are dominated,2,2,1.0
problem are dominated by,2,2,1.0
are dominated by three,2,2,1.0
dominated by three main,2,2,1.0
by three main approaches,2,2,1.0
three main approaches random,2,2,1.0
main approaches random ros,2,2,1.0
approaches random ros random,2,2,1.0
random ros random rus,2,2,1.0
ros random rus and,2,2,1.0
random rus and synthetic,2,2,1.0
rus and synthetic minority,2,2,1.0
and synthetic minority technique,1,1,1.0
minority technique smote in,1,1,1.0
technique smote in ros,2,2,1.0
smote in ros the,2,2,1.0
in ros the class,2,2,1.0
ros the class distribution,2,2,1.0
the class distribution is,2,2,1.0
class distribution is balanced,2,2,1.0
distribution is balanced by,2,2,1.0
is balanced by randomly,2,2,1.0
balanced by randomly adding,2,2,1.0
by randomly adding multiple,2,2,1.0
randomly adding multiple copies,2,2,1.0
adding multiple copies of,2,2,1.0
multiple copies of some,2,2,1.0
copies of some of,2,2,1.0
of some of the,2,2,1.0
some of the minority,2,2,1.0
of the minority classes,2,2,1.0
the minority classes to,2,2,1.0
minority classes to the,2,2,1.0
classes to the training,2,2,1.0
to the training data,2,2,1.0
the training data whereas,2,2,1.0
training data whereas with,2,2,1.0
data whereas with rus,2,2,1.0
whereas with rus a,2,2,1.0
with rus a certain,2,2,1.0
rus a certain number,2,2,1.0
a certain number of,2,2,1.0
certain number of examples,2,2,1.0
number of examples of,2,2,1.0
of examples of the,2,2,1.0
examples of the majority,2,2,1.0
of the majority class,18,5,3.6
the majority class are,3,3,1.0
majority class are randomly,2,2,1.0
class are randomly removed,2,2,1.0
are randomly removed from,2,2,1.0
randomly removed from the,2,2,1.0
removed from the original,2,2,1.0
from the original dataset,2,2,1.0
the original dataset although,2,2,1.0
original dataset although these,2,2,1.0
dataset although these methods,2,2,1.0
although these methods can,2,2,1.0
these methods can the,2,2,1.0
methods can the original,2,2,1.0
can the original dataset,2,2,1.0
the original dataset they,2,2,1.0
original dataset they have,2,2,1.0
dataset they have some,2,2,1.0
they have some since,1,1,1.0
have some since ros,1,1,1.0
some since ros merely,1,1,1.0
since ros merely copies,2,2,1.0
ros merely copies instances,2,2,1.0
merely copies instances no,2,2,1.0
copies instances no new,2,2,1.0
instances no new information,2,2,1.0
no new information is,2,2,1.0
new information is added,2,2,1.0
information is added to,2,2,1.0
is added to the,2,2,1.0
added to the and,1,1,1.0
to the and hence,1,1,1.0
the and hence it,1,1,1.0
and hence it can,2,2,1.0
hence it can lead,2,2,1.0
it can lead to,2,2,1.0
can lead to overfitting,2,2,1.0
lead to overfitting on,2,2,1.0
to overfitting on the,2,2,1.0
overfitting on the other,2,2,1.0
on the other hand,8,5,1.6
the other hand since,2,2,1.0
other hand since rus,2,2,1.0
hand since rus randomly,2,2,1.0
since rus randomly removes,1,1,1.0
rus randomly removes examples,1,1,1.0
randomly removes examples from,1,1,1.0
removes examples from the,1,1,1.0
examples from the majority,3,3,1.0
from the majority class,8,4,2.0
the majority class data,2,2,1.0
majority class data can,2,2,1.0
class data can be,2,2,1.0
data can be discarded,2,2,1.0
can be discarded that,2,2,1.0
be discarded that may,2,2,1.0
discarded that may be,2,2,1.0
that may be important,2,2,1.0
may be important the,2,2,1.0
be important the third,2,2,1.0
important the third option,2,2,1.0
the third option smote,2,2,1.0
third option smote adopts,2,2,1.0
option smote adopts a,2,2,1.0
smote adopts a somewhat,2,2,1.0
adopts a somewhat different,2,2,1.0
a somewhat different approach,2,2,1.0
somewhat different approach based,2,2,1.0
different approach based on,2,2,1.0
approach based on oversampling,4,2,2.0
based on oversampling from,2,2,1.0
on oversampling from the,2,2,1.0
oversampling from the class,1,1,1.0
from the class as,1,1,1.0
the class as smote,1,1,1.0
class as smote is,2,2,1.0
as smote is the,2,2,1.0
smote is the baseline,2,2,1.0
is the baseline method,2,2,1.0
the baseline method used,2,2,1.0
baseline method used for,2,2,1.0
method used for comparisons,2,2,1.0
used for comparisons in,2,2,1.0
for comparisons in the,2,2,1.0
comparisons in the present,2,2,1.0
in the present experiments,2,2,1.0
the present experiments we,2,2,1.0
present experiments we briefly,2,2,1.0
experiments we briefly describe,2,2,1.0
we briefly describe it,2,2,1.0
briefly describe it in,2,2,1.0
describe it in more,2,2,1.0
it in more detail,2,2,1.0
in more detail here,2,2,1.0
more detail here see,2,2,1.0
detail here see section,2,2,1.0
here see section along,2,2,1.0
see section along with,2,2,1.0
section along with important,2,2,1.0
along with important smote,2,2,1.0
with important smote variants,2,2,1.0
important smote variants see,2,2,1.0
smote variants see section,2,2,1.0
variants see section before,2,2,1.0
see section before going,2,2,1.0
section before going on,2,2,1.0
before going on to,2,2,1.0
going on to describe,2,2,1.0
on to describe the,2,2,1.0
to describe the counterfactual,2,2,1.0
describe the counterfactual method,2,2,1.0
the counterfactual method we,2,2,1.0
counterfactual method we have,2,2,1.0
method we have adapted,2,2,1.0
we have adapted from,2,2,1.0
have adapted from the,2,2,1.0
adapted from the xai,2,2,1.0
from the xai literature,4,2,2.0
the xai literature see,2,2,1.0
xai literature see section,2,2,1.0
literature see section finally,2,2,1.0
see section finally we,2,2,1.0
section finally we briefly,2,2,1.0
finally we briefly sketch,2,2,1.0
we briefly sketch the,2,2,1.0
briefly sketch the recent,2,2,1.0
sketch the recent and,2,2,1.0
the recent and very,2,2,1.0
recent and very small,2,2,1.0
and very small literature,2,2,1.0
very small literature that,2,2,1.0
small literature that has,2,2,1.0
literature that has begun,2,2,1.0
that has begun to,2,2,1.0
has begun to apply,2,2,1.0
begun to apply these,2,2,1.0
to apply these counterfactual,2,2,1.0
apply these counterfactual xai,2,2,1.0
these counterfactual xai methods,2,2,1.0
counterfactual xai methods to,2,2,1.0
xai methods to and,2,2,1.0
methods to and data,2,2,1.0
to and data augmentation,2,2,1.0
and data augmentation see,1,1,1.0
data augmentation see section,1,1,1.0
augmentation see section temraz,1,1,1.0
see section temraz keane,2,2,1.0
section temraz keane counterfactual,2,2,1.0
counterfactual data augmentation data,2,2,1.0
data augmentation data sampling,2,2,1.0
augmentation data sampling methods,2,2,1.0
data sampling methods for,2,2,1.0
sampling methods for the,2,2,1.0
methods for the class,2,2,1.0
class imbalance problem smote,2,2,1.0
imbalance problem smote synthetic,2,2,1.0
problem smote synthetic minority,2,2,1.0
smote synthetic minority oversampling,7,4,1.75
synthetic minority oversampling technique,10,5,2.0
minority oversampling technique smote,3,3,1.0
oversampling technique smote oversamples,2,2,1.0
technique smote oversamples the,2,2,1.0
smote oversamples the minority,2,2,1.0
oversamples the minority class,2,2,1.0
the minority class by,3,3,1.0
minority class by creating,2,2,1.0
class by creating synthetic,2,2,1.0
by creating synthetic instances,2,2,1.0
creating synthetic instances rather,2,2,1.0
synthetic instances rather than,2,2,1.0
instances rather than by,2,2,1.0
rather than by oversampling,2,2,1.0
than by oversampling using,2,2,1.0
by oversampling using replacement,2,2,1.0
oversampling using replacement it,2,2,1.0
using replacement it is,2,2,1.0
replacement it is one,2,2,1.0
it is one of,2,2,1.0
is one of the,6,4,1.5
one of the most,7,4,1.75
of the most widely,4,3,1.3333333333333333
the most widely used,4,3,1.3333333333333333
most widely used solutions,2,2,1.0
widely used solutions to,2,2,1.0
used solutions to the,2,2,1.0
solutions to the problem,2,2,1.0
to the problem google,2,2,1.0
the problem google scholar,2,2,1.0
problem google scholar lists,2,2,1.0
google scholar lists over,2,2,1.0
scholar lists over citations,2,2,1.0
lists over citations to,2,2,1.0
over citations to the,2,2,1.0
citations to the original,2,2,1.0
to the original paper,2,2,1.0
the original paper in,2,2,1.0
original paper in smote,2,2,1.0
paper in smote the,2,2,1.0
in smote the new,2,2,1.0
smote the new example,2,2,1.0
the new example in,2,2,1.0
new example in the,2,2,1.0
example in the minority,2,2,1.0
the minority class is,10,4,2.5
minority class is created,1,1,1.0
class is created by,1,1,1.0
is created by between,1,1,1.0
created by between several,1,1,1.0
by between several minority,1,1,1.0
between several minority class,3,3,1.0
several minority class instances,2,2,1.0
minority class instances by,2,2,1.0
class instances by interpolating,2,2,1.0
instances by interpolating instead,2,2,1.0
by interpolating instead of,3,3,1.0
interpolating instead of copying,2,2,1.0
instead of copying instances,2,2,1.0
of copying instances smote,2,2,1.0
copying instances smote avoids,2,2,1.0
instances smote avoids the,2,2,1.0
smote avoids the problem,3,3,1.0
avoids the problem and,3,3,1.0
the problem and creates,1,1,1.0
problem and creates new,1,1,1.0
and creates new synthetic,1,1,1.0
creates new synthetic instances,1,1,1.0
new synthetic instances in,2,2,1.0
synthetic instances in neighborhoods,2,2,1.0
instances in neighborhoods surrounding,2,2,1.0
in neighborhoods surrounding instances,2,2,1.0
neighborhoods surrounding instances in,2,2,1.0
surrounding instances in the,2,2,1.0
the minority class briefly,2,2,1.0
minority class briefly the,2,2,1.0
class briefly the algorithm,2,2,1.0
briefly the algorithm works,2,2,1.0
the algorithm works as,2,2,1.0
algorithm works as follows,2,2,1.0
works as follows assume,2,2,1.0
as follows assume that,2,2,1.0
follows assume that the,2,2,1.0
assume that the minority,4,2,2.0
that the minority class,3,2,1.5
minority class is 𝑃,3,2,1.5
class is 𝑃 and,3,2,1.5
is 𝑃 and the,4,2,2.0
𝑃 and the majority,4,2,2.0
and the majority class,5,3,1.6666666666666667
the majority class is,6,3,2.0
majority class is smote,2,2,1.0
class is smote starts,2,2,1.0
is smote starts by,2,2,1.0
smote starts by randomly,2,2,1.0
starts by randomly selecting,2,2,1.0
by randomly selecting a,2,2,1.0
randomly selecting a minority,2,2,1.0
selecting a minority instance,2,2,1.0
a minority instance 𝑝,2,2,1.0
minority instance 𝑝 from,2,2,1.0
instance 𝑝 from the,2,2,1.0
𝑝 from the minority,4,2,2.0
from the minority class,6,3,2.0
the minority class 𝑃,6,2,3.0
minority class 𝑃 and,2,2,1.0
class 𝑃 and then,2,2,1.0
𝑃 and then determines,1,1,1.0
and then determines 𝑚,1,1,1.0
then determines 𝑚 as,1,1,1.0
determines 𝑚 as the,1,1,1.0
𝑚 as the nearest,2,2,1.0
as the nearest neighbors,2,2,1.0
the nearest neighbors of,5,3,1.6666666666666667
nearest neighbors of 𝑝,6,2,3.0
neighbors of 𝑝 after,2,2,1.0
of 𝑝 after determining,2,2,1.0
𝑝 after determining 𝑚,2,2,1.0
after determining 𝑚 nearest,2,2,1.0
determining 𝑚 nearest neighbors,2,2,1.0
𝑚 nearest neighbors of,4,2,2.0
neighbors of 𝑝 it,2,2,1.0
of 𝑝 it selects,2,2,1.0
𝑝 it selects a,2,2,1.0
it selects a random,2,2,1.0
selects a random neighbor,2,2,1.0
a random neighbor 𝑚,2,2,1.0
random neighbor 𝑚 where,2,2,1.0
neighbor 𝑚 where 𝑚,2,2,1.0
𝑚 where 𝑚 smote,1,1,1.0
where 𝑚 smote creates,1,1,1.0
𝑚 smote creates a,1,1,1.0
smote creates a new,2,2,1.0
creates a new instance,2,2,1.0
a new instance 𝑝,2,2,1.0
new instance 𝑝 using,2,2,1.0
instance 𝑝 using the,2,2,1.0
𝑝 using the following,2,2,1.0
using the following formula,2,2,1.0
the following formula 𝑝,2,2,1.0
following formula 𝑝 𝑝,2,2,1.0
formula 𝑝 𝑝 𝑚,2,2,1.0
𝑝 𝑝 𝑚 where,1,1,1.0
𝑝 𝑚 where 𝛿,1,1,1.0
𝑚 where 𝛿 is,1,1,1.0
where 𝛿 is a,2,2,1.0
𝛿 is a random,2,2,1.0
is a random number,5,4,1.25
a random number between,3,3,1.0
random number between and,3,3,1.0
number between and this,2,2,1.0
between and this new,2,2,1.0
and this new instance,2,2,1.0
this new instance is,2,2,1.0
new instance is then,2,2,1.0
instance is then added,2,2,1.0
is then added to,4,2,2.0
then added to the,4,2,2.0
added to the dataset,4,2,2.0
to the dataset for,2,2,1.0
the dataset for the,2,2,1.0
dataset for the minority,2,2,1.0
for the minority class,6,4,1.5
the minority class one,2,2,1.0
minority class one of,2,2,1.0
class one of the,2,2,1.0
one of the potential,2,2,1.0
of the potential problems,2,2,1.0
the potential problems with,2,2,1.0
potential problems with smote,2,2,1.0
problems with smote is,2,2,1.0
with smote is that,2,2,1.0
smote is that its,2,2,1.0
is that its generation,2,2,1.0
that its generation of,2,2,1.0
its generation of minority,2,2,1.0
generation of minority instances,4,2,2.0
of minority instances is,2,2,1.0
minority instances is done,2,2,1.0
instances is done without,2,2,1.0
is done without reference,2,2,1.0
done without reference to,2,2,1.0
without reference to the,2,2,1.0
reference to the majority,2,2,1.0
to the majority class,14,4,3.5
majority class or indeed,2,2,1.0
class or indeed any,2,2,1.0
or indeed any consideration,1,1,1.0
indeed any consideration that,1,1,1.0
any consideration that some,1,1,1.0
consideration that some minority,1,1,1.0
that some minority instances,2,2,1.0
some minority instances may,2,2,1.0
minority instances may be,2,2,1.0
instances may be better,2,2,1.0
may be better than,2,2,1.0
be better than others,2,2,1.0
better than others to,2,2,1.0
than others to use,2,2,1.0
others to use in,2,2,1.0
to use in this,2,2,1.0
use in this process,2,2,1.0
in this process another,2,2,1.0
this process another issue,2,2,1.0
process another issue is,2,2,1.0
another issue is that,2,2,1.0
issue is that it,2,2,1.0
is that it may,2,2,1.0
that it may introduce,2,2,1.0
it may introduce noise,2,2,1.0
may introduce noise by,2,2,1.0
introduce noise by generating,2,2,1.0
noise by generating interpolated,2,2,1.0
by generating interpolated values,2,2,1.0
generating interpolated values that,2,2,1.0
interpolated values that do,2,2,1.0
values that do not,2,2,1.0
that do not exist,1,1,1.0
do not exist in,1,1,1.0
not exist in the,1,1,1.0
exist in the domain,1,1,1.0
in the domain the,2,2,1.0
the domain the interpolated,2,2,1.0
domain the interpolated value,2,2,1.0
the interpolated value could,2,2,1.0
interpolated value could be,2,2,1.0
value could be accordingly,2,2,1.0
could be accordingly many,2,2,1.0
be accordingly many extensions,2,2,1.0
accordingly many extensions have,2,2,1.0
many extensions have been,2,2,1.0
extensions have been made,2,2,1.0
have been made to,3,3,1.0
been made to smote,2,2,1.0
made to smote that,2,2,1.0
to smote that improve,2,2,1.0
smote that improve on,4,2,2.0
that improve on its,2,2,1.0
improve on its operation,2,2,1.0
on its operation in,2,2,1.0
its operation in the,2,2,1.0
operation in the following,2,2,1.0
in the following we,2,2,1.0
the following we review,2,2,1.0
following we review the,2,2,1.0
we review the smote,2,2,1.0
review the smote variants,2,2,1.0
the smote variants that,2,2,1.0
smote variants that are,2,2,1.0
variants that are closest,2,2,1.0
that are closest to,2,2,1.0
are closest to the,2,2,1.0
closest to the current,2,2,1.0
to the current method,2,2,1.0
the current method proposed,2,2,1.0
current method proposed to,2,2,1.0
method proposed to reveal,2,2,1.0
proposed to reveal how,2,2,1.0
to reveal how it,2,2,1.0
reveal how it differs,2,2,1.0
how it differs temraz,2,2,1.0
it differs temraz keane,2,2,1.0
differs temraz keane counterfactual,2,2,1.0
counterfactual data augmentation smote,6,2,3.0
data augmentation smote variants,2,2,1.0
augmentation smote variants three,2,2,1.0
smote variants three key,2,2,1.0
variants three key insights,2,2,1.0
three key insights there,2,2,1.0
key insights there are,2,2,1.0
insights there are many,2,2,1.0
there are many variants,2,2,1.0
are many variants of,2,2,1.0
many variants of smote,2,2,1.0
variants of smote that,2,2,1.0
of smote that improve,2,2,1.0
that improve on the,2,2,1.0
improve on the original,2,2,1.0
on the original s,2,2,1.0
the original s performance,2,2,1.0
original s performance based,2,2,1.0
s performance based on,2,2,1.0
performance based on several,2,2,1.0
based on several insights,2,2,1.0
on several insights about,2,2,1.0
several insights about how,2,2,1.0
insights about how to,2,2,1.0
about how to solve,2,2,1.0
how to solve the,2,2,1.0
to solve the problem,2,2,1.0
solve the problem so,2,2,1.0
the problem so these,2,2,1.0
problem so these variants,2,2,1.0
so these variants often,2,2,1.0
these variants often hinge,2,2,1.0
variants often hinge on,2,2,1.0
often hinge on regions,1,1,1.0
hinge on regions in,1,1,1.0
on regions in the,1,1,1.0
regions in the minority,6,2,3.0
the minority class they,4,2,2.0
minority class they emphasise,2,2,1.0
class they emphasise the,2,2,1.0
they emphasise the importance,1,1,1.0
emphasise the importance of,1,1,1.0
the importance of focusing,1,1,1.0
importance of focusing on,1,1,1.0
of focusing on the,2,2,1.0
focusing on the border,2,2,1.0
on the border region,2,2,1.0
the border region between,2,2,1.0
border region between the,2,2,1.0
region between the majority,2,2,1.0
between the majority and,2,2,1.0
the majority and minority,5,3,1.6666666666666667
majority and minority classes,6,4,1.5
and minority classes and,2,2,1.0
minority classes and sometimes,2,2,1.0
classes and sometimes analyze,1,1,1.0
and sometimes analyze the,1,1,1.0
sometimes analyze the majority,1,1,1.0
analyze the majority class,1,1,1.0
the majority class with,2,2,1.0
majority class with respect,2,2,1.0
class with respect to,2,2,1.0
with respect to the,10,4,2.5
respect to the minority,2,2,1.0
to the minority to,2,2,1.0
the minority to guide,2,2,1.0
minority to guide smote,2,2,1.0
to guide smote in,2,2,1.0
guide smote in selected,2,2,1.0
smote in selected regions,2,2,1.0
in selected regions one,2,2,1.0
selected regions one critical,2,2,1.0
regions one critical improvement,2,2,1.0
one critical improvement to,2,2,1.0
critical improvement to the,2,2,1.0
improvement to the original,2,2,1.0
to the original smote,2,2,1.0
the original smote method,4,2,2.0
original smote method hinges,2,2,1.0
smote method hinges on,2,2,1.0
method hinges on the,2,2,1.0
hinges on the insight,2,2,1.0
on the insight that,2,2,1.0
the insight that not,2,2,1.0
insight that not all,2,2,1.0
that not all regions,2,2,1.0
not all regions in,2,2,1.0
all regions in the,2,2,1.0
the minority class are,4,4,1.0
minority class are equal,2,2,1.0
class are equal some,2,2,1.0
are equal some may,2,2,1.0
equal some may be,2,2,1.0
some may be more,2,2,1.0
may be more or,1,1,1.0
be more or safer,1,1,1.0
more or safer than,1,1,1.0
or safer than others,2,2,1.0
safer than others within,2,2,1.0
than others within which,2,2,1.0
others within which to,2,2,1.0
within which to apply,2,2,1.0
which to apply smote,2,2,1.0
to apply smote for,2,2,1.0
apply smote for instance,2,2,1.0
smote for instance smote,1,1,1.0
for instance smote clusters,1,1,1.0
instance smote clusters minority,1,1,1.0
smote clusters minority instances,2,2,1.0
clusters minority instances into,2,2,1.0
minority instances into k,2,2,1.0
instances into k clusters,2,2,1.0
into k clusters and,2,2,1.0
k clusters and then,2,2,1.0
clusters and then oversamples,1,1,1.0
and then oversamples from,1,1,1.0
then oversamples from clusters,1,1,1.0
oversamples from clusters with,1,1,1.0
from clusters with the,2,2,1.0
clusters with the most,2,2,1.0
with the most the,1,1,1.0
the most the assumption,1,1,1.0
most the assumption being,1,1,1.0
the assumption being that,2,2,1.0
assumption being that these,2,2,1.0
being that these are,2,2,1.0
that these are safer,2,2,1.0
these are safer regions,2,2,1.0
are safer regions and,2,2,1.0
safer regions and are,2,2,1.0
regions and are less,2,2,1.0
and are less likely,2,2,1.0
are less likely to,2,2,1.0
less likely to generate,4,2,2.0
likely to generate noise,2,2,1.0
to generate noise see,2,2,1.0
generate noise see for,2,2,1.0
noise see for a,2,2,1.0
see for a related,2,2,1.0
for a related solution,2,2,1.0
a related solution other,2,2,1.0
related solution other versions,2,2,1.0
solution other versions of,2,2,1.0
other versions of this,2,2,1.0
versions of this approach,2,2,1.0
of this approach have,2,2,1.0
this approach have used,2,2,1.0
approach have used dbscan,2,2,1.0
have used dbscan a,2,2,1.0
used dbscan a based,2,2,1.0
dbscan a based clustering,2,2,1.0
a based clustering algorithm,2,2,1.0
based clustering algorithm to,2,2,1.0
clustering algorithm to identify,2,2,1.0
algorithm to identify safe,2,2,1.0
to identify safe regions,2,2,1.0
identify safe regions or,2,2,1.0
safe regions or use,2,2,1.0
regions or use representative,2,2,1.0
or use representative points,2,2,1.0
use representative points within,2,2,1.0
representative points within to,1,1,1.0
points within to guide,1,1,1.0
within to guide smote,1,1,1.0
to guide smote some,2,2,1.0
guide smote some methods,2,2,1.0
smote some methods project,2,2,1.0
some methods project the,2,2,1.0
methods project the minority,2,2,1.0
project the minority class,2,2,1.0
the minority class into,2,2,1.0
minority class into a,2,2,1.0
class into a lower,2,2,1.0
into a lower dimension,4,2,2.0
a lower dimension before,2,2,1.0
lower dimension before applying,2,2,1.0
dimension before applying smote,2,2,1.0
before applying smote to,2,2,1.0
applying smote to the,4,2,2.0
smote to the clusters,2,2,1.0
to the clusters found,2,2,1.0
the clusters found for,2,2,1.0
clusters found for instance,2,2,1.0
found for instance somo,2,2,1.0
for instance somo uses,2,2,1.0
instance somo uses a,2,2,1.0
somo uses a map,2,2,1.0
uses a map to,2,2,1.0
a map to transform,2,2,1.0
map to transform datasets,2,2,1.0
to transform datasets into,2,2,1.0
transform datasets into a,2,2,1.0
datasets into a space,1,1,1.0
into a space and,1,1,1.0
a space and uses,1,1,1.0
space and uses a,1,1,1.0
and uses a embedding,2,2,1.0
uses a embedding algorithm,2,2,1.0
a embedding algorithm to,2,2,1.0
embedding algorithm to project,2,2,1.0
algorithm to project into,2,2,1.0
to project into a,2,2,1.0
project into a lower,2,2,1.0
a lower dimension where,2,2,1.0
lower dimension where the,2,2,1.0
dimension where the datasets,2,2,1.0
where the datasets are,2,2,1.0
the datasets are more,2,2,1.0
datasets are more separable,2,2,1.0
are more separable still,2,2,1.0
more separable still others,2,2,1.0
separable still others such,2,2,1.0
still others such as,2,2,1.0
others such as and,1,1,1.0
such as and adasyn,1,1,1.0
as and adasyn explore,1,1,1.0
and adasyn explore different,2,2,1.0
adasyn explore different ways,2,2,1.0
explore different ways to,2,2,1.0
different ways to identify,2,2,1.0
ways to identify regions,2,2,1.0
to identify regions within,2,2,1.0
identify regions within which,2,2,1.0
regions within which to,2,2,1.0
within which to generate,2,2,1.0
which to generate minority,2,2,1.0
to generate minority instances,2,2,1.0
generate minority instances defines,2,2,1.0
minority instances defines a,2,2,1.0
instances defines a geometric,2,2,1.0
defines a geometric region,2,2,1.0
a geometric region around,2,2,1.0
geometric region around each,2,2,1.0
region around each minority,2,2,1.0
around each minority class,2,2,1.0
each minority class instance,2,2,1.0
minority class instance for,2,2,1.0
class instance for generating,2,2,1.0
instance for generating synthetic,2,2,1.0
for generating synthetic datapoints,2,2,1.0
generating synthetic datapoints adasyn,2,2,1.0
synthetic datapoints adasyn proposed,2,2,1.0
datapoints adasyn proposed by,2,2,1.0
adasyn proposed by he,2,2,1.0
proposed by he et,2,2,1.0
by he et al,2,2,1.0
he et al generates,2,2,1.0
et al generates minority,2,2,1.0
al generates minority class,2,2,1.0
generates minority class instances,2,2,1.0
minority class instances according,2,2,1.0
class instances according to,2,2,1.0
instances according to their,2,2,1.0
according to their generating,1,1,1.0
to their generating more,1,1,1.0
their generating more synthetic,1,1,1.0
generating more synthetic data,2,2,1.0
more synthetic data from,2,2,1.0
synthetic data from minority,2,2,1.0
data from minority instances,2,2,1.0
from minority instances that,2,2,1.0
minority instances that are,8,2,4.0
instances that are harder,2,2,1.0
that are harder to,2,2,1.0
are harder to learn,2,2,1.0
harder to learn compared,2,2,1.0
to learn compared to,2,2,1.0
learn compared to minority,2,2,1.0
compared to minority instances,2,2,1.0
to minority instances that,2,2,1.0
instances that are easier,2,2,1.0
that are easier to,2,2,1.0
are easier to learn,2,2,1.0
easier to learn where,2,2,1.0
to learn where is,2,2,1.0
learn where is related,2,2,1.0
where is related to,2,2,1.0
is related to the,4,3,1.3333333333333333
related to the number,2,2,1.0
to the number of,5,5,1.0
the number of temraz,2,2,1.0
number of temraz keane,2,2,1.0
of temraz keane counterfactual,2,2,1.0
counterfactual data augmentation instances,4,2,2.0
data augmentation instances in,4,2,2.0
augmentation instances in the,4,2,2.0
instances in the neighbors,2,2,1.0
in the neighbors that,2,2,1.0
the neighbors that belong,2,2,1.0
neighbors that belong to,3,3,1.0
that belong to the,3,3,1.0
belong to the majority,4,3,1.3333333333333333
the majority class however,4,2,2.0
majority class however these,2,2,1.0
class however these solutions,2,2,1.0
however these solutions owe,2,2,1.0
these solutions owe a,2,2,1.0
solutions owe a lot,2,2,1.0
owe a lot to,2,2,1.0
a lot to another,2,2,1.0
lot to another key,2,2,1.0
to another key insight,2,2,1.0
another key insight namely,2,2,1.0
key insight namely that,2,2,1.0
insight namely that regions,2,2,1.0
namely that regions close,2,2,1.0
that regions close to,2,2,1.0
regions close to the,4,2,2.0
close to the class,1,1,1.0
to the class boundary,2,2,1.0
the class boundary are,1,1,1.0
class boundary are particularly,1,1,1.0
boundary are particularly important,2,2,1.0
are particularly important for,2,2,1.0
particularly important for instance,2,2,1.0
important for instance generation,2,2,1.0
for instance generation smote,2,2,1.0
instance generation smote on,2,2,1.0
generation smote on the,2,2,1.0
smote on the borderline,2,2,1.0
on the borderline the,2,2,1.0
the borderline the idea,2,2,1.0
borderline the idea that,2,2,1.0
the idea that different,2,2,1.0
idea that different regions,2,2,1.0
that different regions in,2,2,1.0
different regions in the,2,2,1.0
regions in the dataset,2,2,1.0
in the dataset need,2,2,1.0
the dataset need to,2,2,1.0
dataset need to be,2,2,1.0
need to be differently,1,1,1.0
to be differently owes,1,1,1.0
be differently owes a,1,1,1.0
differently owes a lot,2,2,1.0
owes a lot to,2,2,1.0
a lot to the,2,2,1.0
lot to the intuition,2,2,1.0
to the intuition that,2,2,1.0
the intuition that minority,2,2,1.0
intuition that minority instances,2,2,1.0
that minority instances close,2,2,1.0
minority instances close to,4,2,2.0
instances close to the,4,2,2.0
close to the decision,14,3,4.666666666666667
to the decision boundary,16,3,5.333333333333333
the decision boundary of,2,2,1.0
decision boundary of the,2,2,1.0
boundary of the classifier,2,2,1.0
of the classifier are,2,2,1.0
the classifier are particularly,2,2,1.0
classifier are particularly important,2,2,1.0
are particularly important to,2,2,1.0
particularly important to successful,2,2,1.0
important to successful classification,1,1,1.0
to successful classification so,1,1,1.0
successful classification so generating,1,1,1.0
classification so generating minority,1,1,1.0
so generating minority in,1,1,1.0
generating minority in this,1,1,1.0
minority in this boundary,1,1,1.0
in this boundary region,2,2,1.0
this boundary region should,2,2,1.0
boundary region should help,2,2,1.0
region should help performance,2,2,1.0
should help performance more,2,2,1.0
help performance more proposed,2,2,1.0
performance more proposed by,2,2,1.0
more proposed by han,2,2,1.0
proposed by han et,2,2,1.0
by han et al,2,2,1.0
han et al realized,2,2,1.0
et al realized this,2,2,1.0
al realized this idea,2,2,1.0
realized this idea by,2,2,1.0
this idea by creating,2,2,1.0
idea by creating instances,2,2,1.0
by creating instances using,2,2,1.0
creating instances using only,2,2,1.0
instances using only minority,2,2,1.0
using only minority instances,2,2,1.0
only minority instances that,2,2,1.0
instances that are close,4,2,2.0
that are close to,4,2,2.0
are close to the,4,2,2.0
the decision boundary so,4,2,2.0
decision boundary so again,2,2,1.0
boundary so again assume,2,2,1.0
so again assume that,2,2,1.0
again assume that the,2,2,1.0
majority class is 𝑋,2,2,1.0
class is 𝑋 and,2,2,1.0
is 𝑋 and the,2,2,1.0
𝑋 and the whole,2,2,1.0
and the whole training,2,2,1.0
the whole training set,2,2,1.0
whole training set is,2,2,1.0
training set is in,2,2,1.0
set is in for,2,2,1.0
is in for every,2,2,1.0
in for every minority,2,2,1.0
for every minority instance,2,2,1.0
every minority instance 𝑝,2,2,1.0
minority instance 𝑝 in,2,2,1.0
instance 𝑝 in the,2,2,1.0
𝑝 in the minority,4,2,2.0
minority class 𝑃 the,2,2,1.0
class 𝑃 the method,2,2,1.0
𝑃 the method calculates,2,2,1.0
the method calculates its,2,2,1.0
method calculates its 𝑚,2,2,1.0
calculates its 𝑚 nearest,2,2,1.0
its 𝑚 nearest neighbors,2,2,1.0
𝑚 nearest neighbors from,2,2,1.0
nearest neighbors from the,4,4,1.0
neighbors from the training,2,2,1.0
from the training set,2,2,1.0
the training set it,2,2,1.0
training set it should,2,2,1.0
set it should be,2,2,1.0
it should be noted,8,3,2.6666666666666665
should be noted that,8,3,2.6666666666666665
be noted that the,2,2,1.0
noted that the number,2,2,1.0
that the number of,3,3,1.0
the number of majority,10,4,2.5
number of majority instances,2,2,1.0
of majority instances among,2,2,1.0
majority instances among the,2,2,1.0
instances among the 𝑚,2,2,1.0
among the 𝑚 nearest,2,2,1.0
the 𝑚 nearest neighbors,4,2,2.0
𝑚 nearest neighbors is,2,2,1.0
nearest neighbors is called,2,2,1.0
neighbors is called as,2,2,1.0
is called as 𝑚,2,2,1.0
called as 𝑚 in,1,1,1.0
as 𝑚 in step,1,1,1.0
𝑚 in step if,2,2,1.0
in step if all,2,2,1.0
step if all the,2,2,1.0
if all the 𝑚,2,2,1.0
all the 𝑚 nearest,2,2,1.0
neighbors of 𝑝 are,2,2,1.0
of 𝑝 are majority,2,2,1.0
𝑝 are majority instances,2,2,1.0
are majority instances 𝑝,1,1,1.0
majority instances 𝑝 is,1,1,1.0
instances 𝑝 is considered,1,1,1.0
𝑝 is considered as,2,2,1.0
is considered as noise,2,2,1.0
considered as noise and,2,2,1.0
as noise and is,2,2,1.0
noise and is not,2,2,1.0
and is not used,2,2,1.0
is not used in,2,2,1.0
not used in the,2,2,1.0
used in the next,2,2,1.0
in the next step,2,2,1.0
the next step if,2,2,1.0
next step if the,2,2,1.0
step if the set,2,2,1.0
if the set of,4,2,2.0
the set of 𝑝,2,2,1.0
set of 𝑝 s,2,2,1.0
of 𝑝 s majority,2,2,1.0
𝑝 s majority nearest,2,2,1.0
s majority nearest neighbors,2,2,1.0
majority nearest neighbors is,2,2,1.0
nearest neighbors is larger,2,2,1.0
neighbors is larger than,2,2,1.0
is larger than that,2,2,1.0
larger than that of,2,2,1.0
than that of its,2,2,1.0
that of its minority,2,2,1.0
of its minority ones,2,2,1.0
its minority ones 𝑚,2,2,1.0
minority ones 𝑚 𝑝,1,1,1.0
ones 𝑚 𝑝 will,1,1,1.0
𝑚 𝑝 will be,2,2,1.0
𝑝 will be easily,2,2,1.0
will be easily and,1,1,1.0
be easily and put,1,1,1.0
easily and put into,1,1,1.0
and put into a,2,2,1.0
put into a danger,2,2,1.0
into a danger set,2,2,1.0
a danger set if,2,2,1.0
danger set if then,1,1,1.0
set if then 𝑝,1,1,1.0
if then 𝑝 is,1,1,1.0
then 𝑝 is safe,2,2,1.0
𝑝 is safe and,2,2,1.0
is safe and does,2,2,1.0
safe and does not,2,2,1.0
and does not participate,2,2,1.0
does not participate in,2,2,1.0
not participate in the,2,2,1.0
participate in the subsequent,2,2,1.0
in the subsequent steps,2,2,1.0
the subsequent steps this,2,2,1.0
subsequent steps this danger,2,2,1.0
steps this danger set,2,2,1.0
this danger set contains,2,2,1.0
danger set contains the,2,2,1.0
set contains the borderline,2,2,1.0
contains the borderline instances,2,2,1.0
the borderline instances of,2,2,1.0
borderline instances of the,2,2,1.0
instances of the minority,2,2,1.0
of the minority class,26,5,5.2
the minority class finally,2,2,1.0
minority class finally for,2,2,1.0
class finally for each,2,2,1.0
finally for each instance,2,2,1.0
for each instance in,2,2,1.0
each instance in the,2,2,1.0
instance in the danger,2,2,1.0
in the danger set,2,2,1.0
the danger set the,2,2,1.0
danger set the neighbors,2,2,1.0
set the neighbors from,2,2,1.0
the neighbors from 𝑃,2,2,1.0
neighbors from 𝑃 are,2,2,1.0
from 𝑃 are found,2,2,1.0
𝑃 are found and,2,2,1.0
are found and the,2,2,1.0
found and the steps,2,2,1.0
and the steps from,2,2,1.0
the steps from the,2,2,1.0
steps from the original,2,2,1.0
from the original smote,2,2,1.0
original smote method are,2,2,1.0
smote method are applied,2,2,1.0
method are applied to,2,2,1.0
are applied to them,2,2,1.0
applied to them to,2,2,1.0
to them to generate,2,2,1.0
them to generate synthetic,1,1,1.0
to generate synthetic instances,5,2,2.5
generate synthetic instances in,1,1,1.0
the minority class this,4,2,2.0
minority class this insight,2,2,1.0
class this insight about,2,2,1.0
this insight about the,2,2,1.0
insight about the importance,2,2,1.0
about the importance of,2,2,1.0
the importance of the,4,2,2.0
importance of the boundary,2,2,1.0
of the boundary regions,2,2,1.0
the boundary regions has,2,2,1.0
boundary regions has been,2,2,1.0
regions has been exploited,2,2,1.0
has been exploited in,2,2,1.0
been exploited in different,2,2,1.0
exploited in different ways,2,2,1.0
in different ways for,2,2,1.0
different ways for example,2,2,1.0
ways for example uses,2,2,1.0
for example uses an,2,2,1.0
example uses an svm,2,2,1.0
uses an svm to,2,2,1.0
an svm to approximate,2,2,1.0
svm to approximate the,2,2,1.0
to approximate the decision,2,2,1.0
approximate the decision and,1,1,1.0
the decision and then,1,1,1.0
decision and then generates,1,1,1.0
and then generates new,2,2,1.0
then generates new synthetic,2,2,1.0
generates new synthetic data,2,2,1.0
new synthetic data along,2,2,1.0
synthetic data along the,2,2,1.0
data along the lines,2,2,1.0
along the lines joining,2,2,1.0
the lines joining each,2,2,1.0
lines joining each temraz,2,2,1.0
joining each temraz keane,2,2,1.0
each temraz keane counterfactual,2,2,1.0
counterfactual data augmentation with,2,2,1.0
data augmentation with its,2,2,1.0
augmentation with its nearest,2,2,1.0
with its nearest neighbors,2,2,1.0
its nearest neighbors using,2,2,1.0
nearest neighbors using interpolation,2,2,1.0
neighbors using interpolation or,2,2,1.0
using interpolation or extrapolation,2,2,1.0
interpolation or extrapolation techniques,2,2,1.0
or extrapolation techniques in,2,2,1.0
extrapolation techniques in a,2,2,1.0
techniques in a similar,1,1,1.0
in a similar vein,1,1,1.0
a similar vein divides,1,1,1.0
similar vein divides minority,1,1,1.0
vein divides minority instances,1,1,1.0
divides minority instances into,2,2,1.0
minority instances into three,2,2,1.0
instances into three groups,2,2,1.0
into three groups security,2,2,1.0
three groups security instances,2,2,1.0
groups security instances border,2,2,1.0
security instances border instances,2,2,1.0
instances border instances and,2,2,1.0
border instances and latent,2,2,1.0
instances and latent noise,2,2,1.0
and latent noise instances,2,2,1.0
latent noise instances and,2,2,1.0
noise instances and then,2,2,1.0
instances and then treats,2,2,1.0
and then treats these,2,2,1.0
then treats these groups,2,2,1.0
treats these groups differently,2,2,1.0
these groups differently when,2,2,1.0
groups differently when generating,2,2,1.0
differently when generating instances,2,2,1.0
when generating instances other,2,2,1.0
generating instances other variants,2,2,1.0
instances other variants in,2,2,1.0
other variants in this,2,2,1.0
variants in this vein,2,2,1.0
in this vein adjust,2,2,1.0
this vein adjust the,2,2,1.0
vein adjust the sampling,2,2,1.0
adjust the sampling rate,2,2,1.0
the sampling rate for,2,2,1.0
sampling rate for some,2,2,1.0
rate for some minority,2,2,1.0
for some minority instances,2,2,1.0
some minority instances those,2,2,1.0
minority instances those close,2,2,1.0
instances those close to,2,2,1.0
those close to the,2,2,1.0
close to the boundary,3,3,1.0
to the boundary to,2,2,1.0
the boundary to improve,2,2,1.0
boundary to improve these,2,2,1.0
to improve these methods,2,2,1.0
improve these methods further,2,2,1.0
these methods further see,2,2,1.0
methods further see and,2,2,1.0
further see and this,2,2,1.0
see and this use,2,2,1.0
and this use of,2,2,1.0
this use of boundary,2,2,1.0
use of boundary regions,2,2,1.0
of boundary regions in,2,2,1.0
boundary regions in the,2,2,1.0
the minority class also,2,2,1.0
minority class also raises,2,2,1.0
class also raises questions,2,2,1.0
also raises questions about,2,2,1.0
raises questions about the,2,2,1.0
questions about the relationship,2,2,1.0
about the relationship of,2,2,1.0
the relationship of majority,2,2,1.0
relationship of majority instances,2,2,1.0
of majority instances to,2,2,1.0
majority instances to minority,2,2,1.0
instances to minority instances,2,2,1.0
to minority instances leading,2,2,1.0
minority instances leading to,2,2,1.0
instances leading to a,2,2,1.0
leading to a third,2,2,1.0
to a third insight,2,2,1.0
a third insight underlying,2,2,1.0
third insight underlying smote,2,2,1.0
insight underlying smote variants,2,2,1.0
underlying smote variants namely,2,2,1.0
smote variants namely that,2,2,1.0
variants namely that oversampling,2,2,1.0
namely that oversampling in,2,2,1.0
that oversampling in the,2,2,1.0
oversampling in the minority,2,2,1.0
the minority class can,5,3,1.6666666666666667
minority class can be,2,2,1.0
class can be by,2,2,1.0
can be by considering,2,2,1.0
be by considering the,2,2,1.0
by considering the majority,2,2,1.0
considering the majority class,2,2,1.0
the majority class using,4,2,2.0
majority class using the,2,2,1.0
class using the majority,2,2,1.0
using the majority class,2,2,1.0
the majority class a,3,3,1.0
majority class a third,2,2,1.0
class a third important,2,2,1.0
a third important insight,2,2,1.0
third important insight in,2,2,1.0
important insight in this,2,2,1.0
insight in this research,2,2,1.0
in this research area,2,2,1.0
this research area which,2,2,1.0
research area which becomes,2,2,1.0
area which becomes more,2,2,1.0
which becomes more apparent,2,2,1.0
becomes more apparent when,2,2,1.0
more apparent when borderlines,2,2,1.0
apparent when borderlines are,2,2,1.0
when borderlines are explored,2,2,1.0
borderlines are explored is,2,2,1.0
are explored is the,2,2,1.0
explored is the idea,2,2,1.0
is the idea that,2,2,1.0
the idea that the,2,2,1.0
idea that the relationship,2,2,1.0
that the relationship between,2,2,1.0
the relationship between the,2,2,1.0
relationship between the majority,2,2,1.0
between the majority class,2,2,1.0
majority class and the,8,4,2.0
class and the minority,4,3,1.3333333333333333
and the minority class,4,3,1.3333333333333333
minority class can also,2,2,1.0
class can also help,2,2,1.0
can also help guide,2,2,1.0
also help guide smote,2,2,1.0
help guide smote earlier,2,2,1.0
guide smote earlier we,2,2,1.0
smote earlier we saw,2,2,1.0
earlier we saw that,2,2,1.0
we saw that does,2,2,1.0
saw that does not,2,2,1.0
that does not interpolate,2,2,1.0
does not interpolate instances,2,2,1.0
not interpolate instances when,2,2,1.0
interpolate instances when the,2,2,1.0
instances when the neighbors,2,2,1.0
when the neighbors show,2,2,1.0
the neighbors show a,2,2,1.0
neighbors show a preponderance,2,2,1.0
show a preponderance of,2,2,1.0
a preponderance of majority,2,2,1.0
preponderance of majority see,1,1,1.0
of majority see also,1,1,1.0
majority see also adysyn,1,1,1.0
see also adysyn this,2,2,1.0
also adysyn this is,2,2,1.0
adysyn this is one,2,2,1.0
this is one way,2,2,1.0
is one way to,2,2,1.0
one way to take,2,2,1.0
way to take the,2,2,1.0
to take the majority,2,2,1.0
take the majority class,2,2,1.0
the majority class into,2,2,1.0
majority class into account,2,2,1.0
class into account other,2,2,1.0
into account other explore,1,1,1.0
account other explore the,1,1,1.0
other explore the relationship,1,1,1.0
explore the relationship between,2,2,1.0
the relationship between classes,2,2,1.0
relationship between classes to,2,2,1.0
between classes to undersample,2,2,1.0
classes to undersample the,2,2,1.0
to undersample the majority,2,2,1.0
majority class using techniques,2,2,1.0
class using techniques or,2,2,1.0
using techniques or to,2,2,1.0
techniques or to guide,2,2,1.0
or to guide the,2,2,1.0
to guide the oversampling,2,2,1.0
guide the oversampling of,2,2,1.0
the oversampling of the,2,2,1.0
oversampling of the minority,2,2,1.0
class for instance finds,1,1,1.0
for instance finds pairs,1,1,1.0
instance finds pairs of,1,1,1.0
finds pairs of instances,2,2,1.0
pairs of instances between,2,2,1.0
of instances between the,2,2,1.0
instances between the minority,2,2,1.0
between the minority and,2,2,1.0
the minority and majority,5,3,1.6666666666666667
minority and majority classes,5,3,1.6666666666666667
and majority classes that,2,2,1.0
majority classes that are,2,2,1.0
classes that are very,2,2,1.0
that are very similar,2,2,1.0
are very similar low,2,2,1.0
very similar low euclidean,2,2,1.0
similar low euclidean distance,2,2,1.0
low euclidean distance a,2,2,1.0
euclidean distance a tomek,2,2,1.0
distance a tomek link,2,2,1.0
a tomek link and,2,2,1.0
tomek link and then,2,2,1.0
link and then removes,2,2,1.0
and then removes the,2,2,1.0
then removes the majority,2,2,1.0
removes the majority instance,2,2,1.0
the majority instance in,2,2,1.0
majority instance in the,2,2,1.0
instance in the pair,2,2,1.0
in the pair by,2,2,1.0
the pair by removing,2,2,1.0
pair by removing these,2,2,1.0
by removing these and,2,2,1.0
removing these and applying,2,2,1.0
these and applying smote,2,2,1.0
and applying smote to,2,2,1.0
smote to the minority,2,2,1.0
to the minority class,16,4,4.0
the minority class it,2,2,1.0
minority class it attempts,2,2,1.0
class it attempts to,2,2,1.0
it attempts to the,2,2,1.0
attempts to the classes,2,2,1.0
to the classes uses,2,2,1.0
the classes uses a,2,2,1.0
classes uses a related,2,2,1.0
uses a related approach,2,2,1.0
a related approach involving,2,2,1.0
related approach involving the,2,2,1.0
approach involving the edited,2,2,1.0
involving the edited nearest,2,2,1.0
the edited nearest neighbour,2,2,1.0
edited nearest neighbour method,2,2,1.0
nearest neighbour method other,2,2,1.0
neighbour method other methods,2,2,1.0
method other methods and,2,2,1.0
other methods and swim,2,2,1.0
methods and swim perform,2,2,1.0
and swim perform explicit,2,2,1.0
swim perform explicit analyses,2,2,1.0
perform explicit analyses of,2,2,1.0
explicit analyses of the,2,2,1.0
analyses of the majority,2,2,1.0
majority class and use,2,2,1.0
class and use this,2,2,1.0
and use this analysis,2,2,1.0
use this analysis to,2,2,1.0
this analysis to minority,2,2,1.0
analysis to minority instance,2,2,1.0
to minority instance generation,2,2,1.0
minority instance generation does,1,1,1.0
instance generation does this,1,1,1.0
generation does this by,1,1,1.0
does this by computing,2,2,1.0
this by computing a,2,2,1.0
by computing a score,2,2,1.0
computing a score for,2,2,1.0
a score for each,2,2,1.0
score for each minority,2,2,1.0
for each minority instance,2,2,1.0
each minority instance where,2,2,1.0
minority instance where safety,2,2,1.0
instance where safety is,2,2,1.0
where safety is based,2,2,1.0
safety is based on,2,2,1.0
is based on the,5,4,1.25
based on the frequency,2,2,1.0
on the frequency of,2,2,1.0
the frequency of majority,2,2,1.0
frequency of majority instances,2,2,1.0
of majority instances in,2,2,1.0
majority instances in the,4,2,2.0
instances in the neighbours,2,2,1.0
in the neighbours and,2,2,1.0
the neighbours and a,2,2,1.0
neighbours and a temraz,2,2,1.0
and a temraz keane,2,2,1.0
a temraz keane counterfactual,2,2,1.0
counterfactual data augmentation ratio,2,2,1.0
data augmentation ratio based,2,2,1.0
augmentation ratio based on,2,2,1.0
ratio based on the,2,2,1.0
based on the score,2,2,1.0
on the score of,2,2,1.0
the score of a,2,2,1.0
score of a minority,2,2,1.0
of a minority instance,2,2,1.0
a minority instance over,2,2,1.0
minority instance over that,2,2,1.0
instance over that of,2,2,1.0
over that of its,2,2,1.0
that of its neighbours,2,2,1.0
of its neighbours s,1,1,1.0
its neighbours s finer,1,1,1.0
neighbours s finer analysis,1,1,1.0
s finer analysis of,2,2,1.0
finer analysis of the,2,2,1.0
analysis of the relationship,2,2,1.0
of the relationship between,2,2,1.0
the relationship between majority,2,2,1.0
relationship between majority and,2,2,1.0
between majority and minority,7,3,2.3333333333333335
majority and minority has,2,2,1.0
and minority has been,2,2,1.0
minority has been shown,2,2,1.0
has been shown to,5,3,1.6666666666666667
been shown to performance,1,1,1.0
shown to performance over,1,1,1.0
to performance over sampling,1,1,1.0
performance over sampling with,2,2,1.0
over sampling with the,2,2,1.0
sampling with the majority,2,2,1.0
with the majority swim,2,2,1.0
the majority swim adopt,2,2,1.0
majority swim adopt a,2,2,1.0
swim adopt a different,1,1,1.0
adopt a different approach,1,1,1.0
a different approach leveraging,1,1,1.0
different approach leveraging information,1,1,1.0
approach leveraging information about,2,2,1.0
leveraging information about the,2,2,1.0
information about the density,2,2,1.0
about the density of,2,2,1.0
the density of majority,2,2,1.0
density of majority instances,2,2,1.0
of majority instances using,2,2,1.0
majority instances using the,2,2,1.0
instances using the mahalanobis,2,2,1.0
using the mahalanobis distances,1,1,1.0
the mahalanobis distances requiring,1,1,1.0
mahalanobis distances requiring generated,1,1,1.0
distances requiring generated minority,1,1,1.0
requiring generated minority instances,2,2,1.0
generated minority instances to,2,2,1.0
minority instances to have,2,2,1.0
instances to have similar,2,2,1.0
to have similar distances,2,2,1.0
have similar distances to,2,2,1.0
similar distances to their,2,2,1.0
distances to their minority,2,2,1.0
to their minority seeds,2,2,1.0
their minority seeds so,2,2,1.0
minority seeds so swim,2,2,1.0
seeds so swim essentially,2,2,1.0
so swim essentially analyses,2,2,1.0
swim essentially analyses the,2,2,1.0
essentially analyses the topology,2,2,1.0
analyses the topology of,2,2,1.0
the topology of the,4,2,2.0
topology of the majority,4,2,2.0
the majority class to,2,2,1.0
majority class to guide,2,2,1.0
class to guide the,2,2,1.0
to guide the generation,2,2,1.0
guide the generation of,2,2,1.0
the generation of minority,2,2,1.0
of minority instances see,2,2,1.0
minority instances see and,2,2,1.0
instances see and for,1,1,1.0
see and for related,1,1,1.0
and for related finally,1,1,1.0
for related finally is,1,1,1.0
related finally is another,1,1,1.0
finally is another method,2,2,1.0
is another method that,2,2,1.0
another method that takes,2,2,1.0
method that takes similarities,2,2,1.0
that takes similarities to,2,2,1.0
takes similarities to majority,2,2,1.0
similarities to majority into,1,1,1.0
to majority into account,1,1,1.0
majority into account in,1,1,1.0
into account in computing,2,2,1.0
account in computing rough,2,2,1.0
in computing rough sets,2,2,1.0
computing rough sets over,2,2,1.0
rough sets over the,2,2,1.0
sets over the minority,2,2,1.0
over the minority class,4,4,1.0
the minority class after,2,2,1.0
minority class after smote,1,1,1.0
class after smote has,1,1,1.0
after smote has been,1,1,1.0
smote has been applied,1,1,1.0
has been applied to,2,2,1.0
been applied to generate,2,2,1.0
applied to generate additional,2,2,1.0
to generate additional minority,2,2,1.0
generate additional minority instances,2,2,1.0
additional minority instances this,2,2,1.0
minority instances this method,2,2,1.0
instances this method acts,2,2,1.0
this method acts like,2,2,1.0
method acts like a,2,2,1.0
acts like a step,2,2,1.0
like a step to,2,2,1.0
a step to generated,1,1,1.0
step to generated instances,1,1,1.0
to generated instances that,1,1,1.0
generated instances that might,2,2,1.0
instances that might be,2,2,1.0
that might be noise,2,2,1.0
might be noise many,2,2,1.0
be noise many of,2,2,1.0
noise many of these,2,2,1.0
many of these methods,2,2,1.0
of these methods improve,2,2,1.0
these methods improve on,2,2,1.0
methods improve on s,2,2,1.0
improve on s performance,2,2,1.0
on s performance and,2,2,1.0
s performance and as,2,2,1.0
performance and as such,2,2,1.0
and as such show,2,2,1.0
as such show that,2,2,1.0
such show that paying,2,2,1.0
show that paying more,2,2,1.0
that paying more attention,2,2,1.0
paying more attention to,2,2,1.0
more attention to the,3,3,1.0
attention to the majority,2,2,1.0
the majority class can,3,3,1.0
majority class can play,2,2,1.0
class can play a,2,2,1.0
can play a key,2,2,1.0
play a key role,2,2,1.0
a key role in,2,2,1.0
key role in instance,2,2,1.0
role in instance generation,2,2,1.0
in instance generation in,2,2,1.0
instance generation in the,2,2,1.0
generation in the minority,2,2,1.0
the minority class we,2,2,1.0
minority class we will,2,2,1.0
class we will see,2,2,1.0
we will see later,2,2,1.0
will see later that,2,2,1.0
see later that while,2,2,1.0
later that while the,2,2,1.0
that while the current,2,2,1.0
while the current counterfactual,2,2,1.0
the current counterfactual methods,2,2,1.0
current counterfactual methods reflect,2,2,1.0
counterfactual methods reflect these,2,2,1.0
methods reflect these three,2,2,1.0
reflect these three key,2,2,1.0
these three key insights,2,2,1.0
three key insights about,2,2,1.0
key insights about out,2,2,1.0
insights about out to,2,2,1.0
about out to improve,2,2,1.0
out to improve on,2,2,1.0
to improve on smote,2,2,1.0
improve on smote it,2,2,1.0
on smote it is,2,2,1.0
smote it is quite,2,2,1.0
it is quite different,2,2,1.0
is quite different from,4,2,2.0
quite different from all,2,2,1.0
different from all of,2,2,1.0
from all of the,2,2,1.0
all of the above,2,2,1.0
of the above methods,2,2,1.0
the above methods in,2,2,1.0
above methods in how,2,2,1.0
methods in how it,2,2,1.0
in how it operates,2,2,1.0
how it operates see,2,2,1.0
it operates see section,2,2,1.0
operates see section but,2,2,1.0
see section but before,2,2,1.0
section but before considering,2,2,1.0
but before considering this,2,2,1.0
before considering this counterfactual,2,2,1.0
considering this counterfactual method,2,2,1.0
this counterfactual method in,2,2,1.0
counterfactual method in detail,2,2,1.0
method in detail we,2,2,1.0
in detail we first,2,2,1.0
detail we first briefly,2,2,1.0
we first briefly review,2,2,1.0
first briefly review how,2,2,1.0
briefly review how it,2,2,1.0
review how it has,2,2,1.0
how it has emerged,2,2,1.0
it has emerged in,2,2,1.0
has emerged in xai,2,2,1.0
emerged in xai counterfactual,2,2,1.0
in xai counterfactual generation,2,2,1.0
xai counterfactual generation in,2,2,1.0
counterfactual generation in xai,2,2,1.0
generation in xai in,2,2,1.0
in xai in this,2,2,1.0
xai in this paper,2,2,1.0
this paper we deploy,2,2,1.0
paper we deploy a,2,2,1.0
we deploy a counterfactual,2,2,1.0
deploy a counterfactual method,2,2,1.0
a counterfactual method to,2,2,1.0
counterfactual method to generate,4,2,2.0
method to generate synthetic,4,2,2.0
generate synthetic instances counterfactual,2,2,1.0
synthetic instances counterfactual methods,2,2,1.0
instances counterfactual methods have,2,2,1.0
counterfactual methods have been,4,2,2.0
methods have been developed,2,2,1.0
have been developed to,3,3,1.0
been developed to generate,2,2,1.0
developed to generate examples,2,2,1.0
to generate examples to,2,2,1.0
generate examples to explain,2,2,1.0
examples to explain the,2,2,1.0
to explain the predictions,2,2,1.0
explain the predictions of,2,2,1.0
the predictions of ml,2,2,1.0
predictions of ml models,2,2,1.0
of ml models and,2,2,1.0
ml models and to,2,2,1.0
models and to provide,2,2,1.0
and to provide algorithmic,2,2,1.0
to provide algorithmic recourse,2,2,1.0
provide algorithmic recourse for,2,2,1.0
algorithmic recourse for temraz,2,2,1.0
recourse for temraz keane,2,2,1.0
for temraz keane counterfactual,2,2,1.0
counterfactual data augmentation trying,2,2,1.0
data augmentation trying to,2,2,1.0
augmentation trying to mitigate,2,2,1.0
trying to mitigate automated,2,2,1.0
to mitigate automated decisions,2,2,1.0
mitigate automated decisions for,2,2,1.0
automated decisions for reviews,2,2,1.0
decisions for reviews see,2,2,1.0
for reviews see the,2,2,1.0
reviews see the classic,2,2,1.0
see the classic counterfactual,2,2,1.0
the classic counterfactual is,1,1,1.0
classic counterfactual is one,1,1,1.0
counterfactual is one that,1,1,1.0
is one that is,2,2,1.0
one that is given,2,2,1.0
that is given when,2,2,1.0
is given when an,2,2,1.0
given when an automated,2,2,1.0
when an automated system,2,2,1.0
an automated system refuses,2,2,1.0
automated system refuses a,2,2,1.0
system refuses a person,2,2,1.0
refuses a person on,2,2,1.0
a person on a,2,2,1.0
person on a loan,2,2,1.0
on a loan application,2,2,1.0
a loan application when,2,2,1.0
loan application when the,2,2,1.0
application when the asks,2,2,1.0
when the asks why,2,2,1.0
the asks why the,2,2,1.0
asks why the system,2,2,1.0
why the system might,2,2,1.0
the system might counterfactually,2,2,1.0
system might counterfactually explain,2,2,1.0
might counterfactually explain that,2,2,1.0
counterfactually explain that if,2,2,1.0
explain that if you,2,2,1.0
that if you a,1,1,1.0
if you a loan,1,1,1.0
you a loan for,1,1,1.0
a loan for less,2,2,1.0
loan for less over,2,2,1.0
for less over a,2,2,1.0
less over a shorter,2,2,1.0
over a shorter term,2,2,1.0
a shorter term then,2,2,1.0
shorter term then you,2,2,1.0
term then you would,2,2,1.0
then you would have,2,2,1.0
you would have been,2,2,1.0
would have been granted,2,2,1.0
have been granted the,2,2,1.0
been granted the loan,2,2,1.0
granted the loan that,2,2,1.0
the loan that is,2,2,1.0
loan that is the,2,2,1.0
that is the counterfactual,2,2,1.0
is the counterfactual explanation,2,2,1.0
the counterfactual explanation tells,2,2,1.0
counterfactual explanation tells users,2,2,1.0
explanation tells users about,2,2,1.0
tells users about the,2,2,1.0
users about the conditions,2,2,1.0
about the conditions under,2,2,1.0
conditions under which outcome,1,1,1.0
under which outcome would,1,1,1.0
which outcome would change,1,1,1.0
outcome would change the,1,1,1.0
would change the closest,2,2,1.0
change the closest world,2,2,1.0
the closest world to,2,2,1.0
closest world to their,2,2,1.0
world to their world,2,2,1.0
to their world in,2,2,1.0
their world in which,2,2,1.0
which the outcome would,2,2,1.0
the outcome would be,2,2,1.0
outcome would be what,2,2,1.0
would be what they,2,2,1.0
be what they desire,2,2,1.0
what they desire counterfactuals,2,2,1.0
they desire counterfactuals have,2,2,1.0
desire counterfactuals have been,2,2,1.0
counterfactuals have been researched,2,2,1.0
have been researched for,2,2,1.0
been researched for some,2,2,1.0
researched for some time,2,2,1.0
for some time in,2,2,1.0
some time in ai,2,2,1.0
time in ai under,2,2,1.0
in ai under diverse,2,2,1.0
ai under diverse names,1,1,1.0
under diverse names for,1,1,1.0
diverse names for in,1,1,1.0
names for in the,1,1,1.0
for in the past,1,1,1.0
in the past they,2,2,1.0
the past they have,2,2,1.0
past they have been,2,2,1.0
they have been called,2,2,1.0
have been called nearest,2,2,1.0
been called nearest unlike,2,2,1.0
called nearest unlike neighbours,2,2,1.0
nearest unlike neighbours nuns,2,2,1.0
unlike neighbours nuns or,2,2,1.0
neighbours nuns or inverse,2,2,1.0
nuns or inverse classifications,2,2,1.0
or inverse classifications recently,2,2,1.0
inverse classifications recently they,2,2,1.0
classifications recently they have,2,2,1.0
recently they have emerged,2,2,1.0
they have emerged as,2,2,1.0
have emerged as a,2,2,1.0
emerged as a hot,2,2,1.0
as a hot topic,2,2,1.0
a hot topic in,3,3,1.0
hot topic in xai,2,2,1.0
topic in xai because,1,1,1.0
in xai because they,1,1,1.0
xai because they appear,1,1,1.0
because they appear to,1,1,1.0
they appear to have,2,2,1.0
appear to have psychological,2,2,1.0
to have psychological benefits,2,2,1.0
have psychological benefits people,2,2,1.0
psychological benefits people naturally,2,2,1.0
benefits people naturally understand,2,2,1.0
people naturally understand them,2,2,1.0
naturally understand them and,2,2,1.0
understand them and legal,2,2,1.0
them and legal benefits,2,2,1.0
and legal benefits they,2,2,1.0
legal benefits they are,2,2,1.0
benefits they are said,2,2,1.0
they are said to,2,2,1.0
are said to be,2,2,1.0
said to be gdpr,2,2,1.0
to be gdpr compliant,2,2,1.0
be gdpr compliant optimization,2,2,1.0
gdpr compliant optimization techniques,2,2,1.0
compliant optimization techniques are,2,2,1.0
optimization techniques are currently,2,2,1.0
techniques are currently the,2,2,1.0
are currently the most,2,2,1.0
currently the most popular,2,2,1.0
the most popular method,2,2,1.0
most popular method for,2,2,1.0
popular method for computing,2,2,1.0
method for computing counterfactuals,2,2,1.0
for computing counterfactuals given,2,2,1.0
computing counterfactuals given a,2,2,1.0
counterfactuals given a test,2,2,1.0
given a test instance,2,2,1.0
a test instance one,2,2,1.0
test instance one encoding,2,2,1.0
instance one encoding the,2,2,1.0
one encoding the loan,1,1,1.0
encoding the loan refusal,1,1,1.0
the loan refusal these,1,1,1.0
loan refusal these optimization,2,2,1.0
refusal these optimization methods,2,2,1.0
these optimization methods search,2,2,1.0
optimization methods search a,2,2,1.0
methods search a sometimes,2,2,1.0
search a sometimes randomly,1,1,1.0
a sometimes randomly generated,1,1,1.0
sometimes randomly generated space,1,1,1.0
randomly generated space of,1,1,1.0
generated space of perturbations,2,2,1.0
space of perturbations of,2,2,1.0
of perturbations of the,2,2,1.0
perturbations of the query,2,2,1.0
of the query synthetic,2,2,1.0
the query synthetic instances,2,2,1.0
query synthetic instances under,2,2,1.0
synthetic instances under a,2,2,1.0
instances under a loss,2,2,1.0
under a loss function,2,2,1.0
a loss function that,2,2,1.0
loss function that balances,1,1,1.0
function that balances proximity,1,1,1.0
that balances proximity to,1,1,1.0
balances proximity to the,1,1,1.0
proximity to the against,2,2,1.0
to the against proximity,2,2,1.0
the against proximity to,2,2,1.0
against proximity to the,2,2,1.0
proximity to the decision,2,2,1.0
the decision boundary for,2,2,1.0
decision boundary for the,2,2,1.0
boundary for the counterfactual,2,2,1.0
for the counterfactual class,2,2,1.0
the counterfactual class the,2,2,1.0
counterfactual class the class,2,2,1.0
class the class that,2,2,1.0
the class that counters,2,2,1.0
class that counters that,2,2,1.0
that counters that of,2,2,1.0
counters that of the,2,2,1.0
that of the query,2,2,1.0
of the query using,2,2,1.0
the query using a,2,2,1.0
query using a scaled,2,2,1.0
using a scaled wachter,2,2,1.0
a scaled wachter et,2,2,1.0
scaled wachter et s,2,2,1.0
wachter et s seminal,2,2,1.0
et s seminal method,2,2,1.0
s seminal method uses,2,2,1.0
seminal method uses gradient,2,2,1.0
method uses gradient descent,2,2,1.0
uses gradient descent to,2,2,1.0
gradient descent to find,2,2,1.0
descent to find the,2,2,1.0
to find the best,2,2,1.0
find the best counterfactual,2,2,1.0
the best counterfactual instance,1,1,1.0
best counterfactual instance for,1,1,1.0
counterfactual instance for a,1,1,1.0
instance for a query,1,1,1.0
for a query though,2,2,1.0
a query though later,2,2,1.0
query though later models,2,2,1.0
though later models have,2,2,1.0
later models have used,2,2,1.0
models have used other,2,2,1.0
have used other techniques,2,2,1.0
used other techniques genetic,2,2,1.0
other techniques genetic algorithms,2,2,1.0
techniques genetic algorithms mothilal,2,2,1.0
genetic algorithms mothilal et,2,2,1.0
algorithms mothilal et al,2,2,1.0
mothilal et al proposed,2,2,1.0
et al proposed the,4,2,2.0
al proposed the diverse,2,2,1.0
proposed the diverse counterfactual,2,2,1.0
the diverse counterfactual explanations,2,2,1.0
diverse counterfactual explanations dice,2,2,1.0
counterfactual explanations dice method,2,2,1.0
explanations dice method as,2,2,1.0
dice method as an,2,2,1.0
method as an extension,2,2,1.0
as an extension to,2,2,1.0
an extension to generate,2,2,1.0
extension to generate a,2,2,1.0
to generate a set,2,2,1.0
generate a set of,4,2,2.0
a set of counterfactual,1,1,1.0
set of counterfactual candidates,1,1,1.0
of counterfactual candidates avoiding,1,1,1.0
counterfactual candidates avoiding the,2,2,1.0
candidates avoiding the problem,2,2,1.0
avoiding the problem of,2,2,1.0
the problem of generating,2,2,1.0
problem of generating sets,2,2,1.0
of generating sets of,2,2,1.0
generating sets of candidates,2,2,1.0
sets of candidates that,2,2,1.0
of candidates that were,2,2,1.0
candidates that were trivial,2,2,1.0
that were trivial variants,2,2,1.0
were trivial variants on,2,2,1.0
trivial variants on one,2,2,1.0
variants on one another,2,2,1.0
on one another the,2,2,1.0
one another the main,2,2,1.0
another the main problem,2,2,1.0
the main problem with,2,2,1.0
main problem with these,2,2,1.0
problem with these optimization,2,2,1.0
with these optimization methods,1,1,1.0
these optimization methods is,1,1,1.0
optimization methods is that,1,1,1.0
methods is that given,1,1,1.0
is that given their,2,2,1.0
that given their blind,2,2,1.0
given their blind perturbation,2,2,1.0
their blind perturbation of,2,2,1.0
blind perturbation of they,2,2,1.0
perturbation of they sometimes,2,2,1.0
of they sometimes generate,2,2,1.0
they sometimes generate invalid,2,2,1.0
sometimes generate invalid keane,2,2,1.0
generate invalid keane counterfactual,2,2,1.0
invalid keane counterfactual data,2,2,1.0
data augmentation points this,2,2,1.0
augmentation points this defect,2,2,1.0
points this defect has,2,2,1.0
this defect has potentially,2,2,1.0
defect has potentially serious,2,2,1.0
has potentially serious for,2,2,1.0
potentially serious for their,2,2,1.0
serious for their use,2,2,1.0
for their use in,2,2,1.0
their use in the,2,2,1.0
use in the problem,1,1,1.0
in the problem as,1,1,1.0
the problem as it,1,1,1.0
problem as it suggests,2,2,1.0
as it suggests that,2,2,1.0
it suggests that they,2,2,1.0
suggests that they might,2,2,1.0
that they might populate,2,2,1.0
they might populate the,2,2,1.0
might populate the minority,2,2,1.0
populate the minority class,2,2,1.0
minority class with noise,1,1,1.0
class with noise with,1,1,1.0
with noise with negative,1,1,1.0
noise with negative effects,1,1,1.0
with negative effects on,1,1,1.0
negative effects on a,2,2,1.0
effects on a classifier,2,2,1.0
on a classifier s,2,2,1.0
classifier s performance however,2,2,1.0
s performance however a,2,2,1.0
performance however a very,2,2,1.0
however a very different,2,2,1.0
a very different approach,2,2,1.0
very different approach to,2,2,1.0
different approach to counterfactual,2,2,1.0
approach to counterfactual generation,2,2,1.0
to counterfactual generation has,2,2,1.0
counterfactual generation has recently,2,2,1.0
generation has recently been,2,2,1.0
has recently been proposed,2,2,1.0
recently been proposed this,2,2,1.0
been proposed this method,2,2,1.0
proposed this method finds,2,2,1.0
this method finds the,2,2,1.0
method finds the s,2,2,1.0
finds the s that,2,2,1.0
the s that takes,2,2,1.0
s that takes part,2,2,1.0
that takes part in,2,2,1.0
takes part in a,2,2,1.0
part in a explanation,2,2,1.0
in a explanation case,2,2,1.0
a explanation case xc,2,2,1.0
explanation case xc an,2,2,1.0
case xc an explanation,2,2,1.0
xc an explanation case,2,2,1.0
an explanation case captures,2,2,1.0
explanation case captures a,2,2,1.0
case captures a counterfactual,2,2,1.0
captures a counterfactual between,1,1,1.0
a counterfactual between existing,1,1,1.0
counterfactual between existing instances,1,1,1.0
between existing instances in,2,2,1.0
existing instances in the,2,2,1.0
instances in the dataset,2,2,1.0
in the dataset that,2,2,1.0
the dataset that are,2,2,1.0
dataset that are in,2,2,1.0
that are in opposing,2,2,1.0
are in opposing classes,2,2,1.0
in opposing classes either,2,2,1.0
opposing classes either side,2,2,1.0
classes either side of,2,2,1.0
either side of a,4,2,2.0
side of a decision,2,2,1.0
of a decision boundary,2,2,1.0
a decision boundary with,2,2,1.0
decision boundary with the,4,2,2.0
boundary with the constraint,2,2,1.0
with the constraint that,2,2,1.0
the constraint that the,4,2,2.0
constraint that the pair,2,2,1.0
that the pair of,2,2,1.0
the pair of instances,2,2,1.0
pair of instances differ,2,2,1.0
of instances differ by,2,2,1.0
instances differ by at,2,2,1.0
differ by at most,2,2,1.0
by at most two,2,2,1.0
at most two for,1,1,1.0
most two for example,1,1,1.0
two for example the,1,1,1.0
for example the loan,2,2,1.0
example the loan dataset,2,2,1.0
the loan dataset could,2,2,1.0
loan dataset could contain,2,2,1.0
dataset could contain two,2,2,1.0
could contain two existing,2,2,1.0
contain two existing cases,2,2,1.0
two existing cases that,2,2,1.0
existing cases that are,2,2,1.0
cases that are counterfactually,2,2,1.0
that are counterfactually related,2,2,1.0
are counterfactually related one,2,2,1.0
counterfactually related one about,2,2,1.0
related one about a,2,2,1.0
one about a old,2,2,1.0
about a old female,2,2,1.0
a old female accountant,2,2,1.0
old female accountant earning,2,2,1.0
female accountant earning who,2,2,1.0
accountant earning who was,4,2,2.0
earning who was refused,4,2,2.0
who was refused a,4,2,2.0
was refused a loan,4,2,2.0
refused a loan that,2,2,1.0
a loan that is,2,2,1.0
loan that is counterfactually,2,2,1.0
that is counterfactually related,3,2,1.5
is counterfactually related to,3,2,1.5
counterfactually related to another,2,2,1.0
related to another instance,2,2,1.0
to another instance with,2,2,1.0
another instance with a,2,2,1.0
instance with a different,2,2,1.0
with a different outcome,2,2,1.0
a different outcome namely,2,2,1.0
different outcome namely a,2,2,1.0
outcome namely a old,2,2,1.0
namely a old accountant,1,1,1.0
a old accountant earning,1,1,1.0
old accountant earning who,1,1,1.0
earning who was granted,2,2,1.0
who was granted a,2,2,1.0
was granted a loan,2,2,1.0
granted a loan differences,2,2,1.0
a loan differences shown,2,2,1.0
loan differences shown in,2,2,1.0
differences shown in italics,2,2,1.0
shown in italics this,2,2,1.0
in italics this explanatory,2,2,1.0
italics this explanatory case,2,2,1.0
this explanatory case implicitly,2,2,1.0
explanatory case implicitly suggests,2,2,1.0
case implicitly suggests that,2,2,1.0
implicitly suggests that if,2,2,1.0
suggests that if one,2,2,1.0
that if one earns,2,2,1.0
if one earns rather,2,2,1.0
one earns rather than,2,2,1.0
earns rather than then,2,2,1.0
rather than then the,4,2,2.0
than then the loan,4,2,2.0
then the loan is,1,1,1.0
the loan is likely,1,1,1.0
loan is likely to,1,1,1.0
is likely to be,5,3,1.6666666666666667
likely to be granted,2,2,1.0
to be granted rather,2,2,1.0
be granted rather than,2,2,1.0
granted rather than refused,2,2,1.0
rather than refused so,2,2,1.0
than refused so if,2,2,1.0
refused so if i,2,2,1.0
so if i am,4,2,2.0
if i am a,2,2,1.0
i am a old,2,2,1.0
am a old male,2,2,1.0
a old male teacher,2,2,1.0
old male teacher earning,2,2,1.0
male teacher earning who,2,2,1.0
teacher earning who was,2,2,1.0
refused a loan then,2,2,1.0
a loan then this,2,2,1.0
loan then this algorithm,2,2,1.0
then this algorithm could,2,2,1.0
this algorithm could find,2,2,1.0
algorithm could find this,2,2,1.0
could find this as,2,2,1.0
find this as a,2,2,1.0
this as a nearest,2,2,1.0
as a nearest neighbour,2,2,1.0
a nearest neighbour and,2,2,1.0
nearest neighbour and suggest,2,2,1.0
neighbour and suggest that,2,2,1.0
and suggest that if,2,2,1.0
suggest that if this,2,2,1.0
that if this earned,2,2,1.0
if this earned more,2,2,1.0
this earned more rather,2,2,1.0
earned more rather than,2,2,1.0
more rather than then,2,2,1.0
then the loan would,2,2,1.0
the loan would be,2,2,1.0
loan would be granted,2,2,1.0
would be granted in,2,2,1.0
be granted in the,2,2,1.0
granted in the xai,2,2,1.0
in the xai context,2,2,1.0
the xai context this,2,2,1.0
xai context this method,2,2,1.0
context this method has,2,2,1.0
this method has been,2,2,1.0
method has been shown,2,2,1.0
been shown to generate,2,2,1.0
shown to generate close,2,2,1.0
to generate close plausible,2,2,1.0
generate close plausible counterfactuals,2,2,1.0
close plausible counterfactuals and,2,2,1.0
plausible counterfactuals and appears,2,2,1.0
counterfactuals and appears to,2,2,1.0
and appears to avoid,2,2,1.0
appears to avoid the,2,2,1.0
to avoid the pitfalls,2,2,1.0
avoid the pitfalls that,2,2,1.0
the pitfalls that arise,2,2,1.0
pitfalls that arise in,2,2,1.0
that arise in techniques,1,1,1.0
arise in techniques see,1,1,1.0
in techniques see from,1,1,1.0
techniques see from a,2,2,1.0
see from a data,2,2,1.0
from a data augmentation,4,2,2.0
a data augmentation perspective,4,2,2.0
data augmentation perspective this,2,2,1.0
augmentation perspective this method,2,2,1.0
perspective this method can,2,2,1.0
this method can be,3,3,1.0
method can be seen,3,3,1.0
can be seen as,3,3,1.0
be seen as supporting,2,2,1.0
seen as supporting the,2,2,1.0
as supporting the creation,2,2,1.0
supporting the creation of,2,2,1.0
the creation of synthetic,2,2,1.0
creation of synthetic in,2,2,1.0
of synthetic in the,3,3,1.0
the minority class using,9,3,3.0
minority class using information,2,2,1.0
class using information from,2,2,1.0
using information from these,2,2,1.0
information from these known,2,2,1.0
from these known counterfactual,2,2,1.0
these known counterfactual pairs,2,2,1.0
known counterfactual pairs however,2,2,1.0
counterfactual pairs however few,2,2,1.0
pairs however few xai,2,2,1.0
however few xai techniques,2,2,1.0
few xai techniques have,2,2,1.0
xai techniques have been,2,2,1.0
techniques have been applied,2,2,1.0
have been applied to,2,2,1.0
been applied to data,2,2,1.0
applied to data augmentation,2,2,1.0
to data augmentation problems,4,2,2.0
data augmentation problems in,2,2,1.0
augmentation problems in the,2,2,1.0
problems in the next,2,2,1.0
in the next subsection,2,2,1.0
the next subsection we,2,2,1.0
next subsection we briefly,2,2,1.0
subsection we briefly sketch,2,2,1.0
we briefly sketch this,2,2,1.0
briefly sketch this small,2,2,1.0
sketch this small literature,2,2,1.0
this small literature on,2,2,1.0
small literature on the,2,2,1.0
literature on the topic,2,2,1.0
on the topic temraz,2,2,1.0
the topic temraz keane,2,2,1.0
topic temraz keane counterfactual,2,2,1.0
counterfactual data augmentation using,4,2,2.0
data augmentation using counterfactuals,2,2,1.0
augmentation using counterfactuals for,2,2,1.0
using counterfactuals for data,2,2,1.0
counterfactuals for data augmentation,2,2,1.0
for data augmentation beyond,2,2,1.0
data augmentation beyond xai,2,2,1.0
augmentation beyond xai our,2,2,1.0
beyond xai our hypothesis,2,2,1.0
xai our hypothesis is,2,2,1.0
our hypothesis is that,2,2,1.0
hypothesis is that counterfactual,2,2,1.0
is that counterfactual methods,2,2,1.0
that counterfactual methods can,2,2,1.0
counterfactual methods can also,2,2,1.0
methods can also play,2,2,1.0
can also play a,2,2,1.0
also play a role,2,2,1.0
play a role in,2,2,1.0
a role in data,2,2,1.0
role in data to,1,1,1.0
in data to solve,1,1,1.0
data to solve problems,3,2,1.5
to solve problems that,2,2,1.0
solve problems that generated,2,2,1.0
problems that generated synthetic,2,2,1.0
that generated synthetic counterfactual,1,1,1.0
generated synthetic counterfactual cases,1,1,1.0
synthetic counterfactual cases could,1,1,1.0
counterfactual cases could the,1,1,1.0
cases could the predictive,1,1,1.0
could the predictive accuracy,1,1,1.0
the predictive accuracy of,2,2,1.0
predictive accuracy of ai,2,2,1.0
accuracy of ai models,2,2,1.0
of ai models although,2,2,1.0
ai models although there,2,2,1.0
models although there are,2,2,1.0
although there are now,2,2,1.0
there are now of,2,2,1.0
are now of papers,2,2,1.0
now of papers on,2,2,1.0
of papers on in,1,1,1.0
papers on in xai,1,1,1.0
on in xai only,1,1,1.0
in xai only a,2,2,1.0
xai only a handful,2,2,1.0
only a handful of,2,2,1.0
a handful of papers,2,2,1.0
handful of papers consider,2,2,1.0
of papers consider their,2,2,1.0
papers consider their use,2,2,1.0
consider their use in,2,2,1.0
their use in data,2,2,1.0
use in data augmentation,2,2,1.0
in data augmentation in,2,2,1.0
data augmentation in evaluating,2,2,1.0
augmentation in evaluating xai,2,2,1.0
in evaluating xai counterfactual,2,2,1.0
evaluating xai counterfactual methods,2,2,1.0
xai counterfactual methods mothilal,2,2,1.0
counterfactual methods mothilal et,2,2,1.0
methods mothilal et al,2,2,1.0
mothilal et al suggested,2,2,1.0
et al suggested that,3,3,1.0
al suggested that a,2,2,1.0
suggested that a good,2,2,1.0
that a good method,2,2,1.0
a good method should,2,2,1.0
good method should generate,2,2,1.0
method should generate a,2,2,1.0
should generate a set,2,2,1.0
a set of counterfactuals,2,2,1.0
set of counterfactuals that,2,2,1.0
of counterfactuals that can,2,2,1.0
counterfactuals that can substitute,2,2,1.0
that can substitute for,2,2,1.0
can substitute for the,2,2,1.0
substitute for the original,2,2,1.0
for the original dataset,2,2,1.0
the original dataset calling,2,2,1.0
original dataset calling it,2,2,1.0
dataset calling it that,1,1,1.0
calling it that is,1,1,1.0
it that is if,1,1,1.0
that is if the,2,2,1.0
is if the set,2,2,1.0
the set of generated,2,2,1.0
set of generated counterfactuals,2,2,1.0
of generated counterfactuals were,2,2,1.0
generated counterfactuals were plausible,2,2,1.0
counterfactuals were plausible and,2,2,1.0
were plausible and close,2,2,1.0
plausible and close to,2,2,1.0
and close to the,2,2,1.0
close to the original,2,2,1.0
to the original data,4,3,1.3333333333333333
the original data then,2,2,1.0
original data then their,2,2,1.0
data then their predictive,2,2,1.0
then their predictive performance,2,2,1.0
their predictive performance should,2,2,1.0
predictive performance should parallel,2,2,1.0
performance should parallel that,2,2,1.0
should parallel that of,2,2,1.0
parallel that of the,2,2,1.0
that of the original,2,2,1.0
of the original dataset,4,2,2.0
the original dataset however,1,1,1.0
original dataset however mothilal,1,1,1.0
dataset however mothilal et,1,1,1.0
however mothilal et al,1,1,1.0
mothilal et al did,2,2,1.0
et al did not,2,2,1.0
al did not consider,2,2,1.0
did not consider using,2,2,1.0
not consider using their,2,2,1.0
consider using their counterfactual,2,2,1.0
using their counterfactual method,2,2,1.0
their counterfactual method for,2,2,1.0
for data augmentation in,2,2,1.0
data augmentation in a,2,2,1.0
augmentation in a student,2,2,1.0
in a student project,2,2,1.0
a student project hasan,2,2,1.0
student project hasan did,2,2,1.0
project hasan did and,2,2,1.0
hasan did and tried,2,2,1.0
did and tried to,2,2,1.0
and tried to determine,2,2,1.0
tried to determine whether,2,2,1.0
to determine whether an,2,2,1.0
determine whether an augmented,2,2,1.0
whether an augmented dataset,2,2,1.0
an augmented dataset based,2,2,1.0
augmented dataset based on,2,2,1.0
dataset based on generated,1,1,1.0
based on generated could,1,1,1.0
on generated could act,1,1,1.0
generated could act as,1,1,1.0
could act as a,2,2,1.0
act as a proxy,2,2,1.0
as a proxy dataset,2,2,1.0
a proxy dataset but,2,2,1.0
proxy dataset but only,2,2,1.0
dataset but only found,2,2,1.0
but only found modest,2,2,1.0
only found modest success,2,2,1.0
found modest success a,2,2,1.0
modest success a selection,2,2,1.0
success a selection of,2,2,1.0
a selection of other,2,2,1.0
selection of other papers,2,2,1.0
of other papers in,2,2,1.0
other papers in diverse,2,2,1.0
papers in diverse areas,2,2,1.0
in diverse areas have,2,2,1.0
diverse areas have also,2,2,1.0
areas have also circled,2,2,1.0
have also circled the,2,2,1.0
also circled the issue,2,2,1.0
circled the issue of,2,2,1.0
the issue of using,2,2,1.0
issue of using counterfactual,2,2,1.0
of using counterfactual techniques,2,2,1.0
using counterfactual techniques for,2,2,1.0
counterfactual techniques for data,2,2,1.0
techniques for data augmentation,2,2,1.0
for data augmentation subbaswamy,2,2,1.0
data augmentation subbaswamy and,2,2,1.0
augmentation subbaswamy and saria,2,2,1.0
subbaswamy and saria considered,2,2,1.0
and saria considered the,2,2,1.0
saria considered the problem,2,2,1.0
considered the problem of,2,2,1.0
the problem of dataset,2,2,1.0
problem of dataset shift,2,2,1.0
of dataset shift where,2,2,1.0
dataset shift where there,2,2,1.0
shift where there is,2,2,1.0
where there is a,2,2,1.0
there is a divergence,2,2,1.0
is a divergence between,2,2,1.0
a divergence between the,2,2,1.0
divergence between the context,2,2,1.0
between the context in,2,2,1.0
the context in which,2,2,1.0
context in which a,2,2,1.0
in which a model,2,2,1.0
which a model was,2,2,1.0
a model was trained,1,1,1.0
model was trained and,1,1,1.0
was trained and tested,1,1,1.0
trained and tested they,1,1,1.0
and tested they use,2,2,1.0
tested they use the,2,2,1.0
they use the notion,2,2,1.0
use the notion of,2,2,1.0
the notion of counterfactual,2,2,1.0
notion of counterfactual risk,2,2,1.0
of counterfactual risk to,2,2,1.0
counterfactual risk to diagnose,2,2,1.0
risk to diagnose this,2,2,1.0
to diagnose this problem,2,2,1.0
diagnose this problem using,2,2,1.0
this problem using causal,2,2,1.0
problem using causal models,2,2,1.0
using causal models zeng,2,2,1.0
causal models zeng et,2,2,1.0
models zeng et al,2,2,1.0
zeng et al proposed,2,2,1.0
al proposed the counterfactual,2,2,1.0
proposed the counterfactual generator,2,2,1.0
the counterfactual generator which,2,2,1.0
counterfactual generator which generates,2,2,1.0
generator which generates counterfactual,2,2,1.0
which generates counterfactual examples,2,2,1.0
generates counterfactual examples for,2,2,1.0
counterfactual examples for data,1,1,1.0
examples for data and,1,1,1.0
for data and found,1,1,1.0
data and found that,2,2,1.0
and found that generated,2,2,1.0
found that generated counterfactuals,2,2,1.0
that generated counterfactuals improved,2,2,1.0
generated counterfactuals improved the,2,2,1.0
counterfactuals improved the generalizability,2,2,1.0
improved the generalizability of,2,2,1.0
the generalizability of models,2,2,1.0
generalizability of models under,2,2,1.0
of models under limited,2,2,1.0
models under limited observational,2,2,1.0
under limited observational examples,2,2,1.0
limited observational examples pitis,2,2,1.0
observational examples pitis et,2,2,1.0
examples pitis et al,2,2,1.0
pitis et al proposed,1,1,1.0
et al proposed counterfactual,1,1,1.0
al proposed counterfactual data,1,1,1.0
proposed counterfactual data augmentation,1,1,1.0
counterfactual data augmentation coda,2,2,1.0
data augmentation coda for,2,2,1.0
augmentation coda for generating,2,2,1.0
coda for generating counterfactual,2,2,1.0
for generating counterfactual experiences,2,2,1.0
generating counterfactual experiences in,2,2,1.0
counterfactual experiences in reinforcement,2,2,1.0
experiences in reinforcement learning,2,2,1.0
in reinforcement learning rl,2,2,1.0
reinforcement learning rl in,2,2,1.0
learning rl in which,2,2,1.0
rl in which the,2,2,1.0
in which the method,2,2,1.0
which the method increases,2,2,1.0
the method increases the,2,2,1.0
method increases the size,2,2,1.0
increases the size of,2,2,1.0
the size of available,2,2,1.0
size of available training,2,2,1.0
of available training data,2,2,1.0
available training data with,2,2,1.0
training data with counterfactual,2,2,1.0
data with counterfactual examples,2,2,1.0
with counterfactual examples by,2,2,1.0
counterfactual examples by stitching,2,2,1.0
examples by stitching subsamples,1,1,1.0
by stitching subsamples from,1,1,1.0
stitching subsamples from the,1,1,1.0
subsamples from the environment,2,2,1.0
from the environment they,2,2,1.0
the environment they found,2,2,1.0
environment they found that,2,2,1.0
they found that coda,2,2,1.0
found that coda significantly,2,2,1.0
that coda significantly improved,2,2,1.0
coda significantly improved the,2,2,1.0
significantly improved the performance,2,2,1.0
improved the performance of,2,2,1.0
the performance of rl,2,2,1.0
performance of rl agents,2,2,1.0
of rl agents in,2,2,1.0
rl agents in tasks,2,2,1.0
agents in tasks for,2,2,1.0
in tasks for and,2,2,1.0
tasks for and keane,2,2,1.0
for and keane counterfactual,2,2,1.0
and keane counterfactual data,2,2,1.0
counterfactual data augmentation conditioned,2,2,1.0
data augmentation conditioned settings,2,2,1.0
augmentation conditioned settings the,2,2,1.0
conditioned settings the problem,2,2,1.0
settings the problem with,2,2,1.0
the problem with these,2,2,1.0
problem with these papers,2,2,1.0
with these papers is,2,2,1.0
these papers is that,2,2,1.0
papers is that they,2,2,1.0
is that they use,2,2,1.0
that they use bespoke,2,2,1.0
they use bespoke counterfactual,2,2,1.0
use bespoke counterfactual developed,1,1,1.0
bespoke counterfactual developed for,1,1,1.0
counterfactual developed for specific,1,1,1.0
developed for specific task,2,2,1.0
for specific task domains,2,2,1.0
specific task domains rather,2,2,1.0
task domains rather than,2,2,1.0
domains rather than the,2,2,1.0
rather than the tried,2,2,1.0
than the tried and,2,2,1.0
the tried and tested,2,2,1.0
tried and tested techniques,1,1,1.0
and tested techniques from,1,1,1.0
tested techniques from the,1,1,1.0
techniques from the xai,1,1,1.0
the xai literature therefore,2,2,1.0
xai literature therefore their,2,2,1.0
literature therefore their performance,2,2,1.0
therefore their performance robustness,2,2,1.0
their performance robustness and,2,2,1.0
performance robustness and generalizability,2,2,1.0
robustness and generalizability is,2,2,1.0
and generalizability is at,2,2,1.0
generalizability is at best,2,2,1.0
is at best however,2,2,1.0
at best however one,2,2,1.0
best however one study,2,2,1.0
however one study has,2,2,1.0
one study has applied,2,2,1.0
study has applied to,2,2,1.0
has applied to a,2,2,1.0
applied to a xai,2,2,1.0
to a xai counterfactual,2,2,1.0
a xai counterfactual method,2,2,1.0
xai counterfactual method to,2,2,1.0
counterfactual method to the,4,2,2.0
method to the problem,2,2,1.0
to the problem of,2,2,1.0
the problem of data,2,2,1.0
problem of data augmentation,2,2,1.0
of data augmentation temraz,2,2,1.0
data augmentation temraz et,2,2,1.0
augmentation temraz et al,2,2,1.0
temraz et al used,2,2,1.0
et al used the,2,2,1.0
al used the present,2,2,1.0
used the present counterfactual,2,2,1.0
the present counterfactual method,2,2,1.0
present counterfactual method to,2,2,1.0
to generate synthetic data,4,3,1.3333333333333333
generate synthetic data for,2,2,1.0
synthetic data for a,2,2,1.0
data for a prediction,2,2,1.0
for a prediction problem,2,2,1.0
a prediction problem their,2,2,1.0
prediction problem their problem,2,2,1.0
problem their problem domain,2,2,1.0
their problem domain involved,1,1,1.0
problem domain involved a,1,1,1.0
domain involved a model,1,1,1.0
involved a model for,1,1,1.0
a model for grass,2,2,1.0
model for grass growth,2,2,1.0
for grass growth prediction,2,2,1.0
grass growth prediction that,2,2,1.0
growth prediction that relies,2,2,1.0
prediction that relies on,2,2,1.0
that relies on an,2,2,1.0
relies on an historical,2,2,1.0
on an historical dataset,2,2,1.0
an historical dataset of,2,2,1.0
historical dataset of specific,2,2,1.0
dataset of specific measurements,2,2,1.0
of specific measurements of,2,2,1.0
specific measurements of climate,2,2,1.0
measurements of climate and,2,2,1.0
of climate and grass,2,2,1.0
climate and grass growth,2,2,1.0
and grass growth on,2,2,1.0
grass growth on dairy,2,2,1.0
growth on dairy farms,2,2,1.0
on dairy farms in,2,2,1.0
dairy farms in ireland,2,2,1.0
farms in ireland covering,2,2,1.0
in ireland covering the,2,2,1.0
ireland covering the model,2,2,1.0
covering the model does,2,2,1.0
the model does reasonably,2,2,1.0
model does reasonably well,2,2,1.0
does reasonably well at,2,2,1.0
reasonably well at predicting,2,2,1.0
well at predicting grass,4,2,2.0
at predicting grass growth,4,2,2.0
predicting grass growth for,4,2,2.0
grass growth for individual,2,2,1.0
growth for individual farms,2,2,1.0
for individual farms in,2,2,1.0
individual farms in the,2,2,1.0
farms in the coming,2,2,1.0
in the coming week,2,2,1.0
the coming week using,2,2,1.0
coming week using this,2,2,1.0
week using this historical,2,2,1.0
using this historical data,2,2,1.0
this historical data but,2,2,1.0
historical data but with,2,2,1.0
data but with climate,2,2,1.0
but with climate change,2,2,1.0
with climate change there,2,2,1.0
climate change there are,2,2,1.0
change there are an,2,2,1.0
there are an increasing,2,2,1.0
are an increasing number,2,2,1.0
an increasing number of,2,2,1.0
increasing number of events,2,2,1.0
number of events events,2,2,1.0
of events events that,2,2,1.0
events events that diverge,2,2,1.0
events that diverge significantly,2,2,1.0
that diverge significantly from,2,2,1.0
diverge significantly from the,2,2,1.0
significantly from the recorded,1,1,1.0
from the recorded in,1,1,1.0
the recorded in the,1,1,1.0
recorded in the historical,2,2,1.0
in the historical data,2,2,1.0
the historical data extreme,2,2,1.0
historical data extreme values,2,2,1.0
data extreme values for,2,2,1.0
extreme values for key,2,2,1.0
values for key weather,2,2,1.0
for key weather variables,1,1,1.0
key weather variables like,1,1,1.0
weather variables like solar,1,1,1.0
variables like solar or,1,1,1.0
like solar or soil,1,1,1.0
solar or soil moisture,1,1,1.0
or soil moisture for,2,2,1.0
soil moisture for example,2,2,1.0
moisture for example in,2,2,1.0
for example in there,2,2,1.0
example in there was,2,2,1.0
in there was a,2,2,1.0
there was a significant,2,2,1.0
was a significant drought,2,2,1.0
a significant drought across,2,2,1.0
significant drought across europe,2,2,1.0
drought across europe that,2,2,1.0
across europe that effectively,2,2,1.0
europe that effectively halted,2,2,1.0
that effectively halted grass,2,2,1.0
effectively halted grass growth,2,2,1.0
halted grass growth in,2,2,1.0
grass growth in ireland,2,2,1.0
growth in ireland during,2,2,1.0
in ireland during what,2,2,1.0
ireland during what is,2,2,1.0
during what is usually,2,2,1.0
what is usually the,2,2,1.0
is usually the of,2,2,1.0
usually the of july,2,2,1.0
the of july if,2,2,1.0
of july if soil,2,2,1.0
july if soil moisture,2,2,1.0
if soil moisture drops,2,2,1.0
soil moisture drops then,2,2,1.0
moisture drops then grass,2,2,1.0
drops then grass stops,2,2,1.0
then grass stops growing,2,2,1.0
grass stops growing indeed,2,2,1.0
stops growing indeed high,2,2,1.0
growing indeed high solar,2,2,1.0
indeed high solar radiation,2,2,1.0
high solar radiation will,2,2,1.0
solar radiation will burn,2,2,1.0
radiation will burn grass,2,2,1.0
will burn grass the,1,1,1.0
burn grass the model,1,1,1.0
grass the model does,1,1,1.0
the model does not,2,2,1.0
model does not do,2,2,1.0
does not do very,2,2,1.0
not do very well,2,2,1.0
do very well at,2,2,1.0
very well at predicting,2,2,1.0
grass growth for these,2,2,1.0
growth for these months,1,1,1.0
for these months of,1,1,1.0
these months of because,1,1,1.0
months of because they,2,2,1.0
of because they are,2,2,1.0
because they are historically,2,2,1.0
they are historically unique,2,2,1.0
are historically unique temraz,2,2,1.0
historically unique temraz et,2,2,1.0
unique temraz et al,2,2,1.0
temraz et al defined,2,2,1.0
et al defined a,2,2,1.0
al defined a class,1,1,1.0
defined a class boundary,1,1,1.0
a class boundary in,1,1,1.0
class boundary in the,2,2,1.0
boundary in the dataset,2,2,1.0
in the dataset creating,2,2,1.0
the dataset creating a,2,2,1.0
dataset creating a division,2,2,1.0
creating a division between,2,2,1.0
a division between normal,2,2,1.0
division between normal cases,2,2,1.0
between normal cases with,2,2,1.0
normal cases with values,2,2,1.0
cases with values within,2,2,1.0
with values within standard,2,2,1.0
values within standard deviations,2,2,1.0
within standard deviations of,2,2,1.0
standard deviations of historical,4,2,2.0
deviations of historical means,4,2,2.0
of historical means and,2,2,1.0
historical means and cases,2,2,1.0
means and cases with,2,2,1.0
and cases with values,2,2,1.0
cases with values standard,2,2,1.0
with values standard deviations,2,2,1.0
values standard deviations of,2,2,1.0
of historical means from,2,2,1.0
historical means from a,2,2,1.0
means from a perspective,1,1,1.0
from a perspective these,1,1,1.0
a perspective these normal,1,1,1.0
perspective these normal cases,2,2,1.0
these normal cases were,2,2,1.0
normal cases were the,2,2,1.0
cases were the majority,2,2,1.0
were the majority class,2,2,1.0
class and the cases,2,2,1.0
and the cases an,2,2,1.0
the cases an unpublished,2,2,1.0
cases an unpublished paper,2,2,1.0
an unpublished paper reports,2,2,1.0
unpublished paper reports an,2,2,1.0
paper reports an identical,2,2,1.0
reports an identical method,2,2,1.0
an identical method to,2,2,1.0
identical method to wachter,2,2,1.0
method to wachter et,2,2,1.0
to wachter et s,2,2,1.0
wachter et s counterfactual,2,2,1.0
et s counterfactual optimization,2,2,1.0
s counterfactual optimization method,2,2,1.0
counterfactual optimization method for,2,2,1.0
optimization method for data,2,2,1.0
for data augmentation however,2,2,1.0
data augmentation however this,2,2,1.0
augmentation however this paper,2,2,1.0
however this paper does,2,2,1.0
this paper does not,2,2,1.0
paper does not reference,2,2,1.0
does not reference wachter,2,2,1.0
not reference wachter et,2,2,1.0
reference wachter et al,2,2,1.0
wachter et al or,2,2,1.0
et al or any,2,2,1.0
al or any of,2,2,1.0
or any of the,2,2,1.0
any of the xai,2,2,1.0
of the xai literature,2,2,1.0
the xai literature temraz,2,2,1.0
xai literature temraz keane,2,2,1.0
literature temraz keane counterfactual,2,2,1.0
counterfactual data augmentation are,2,2,1.0
data augmentation are the,2,2,1.0
augmentation are the minority,2,2,1.0
are the minority class,2,2,1.0
minority class they then,2,2,1.0
class they then used,2,2,1.0
they then used the,2,2,1.0
then used the counterfactual,2,2,1.0
used the counterfactual method,1,1,1.0
the counterfactual method to,1,1,1.0
counterfactual method to create,1,1,1.0
method to create new,1,1,1.0
to create new synthetic,2,2,1.0
create new synthetic cases,2,2,1.0
new synthetic cases and,2,2,1.0
synthetic cases and showed,2,2,1.0
cases and showed that,2,2,1.0
and showed that the,2,2,1.0
showed that the model,2,2,1.0
that the model s,2,2,1.0
the model s performance,2,2,1.0
model s performance specifically,2,2,1.0
s performance specifically improved,2,2,1.0
performance specifically improved on,2,2,1.0
specifically improved on predicting,2,2,1.0
improved on predicting events,2,2,1.0
on predicting events in,2,2,1.0
predicting events in using,2,2,1.0
events in using these,2,2,1.0
in using these minority,1,1,1.0
using these minority interestingly,1,1,1.0
these minority interestingly temraz,1,1,1.0
minority interestingly temraz et,1,1,1.0
interestingly temraz et al,2,2,1.0
temraz et al s,2,2,1.0
et al s experiments,2,2,1.0
al s experiments showed,2,2,1.0
s experiments showed that,2,2,1.0
experiments showed that that,2,2,1.0
showed that that the,2,2,1.0
that that the did,1,1,1.0
that the did better,1,1,1.0
the did better than,1,1,1.0
did better than in,2,2,1.0
better than in this,2,2,1.0
than in this problem,2,2,1.0
in this problem domain,2,2,1.0
this problem domain specifically,2,2,1.0
problem domain specifically the,1,1,1.0
domain specifically the dice,1,1,1.0
specifically the dice method,1,1,1.0
the dice method however,1,1,1.0
dice method however this,2,2,1.0
method however this work,2,2,1.0
however this work only,2,2,1.0
this work only considers,2,2,1.0
work only considers one,2,2,1.0
only considers one specific,2,2,1.0
considers one specific problem,2,2,1.0
one specific problem domain,2,2,1.0
specific problem domain classifier,2,2,1.0
problem domain classifier and,2,2,1.0
domain classifier and dataset,2,2,1.0
classifier and dataset it,2,2,1.0
and dataset it remains,2,2,1.0
dataset it remains to,2,2,1.0
it remains to be,2,2,1.0
remains to be seen,2,2,1.0
to be seen whether,2,2,1.0
be seen whether this,2,2,1.0
seen whether this counterfactual,2,2,1.0
whether this counterfactual approach,2,2,1.0
this counterfactual approach generalizes,2,2,1.0
counterfactual approach generalizes to,2,2,1.0
approach generalizes to other,2,2,1.0
generalizes to other problem,1,1,1.0
to other problem domains,1,1,1.0
other problem domains classifiers,1,1,1.0
problem domains classifiers and,1,1,1.0
domains classifiers and datasets,2,2,1.0
classifiers and datasets and,2,2,1.0
and datasets and specifically,2,2,1.0
datasets and specifically to,2,2,1.0
and specifically to datasets,2,2,1.0
specifically to datasets where,2,2,1.0
to datasets where problems,2,2,1.0
datasets where problems arise,2,2,1.0
where problems arise hence,2,2,1.0
problems arise hence this,2,2,1.0
arise hence this is,2,2,1.0
hence this is the,2,2,1.0
this is the aim,2,2,1.0
is the aim for,2,2,1.0
the aim for the,2,2,1.0
aim for the remainder,2,2,1.0
for the remainder of,2,2,1.0
the remainder of this,3,3,1.0
remainder of this paper,3,3,1.0
of this paper the,2,2,1.0
this paper the counterfactual,2,2,1.0
paper the counterfactual augmentation,2,2,1.0
the counterfactual augmentation algorithm,2,2,1.0
counterfactual augmentation algorithm cfa,2,2,1.0
augmentation algorithm cfa this,2,2,1.0
algorithm cfa this paper,2,2,1.0
cfa this paper advances,2,2,1.0
this paper advances the,2,2,1.0
paper advances the use,2,2,1.0
advances the use of,2,2,1.0
the use of counterfactual,2,2,1.0
use of counterfactual methods,2,2,1.0
of counterfactual methods for,2,2,1.0
counterfactual methods for data,2,2,1.0
methods for data augmentation,2,2,1.0
for data augmentation as,2,2,1.0
data augmentation as a,2,2,1.0
augmentation as a solution,2,2,1.0
as a solution to,2,2,1.0
a solution to the,1,1,1.0
solution to the minority,1,1,1.0
minority class with more,2,2,1.0
class with more synthetic,2,2,1.0
with more synthetic data,2,2,1.0
more synthetic data to,2,2,1.0
synthetic data to solve,2,2,1.0
to solve problems the,2,2,1.0
solve problems the of,1,1,1.0
problems the of this,1,1,1.0
the of this xai,1,1,1.0
of this xai method,2,2,1.0
this xai method to,2,2,1.0
xai method to data,2,2,1.0
method to data augmentation,2,2,1.0
to data augmentation was,2,2,1.0
data augmentation was motivated,2,2,1.0
augmentation was motivated by,2,2,1.0
was motivated by the,2,2,1.0
motivated by the observation,1,1,1.0
by the observation that,1,1,1.0
the observation that it,1,1,1.0
observation that it seemed,1,1,1.0
that it seemed to,2,2,1.0
it seemed to generate,2,2,1.0
seemed to generate plausible,2,2,1.0
to generate plausible synthetic,2,2,1.0
generate plausible synthetic datapoints,2,2,1.0
plausible synthetic datapoints for,4,2,2.0
synthetic datapoints for explanatory,2,2,1.0
datapoints for explanatory purposes,2,2,1.0
for explanatory purposes furthermore,2,2,1.0
explanatory purposes furthermore the,2,2,1.0
purposes furthermore the evaluation,1,1,1.0
furthermore the evaluation metrics,1,1,1.0
the evaluation metrics in,1,1,1.0
evaluation metrics in xai,1,1,1.0
metrics in xai showed,2,2,1.0
in xai showed that,2,2,1.0
xai showed that these,2,2,1.0
showed that these datapoints,2,2,1.0
that these datapoints were,2,2,1.0
these datapoints were generally,2,2,1.0
datapoints were generally valid,2,2,1.0
were generally valid and,2,2,1.0
generally valid and close,4,2,2.0
valid and close to,4,2,2.0
and close to existing,4,2,2.0
close to existing datapoints,2,2,1.0
to existing datapoints accordingly,2,2,1.0
existing datapoints accordingly the,2,2,1.0
datapoints accordingly the extension,2,2,1.0
accordingly the extension of,2,2,1.0
the extension of these,2,2,1.0
extension of these techniques,2,2,1.0
of these techniques to,2,2,1.0
these techniques to data,2,2,1.0
techniques to data augmentation,2,2,1.0
data augmentation problems seemed,2,2,1.0
augmentation problems seemed like,2,2,1.0
problems seemed like a,2,2,1.0
seemed like a promising,2,2,1.0
like a promising avenue,2,2,1.0
a promising avenue of,2,2,1.0
promising avenue of research,2,2,1.0
avenue of research in,2,2,1.0
of research in this,2,2,1.0
research in this section,2,2,1.0
in this section we,7,4,1.75
this section we describe,2,2,1.0
section we describe a,2,2,1.0
we describe a new,2,2,1.0
describe a new oversampling,2,2,1.0
a new oversampling method,2,2,1.0
new oversampling method using,2,2,1.0
oversampling method using a,2,2,1.0
method using a reasoning,1,1,1.0
using a reasoning to,1,1,1.0
a reasoning to generating,1,1,1.0
reasoning to generating synthetic,1,1,1.0
to generating synthetic counterfactuals,4,2,2.0
generating synthetic counterfactuals in,4,2,2.0
synthetic counterfactuals in the,4,2,2.0
counterfactuals in the minority,6,2,3.0
minority class to be,3,3,1.0
class to be applied,2,2,1.0
to be applied to,2,2,1.0
be applied to binary,2,2,1.0
applied to binary problems,1,1,1.0
to binary problems consider,1,1,1.0
binary problems consider a,1,1,1.0
problems consider a simple,2,2,1.0
consider a simple scenario,2,2,1.0
a simple scenario to,2,2,1.0
simple scenario to show,2,2,1.0
scenario to show how,2,2,1.0
to show how this,2,2,1.0
show how this method,2,2,1.0
how this method operates,2,2,1.0
this method operates temraz,2,2,1.0
method operates temraz keane,2,2,1.0
operates temraz keane counterfactual,2,2,1.0
counterfactual data augmentation a,4,2,2.0
data augmentation a counterfactual,2,2,1.0
augmentation a counterfactual example,2,2,1.0
a counterfactual example for,2,2,1.0
counterfactual example for class,2,2,1.0
example for class imbalances,2,2,1.0
for class imbalances imagine,2,2,1.0
class imbalances imagine an,2,2,1.0
imbalances imagine an ml,2,2,1.0
imagine an ml classifier,2,2,1.0
an ml classifier being,2,2,1.0
ml classifier being used,2,2,1.0
classifier being used to,2,2,1.0
being used to predict,2,2,1.0
used to predict whether,4,2,2.0
to predict whether farm,2,2,1.0
predict whether farm animals,2,2,1.0
whether farm animals are,2,2,1.0
farm animals are likely,2,2,1.0
animals are likely to,2,2,1.0
are likely to be,2,2,1.0
likely to be healthy,2,2,1.0
to be healthy or,2,2,1.0
be healthy or fall,2,2,1.0
healthy or fall ill,2,2,1.0
or fall ill mastitis,2,2,1.0
fall ill mastitis in,2,2,1.0
ill mastitis in cows,2,2,1.0
mastitis in cows see,2,2,1.0
in cows see the,2,2,1.0
cows see the dataset,2,2,1.0
see the dataset recording,2,2,1.0
the dataset recording a,2,2,1.0
dataset recording a herd,2,2,1.0
recording a herd of,2,2,1.0
a herd of cows,2,2,1.0
herd of cows on,2,2,1.0
of cows on most,2,2,1.0
cows on most farms,2,2,1.0
on most farms will,2,2,1.0
most farms will be,2,2,1.0
farms will be imbalanced,2,2,1.0
will be imbalanced in,2,2,1.0
be imbalanced in that,2,2,1.0
imbalanced in that most,2,2,1.0
in that most cows,2,2,1.0
that most cows will,2,2,1.0
most cows will tend,2,2,1.0
cows will tend to,2,2,1.0
will tend to be,2,2,1.0
tend to be healthy,2,2,1.0
to be healthy rather,2,2,1.0
be healthy rather than,2,2,1.0
healthy rather than an,2,2,1.0
rather than an analysis,2,2,1.0
than an analysis of,2,2,1.0
an analysis of this,2,2,1.0
analysis of this dataset,2,2,1.0
of this dataset shows,2,2,1.0
this dataset shows that,2,2,1.0
dataset shows that some,2,2,1.0
shows that some pairs,2,2,1.0
that some pairs of,2,2,1.0
some pairs of instances,2,2,1.0
pairs of instances instance,2,2,1.0
of instances instance pairs,2,2,1.0
instances instance pairs be,2,2,1.0
instance pairs be counterfactually,2,2,1.0
pairs be counterfactually related,2,2,1.0
be counterfactually related to,2,2,1.0
counterfactually related to one,2,2,1.0
related to one another,2,2,1.0
to one another for,2,2,1.0
one another for example,2,2,1.0
another for example a,2,2,1.0
for example a majority,2,2,1.0
example a majority instance,2,2,1.0
a majority instance of,2,2,1.0
majority instance of a,2,2,1.0
instance of a certain,2,2,1.0
of a certain breed,2,2,1.0
a certain breed age,2,2,1.0
certain breed age and,2,2,1.0
breed age and that,1,1,1.0
age and that is,1,1,1.0
and that is classed,1,1,1.0
that is classed as,4,2,2.0
is classed as healthy,2,2,1.0
classed as healthy can,2,2,1.0
as healthy can be,2,2,1.0
healthy can be counterfactually,2,2,1.0
can be counterfactually paired,2,2,1.0
be counterfactually paired with,2,2,1.0
counterfactually paired with a,2,2,1.0
paired with a minority,2,2,1.0
with a minority instance,2,2,1.0
a minority instance that,2,2,1.0
minority instance that is,4,2,2.0
instance that is of,2,2,1.0
that is of the,2,2,1.0
is of the same,2,2,1.0
of the same breed,2,2,1.0
the same breed age,2,2,1.0
same breed age but,2,2,1.0
breed age but with,2,2,1.0
age but with a,2,2,1.0
but with a different,2,2,1.0
with a different they,1,1,1.0
a different they have,1,1,1.0
different they have ill,1,1,1.0
they have ill several,2,2,1.0
have ill several times,2,2,1.0
ill several times that,2,2,1.0
several times that is,2,2,1.0
times that is classed,2,2,1.0
is classed as likely,2,2,1.0
classed as likely to,2,2,1.0
as likely to be,2,2,1.0
likely to be this,2,2,1.0
to be this which,2,2,1.0
be this which we,2,2,1.0
this which we call,2,2,1.0
which we call a,2,2,1.0
we call a native,2,2,1.0
call a native counterfactual,2,2,1.0
a native counterfactual tells,2,2,1.0
native counterfactual tells us,2,2,1.0
counterfactual tells us that,2,2,1.0
tells us that a,2,2,1.0
us that a difference,2,2,1.0
that a difference in,2,2,1.0
a difference in the,2,2,1.0
difference in the feature,2,2,1.0
in the feature can,2,2,1.0
the feature can change,2,2,1.0
feature can change the,2,2,1.0
can change the class,2,2,1.0
change the class of,2,2,1.0
the class of a,2,2,1.0
class of a cow,2,2,1.0
of a cow from,2,2,1.0
a cow from healthy,2,2,1.0
cow from healthy to,2,2,1.0
from healthy to so,2,2,1.0
healthy to so if,2,2,1.0
to so if we,2,2,1.0
so if we want,2,2,1.0
if we want to,2,2,1.0
we want to fix,2,2,1.0
want to fix the,2,2,1.0
to fix the class,2,2,1.0
fix the class imbalance,2,2,1.0
the class imbalance in,2,2,1.0
class imbalance in this,3,3,1.0
imbalance in this dataset,2,2,1.0
in this dataset using,1,1,1.0
this dataset using our,1,1,1.0
dataset using our counterfactual,1,1,1.0
using our counterfactual method,1,1,1.0
our counterfactual method then,2,2,1.0
counterfactual method then one,2,2,1.0
method then one can,2,2,1.0
then one can generate,2,2,1.0
one can generate a,2,2,1.0
can generate a new,4,2,2.0
generate a new minority,2,2,1.0
a new minority instance,4,2,2.0
new minority instance by,2,2,1.0
minority instance by this,1,1,1.0
instance by this known,1,1,1.0
by this known counterfactual,1,1,1.0
this known counterfactual relationship,1,1,1.0
known counterfactual relationship imagine,2,2,1.0
counterfactual relationship imagine we,2,2,1.0
relationship imagine we pick,2,2,1.0
imagine we pick another,2,2,1.0
we pick another majority,2,2,1.0
pick another majority instance,2,2,1.0
another majority instance a,2,2,1.0
majority instance a cow,2,2,1.0
instance a cow that,2,2,1.0
a cow that has,2,2,1.0
cow that has no,2,2,1.0
that has no existing,2,2,1.0
has no existing counterfactual,2,2,1.0
no existing counterfactual pair,2,2,1.0
existing counterfactual pair and,2,2,1.0
counterfactual pair and find,2,2,1.0
pair and find a,2,2,1.0
and find a nearest,2,2,1.0
find a nearest neighbour,2,2,1.0
a nearest neighbour to,2,2,1.0
nearest neighbour to that,2,2,1.0
neighbour to that is,2,2,1.0
to that is part,2,2,1.0
that is part of,2,2,1.0
is part of known,2,2,1.0
part of known the,2,2,1.0
of known the pair,2,2,1.0
known the pair using,2,2,1.0
the pair using this,2,2,1.0
pair using this and,2,2,1.0
using this and the,2,2,1.0
this and the native,2,2,1.0
and the native we,2,2,1.0
the native we can,2,2,1.0
native we can generate,2,2,1.0
we can generate a,2,2,1.0
generate a new synthetic,4,2,2.0
a new synthetic minority,2,2,1.0
new synthetic minority instance,1,1,1.0
synthetic minority instance using,1,1,1.0
minority instance using the,1,1,1.0
instance using the of,1,1,1.0
using the of and,1,1,1.0
the of and the,1,1,1.0
of and the from,2,2,1.0
and the from so,2,2,1.0
the from so this,2,2,1.0
from so this new,2,2,1.0
so this new instance,2,2,1.0
this new instance would,2,2,1.0
new instance would have,2,2,1.0
instance would have the,2,2,1.0
would have the of,2,2,1.0
have the of for,2,2,1.0
the of for breed,2,2,1.0
of for breed age,2,2,1.0
for breed age and,2,2,1.0
breed age and and,2,2,1.0
age and and the,2,2,1.0
and and the from,1,1,1.0
and the from for,1,1,1.0
the from for along,1,1,1.0
from for along with,2,2,1.0
for along with the,2,2,1.0
along with the prediction,2,2,1.0
with the prediction that,2,2,1.0
the prediction that it,2,2,1.0
prediction that it will,2,2,1.0
that it will be,2,2,1.0
it will be so,2,2,1.0
will be so we,2,2,1.0
be so we have,2,2,1.0
so we have now,2,2,1.0
we have now created,2,2,1.0
have now created a,2,2,1.0
now created a new,2,2,1.0
created a new minority,2,2,1.0
new minority instance that,2,2,1.0
instance that is counterfactually,1,1,1.0
counterfactually related to where,1,1,1.0
related to where the,2,2,1.0
to where the class,2,2,1.0
where the class of,2,2,1.0
the class of this,2,2,1.0
class of this new,2,2,1.0
of this new instance,2,2,1.0
this new instance needs,2,2,1.0
new instance needs to,2,2,1.0
instance needs to be,2,2,1.0
needs to be verified,2,2,1.0
to be verified by,2,2,1.0
be verified by the,2,2,1.0
verified by the underlying,2,2,1.0
by the underlying ml,2,2,1.0
the underlying ml model,2,2,1.0
underlying ml model this,2,2,1.0
ml model this example,2,2,1.0
model this example keane,2,2,1.0
this example keane smyth,2,2,1.0
example keane smyth have,2,2,1.0
keane smyth have shown,2,2,1.0
smyth have shown good,2,2,1.0
have shown good native,2,2,1.0
shown good native those,2,2,1.0
good native those with,2,2,1.0
native those with differences,2,2,1.0
those with differences between,2,2,1.0
with differences between them,2,2,1.0
differences between them are,2,2,1.0
between them are quite,2,2,1.0
them are quite rare,2,2,1.0
are quite rare in,2,2,1.0
quite rare in most,2,2,1.0
rare in most datasets,2,2,1.0
in most datasets of,2,2,1.0
most datasets of all,2,2,1.0
datasets of all instances,2,2,1.0
of all instances but,2,2,1.0
all instances but with,2,2,1.0
instances but with some,2,2,1.0
but with some tolerance,2,2,1.0
with some tolerance in,2,2,1.0
some tolerance in they,2,2,1.0
tolerance in they can,2,2,1.0
in they can be,2,2,1.0
they can be increased,2,2,1.0
can be increased to,2,2,1.0
be increased to temraz,2,2,1.0
increased to temraz keane,2,2,1.0
to temraz keane counterfactual,2,2,1.0
counterfactual data augmentation describes,2,2,1.0
data augmentation describes the,2,2,1.0
augmentation describes the generation,2,2,1.0
describes the generation of,2,2,1.0
the generation of one,2,2,1.0
generation of one minority,2,2,1.0
of one minority instance,2,2,1.0
one minority instance in,2,2,1.0
minority instance in our,2,2,1.0
instance in our experiments,2,2,1.0
in our experiments we,2,2,1.0
our experiments we do,2,2,1.0
experiments we do this,2,2,1.0
we do this iteratively,1,1,1.0
do this iteratively for,1,1,1.0
this iteratively for all,1,1,1.0
iteratively for all those,1,1,1.0
for all those majority,2,2,1.0
all those majority instances,2,2,1.0
those majority instances that,2,2,1.0
majority instances that are,2,2,1.0
instances that are not,2,2,1.0
that are not paired,2,2,1.0
are not paired in,2,2,1.0
not paired in native,2,2,1.0
paired in native counterfactuals,2,2,1.0
in native counterfactuals a,2,2,1.0
native counterfactuals a step,2,2,1.0
counterfactuals a step that,2,2,1.0
a step that results,2,2,1.0
step that results in,2,2,1.0
that results in the,2,2,1.0
results in the generation,2,2,1.0
in the generation of,3,3,1.0
the generation of many,2,2,1.0
generation of many more,2,2,1.0
of many more minority,2,2,1.0
many more minority instances,2,2,1.0
more minority instances close,2,2,1.0
the decision boundary with,2,2,1.0
boundary with the majority,2,2,1.0
the majority class in,5,4,1.25
majority class in the,3,3,1.0
class in the next,2,2,1.0
in the next we,2,2,1.0
the next we describe,2,2,1.0
next we describe the,2,2,1.0
we describe the algorithm,2,2,1.0
describe the algorithm more,2,2,1.0
the algorithm more formally,2,2,1.0
algorithm more formally the,2,2,1.0
more formally the method,2,2,1.0
formally the method counterfactual,2,2,1.0
the method counterfactual augmentation,2,2,1.0
method counterfactual augmentation cfa,4,2,2.0
counterfactual augmentation cfa the,2,2,1.0
augmentation cfa the counterfactual,2,2,1.0
cfa the counterfactual augmentation,2,2,1.0
the counterfactual augmentation cfa,2,2,1.0
counterfactual augmentation cfa is,2,2,1.0
augmentation cfa is a,2,2,1.0
cfa is a technique,2,2,1.0
is a technique for,2,2,1.0
a technique for generating,2,2,1.0
technique for generating synthetic,2,2,1.0
for generating synthetic examples,2,2,1.0
generating synthetic examples in,2,2,1.0
synthetic examples in the,2,2,1.0
examples in the minority,3,3,1.0
minority class using counterfactual,2,2,1.0
class using counterfactual methods,2,2,1.0
using counterfactual methods see,2,2,1.0
counterfactual methods see for,2,2,1.0
methods see for the,2,2,1.0
see for the method,2,2,1.0
for the method used,2,2,1.0
the method used in,2,2,1.0
method used in xai,2,2,1.0
used in xai the,2,2,1.0
in xai the cfa,2,2,1.0
xai the cfa method,2,2,1.0
the cfa method generates,2,2,1.0
cfa method generates synthetic,2,2,1.0
method generates synthetic counterfactual,2,2,1.0
counterfactual instances in three,2,2,1.0
instances in three main,2,2,1.0
in three main steps,2,2,1.0
three main steps see,2,2,1.0
main steps see figure,2,2,1.0
steps see figure figure,2,2,1.0
see figure figure counterfactual,2,2,1.0
figure figure counterfactual augmentation,2,2,1.0
figure counterfactual augmentation cfa,2,2,1.0
counterfactual augmentation cfa an,1,1,1.0
augmentation cfa an unpaired,1,1,1.0
cfa an unpaired instance,1,1,1.0
an unpaired instance 𝒙,1,1,1.0
unpaired instance 𝒙 grey,4,2,2.0
instance 𝒙 grey circle,4,2,2.0
𝒙 grey circle finds,2,2,1.0
grey circle finds a,2,2,1.0
circle finds a nearest,2,2,1.0
finds a nearest neighbor,2,2,1.0
a nearest neighbor 𝒙,2,2,1.0
nearest neighbor 𝒙 blue,2,2,1.0
neighbor 𝒙 blue circle,2,2,1.0
𝒙 blue circle taking,2,2,1.0
blue circle taking part,2,2,1.0
circle taking part in,2,2,1.0
taking part in a,4,2,2.0
part in a good,2,2,1.0
in a good native,2,2,1.0
a good native in,2,2,1.0
good native in the,2,2,1.0
native in the dataset,3,2,1.5
in the dataset 𝒄𝒇,2,2,1.0
the dataset 𝒄𝒇 𝒙,4,2,2.0
dataset 𝒄𝒇 𝒙 𝒑,4,2,2.0
𝒄𝒇 𝒙 𝒑 pairing,2,2,1.0
𝒙 𝒑 pairing of,2,2,1.0
𝒑 pairing of blue,2,2,1.0
pairing of blue circle,2,2,1.0
of blue circle and,2,2,1.0
blue circle and yellow,2,2,1.0
circle and yellow box,2,2,1.0
and yellow box and,2,2,1.0
yellow box and then,2,2,1.0
box and then uses,2,2,1.0
and then uses the,2,2,1.0
then uses the of,2,2,1.0
uses the of the,2,2,1.0
the of the 𝒑,2,2,1.0
of the 𝒑 box,1,1,1.0
the 𝒑 box to,1,1,1.0
𝒑 box to generate,1,1,1.0
box to generate a,2,2,1.0
to generate a new,2,2,1.0
a new synthetic 𝒑,1,1,1.0
new synthetic 𝒑 green,1,1,1.0
synthetic 𝒑 green box,1,1,1.0
𝒑 green box combining,2,2,1.0
green box combining them,2,2,1.0
box combining them with,2,2,1.0
combining them with the,2,2,1.0
them with the of,2,2,1.0
with the of the,2,2,1.0
the of the original,2,2,1.0
of the original unpaired,2,2,1.0
the original unpaired instance,4,2,2.0
original unpaired instance 𝒙,2,2,1.0
𝒙 grey circle the,2,2,1.0
grey circle the generated,2,2,1.0
circle the generated synthetic,2,2,1.0
the generated synthetic instance,2,2,1.0
generated synthetic instance 𝒑,2,2,1.0
synthetic instance 𝒑 green,2,2,1.0
instance 𝒑 green box,2,2,1.0
𝒑 green box is,2,2,1.0
green box is then,2,2,1.0
box is then added,2,2,1.0
to the dataset to,2,2,1.0
the dataset to improve,2,2,1.0
dataset to improve future,2,2,1.0
to improve future prediction,2,2,1.0
improve future prediction temraz,2,2,1.0
future prediction temraz keane,2,2,1.0
prediction temraz keane counterfactual,2,2,1.0
counterfactual data augmentation i,2,2,1.0
data augmentation i good,2,2,1.0
augmentation i good native,2,2,1.0
i good native counterfactuals,2,2,1.0
good native counterfactuals 𝑐𝑓,2,2,1.0
native counterfactuals 𝑐𝑓 𝑥,4,2,2.0
counterfactuals 𝑐𝑓 𝑥 𝑝,4,2,2.0
𝑐𝑓 𝑥 𝑝 are,2,2,1.0
𝑥 𝑝 are initially,2,2,1.0
𝑝 are initially computed,2,2,1.0
are initially computed over,2,2,1.0
initially computed over the,2,2,1.0
computed over the whole,2,2,1.0
over the whole dataset,2,2,1.0
the whole dataset 𝑇,2,2,1.0
whole dataset 𝑇 identifying,2,2,1.0
dataset 𝑇 identifying combinations,2,2,1.0
𝑇 identifying combinations of,2,2,1.0
identifying combinations of instances,2,2,1.0
combinations of instances 𝑥,2,2,1.0
of instances 𝑥 from,2,2,1.0
instances 𝑥 from the,2,2,1.0
𝑥 from the majority,2,2,1.0
majority class and 𝑝,2,2,1.0
class and 𝑝 from,2,2,1.0
and 𝑝 from the,2,2,1.0
the minority class ii,2,2,1.0
minority class ii given,2,2,1.0
class ii given an,2,2,1.0
ii given an unpaired,2,2,1.0
given an unpaired instance,2,2,1.0
an unpaired instance 𝑥,2,2,1.0
unpaired instance 𝑥 its,2,2,1.0
instance 𝑥 its paired,2,2,1.0
𝑥 its paired instance,2,2,1.0
its paired instance 𝑥,2,2,1.0
paired instance 𝑥 is,2,2,1.0
instance 𝑥 is found,2,2,1.0
𝑥 is found and,2,2,1.0
is found and used,2,2,1.0
found and used to,2,2,1.0
and used to identify,2,2,1.0
used to identify a,2,2,1.0
to identify a close,2,2,1.0
identify a close existing,2,2,1.0
a close existing counterfactual,2,2,1.0
close existing counterfactual pair,2,2,1.0
existing counterfactual pair 𝑐𝑓,2,2,1.0
counterfactual pair 𝑐𝑓 𝑥,4,2,2.0
pair 𝑐𝑓 𝑥 𝑝,4,2,2.0
𝑐𝑓 𝑥 𝑝 and,2,2,1.0
𝑥 𝑝 and then,2,2,1.0
𝑝 and then iii,2,2,1.0
and then iii a,2,2,1.0
then iii a new,2,2,1.0
iii a new synthetic,2,2,1.0
a new synthetic counterfactual,3,2,1.5
new synthetic counterfactual instance,2,2,1.0
synthetic counterfactual instance 𝑝,2,2,1.0
counterfactual instance 𝑝 is,2,2,1.0
instance 𝑝 is produced,2,2,1.0
𝑝 is produced in,2,2,1.0
is produced in the,2,2,1.0
produced in the minority,2,2,1.0
minority class using features,2,2,1.0
class using features from,2,2,1.0
using features from the,2,2,1.0
features from the original,2,2,1.0
from the original unpaired,2,2,1.0
original unpaired instance 𝑥,2,2,1.0
unpaired instance 𝑥 and,2,2,1.0
instance 𝑥 and values,2,2,1.0
𝑥 and values from,2,2,1.0
and values from more,2,2,1.0
values from more formally,2,2,1.0
from more formally definitions,2,2,1.0
more formally definitions 𝑋,2,2,1.0
formally definitions 𝑋 is,2,2,1.0
definitions 𝑋 is a,2,2,1.0
𝑋 is a majority,2,2,1.0
is a majority class,2,2,1.0
a majority class 𝑐𝑙𝑎𝑠𝑠efg,2,2,1.0
majority class 𝑐𝑙𝑎𝑠𝑠efg 𝑃,2,2,1.0
class 𝑐𝑙𝑎𝑠𝑠efg 𝑃 is,2,2,1.0
𝑐𝑙𝑎𝑠𝑠efg 𝑃 is a,2,2,1.0
𝑃 is a minority,2,2,1.0
is a minority class,2,2,1.0
a minority class 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
minority class 𝑐𝑙𝑎𝑠𝑠e 𝑥,1,1,1.0
class 𝑐𝑙𝑎𝑠𝑠e 𝑥 𝑥,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑥 𝑥 where,1,1,1.0
𝑥 𝑥 where 𝑥,2,2,1.0
𝑥 where 𝑥 is,2,2,1.0
where 𝑥 is a,2,2,1.0
𝑥 is a paired,4,2,2.0
is a paired instance,4,2,2.0
a paired instance in,2,2,1.0
paired instance in 𝑐𝑙𝑎𝑠𝑠efg,2,2,1.0
instance in 𝑐𝑙𝑎𝑠𝑠efg 𝑥,2,2,1.0
in 𝑐𝑙𝑎𝑠𝑠efg 𝑥 𝑥j,1,1,1.0
𝑐𝑙𝑎𝑠𝑠efg 𝑥 𝑥j 𝑥k,1,1,1.0
𝑥 𝑥j 𝑥k 𝑥l,3,2,1.5
𝑥j 𝑥k 𝑥l 𝑥,4,2,2.0
𝑥k 𝑥l 𝑥 where,2,2,1.0
𝑥l 𝑥 where 𝑥,2,2,1.0
𝑥 where 𝑥 𝑥,1,1,1.0
where 𝑥 𝑥 𝑝,2,1,2.0
𝑥 𝑥 𝑝 𝑥,1,1,1.0
𝑥 𝑝 𝑥 is,2,2,1.0
𝑝 𝑥 is an,2,2,1.0
𝑥 is an unpaired,2,2,1.0
is an unpaired instance,2,2,1.0
an unpaired instance in,2,2,1.0
unpaired instance in 𝑐𝑙𝑎𝑠𝑠efg,2,2,1.0
instance in 𝑐𝑙𝑎𝑠𝑠efg where,2,2,1.0
in 𝑐𝑙𝑎𝑠𝑠efg where 𝑥,2,2,1.0
𝑐𝑙𝑎𝑠𝑠efg where 𝑥 𝑥,1,1,1.0
𝑥 𝑥 𝑝 𝑝,1,1,1.0
𝑥 𝑝 𝑝 𝑝,1,1,1.0
𝑝 𝑝 𝑝 where,1,1,1.0
𝑝 𝑝 where 𝑝,2,2,1.0
𝑝 where 𝑝 is,2,2,1.0
where 𝑝 is a,2,2,1.0
𝑝 is a counterfactual,4,2,2.0
is a counterfactual instance,4,2,2.0
a counterfactual instance in,2,2,1.0
counterfactual instance in 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
instance in 𝑐𝑙𝑎𝑠𝑠e 𝑝j,1,1,1.0
in 𝑐𝑙𝑎𝑠𝑠e 𝑝j 𝑝k,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑝j 𝑝k 𝑝l,1,1,1.0
𝑝j 𝑝k 𝑝l 𝑝,4,2,2.0
𝑝k 𝑝l 𝑝 where,2,2,1.0
𝑝l 𝑝 where 𝑥,1,1,1.0
𝑝 where 𝑥 𝑝,1,1,1.0
where 𝑥 𝑝 𝑝,1,1,1.0
𝑥 𝑝 𝑝 is,2,2,1.0
𝑝 𝑝 is a,2,2,1.0
𝑝 is a synthetic,2,2,1.0
is a synthetic counterfactual,2,2,1.0
a synthetic counterfactual instance,2,2,1.0
synthetic counterfactual instance generated,2,2,1.0
counterfactual instance generated to,2,2,1.0
instance generated to be,2,2,1.0
generated to be added,2,2,1.0
to be added to,2,2,1.0
be added to 𝑐𝑙𝑎𝑠𝑠e,2,2,1.0
added to 𝑐𝑙𝑎𝑠𝑠e is,2,2,1.0
to 𝑐𝑙𝑎𝑠𝑠e is 𝑐𝑓,2,2,1.0
𝑐𝑙𝑎𝑠𝑠e is 𝑐𝑓 𝑥,2,2,1.0
is 𝑐𝑓 𝑥 𝑝,2,2,1.0
𝑐𝑓 𝑥 𝑝 𝑡𝑎𝑟𝑔𝑒𝑡,2,2,1.0
𝑥 𝑝 𝑡𝑎𝑟𝑔𝑒𝑡 𝑥,2,2,1.0
𝑝 𝑡𝑎𝑟𝑔𝑒𝑡 𝑥 𝑝,1,1,1.0
𝑡𝑎𝑟𝑔𝑒𝑡 𝑥 𝑝 neighbors,1,1,1.0
𝑥 𝑝 neighbors assume,1,1,1.0
𝑝 neighbors assume that,2,2,1.0
neighbors assume that 𝑥,2,2,1.0
assume that 𝑥 is,2,2,1.0
that 𝑥 is a,2,2,1.0
a paired instance which,2,2,1.0
paired instance which belongs,2,2,1.0
instance which belongs to,4,2,2.0
which belongs to the,4,2,2.0
belongs to the majority,5,3,1.6666666666666667
the majority class 𝑋,2,2,1.0
majority class 𝑋 and,2,2,1.0
class 𝑋 and 𝑝,2,2,1.0
𝑋 and 𝑝 is,2,2,1.0
and 𝑝 is a,2,2,1.0
a counterfactual instance which,2,2,1.0
counterfactual instance which belongs,2,2,1.0
belongs to the minority,3,3,1.0
minority class 𝑃 𝑥j,1,1,1.0
class 𝑃 𝑥j 𝑥k,1,1,1.0
𝑃 𝑥j 𝑥k 𝑥l,1,1,1.0
𝑥k 𝑥l 𝑥 𝑝j,1,1,1.0
𝑥l 𝑥 𝑝j 𝑝k,1,1,1.0
𝑥 𝑝j 𝑝k 𝑝l,1,1,1.0
𝑝k 𝑝l 𝑝 the,2,2,1.0
𝑝l 𝑝 the procedure,2,2,1.0
𝑝 the procedure for,2,2,1.0
the procedure for cfa,2,2,1.0
procedure for cfa is,2,2,1.0
for cfa is as,2,2,1.0
cfa is as follows,2,2,1.0
is as follows temraz,2,2,1.0
as follows temraz keane,2,2,1.0
follows temraz keane counterfactual,2,2,1.0
counterfactual data augmentation step,4,2,2.0
data augmentation step compute,2,2,1.0
augmentation step compute the,2,2,1.0
step compute the for,2,2,1.0
compute the for the,2,2,1.0
the for the dataset,2,2,1.0
for the dataset 𝒄𝒇,2,2,1.0
𝒄𝒇 𝒙 𝒑 cfa,2,2,1.0
𝒙 𝒑 cfa first,2,2,1.0
𝒑 cfa first finds,2,2,1.0
cfa first finds all,2,2,1.0
first finds all possible,2,2,1.0
finds all possible good,2,2,1.0
all possible good native,2,2,1.0
possible good native counterfactual,2,2,1.0
good native counterfactual pairs,2,2,1.0
native counterfactual pairs 𝑐𝑓,2,2,1.0
counterfactual pairs 𝑐𝑓 𝑥,2,2,1.0
pairs 𝑐𝑓 𝑥 𝑝,2,2,1.0
𝑐𝑓 𝑥 𝑝 between,2,2,1.0
𝑥 𝑝 between instances,2,2,1.0
𝑝 between instances that,2,2,1.0
between instances that already,2,2,1.0
instances that already exist,2,2,1.0
that already exist in,2,2,1.0
already exist in a,2,2,1.0
exist in a 𝑇,1,1,1.0
in a 𝑇 these,1,1,1.0
a 𝑇 these native,1,1,1.0
𝑇 these native counterfactuals,2,2,1.0
these native counterfactuals pair,2,2,1.0
native counterfactuals pair an,2,2,1.0
counterfactuals pair an instance,2,2,1.0
pair an instance in,2,2,1.0
an instance in the,2,2,1.0
instance in the majority,2,2,1.0
in the majority space,2,2,1.0
the majority space called,2,2,1.0
majority space called the,2,2,1.0
space called the paired,2,2,1.0
called the paired instance,2,2,1.0
the paired instance and,2,2,1.0
paired instance and its,2,2,1.0
instance and its instance,2,2,1.0
and its instance in,2,2,1.0
its instance in the,2,2,1.0
instance in the minority,4,2,2.0
in the minority space,2,2,1.0
the minority space called,2,2,1.0
minority space called the,2,2,1.0
space called the counterfactual,2,2,1.0
called the counterfactual instance,2,2,1.0
the counterfactual instance in,2,2,1.0
counterfactual instance in other,2,2,1.0
instance in other words,2,2,1.0
in other words for,2,2,1.0
other words for every,2,2,1.0
words for every 𝑥,2,2,1.0
for every 𝑥 in,2,2,1.0
every 𝑥 in the,2,2,1.0
𝑥 in the majority,2,2,1.0
in the majority class,6,3,2.0
the majority class 𝑥,2,2,1.0
majority class 𝑥 we,2,2,1.0
class 𝑥 we find,2,2,1.0
𝑥 we find its,2,2,1.0
we find its counterfactual,2,2,1.0
find its counterfactual 𝑝,2,2,1.0
its counterfactual 𝑝 from,2,2,1.0
counterfactual 𝑝 from 𝑝,2,2,1.0
𝑝 from 𝑝 in,2,2,1.0
from 𝑝 in the,2,2,1.0
the minority class these,2,2,1.0
minority class these native,2,2,1.0
class these native counterfactuals,2,2,1.0
these native counterfactuals 𝑐𝑓,2,2,1.0
𝑐𝑓 𝑥 𝑝 pair,2,2,1.0
𝑥 𝑝 pair instances,2,2,1.0
𝑝 pair instances either,2,2,1.0
pair instances either side,2,2,1.0
instances either side of,4,2,2.0
either side of decision,2,2,1.0
side of decision boundary,2,2,1.0
of decision boundary they,2,2,1.0
decision boundary they are,2,2,1.0
boundary they are called,2,2,1.0
they are called native,2,2,1.0
are called native in,1,1,1.0
called native in one,1,1,1.0
native in one sense,1,1,1.0
in one sense they,2,2,1.0
one sense they already,2,2,1.0
sense they already exist,2,2,1.0
they already exist in,2,2,1.0
already exist in the,2,2,1.0
exist in the dataset,2,2,1.0
in the dataset each,2,2,1.0
the dataset each of,2,2,1.0
dataset each of these,2,2,1.0
each of these native,2,2,1.0
of these native pairs,2,2,1.0
these native pairs has,2,2,1.0
native pairs has a,2,2,1.0
pairs has a set,2,2,1.0
has a set of,2,2,1.0
a set of and,2,2,1.0
set of and a,2,2,1.0
of and a set,2,2,1.0
and a set of,2,2,1.0
a set of where,2,2,1.0
set of where the,2,2,1.0
of where the differences,2,2,1.0
where the differences determine,2,2,1.0
the differences determine the,2,2,1.0
differences determine the class,2,2,1.0
determine the class change,2,2,1.0
the class change over,2,2,1.0
class change over the,2,2,1.0
change over the decision,2,2,1.0
over the decision boundary,2,2,1.0
the decision boundary step,2,2,1.0
decision boundary step for,2,2,1.0
boundary step for each,2,2,1.0
step for each unpaired,2,2,1.0
for each unpaired instance,2,2,1.0
each unpaired instance 𝒙,2,2,1.0
unpaired instance 𝒙 from,2,2,1.0
instance 𝒙 from the,2,2,1.0
𝒙 from the majority,2,2,1.0
the majority class find,2,2,1.0
majority class find its,2,2,1.0
class find its paired,2,2,1.0
find its paired instance,2,2,1.0
its paired instance x,2,2,1.0
paired instance x taking,2,2,1.0
instance x taking part,2,2,1.0
x taking part in,2,2,1.0
part in a native,2,2,1.0
in a native counterfactual,4,2,2.0
a native counterfactual 𝒄𝒇,2,2,1.0
native counterfactual 𝒄𝒇 𝒙,2,2,1.0
counterfactual 𝒄𝒇 𝒙 𝒑,2,2,1.0
𝒄𝒇 𝒙 𝒑 for,2,2,1.0
𝒙 𝒑 for each,2,2,1.0
𝒑 for each instance,1,1,1.0
for each instance 𝑥,1,1,1.0
each instance 𝑥 cfa,1,1,1.0
instance 𝑥 cfa uses,2,2,1.0
𝑥 cfa uses a,2,2,1.0
cfa uses a to,2,2,1.0
uses a to find,2,2,1.0
a to find its,2,2,1.0
to find its nearest,2,2,1.0
find its nearest neighboring,2,2,1.0
its nearest neighboring 𝑥,2,2,1.0
nearest neighboring 𝑥 a,2,2,1.0
neighboring 𝑥 a paired,2,2,1.0
𝑥 a paired involved,1,1,1.0
a paired involved in,1,1,1.0
paired involved in a,1,1,1.0
involved in a native,2,2,1.0
a native counterfactual pair,2,2,1.0
native counterfactual pair 𝑐𝑓,2,2,1.0
𝑐𝑓 𝑥 𝑝 by,2,2,1.0
𝑥 𝑝 by definition,2,2,1.0
𝑝 by definition 𝑥,2,2,1.0
by definition 𝑥 belongs,2,2,1.0
definition 𝑥 belongs to,2,2,1.0
𝑥 belongs to the,2,2,1.0
majority class and does,2,2,1.0
class and does not,2,2,1.0
and does not occur,2,2,1.0
does not occur any,2,2,1.0
not occur any native,2,2,1.0
occur any native counterfactual,2,2,1.0
any native counterfactual pairs,2,2,1.0
native counterfactual pairs notably,2,2,1.0
counterfactual pairs notably this,1,1,1.0
pairs notably this means,1,1,1.0
notably this means that,1,1,1.0
this means that all,1,1,1.0
means that all the,2,2,1.0
that all the synthetic,2,2,1.0
all the synthetic datapoints,2,2,1.0
the synthetic datapoints generated,2,2,1.0
synthetic datapoints generated by,2,2,1.0
datapoints generated by cfa,2,2,1.0
generated by cfa come,2,2,1.0
by cfa come from,2,2,1.0
cfa come from these,2,2,1.0
come from these instances,2,2,1.0
from these instances in,2,2,1.0
these instances in the,2,2,1.0
instances in the majority,2,2,1.0
the majority class that,3,3,1.0
majority class that are,3,3,1.0
class that are not,2,2,1.0
that are not already,2,2,1.0
are not already to,2,2,1.0
not already to instances,2,2,1.0
already to instances in,2,2,1.0
to instances in the,2,2,1.0
the minority class euclidean,2,2,1.0
minority class euclidean distance,2,2,1.0
class euclidean distance is,2,2,1.0
euclidean distance is used,2,2,1.0
distance is used in,2,2,1.0
is used in finding,2,2,1.0
used in finding these,2,2,1.0
in finding these nearest,2,2,1.0
finding these nearest neighbors,2,2,1.0
these nearest neighbors euclidean,2,2,1.0
nearest neighbors euclidean distance,2,2,1.0
neighbors euclidean distance ed,2,2,1.0
euclidean distance ed 𝑝,1,1,1.0
distance ed 𝑝 ke,1,1,1.0
ed 𝑝 ke temraz,1,1,1.0
𝑝 ke temraz keane,1,1,1.0
ke temraz keane counterfactual,1,1,1.0
data augmentation step transfer,2,2,1.0
augmentation step transfer from,2,2,1.0
step transfer from 𝒑,2,2,1.0
transfer from 𝒑 to,2,2,1.0
from 𝒑 to 𝒑,2,2,1.0
𝒑 to 𝒑 and,2,2,1.0
to 𝒑 and from,2,2,1.0
𝒑 and from 𝒙,2,2,1.0
and from 𝒙 𝐭o,2,2,1.0
from 𝒙 𝐭o 𝒑,2,2,1.0
𝒙 𝐭o 𝒑 having,2,2,1.0
𝐭o 𝒑 having identified,2,2,1.0
𝒑 having identified a,2,2,1.0
having identified a native,1,1,1.0
identified a native counterfactual,1,1,1.0
a native counterfactual 𝑐𝑓,1,1,1.0
native counterfactual 𝑐𝑓 𝑥,2,2,1.0
counterfactual 𝑐𝑓 𝑥 𝑝,2,2,1.0
𝑐𝑓 𝑥 𝑝 for,2,2,1.0
𝑥 𝑝 for 𝑥,2,2,1.0
𝑝 for 𝑥 cfa,2,2,1.0
for 𝑥 cfa generates,2,2,1.0
𝑥 cfa generates a,2,2,1.0
cfa generates a synthetic,2,2,1.0
generates a synthetic instance,1,1,1.0
a synthetic instance in,1,1,1.0
synthetic instance in the,1,1,1.0
the minority class 𝑝,2,2,1.0
minority class 𝑝 using,2,2,1.0
class 𝑝 using from,2,2,1.0
𝑝 using from and,2,2,1.0
using from and 𝑝,2,2,1.0
from and 𝑝 such,2,2,1.0
and 𝑝 such that,2,2,1.0
𝑝 such that for,2,2,1.0
such that for each,2,2,1.0
that for each of,2,2,1.0
for each of the,9,3,3.0
each of the between,4,2,2.0
of the between 𝑥,4,2,2.0
the between 𝑥 and,4,2,2.0
between 𝑥 and 𝑝,4,2,2.0
𝑥 and 𝑝 take,4,2,2.0
and 𝑝 take the,4,2,2.0
𝑝 take the values,4,2,2.0
take the values from,4,2,2.0
the values from 𝑝,2,2,1.0
values from 𝑝 into,2,2,1.0
from 𝑝 into the,2,2,1.0
𝑝 into the synthetic,2,2,1.0
into the synthetic counterfactual,2,2,1.0
the synthetic counterfactual case,2,2,1.0
synthetic counterfactual case 𝑝,2,2,1.0
counterfactual case 𝑝 for,2,2,1.0
case 𝑝 for each,2,2,1.0
𝑝 for each of,2,2,1.0
the values from 𝑥,2,2,1.0
values from 𝑥 into,2,2,1.0
from 𝑥 into the,2,2,1.0
𝑥 into the new,2,2,1.0
into the new counterfactual,2,2,1.0
the new counterfactual case,2,2,1.0
new counterfactual case 𝑝,2,2,1.0
counterfactual case 𝑝 it,2,2,1.0
case 𝑝 it should,2,2,1.0
𝑝 it should be,2,2,1.0
be noted that tolerance,2,2,1.0
noted that tolerance is,2,2,1.0
that tolerance is one,2,2,1.0
tolerance is one parameter,2,2,1.0
is one parameter in,2,2,1.0
one parameter in cfa,2,2,1.0
parameter in cfa algorithm,2,2,1.0
in cfa algorithm which,2,2,1.0
cfa algorithm which is,2,2,1.0
algorithm which is used,2,2,1.0
which is used to,4,3,1.3333333333333333
is used to improve,2,2,1.0
used to improve the,2,2,1.0
to improve the availability,2,2,1.0
improve the availability of,2,2,1.0
the availability of good,2,2,1.0
availability of good native,2,2,1.0
of good native counterfactuals,2,2,1.0
good native counterfactuals in,2,2,1.0
native counterfactuals in the,4,2,2.0
counterfactuals in the dataset,4,2,2.0
in the dataset without,2,2,1.0
the dataset without tolerance,2,2,1.0
dataset without tolerance fewer,2,2,1.0
without tolerance fewer counterfactuals,2,2,1.0
tolerance fewer counterfactuals would,2,2,1.0
fewer counterfactuals would be,2,2,1.0
counterfactuals would be found,2,2,1.0
would be found and,4,2,2.0
be found and the,4,2,2.0
found and the generative,4,2,2.0
and the generative benefits,4,2,2.0
the generative benefits of,4,2,2.0
generative benefits of them,4,2,2.0
benefits of them would,4,2,2.0
of them would likely,4,2,2.0
them would likely diminish,4,2,2.0
would likely diminish in,2,2,1.0
likely diminish in finding,2,2,1.0
diminish in finding and,1,1,1.0
in finding and between,3,2,1.5
finding and between two,3,2,1.5
and between two instances,4,2,2.0
between two instances for,4,2,2.0
two instances for a,4,2,2.0
instances for a native,4,2,2.0
for a native counterfactual,4,2,2.0
a native counterfactual cfa,2,2,1.0
native counterfactual cfa computes,1,1,1.0
counterfactual cfa computes a,1,1,1.0
cfa computes a by,1,1,1.0
computes a by finding,1,1,1.0
a by finding the,1,1,1.0
by finding the mean,2,2,1.0
finding the mean 𝜇,2,2,1.0
the mean 𝜇 and,2,2,1.0
mean 𝜇 and standard,2,2,1.0
𝜇 and standard deviation,2,2,1.0
and standard deviation 𝜎,2,2,1.0
standard deviation 𝜎 for,2,2,1.0
deviation 𝜎 for each,2,2,1.0
𝜎 for each feature,2,2,1.0
for each feature then,2,2,1.0
each feature then it,2,2,1.0
feature then it allows,2,2,1.0
then it allows features,2,2,1.0
it allows features to,2,2,1.0
allows features to match,2,2,1.0
features to match if,4,2,2.0
to match if their,4,2,2.0
match if their values,4,2,2.0
if their values are,4,2,2.0
their values are within,4,2,2.0
values are within of,4,2,2.0
are within of the,4,2,2.0
within of the standard,4,2,2.0
of the standard deviation,4,2,2.0
the standard deviation from,4,2,2.0
standard deviation from the,4,2,2.0
deviation from the mean,3,2,1.5
from the mean all,3,2,1.5
the mean all the,3,2,1.5
mean all the values,3,2,1.5
all the values for,4,2,2.0
the values for that,4,2,2.0
values for that feature,3,2,1.5
for that feature two,2,2,1.0
that feature two subtle,2,2,1.0
feature two subtle differences,2,2,1.0
two subtle differences that,2,2,1.0
subtle differences that distinguish,2,2,1.0
differences that distinguish this,2,2,1.0
that distinguish this data,2,2,1.0
distinguish this data augmentation,2,2,1.0
this data augmentation version,2,2,1.0
data augmentation version of,2,2,1.0
augmentation version of the,2,2,1.0
version of the algorithm,1,1,1.0
of the algorithm from,1,1,1.0
the algorithm from its,1,1,1.0
algorithm from its xai,1,1,1.0
from its xai counterpart,2,2,1.0
its xai counterpart first,2,2,1.0
xai counterpart first although,2,2,1.0
counterpart first although both,2,2,1.0
first although both algorithms,2,2,1.0
although both algorithms adopt,2,2,1.0
both algorithms adopt the,2,2,1.0
algorithms adopt the same,2,2,1.0
adopt the same definition,2,2,1.0
the same definition of,2,2,1.0
same definition of a,2,2,1.0
definition of a good,2,2,1.0
of a good pairing,1,1,1.0
a good pairing the,1,1,1.0
good pairing the do,1,1,1.0
pairing the do so,2,2,1.0
the do so for,2,2,1.0
do so for different,2,2,1.0
so for different reasons,2,2,1.0
for different reasons on,2,2,1.0
different reasons on psychological,2,2,1.0
reasons on psychological grounds,2,2,1.0
on psychological grounds keane,2,2,1.0
psychological grounds keane and,2,2,1.0
grounds keane and smyth,1,1,1.0
keane and smyth defined,1,1,1.0
and smyth defined a,1,1,1.0
smyth defined a good,1,1,1.0
defined a good counterfactual,2,2,1.0
a good counterfactual to,2,2,1.0
good counterfactual to be,2,2,1.0
counterfactual to be one,2,2,1.0
to be one with,2,2,1.0
be one with no,2,2,1.0
one with no more,2,2,1.0
with no more than,2,2,1.0
no more than two,4,2,2.0
more than two from,2,2,1.0
than two from the,2,2,1.0
two from the xai,2,2,1.0
from the xai perspective,2,2,1.0
the xai perspective researchers,2,2,1.0
xai perspective researchers argue,2,2,1.0
perspective researchers argue that,2,2,1.0
researchers argue that sparse,2,2,1.0
argue that sparse counterfactuals,2,2,1.0
that sparse counterfactuals with,2,2,1.0
sparse counterfactuals with fewer,2,2,1.0
counterfactuals with fewer feature,2,2,1.0
with fewer feature differences,2,2,1.0
fewer feature differences are,2,2,1.0
feature differences are better,2,2,1.0
differences are better because,2,2,1.0
are better because people,2,2,1.0
better because people find,2,2,1.0
because people find them,2,2,1.0
people find them more,2,2,1.0
find them more understandable,2,2,1.0
them more understandable confirmed,2,2,1.0
more understandable confirmed by,2,2,1.0
understandable confirmed by user,2,2,1.0
confirmed by user studies,2,2,1.0
by user studies from,2,2,1.0
user studies from a,2,2,1.0
studies from a data,2,2,1.0
data augmentation perspective basing,2,2,1.0
augmentation perspective basing synthetic,2,2,1.0
perspective basing synthetic counterfactuals,2,2,1.0
basing synthetic counterfactuals on,2,2,1.0
synthetic counterfactuals on sparse,2,2,1.0
counterfactuals on sparse pairs,2,2,1.0
on sparse pairs also,2,2,1.0
sparse pairs also makes,2,2,1.0
pairs also makes sense,2,2,1.0
also makes sense because,2,2,1.0
makes sense because the,2,2,1.0
sense because the implicit,2,2,1.0
because the implicit causal,2,2,1.0
the implicit causal dependencies,2,2,1.0
implicit causal dependencies between,2,2,1.0
causal dependencies between matched,2,2,1.0
dependencies between matched and,2,2,1.0
between matched and difference,2,2,1.0
matched and difference features,2,2,1.0
and difference features are,2,2,1.0
difference features are more,2,2,1.0
features are more decision,1,1,1.0
are more decision boundary,1,1,1.0
more decision boundary temraz,1,1,1.0
decision boundary temraz keane,1,1,1.0
boundary temraz keane counterfactual,1,1,1.0
counterfactual data augmentation likely,2,2,1.0
data augmentation likely to,2,2,1.0
augmentation likely to be,2,2,1.0
likely to be preserved,2,2,1.0
to be preserved in,2,2,1.0
be preserved in hence,2,2,1.0
preserved in hence generated,2,2,1.0
in hence generated synthetic,2,2,1.0
hence generated synthetic using,1,1,1.0
generated synthetic using these,1,1,1.0
synthetic using these sparse,1,1,1.0
using these sparse pairs,1,1,1.0
these sparse pairs should,2,2,1.0
sparse pairs should be,2,2,1.0
pairs should be more,2,2,1.0
should be more likely,2,2,1.0
be more likely to,4,2,2.0
more likely to be,6,4,1.5
likely to be valid,2,2,1.0
to be valid and,2,2,1.0
be valid and second,2,2,1.0
valid and second there,2,2,1.0
and second there is,2,2,1.0
second there is a,2,2,1.0
there is a critical,2,2,1.0
is a critical between,1,1,1.0
a critical between the,1,1,1.0
critical between the xai,1,1,1.0
between the xai and,2,2,1.0
the xai and data,2,2,1.0
xai and data augmentation,2,2,1.0
and data augmentation contexts,2,2,1.0
data augmentation contexts with,2,2,1.0
augmentation contexts with respect,2,2,1.0
contexts with respect to,2,2,1.0
respect to the selection,2,2,1.0
to the selection of,2,2,1.0
the selection of test,2,2,1.0
selection of test instances,2,2,1.0
of test instances in,2,2,1.0
test instances in xai,2,2,1.0
instances in xai the,2,2,1.0
in xai the test,2,2,1.0
xai the test instance,2,2,1.0
the test instance is,4,2,2.0
test instance is typically,2,2,1.0
instance is typically a,2,2,1.0
is typically a novel,2,2,1.0
typically a novel problem,2,2,1.0
a novel problem for,2,2,1.0
novel problem for which,2,2,1.0
problem for which a,2,2,1.0
for which a classifier,2,2,1.0
which a classifier has,2,2,1.0
a classifier has made,2,2,1.0
classifier has made a,2,2,1.0
has made a prediction,2,2,1.0
made a prediction a,2,2,1.0
a prediction a prediction,2,2,1.0
prediction a prediction that,2,2,1.0
a prediction that needs,2,2,1.0
prediction that needs to,2,2,1.0
that needs to be,2,2,1.0
needs to be counterfactually,2,2,1.0
to be counterfactually explained,2,2,1.0
be counterfactually explained hence,2,2,1.0
counterfactually explained hence typically,2,2,1.0
explained hence typically the,2,2,1.0
hence typically the test,2,2,1.0
typically the test instance,2,2,1.0
test instance is not,2,2,1.0
instance is not already,2,2,1.0
is not already in,2,2,1.0
not already in the,2,2,1.0
already in the training,2,2,1.0
in the training data,6,2,3.0
the training data in,2,2,1.0
training data in data,2,2,1.0
data in data augmentation,2,2,1.0
in data augmentation the,4,2,2.0
data augmentation the test,2,2,1.0
augmentation the test instances,2,2,1.0
the test instances used,2,2,1.0
test instances used to,2,2,1.0
instances used to generate,4,2,2.0
used to generate synthetic,6,4,1.5
to generate synthetic have,2,2,1.0
generate synthetic have to,2,2,1.0
synthetic have to be,2,2,1.0
have to be in,2,2,1.0
to be in the,3,3,1.0
be in the training,2,2,1.0
in the training dataset,4,3,1.3333333333333333
the training dataset specifically,2,2,1.0
training dataset specifically they,2,2,1.0
dataset specifically they are,2,2,1.0
specifically they are all,2,2,1.0
they are all the,2,2,1.0
are all the majority,2,2,1.0
all the majority class,2,2,1.0
the majority class instances,2,2,1.0
majority class instances in,2,2,1.0
class instances in the,2,2,1.0
instances in the training,2,2,1.0
the training data that,2,2,1.0
training data that do,2,2,1.0
data that do not,2,2,1.0
that do not take,2,2,1.0
do not take part,2,2,1.0
not take part in,2,2,1.0
take part in native,2,2,1.0
part in native counterfactuals,1,1,1.0
in native counterfactuals this,1,1,1.0
native counterfactuals this is,1,1,1.0
counterfactuals this is why,1,1,1.0
this is why the,3,3,1.0
is why the test,2,2,1.0
why the test instances,2,2,1.0
the test instances are,4,2,2.0
test instances are called,2,2,1.0
instances are called unpaired,2,2,1.0
are called unpaired instances,2,2,1.0
called unpaired instances for,2,2,1.0
unpaired instances for data,2,2,1.0
instances for data augmentation,2,2,1.0
for data augmentation purposes,2,2,1.0
data augmentation purposes the,2,2,1.0
augmentation purposes the test,2,2,1.0
purposes the test instances,2,2,1.0
test instances are a,2,2,1.0
instances are a residual,2,2,1.0
are a residual set,2,2,1.0
a residual set of,2,2,1.0
residual set of majority,2,2,1.0
set of majority instances,2,2,1.0
of majority instances left,2,2,1.0
majority instances left after,2,2,1.0
instances left after the,2,2,1.0
left after the native,2,2,1.0
after the native have,2,2,1.0
the native have been,2,2,1.0
native have been identified,2,2,1.0
have been identified how,2,2,1.0
been identified how counterfactuals,2,2,1.0
identified how counterfactuals differ,2,2,1.0
how counterfactuals differ from,2,2,1.0
counterfactuals differ from smote,2,2,1.0
differ from smote variants,2,2,1.0
from smote variants it,2,2,1.0
smote variants it should,2,2,1.0
variants it should be,2,2,1.0
it should be apparent,2,2,1.0
should be apparent that,2,2,1.0
be apparent that this,2,2,1.0
apparent that this counterfactual,2,2,1.0
that this counterfactual method,2,2,1.0
this counterfactual method is,3,2,1.5
counterfactual method is quite,3,2,1.5
method is quite different,2,2,1.0
quite different from smote,2,2,1.0
different from smote and,2,2,1.0
from smote and its,2,2,1.0
smote and its variants,2,2,1.0
and its variants though,2,2,1.0
its variants though it,2,2,1.0
variants though it is,2,2,1.0
though it is consistent,2,2,1.0
it is consistent with,2,2,1.0
is consistent with many,2,2,1.0
consistent with many insights,2,2,1.0
with many insights from,2,2,1.0
many insights from the,2,2,1.0
insights from the literature,1,1,1.0
from the literature first,1,1,1.0
the literature first by,1,1,1.0
literature first by definition,2,2,1.0
first by definition the,2,2,1.0
by definition the counterfactual,2,2,1.0
definition the counterfactual method,2,2,1.0
the counterfactual method addresses,2,2,1.0
counterfactual method addresses regions,2,2,1.0
method addresses regions close,2,2,1.0
addresses regions close to,2,2,1.0
the decision boundary a,2,2,1.0
decision boundary a good,2,2,1.0
boundary a good counterfactual,2,2,1.0
a good counterfactual records,2,2,1.0
good counterfactual records the,2,2,1.0
counterfactual records the minimal,2,2,1.0
records the minimal that,2,2,1.0
the minimal that result,2,2,1.0
minimal that result in,2,2,1.0
that result in a,2,2,1.0
result in a class,2,2,1.0
in a class change,2,2,1.0
a class change as,2,2,1.0
class change as in,2,2,1.0
change as in and,2,2,1.0
as in and second,2,2,1.0
in and second this,2,2,1.0
and second this method,2,2,1.0
second this method relies,2,2,1.0
this method relies on,2,2,1.0
method relies on native,2,2,1.0
relies on native in,1,1,1.0
on native in the,1,1,1.0
in the dataset pairings,2,2,1.0
the dataset pairings between,2,2,1.0
dataset pairings between existing,2,2,1.0
pairings between existing majority,2,2,1.0
between existing majority and,2,2,1.0
existing majority and minority,2,2,1.0
majority and minority instances,5,2,2.5
and minority instances and,1,1,1.0
minority instances and as,1,1,1.0
instances and as such,1,1,1.0
and as such is,2,2,1.0
as such is exploiting,2,2,1.0
such is exploiting relationships,2,2,1.0
is exploiting relationships between,2,2,1.0
exploiting relationships between both,2,2,1.0
relationships between both classes,2,2,1.0
between both classes as,2,2,1.0
both classes as in,2,2,1.0
classes as in adasyn,2,2,1.0
as in adasyn third,2,2,1.0
in adasyn third we,2,2,1.0
adasyn third we are,2,2,1.0
third we are highly,2,2,1.0
we are highly selective,2,2,1.0
are highly selective in,2,2,1.0
highly selective in the,2,2,1.0
selective in the minority,2,2,1.0
in the minority instances,2,2,1.0
the minority instances used,2,2,1.0
minority instances used to,2,2,1.0
generate synthetic instances as,2,2,1.0
synthetic instances as in,2,2,1.0
instances as in the,2,2,1.0
as in the many,2,2,1.0
in the many smote,2,2,1.0
the many smote variants,2,2,1.0
many smote variants that,2,2,1.0
smote variants that is,2,2,1.0
variants that is we,2,2,1.0
that is we only,2,2,1.0
is we only work,2,2,1.0
we only work of,2,2,1.0
only work of those,2,2,1.0
work of those involved,1,1,1.0
of those involved in,1,1,1.0
those involved in known,1,1,1.0
involved in known counterfactuals,1,1,1.0
in known counterfactuals with,2,2,1.0
known counterfactuals with two,2,2,1.0
counterfactuals with two feature,2,2,1.0
with two feature differences,2,2,1.0
two feature differences however,2,2,1.0
feature differences however this,2,2,1.0
differences however this counterfactual,2,2,1.0
however this counterfactual method,1,1,1.0
method is quite keane,1,1,1.0
is quite keane counterfactual,2,2,1.0
quite keane counterfactual data,2,2,1.0
counterfactual data augmentation ferent,2,2,1.0
data augmentation ferent in,2,2,1.0
augmentation ferent in many,2,2,1.0
ferent in many other,2,2,1.0
in many other significant,2,2,1.0
many other significant respects,2,2,1.0
other significant respects first,2,2,1.0
significant respects first it,2,2,1.0
respects first it does,2,2,1.0
first it does not,2,2,1.0
it does not use,2,2,1.0
does not use interpolation,2,2,1.0
not use interpolation between,1,1,1.0
use interpolation between instances,1,1,1.0
interpolation between instances but,1,1,1.0
between instances but rather,1,1,1.0
instances but rather uses,2,2,1.0
but rather uses the,2,2,1.0
rather uses the as,2,2,1.0
uses the as a,2,2,1.0
the as a template,2,2,1.0
as a template for,2,2,1.0
a template for generating,2,2,1.0
template for generating new,2,2,1.0
for generating new minority,2,2,1.0
generating new minority instances,2,2,1.0
new minority instances ii,2,2,1.0
minority instances ii it,2,2,1.0
instances ii it does,2,2,1.0
ii it does not,2,2,1.0
it does not rely,2,2,1.0
does not rely on,4,2,2.0
not rely on the,2,2,1.0
rely on the topology,2,2,1.0
on the topology of,2,2,1.0
the majority class as,3,3,1.0
majority class as in,2,2,1.0
class as in swim,2,2,1.0
as in swim but,2,2,1.0
in swim but acts,2,2,1.0
swim but acts in,2,2,1.0
but acts in a,2,2,1.0
acts in a very,2,2,1.0
in a very local,2,2,1.0
a very local way,2,2,1.0
very local way using,2,2,1.0
local way using the,2,2,1.0
way using the counterfactual,2,2,1.0
using the counterfactual relation,2,2,1.0
the counterfactual relation between,2,2,1.0
counterfactual relation between a,2,2,1.0
relation between a single,2,2,1.0
between a single majority,2,2,1.0
a single majority instance,2,2,1.0
single majority instance and,2,2,1.0
majority instance and a,2,2,1.0
instance and a minority,2,2,1.0
and a minority one,2,2,1.0
a minority one iii,2,2,1.0
minority one iii does,2,2,1.0
one iii does not,2,2,1.0
iii does not rely,2,2,1.0
not rely on any,2,2,1.0
rely on any clustering,2,2,1.0
on any clustering analysis,2,2,1.0
any clustering analysis of,2,2,1.0
clustering analysis of the,2,2,1.0
analysis of the majority,2,2,1.0
of the majority and,3,3,1.0
and minority classes as,2,2,1.0
minority classes as such,2,2,1.0
classes as such it,2,2,1.0
as such it represents,2,2,1.0
such it represents quite,2,2,1.0
it represents quite a,2,2,1.0
represents quite a novel,2,2,1.0
quite a novel departure,2,2,1.0
a novel departure relative,2,2,1.0
novel departure relative to,2,2,1.0
departure relative to existing,2,2,1.0
relative to existing smote,2,2,1.0
to existing smote variants,2,2,1.0
existing smote variants competitive,2,2,1.0
smote variants competitive tests,2,2,1.0
variants competitive tests of,2,2,1.0
competitive tests of data,2,2,1.0
tests of data augmentation,2,2,1.0
of data augmentation methods,2,2,1.0
data augmentation methods in,2,2,1.0
augmentation methods in the,2,2,1.0
methods in the current,2,2,1.0
in the current study,2,2,1.0
the current study we,2,2,1.0
current study we competitively,2,2,1.0
study we competitively test,2,2,1.0
we competitively test the,2,2,1.0
competitively test the counterfactual,2,2,1.0
test the counterfactual method,2,2,1.0
the counterfactual method cfa,4,2,2.0
counterfactual method cfa against,2,2,1.0
method cfa against the,2,2,1.0
cfa against the benchmark,2,2,1.0
against the benchmark techniques,2,2,1.0
the benchmark techniques in,2,2,1.0
benchmark techniques in the,2,2,1.0
techniques in the literature,2,2,1.0
in the literature using,2,2,1.0
the literature using six,2,2,1.0
literature using six oversampling,1,1,1.0
using six oversampling methods,1,1,1.0
six oversampling methods smote,1,1,1.0
oversampling methods smote adasyn,1,1,1.0
methods smote adasyn these,1,1,1.0
smote adasyn these specific,1,1,1.0
adasyn these specific methods,1,1,1.0
these specific methods were,2,2,1.0
specific methods were chosen,2,2,1.0
methods were chosen based,2,2,1.0
were chosen based of,2,2,1.0
chosen based of their,2,2,1.0
based of their conceptual,2,2,1.0
of their conceptual closeness,2,2,1.0
their conceptual closeness to,2,2,1.0
conceptual closeness to the,2,2,1.0
closeness to the cfa,2,2,1.0
to the cfa method,2,2,1.0
the cfa method their,2,2,1.0
cfa method their popularity,2,2,1.0
method their popularity amongst,2,2,1.0
their popularity amongst smote,2,2,1.0
popularity amongst smote variants,2,2,1.0
amongst smote variants and,2,2,1.0
smote variants and their,2,2,1.0
variants and their as,2,2,1.0
and their as the,1,1,1.0
their as the six,1,1,1.0
as the six techniques,1,1,1.0
the six techniques were,2,2,1.0
six techniques were tested,2,2,1.0
techniques were tested on,2,2,1.0
were tested on a,2,2,1.0
tested on a representative,2,2,1.0
on a representative selection,1,1,1.0
a representative selection of,1,1,1.0
representative selection of keel,1,1,1.0
selection of keel datasets,1,1,1.0
of keel datasets from,2,2,1.0
keel datasets from which,2,2,1.0
datasets from which were,2,2,1.0
from which were produced,2,2,1.0
which were produced with,2,2,1.0
were produced with four,2,2,1.0
produced with four different,2,2,1.0
with four different ml,2,2,1.0
four different ml classifiers,2,2,1.0
different ml classifiers including,2,2,1.0
ml classifiers including random,2,2,1.0
classifiers including random forest,2,2,1.0
including random forest rf,2,2,1.0
random forest rf neighbor,2,2,1.0
forest rf neighbor logistic,2,2,1.0
rf neighbor logistic regression,2,2,1.0
neighbor logistic regression lr,2,2,1.0
logistic regression lr and,2,2,1.0
regression lr and multilayer,2,2,1.0
lr and multilayer perceptron,2,2,1.0
and multilayer perceptron mlp,2,2,1.0
multilayer perceptron mlp models,2,2,1.0
perceptron mlp models several,2,2,1.0
mlp models several alternative,2,2,1.0
models several alternative ml,2,2,1.0
several alternative ml models,2,2,1.0
alternative ml models were,2,2,1.0
ml models were used,2,2,1.0
models were used because,2,2,1.0
were used because models,1,1,1.0
used because models find,1,1,1.0
because models find different,1,1,1.0
models find different decision,2,2,1.0
find different decision boundaries,2,2,1.0
different decision boundaries for,2,2,1.0
decision boundaries for a,2,2,1.0
boundaries for a given,2,2,1.0
for a given dataset,3,3,1.0
a given dataset differences,2,2,1.0
given dataset differences that,2,2,1.0
dataset differences that could,2,2,1.0
differences that could impact,2,2,1.0
that could impact the,2,2,1.0
could impact the success,2,2,1.0
impact the success of,2,2,1.0
the success of the,4,2,2.0
success of the counterfactual,2,2,1.0
of the counterfactual method,2,2,1.0
the counterfactual method as,2,2,1.0
counterfactual method as it,2,2,1.0
method as it relies,2,2,1.0
as it relies heavily,2,2,1.0
it relies heavily on,2,2,1.0
relies heavily on a,2,2,1.0
heavily on a model,2,2,1.0
on a model s,2,2,1.0
a model s decision,2,2,1.0
model s decision boundary,2,2,1.0
s decision boundary the,2,2,1.0
decision boundary the for,2,2,1.0
boundary the for a,2,2,1.0
the for a given,2,2,1.0
for a given classifier,4,2,2.0
a given classifier recorded,2,2,1.0
given classifier recorded the,2,2,1.0
classifier recorded the performance,2,2,1.0
recorded the performance of,2,2,1.0
the performance of the,12,6,2.0
performance of the model,2,2,1.0
of the model on,2,2,1.0
the model on a,2,2,1.0
model on a given,2,2,1.0
on a given dataset,2,2,1.0
a given dataset without,2,2,1.0
given dataset without any,2,2,1.0
dataset without any data,2,2,1.0
without any data augmentation,2,2,1.0
any data augmentation applied,2,2,1.0
data augmentation applied several,2,2,1.0
augmentation applied several standard,2,2,1.0
applied several standard measures,2,2,1.0
several standard measures were,2,2,1.0
standard measures were used,2,2,1.0
measures were used to,2,2,1.0
were used to assess,2,2,1.0
used to assess the,3,3,1.0
to assess the of,1,1,1.0
assess the of the,1,1,1.0
the of the four,1,1,1.0
of the four methods,2,2,1.0
the four methods namely,2,2,1.0
four methods namely precision,2,2,1.0
methods namely precision recall,2,2,1.0
namely precision recall and,2,2,1.0
precision recall and plots,2,2,1.0
recall and plots of,2,2,1.0
and plots of roc,2,2,1.0
plots of roc curves,2,2,1.0
of roc curves temraz,2,2,1.0
roc curves temraz keane,2,2,1.0
curves temraz keane counterfactual,2,2,1.0
counterfactual data augmentation method,4,2,2.0
data augmentation method datasets,2,2,1.0
augmentation method datasets setup,2,2,1.0
method datasets setup table,2,2,1.0
datasets setup table shows,2,2,1.0
setup table shows the,2,2,1.0
table shows the main,2,2,1.0
shows the main characteristics,2,2,1.0
the main characteristics of,2,2,1.0
main characteristics of the,2,2,1.0
characteristics of the datasets,3,3,1.0
of the datasets drawn,2,2,1.0
the datasets drawn from,2,2,1.0
datasets drawn from both,2,2,1.0
drawn from both uci,2,2,1.0
from both uci and,2,2,1.0
both uci and keel,2,2,1.0
uci and keel repositories,2,2,1.0
and keel repositories as,2,2,1.0
keel repositories as the,2,2,1.0
repositories as the focus,2,2,1.0
as the focus is,2,2,1.0
the focus is on,2,2,1.0
focus is on binary,2,2,1.0
is on binary classification,2,2,1.0
on binary classification problems,2,2,1.0
binary classification problems and,2,2,1.0
classification problems and some,2,2,1.0
problems and some of,2,2,1.0
and some of these,2,2,1.0
some of these datasets,1,1,1.0
of these datasets are,1,1,1.0
these datasets are they,1,1,1.0
datasets are they were,1,1,1.0
are they were converted,2,2,1.0
they were converted to,2,2,1.0
were converted to binary,4,2,2.0
converted to binary classes,2,2,1.0
to binary classes the,2,2,1.0
binary classes the ovo,2,2,1.0
classes the ovo or,2,2,1.0
the ovo or and,2,2,1.0
ovo or and ovr,1,1,1.0
or and ovr or,1,1,1.0
and ovr or methods,1,1,1.0
ovr or methods were,2,2,1.0
or methods were used,2,2,1.0
methods were used to,2,2,1.0
were used to do,2,2,1.0
used to do this,2,2,1.0
to do this conversion,2,2,1.0
do this conversion the,2,2,1.0
this conversion the ovo,2,2,1.0
conversion the ovo method,2,2,1.0
the ovo method splits,2,2,1.0
ovo method splits a,2,2,1.0
method splits a classification,2,2,1.0
splits a classification into,2,2,1.0
a classification into one,2,2,1.0
classification into one binary,2,2,1.0
into one binary classification,2,2,1.0
one binary classification dataset,2,2,1.0
binary classification dataset for,2,2,1.0
classification dataset for each,2,2,1.0
dataset for each pair,2,2,1.0
for each pair of,2,2,1.0
each pair of classes,2,2,1.0
pair of classes whereas,2,2,1.0
of classes whereas the,2,2,1.0
classes whereas the ovr,2,2,1.0
whereas the ovr approach,2,2,1.0
the ovr approach selects,2,2,1.0
ovr approach selects one,2,2,1.0
approach selects one of,2,2,1.0
selects one of the,2,2,1.0
one of the multiple,2,2,1.0
of the multiple classes,2,2,1.0
the multiple classes and,2,2,1.0
multiple classes and predicts,2,2,1.0
classes and predicts it,2,2,1.0
and predicts it against,1,1,1.0
predicts it against all,1,1,1.0
it against all other,1,1,1.0
against all other classes,1,1,1.0
all other classes so,2,2,1.0
other classes so that,2,2,1.0
classes so that one,2,2,1.0
so that one of,2,2,1.0
that one of the,2,2,1.0
one of the classes,2,2,1.0
of the classes is,2,2,1.0
the classes is treated,2,2,1.0
classes is treated as,2,2,1.0
is treated as the,4,2,2.0
treated as the positive,2,2,1.0
as the positive minority,2,2,1.0
the positive minority class,2,2,1.0
positive minority class and,2,2,1.0
minority class and all,6,2,3.0
class and all other,6,2,3.0
and all other classes,6,2,3.0
all other classes are,6,2,3.0
other classes are treated,6,2,3.0
classes are treated as,6,2,3.0
are treated as the,6,2,3.0
treated as the negative,6,2,3.0
as the negative class,7,3,2.3333333333333335
the negative class majority,2,2,1.0
negative class majority in,2,2,1.0
class majority in this,2,2,1.0
majority in this paper,2,2,1.0
in this paper the,2,2,1.0
this paper the datasets,2,2,1.0
paper the datasets were,2,2,1.0
the datasets were modified,2,2,1.0
datasets were modified using,1,1,1.0
were modified using both,1,1,1.0
modified using both methods,1,1,1.0
using both methods one,1,1,1.0
both methods one method,2,2,1.0
methods one method per,2,2,1.0
one method per dataset,2,2,1.0
method per dataset to,2,2,1.0
per dataset to vary,2,2,1.0
dataset to vary the,2,2,1.0
to vary the class,2,2,1.0
vary the class ratio,2,2,1.0
the class ratio of,2,2,1.0
class ratio of class,2,2,1.0
ratio of class imbalance,2,2,1.0
of class imbalance among,2,2,1.0
class imbalance among the,2,2,1.0
imbalance among the datasets,2,2,1.0
among the datasets see,2,2,1.0
the datasets see table,2,2,1.0
datasets see table the,2,2,1.0
see table the base,2,2,1.0
table the base datasets,2,2,1.0
the base datasets were,2,2,1.0
base datasets were abalone,2,2,1.0
datasets were abalone dataset,2,2,1.0
were abalone dataset a,2,2,1.0
abalone dataset a dataset,2,2,1.0
dataset a dataset analyzed,2,2,1.0
a dataset analyzed to,2,2,1.0
dataset analyzed to find,2,2,1.0
analyzed to find the,2,2,1.0
to find the age,2,2,1.0
find the age of,2,2,1.0
the age of abalone,2,2,1.0
age of abalone from,2,2,1.0
of abalone from physical,2,2,1.0
abalone from physical measurements,2,2,1.0
from physical measurements consisting,2,2,1.0
physical measurements consisting of,2,2,1.0
measurements consisting of classes,2,2,1.0
consisting of classes that,6,2,3.0
of classes that was,2,2,1.0
classes that was modified,2,2,1.0
that was modified using,2,2,1.0
was modified using and,2,2,1.0
modified using and glass,2,2,1.0
using and glass dataset,2,2,1.0
and glass dataset a,2,2,1.0
glass dataset a dataset,2,2,1.0
dataset a dataset used,10,2,5.0
a dataset used to,10,2,5.0
dataset used to classify,4,2,2.0
used to classify based,2,2,1.0
to classify based on,2,2,1.0
classify based on the,2,2,1.0
based on the chemical,2,2,1.0
on the chemical consisting,1,1,1.0
the chemical consisting of,1,1,1.0
chemical consisting of classes,1,1,1.0
of classes that modified,4,2,2.0
classes that modified using,4,2,2.0
that modified using to,2,2,1.0
modified using to treat,2,2,1.0
using to treat the,2,2,1.0
to treat the class,2,2,1.0
treat the class as,2,2,1.0
the class as the,2,2,1.0
class as the minority,2,2,1.0
as the minority class,4,2,2.0
the minority class and,7,3,2.3333333333333335
the negative class yeast,2,2,1.0
negative class yeast dataset,2,2,1.0
class yeast dataset a,2,2,1.0
yeast dataset a dataset,2,2,1.0
dataset used to predict,4,2,2.0
used to predict the,2,2,1.0
to predict the cellular,2,2,1.0
predict the cellular localization,2,2,1.0
the cellular localization sites,2,2,1.0
cellular localization sites of,2,2,1.0
localization sites of proteins,2,2,1.0
sites of proteins consisting,2,2,1.0
of proteins consisting of,2,2,1.0
proteins consisting of classes,2,2,1.0
that modified using and,2,2,1.0
modified using and pima,2,2,1.0
using and pima indians,2,2,1.0
and pima indians diabetes,2,2,1.0
pima indians diabetes dataset,2,2,1.0
indians diabetes dataset a,2,2,1.0
diabetes dataset a dataset,2,2,1.0
to predict whether or,2,2,1.0
predict whether or not,2,2,1.0
whether or not a,2,2,1.0
or not a patient,2,2,1.0
not a patient has,2,2,1.0
a patient has diabetes,2,2,1.0
patient has diabetes based,2,2,1.0
has diabetes based on,2,2,1.0
diabetes based on certain,2,2,1.0
based on certain diagnostic,2,2,1.0
on certain diagnostic measurements,2,2,1.0
certain diagnostic measurements included,2,2,1.0
diagnostic measurements included in,2,2,1.0
measurements included in the,2,2,1.0
included in the dataset,2,2,1.0
in the dataset phoneme,2,2,1.0
the dataset phoneme dataset,2,2,1.0
dataset phoneme dataset a,2,2,1.0
phoneme dataset a dataset,2,2,1.0
dataset used to distinguish,2,2,1.0
used to distinguish between,2,2,1.0
to distinguish between nasal,2,2,1.0
distinguish between nasal and,2,2,1.0
between nasal and oral,2,2,1.0
nasal and oral sounds,2,2,1.0
and oral sounds temraz,2,2,1.0
oral sounds temraz keane,2,2,1.0
sounds temraz keane counterfactual,2,2,1.0
counterfactual data augmentation vehicle,2,2,1.0
data augmentation vehicle dataset,2,2,1.0
augmentation vehicle dataset the,2,2,1.0
vehicle dataset the vehicle,2,2,1.0
dataset the vehicle data,2,2,1.0
the vehicle data set,2,2,1.0
vehicle data set is,2,2,1.0
data set is a,2,2,1.0
set is a dataset,2,2,1.0
is a dataset with,4,2,2.0
a dataset with classes,5,2,2.5
dataset with classes the,3,2,1.5
with classes the problem,3,2,1.5
classes the problem is,3,2,1.5
the problem is to,7,3,2.3333333333333335
problem is to classify,6,2,3.0
is to classify a,2,2,1.0
to classify a given,2,2,1.0
classify a given silhouette,2,2,1.0
a given silhouette as,2,2,1.0
given silhouette as one,2,2,1.0
silhouette as one of,2,2,1.0
as one of four,2,2,1.0
one of four types,2,2,1.0
of four types of,2,2,1.0
four types of vehicle,2,2,1.0
types of vehicle using,2,2,1.0
of vehicle using a,2,2,1.0
vehicle using a set,2,2,1.0
using a set of,2,2,1.0
a set of features,2,2,1.0
set of features from,1,1,1.0
of features from the,1,1,1.0
features from the silhouette,1,1,1.0
from the silhouette again,2,2,1.0
the silhouette again this,2,2,1.0
silhouette again this dataset,2,2,1.0
again this dataset was,2,2,1.0
this dataset was modified,2,2,1.0
dataset was modified using,2,2,1.0
was modified using so,2,2,1.0
modified using so that,2,2,1.0
using so that the,2,2,1.0
so that the class,2,2,1.0
that the class van,2,2,1.0
the class van is,2,2,1.0
class van is treated,2,2,1.0
van is treated as,2,2,1.0
treated as the minority,2,2,1.0
the negative class ecoli,2,2,1.0
negative class ecoli dataset,2,2,1.0
class ecoli dataset this,2,2,1.0
ecoli dataset this dataset,2,2,1.0
dataset this dataset is,2,2,1.0
this dataset is a,2,2,1.0
dataset is a dataset,2,2,1.0
is to classify ecoli,2,2,1.0
to classify ecoli proteins,2,2,1.0
classify ecoli proteins using,2,2,1.0
ecoli proteins using their,2,2,1.0
proteins using their amino,2,2,1.0
using their amino acid,2,2,1.0
their amino acid sequences,2,2,1.0
amino acid sequences in,2,2,1.0
acid sequences in their,2,2,1.0
sequences in their cell,2,2,1.0
in their cell localization,2,2,1.0
their cell localization sites,2,2,1.0
cell localization sites dataset,2,2,1.0
localization sites dataset a,2,2,1.0
sites dataset a dataset,2,2,1.0
used to classify all,2,2,1.0
to classify all the,2,2,1.0
classify all the blocks,2,2,1.0
all the blocks of,2,2,1.0
the blocks of the,2,2,1.0
blocks of the page,2,2,1.0
of the page layout,2,2,1.0
the page layout of,2,2,1.0
page layout of a,2,2,1.0
layout of a document,2,2,1.0
of a document that,2,2,1.0
a document that has,2,2,1.0
document that has been,2,2,1.0
that has been detected,2,2,1.0
has been detected by,2,2,1.0
been detected by a,2,2,1.0
detected by a segmentation,2,2,1.0
by a segmentation process,2,2,1.0
a segmentation process wine,2,2,1.0
segmentation process wine quality,2,2,1.0
process wine quality dataset,2,2,1.0
wine quality dataset red,2,2,1.0
quality dataset red and,2,2,1.0
dataset red and white,2,2,1.0
red and white two,2,2,1.0
and white two datasets,2,2,1.0
white two datasets related,2,2,1.0
two datasets related to,2,2,1.0
datasets related to red,2,2,1.0
related to red and,2,2,1.0
to red and white,2,2,1.0
red and white vinho,2,2,1.0
and white vinho verde,2,2,1.0
white vinho verde wine,2,2,1.0
vinho verde wine samples,2,2,1.0
verde wine samples the,2,2,1.0
wine samples the problem,2,2,1.0
samples the problem is,2,2,1.0
is to classify wine,2,2,1.0
to classify wine quality,2,2,1.0
classify wine quality based,2,2,1.0
wine quality based on,2,2,1.0
quality based on physicochemical,2,2,1.0
based on physicochemical tests,2,2,1.0
on physicochemical tests poker,2,2,1.0
physicochemical tests poker dataset,2,2,1.0
tests poker dataset a,2,2,1.0
poker dataset a dataset,2,2,1.0
dataset a dataset with,2,2,1.0
dataset with classes used,2,2,1.0
with classes used to,2,2,1.0
classes used to predict,2,2,1.0
used to predict poker,2,2,1.0
to predict poker hands,2,2,1.0
predict poker hands the,2,2,1.0
poker hands the overall,2,2,1.0
hands the overall performance,2,2,1.0
the overall performance of,2,2,1.0
overall performance of each,2,2,1.0
performance of each classifier,2,2,1.0
of each classifier was,2,2,1.0
each classifier was tested,2,2,1.0
classifier was tested using,2,2,1.0
was tested using cross,2,2,1.0
tested using cross validation,2,2,1.0
using cross validation with,2,2,1.0
cross validation with k,2,2,1.0
validation with k so,2,2,1.0
with k so each,2,2,1.0
k so each dataset,2,2,1.0
so each dataset is,2,2,1.0
each dataset is randomly,2,2,1.0
dataset is randomly partitioned,2,2,1.0
is randomly partitioned into,2,2,1.0
randomly partitioned into disjoint,2,2,1.0
partitioned into disjoint subsets,2,2,1.0
into disjoint subsets where,2,2,1.0
disjoint subsets where each,2,2,1.0
subsets where each subset,1,1,1.0
where each subset included,1,1,1.0
each subset included equal,1,1,1.0
subset included equal size,1,1,1.0
included equal size of,1,1,1.0
equal size of data,2,2,1.0
size of data then,2,2,1.0
of data then a,2,2,1.0
data then a single,2,2,1.0
then a single subset,2,2,1.0
a single subset was,2,2,1.0
single subset was retained,2,2,1.0
subset was retained as,2,2,1.0
was retained as a,2,2,1.0
retained as a test,2,2,1.0
as a test set,2,2,1.0
a test set with,2,2,1.0
test set with the,2,2,1.0
set with the remaining,2,2,1.0
with the remaining subsets,2,2,1.0
the remaining subsets being,2,2,1.0
remaining subsets being used,2,2,1.0
subsets being used as,2,2,1.0
being used as the,2,2,1.0
used as the training,2,2,1.0
as the training data,2,2,1.0
the training data different,2,2,1.0
training data different datasets,2,2,1.0
data different datasets have,2,2,1.0
different datasets have minority,2,2,1.0
datasets have minority classes,2,2,1.0
have minority classes see,2,2,1.0
minority classes see table,2,2,1.0
classes see table for,2,2,1.0
see table for details,2,2,1.0
table for details for,2,2,1.0
for details for each,2,2,1.0
details for each of,2,2,1.0
each of the smote,2,2,1.0
of the smote methods,2,2,1.0
the smote methods we,2,2,1.0
smote methods we split,2,2,1.0
methods we split each,2,2,1.0
we split each dataset,1,1,1.0
split each dataset into,1,1,1.0
each dataset into training,1,1,1.0
dataset into training and,1,1,1.0
into training and validation,2,2,1.0
training and validation folds,2,2,1.0
and validation folds then,2,2,1.0
validation folds then on,2,2,1.0
folds then on each,2,2,1.0
then on each fold,2,2,1.0
on each fold we,2,2,1.0
each fold we oversample,2,2,1.0
fold we oversample the,2,2,1.0
we oversample the minority,4,2,2.0
the minority class train,2,2,1.0
minority class train the,2,2,1.0
class train the classifier,2,2,1.0
train the classifier on,2,2,1.0
the classifier on the,4,2,2.0
classifier on the training,2,2,1.0
on the training folds,2,2,1.0
the training folds and,2,2,1.0
training folds and finally,2,2,1.0
folds and finally validate,2,2,1.0
and finally validate the,2,2,1.0
finally validate the classifier,2,2,1.0
validate the classifier on,2,2,1.0
classifier on the remaining,2,2,1.0
on the remaining fold,2,2,1.0
the remaining fold for,2,2,1.0
remaining fold for cfa,2,2,1.0
fold for cfa the,2,2,1.0
for cfa the native,2,2,1.0
cfa the native in,1,1,1.0
the native in the,1,1,1.0
native in the training,1,1,1.0
the training data were,2,2,1.0
training data were computed,2,2,1.0
data were computed and,2,2,1.0
were computed and then,2,2,1.0
computed and then all,2,2,1.0
and then all the,2,2,1.0
then all the remaining,2,2,1.0
all the remaining unpaired,2,2,1.0
the remaining unpaired were,2,2,1.0
remaining unpaired were run,2,2,1.0
unpaired were run through,2,2,1.0
were run through cfa,2,2,1.0
run through cfa to,2,2,1.0
through cfa to create,2,2,1.0
cfa to create the,2,2,1.0
to create the synthetic,2,2,1.0
create the synthetic counterfactual,2,2,1.0
the synthetic counterfactual instances,2,2,1.0
minority class this augmented,2,2,1.0
class this augmented dataset,2,2,1.0
this augmented dataset was,2,2,1.0
augmented dataset was then,2,2,1.0
dataset was then used,2,2,1.0
was then used for,2,2,1.0
then used for testing,2,2,1.0
used for testing these,2,2,1.0
for testing these generated,2,2,1.0
testing these generated datasets,1,1,1.0
these generated datasets from,1,1,1.0
generated datasets from cfa,1,1,1.0
datasets from cfa were,1,1,1.0
from cfa were compared,2,2,1.0
cfa were compared with,2,2,1.0
were compared with the,2,2,1.0
compared with the original,2,2,1.0
with the original dataset,2,2,1.0
the original dataset without,2,2,1.0
original dataset without data,2,2,1.0
dataset without data augmentation,2,2,1.0
without data augmentation and,2,2,1.0
data augmentation and the,2,2,1.0
augmentation and the datasets,1,1,1.0
and the datasets generated,1,1,1.0
the datasets generated by,1,1,1.0
datasets generated by temraz,1,1,1.0
generated by temraz keane,2,2,1.0
by temraz keane counterfactual,2,2,1.0
data augmentation smote adasyn,6,2,3.0
augmentation smote adasyn and,5,2,2.5
smote adasyn and for,2,2,1.0
adasyn and for our,2,2,1.0
and for our we,1,1,1.0
for our we oversample,1,1,1.0
our we oversample the,1,1,1.0
minority class using each,2,2,1.0
class using each of,2,2,1.0
using each of the,2,2,1.0
each of the data,4,2,2.0
of the data augmentation,3,2,1.5
the data augmentation methods,3,2,1.5
data augmentation methods until,1,1,1.0
augmentation methods until we,1,1,1.0
methods until we have,2,2,1.0
until we have the,2,2,1.0
we have the same,2,2,1.0
have the same number,2,2,1.0
the same number of,3,3,1.0
same number of instances,2,2,1.0
number of instances in,2,2,1.0
of instances in each,2,2,1.0
instances in each class,2,2,1.0
in each class as,2,2,1.0
each class as a,2,2,1.0
class as a result,2,2,1.0
as a result fully,2,2,1.0
a result fully balanced,2,2,1.0
result fully balanced datasets,2,2,1.0
fully balanced datasets were,2,2,1.0
balanced datasets were created,2,2,1.0
datasets were created for,2,2,1.0
were created for each,2,2,1.0
created for each classifier,2,2,1.0
for each classifier based,2,2,1.0
each classifier based on,2,2,1.0
classifier based on the,2,2,1.0
based on the original,2,2,1.0
on the original imbalanced,2,2,1.0
the original imbalanced dataset,2,2,1.0
original imbalanced dataset was,2,2,1.0
imbalanced dataset was also,1,1,1.0
dataset was also run,1,1,1.0
was also run as,1,1,1.0
also run as a,1,1,1.0
run as a baseline,2,2,1.0
as a baseline without,2,2,1.0
a baseline without using,2,2,1.0
baseline without using any,2,2,1.0
without using any data,2,2,1.0
using any data augmentation,2,2,1.0
any data augmentation method,2,2,1.0
data augmentation method finally,2,2,1.0
augmentation method finally to,2,2,1.0
method finally to determine,2,2,1.0
finally to determine the,2,2,1.0
to determine the optimal,2,2,1.0
determine the optimal values,2,2,1.0
the optimal values for,2,2,1.0
optimal values for all,2,2,1.0
values for all classifiers,2,2,1.0
for all classifiers we,2,2,1.0
all classifiers we applied,2,2,1.0
classifiers we applied hyperparameters,2,2,1.0
we applied hyperparameters tuning,2,2,1.0
applied hyperparameters tuning using,2,2,1.0
hyperparameters tuning using gridsearchcv,2,2,1.0
tuning using gridsearchcv function,2,2,1.0
using gridsearchcv function from,2,2,1.0
gridsearchcv function from gridsearchcv,2,2,1.0
function from gridsearchcv performs,2,2,1.0
from gridsearchcv performs across,2,2,1.0
gridsearchcv performs across all,2,2,1.0
performs across all hyperparameter,2,2,1.0
across all hyperparameter combinations,2,2,1.0
all hyperparameter combinations and,2,2,1.0
hyperparameter combinations and finds,2,2,1.0
combinations and finds the,2,2,1.0
and finds the best,2,2,1.0
finds the best score,2,2,1.0
the best score for,2,2,1.0
best score for a,2,2,1.0
score for a given,2,2,1.0
a given classifier to,2,2,1.0
given classifier to achieve,2,2,1.0
classifier to achieve this,2,2,1.0
to achieve this we,2,2,1.0
achieve this we defined,2,2,1.0
this we defined our,2,2,1.0
we defined our grid,2,2,1.0
defined our grid of,2,2,1.0
our grid of parameters,2,2,1.0
grid of parameters for,2,2,1.0
of parameters for each,2,2,1.0
parameters for each method,1,1,1.0
for each method rf,1,1,1.0
each method rf lr,1,1,1.0
method rf lr mlp,2,2,1.0
rf lr mlp and,2,2,1.0
lr mlp and oversampling,2,2,1.0
mlp and oversampling methods,2,2,1.0
and oversampling methods smote,2,2,1.0
oversampling methods smote its,4,2,2.0
methods smote its variants,4,2,2.0
smote its variants and,2,2,1.0
its variants and then,2,2,1.0
variants and then ran,2,2,1.0
and then ran the,2,2,1.0
then ran the grid,2,2,1.0
ran the grid search,2,2,1.0
the grid search see,2,2,1.0
grid search see table,2,2,1.0
search see table for,2,2,1.0
see table for a,2,2,1.0
table for a full,2,2,1.0
for a full description,4,2,2.0
a full description table,2,2,1.0
full description table a,2,2,1.0
description table a grid,2,2,1.0
table a grid of,2,2,1.0
a grid of hyperparameter,2,2,1.0
grid of hyperparameter values,2,2,1.0
of hyperparameter values for,2,2,1.0
hyperparameter values for each,2,2,1.0
values for each classifier,2,2,1.0
for each classifier and,2,2,1.0
each classifier and oversampling,2,2,1.0
classifier and oversampling methods,2,2,1.0
and oversampling methods learner,2,2,1.0
oversampling methods learner parameter,2,2,1.0
methods learner parameter variants,2,2,1.0
learner parameter variants random,2,2,1.0
parameter variants random forest,2,2,1.0
variants random forest rf,2,2,1.0
random forest rf 𝑚𝑎𝑥r,1,1,1.0
forest rf 𝑚𝑎𝑥r neighbors,1,1,1.0
rf 𝑚𝑎𝑥r neighbors 𝑛,1,1,1.0
𝑚𝑎𝑥r neighbors 𝑛 xuyz,1,1,1.0
neighbors 𝑛 xuyz logistic,2,2,1.0
𝑛 xuyz logistic regression,2,2,1.0
xuyz logistic regression lr,2,2,1.0
logistic regression lr 𝑚𝑎𝑥,2,2,1.0
regression lr 𝑚𝑎𝑥 t,2,2,1.0
lr 𝑚𝑎𝑥 t multilayer,1,1,1.0
𝑚𝑎𝑥 t multilayer perceptron,1,1,1.0
t multilayer perceptron mlp,1,1,1.0
multilayer perceptron mlp oversampling,1,1,1.0
perceptron mlp oversampling methods,1,1,1.0
mlp oversampling methods smote,1,1,1.0
smote its variants 𝑘,2,2,1.0
its variants 𝑘 f,2,2,1.0
variants 𝑘 f xuyz,2,2,1.0
𝑘 f xuyz temraz,2,2,1.0
f xuyz temraz keane,2,2,1.0
xuyz temraz keane counterfactual,2,2,1.0
counterfactual data augmentation table,8,2,4.0
data augmentation table datasets,2,2,1.0
augmentation table datasets dataset,2,2,1.0
table datasets dataset variants,2,2,1.0
datasets dataset variants used,2,2,1.0
dataset variants used in,2,2,1.0
variants used in the,2,2,1.0
used in the experiment,2,2,1.0
in the experiment imbalance,2,2,1.0
the experiment imbalance ratio,2,2,1.0
experiment imbalance ratio id,2,2,1.0
imbalance ratio id dataset,2,2,1.0
ratio id dataset features,2,2,1.0
id dataset features instances,2,2,1.0
dataset features instances minority,2,2,1.0
features instances minority majority,2,2,1.0
instances minority majority ir,2,2,1.0
minority majority ir pima,2,2,1.0
majority ir pima phoneme,2,2,1.0
ir pima phoneme vehicle,2,2,1.0
pima phoneme vehicle metrics,2,2,1.0
phoneme vehicle metrics measures,2,2,1.0
vehicle metrics measures in,2,2,1.0
metrics measures in binary,2,2,1.0
measures in binary classification,2,2,1.0
in binary classification problems,2,2,1.0
binary classification problems the,2,2,1.0
classification problems the labels,2,2,1.0
problems the labels can,2,2,1.0
the labels can be,2,2,1.0
labels can be either,2,2,1.0
can be either positive,2,2,1.0
be either positive or,2,2,1.0
either positive or negative,1,1,1.0
positive or negative so,1,1,1.0
or negative so the,1,1,1.0
negative so the prediction,1,1,1.0
so the prediction made,2,2,1.0
the prediction made by,2,2,1.0
prediction made by the,2,2,1.0
made by the classifier,2,2,1.0
by the classifier is,2,2,1.0
the classifier is represented,2,2,1.0
classifier is represented as,2,2,1.0
is represented as a,2,2,1.0
represented as a confusion,2,2,1.0
as a confusion matrix,2,2,1.0
a confusion matrix see,2,2,1.0
confusion matrix see table,2,2,1.0
matrix see table the,2,2,1.0
see table the confusion,2,2,1.0
table the confusion temraz,2,2,1.0
the confusion temraz keane,2,2,1.0
confusion temraz keane counterfactual,2,2,1.0
counterfactual data augmentation matrix,2,2,1.0
data augmentation matrix summarizes,2,2,1.0
augmentation matrix summarizes the,2,2,1.0
matrix summarizes the performance,2,2,1.0
summarizes the performance of,3,3,1.0
the performance of classifiers,4,3,1.3333333333333333
performance of classifiers for,2,2,1.0
of classifiers for the,2,2,1.0
classifiers for the four,2,2,1.0
for the four possible,2,2,1.0
the four possible outcomes,2,2,1.0
four possible outcomes of,2,2,1.0
possible outcomes of a,2,2,1.0
outcomes of a given,2,2,1.0
of a given a,1,1,1.0
a given a true,1,1,1.0
given a true positive,1,1,1.0
a true positive tp,2,2,1.0
true positive tp true,2,2,1.0
positive tp true negative,2,2,1.0
tp true negative tn,2,2,1.0
true negative tn false,2,2,1.0
negative tn false positive,2,2,1.0
tn false positive fp,2,2,1.0
false positive fp and,2,2,1.0
positive fp and false,2,2,1.0
fp and false negative,2,2,1.0
and false negative fn,2,2,1.0
false negative fn was,1,1,1.0
negative fn was not,1,1,1.0
fn was not used,1,1,1.0
was not used as,2,2,1.0
not used as a,2,2,1.0
used as a measure,2,2,1.0
as a measure because,2,2,1.0
a measure because as,2,2,1.0
measure because as discussed,2,2,1.0
because as discussed earlier,2,2,1.0
as discussed earlier it,2,2,1.0
discussed earlier it can,1,1,1.0
earlier it can be,1,1,1.0
it can be spuriously,1,1,1.0
can be spuriously high,1,1,1.0
be spuriously high for,2,2,1.0
spuriously high for datasets,1,1,1.0
high for datasets it,1,1,1.0
for datasets it should,1,1,1.0
datasets it should be,2,2,1.0
be noted that all,3,3,1.0
noted that all datasets,2,2,1.0
that all datasets used,2,2,1.0
all datasets used in,2,2,1.0
datasets used in our,3,3,1.0
used in our experiments,1,1,1.0
in our experiments were,1,1,1.0
our experiments were converted,1,1,1.0
experiments were converted to,1,1,1.0
converted to binary datasets,2,2,1.0
to binary datasets using,2,2,1.0
binary datasets using two,2,2,1.0
datasets using two of,2,2,1.0
using two of the,2,2,1.0
two of the most,2,2,1.0
of the most strategies,2,2,1.0
the most strategies and,2,2,1.0
most strategies and see,2,2,1.0
strategies and see section,2,2,1.0
and see section for,2,2,1.0
see section for a,2,2,1.0
section for a full,2,2,1.0
a full description hence,2,2,1.0
full description hence the,2,2,1.0
description hence the evaluation,2,2,1.0
hence the evaluation metrics,2,2,1.0
the evaluation metrics used,2,2,1.0
evaluation metrics used were,2,2,1.0
metrics used were as,2,2,1.0
used were as precision,2,2,1.0
were as precision recall,2,2,1.0
as precision recall auc,2,2,1.0
precision recall auc and,2,2,1.0
recall auc and defined,2,2,1.0
auc and defined as,2,2,1.0
and defined as follows,2,2,1.0
defined as follows 𝑇𝑃,1,1,1.0
as follows 𝑇𝑃 𝑇𝑃,1,1,1.0
follows 𝑇𝑃 𝑇𝑃 𝐴𝑈𝐶,1,1,1.0
𝑇𝑃 𝑇𝑃 𝐴𝑈𝐶 table,1,1,1.0
𝑇𝑃 𝐴𝑈𝐶 table confusion,1,1,1.0
𝐴𝑈𝐶 table confusion matrix,1,1,1.0
table confusion matrix for,3,3,1.0
confusion matrix for classifications,2,2,1.0
matrix for classifications receiver,2,2,1.0
for classifications receiver operating,2,2,1.0
classifications receiver operating characteristic,2,2,1.0
receiver operating characteristic curves,2,2,1.0
operating characteristic curves roc,2,2,1.0
characteristic curves roc curve,2,2,1.0
curves roc curve were,2,2,1.0
roc curve were reported,2,2,1.0
curve were reported as,2,2,1.0
were reported as they,2,2,1.0
reported as they are,4,2,2.0
as they are often,2,2,1.0
they are often used,2,2,1.0
are often used to,2,2,1.0
often used to evaluate,2,2,1.0
used to evaluate classification,2,2,1.0
to evaluate classification models,2,2,1.0
evaluate classification models for,2,2,1.0
classification models for imbalanced,2,2,1.0
models for imbalanced data,2,2,1.0
for imbalanced data sets,11,5,2.2
imbalanced data sets the,3,3,1.0
data sets the roc,2,2,1.0
sets the roc curve,2,2,1.0
the roc curve is,2,2,1.0
roc curve is a,3,3,1.0
curve is a graph,3,3,1.0
is a graph in,3,3,1.0
a graph in which,3,3,1.0
graph in which true,2,2,1.0
in which true positive,2,2,1.0
which true positive tp,2,2,1.0
true positive tp rate,2,2,1.0
positive tp rate is,2,2,1.0
tp rate is plotted,3,3,1.0
rate is plotted on,6,3,2.0
is plotted on the,6,3,2.0
plotted on the and,3,3,1.0
on the and false,2,2,1.0
the and false positive,2,2,1.0
and false positive fp,2,2,1.0
false positive fp rate,2,2,1.0
positive fp rate is,2,2,1.0
fp rate is plotted,3,3,1.0
plotted on the one,2,2,1.0
on the one advantage,2,2,1.0
the one advantage of,2,2,1.0
one advantage of roc,2,2,1.0
advantage of roc curves,2,2,1.0
of roc curves is,2,2,1.0
roc curves is that,2,2,1.0
curves is that they,2,2,1.0
is that they are,2,2,1.0
that they are not,2,2,1.0
they are not affected,2,2,1.0
are not affected by,2,2,1.0
not affected by the,2,2,1.0
affected by the class,2,2,1.0
by the class ratio,2,2,1.0
the class ratio between,2,2,1.0
class ratio between minority,2,2,1.0
ratio between minority and,2,2,1.0
between minority and majority,3,3,1.0
minority and majority instances,2,2,1.0
and majority instances in,2,2,1.0
instances in the datasets,2,2,1.0
in the datasets see,2,2,1.0
the datasets see figures,2,2,1.0
datasets see figures for,2,2,1.0
see figures for results,2,2,1.0
figures for results area,2,2,1.0
for results area under,2,2,1.0
results area under curve,2,2,1.0
area under curve auc,3,3,1.0
under curve auc predicted,2,2,1.0
curve auc predicted class,2,2,1.0
auc predicted class actual,2,2,1.0
predicted class actual class,2,2,1.0
class actual class p,2,2,1.0
actual class p n,2,2,1.0
class p n p,2,2,1.0
p n p true,2,2,1.0
n p true positive,2,2,1.0
p true positive tp,2,2,1.0
true positive tp false,2,2,1.0
positive tp false negative,2,2,1.0
tp false negative fn,2,2,1.0
false negative fn n,2,2,1.0
negative fn n false,2,2,1.0
fn n false positive,2,2,1.0
n false positive fp,2,2,1.0
false positive fp true,2,2,1.0
positive fp true negative,2,2,1.0
fp true negative tn,2,2,1.0
true negative tn temraz,2,2,1.0
negative tn temraz keane,2,2,1.0
tn temraz keane counterfactual,2,2,1.0
counterfactual data augmentation scores,2,2,1.0
data augmentation scores were,2,2,1.0
augmentation scores were also,2,2,1.0
scores were also reported,2,2,1.0
were also reported as,2,2,1.0
also reported as they,2,2,1.0
as they are used,2,2,1.0
they are used to,2,2,1.0
are used to measure,2,2,1.0
used to measure the,2,2,1.0
to measure the area,2,2,1.0
measure the area that,2,2,1.0
the area that lies,2,2,1.0
area that lies under,2,2,1.0
that lies under the,2,2,1.0
lies under the roc,2,2,1.0
under the roc curve,6,4,1.5
the roc curve results,2,2,1.0
roc curve results discussion,2,2,1.0
curve results discussion overall,2,2,1.0
results discussion overall the,2,2,1.0
discussion overall the counterfactual,2,2,1.0
overall the counterfactual method,2,2,1.0
counterfactual method cfa performs,2,2,1.0
method cfa performs better,2,2,1.0
cfa performs better than,2,2,1.0
performs better than all,2,2,1.0
better than all other,2,2,1.0
than all other methods,2,2,1.0
all other methods on,2,2,1.0
other methods on the,2,2,1.0
methods on the main,2,2,1.0
on the main metrics,2,2,1.0
the main metrics reported,2,2,1.0
main metrics reported for,2,2,1.0
metrics reported for most,2,2,1.0
reported for most of,2,2,1.0
for most of the,3,3,1.0
most of the classifiers,2,2,1.0
of the classifiers tested,1,1,1.0
the classifiers tested see,1,1,1.0
classifiers tested see recall,1,1,1.0
tested see recall the,1,1,1.0
see recall the datasets,1,1,1.0
recall the datasets were,2,2,1.0
the datasets were tested,2,2,1.0
datasets were tested on,2,2,1.0
were tested on four,2,2,1.0
tested on four classifiers,2,2,1.0
on four classifiers rf,2,2,1.0
four classifiers rf lr,4,2,2.0
classifiers rf lr mlp,4,2,2.0
rf lr mlp with,2,2,1.0
lr mlp with the,2,2,1.0
mlp with the baseline,2,2,1.0
with the baseline no,2,2,1.0
the baseline no data,2,2,1.0
baseline no data augmentation,2,2,1.0
no data augmentation smote,2,2,1.0
smote adasyn and cfa,5,2,2.5
adasyn and cfa tables,1,1,1.0
and cfa tables report,2,2,1.0
cfa tables report the,2,2,1.0
tables report the main,2,2,1.0
report the main metric,2,2,1.0
the main metric for,2,2,1.0
main metric for each,1,1,1.0
metric for each on,1,1,1.0
for each on the,1,1,1.0
each on the datasets,1,1,1.0
on the datasets for,2,2,1.0
the datasets for the,3,3,1.0
datasets for the rf,2,2,1.0
for the rf classifier,4,2,2.0
the rf classifier table,2,2,1.0
rf classifier table the,2,2,1.0
classifier table the results,4,2,2.0
table the results show,2,2,1.0
the results show that,3,3,1.0
results show that cfa,2,2,1.0
show that cfa does,2,2,1.0
that cfa does better,4,2,2.0
cfa does better than,5,2,2.5
does better than all,5,2,2.5
better than all the,5,2,2.5
than all the other,5,2,2.5
all the other methods,6,2,3.0
the other methods in,6,2,3.0
other methods in datasets,2,2,1.0
methods in datasets out,2,2,1.0
in datasets out of,2,2,1.0
datasets out of with,2,2,1.0
out of with being,1,1,1.0
of with being the,1,1,1.0
with being the next,2,1,2.0
being the next best,7,2,3.5
the next best in,2,2,1.0
next best in only,2,2,1.0
best in only datasets,2,2,1.0
in only datasets both,2,2,1.0
only datasets both adasyn,2,2,1.0
datasets both adasyn and,2,2,1.0
both adasyn and had,2,2,1.0
adasyn and had the,2,2,1.0
and had the highest,4,2,2.0
had the highest for,2,2,1.0
the highest for one,2,2,1.0
highest for one dataset,2,2,1.0
for one dataset for,2,2,1.0
one dataset for each,2,2,1.0
dataset for each for,2,2,1.0
for each for the,2,2,1.0
each for the classifier,2,2,1.0
for the classifier table,2,2,1.0
the classifier table it,2,2,1.0
classifier table it is,2,2,1.0
table it is observed,2,2,1.0
it is observed that,2,2,1.0
is observed that cfa,2,2,1.0
observed that cfa also,2,2,1.0
that cfa also achieved,2,2,1.0
cfa also achieved a,2,2,1.0
also achieved a greater,2,2,1.0
achieved a greater in,1,1,1.0
a greater in the,1,1,1.0
greater in the results,1,1,1.0
in the results also,2,2,1.0
the results also show,2,2,1.0
results also show that,2,2,1.0
also show that for,2,2,1.0
show that for cfa,2,2,1.0
that for cfa does,2,2,1.0
for cfa does better,2,2,1.0
other methods in out,4,2,2.0
methods in out of,4,2,2.0
in out of datasets,12,2,6.0
out of datasets with,4,2,2.0
of datasets with adasyn,2,2,1.0
datasets with adasyn being,2,2,1.0
with adasyn being the,2,2,1.0
adasyn being the next,1,1,1.0
the next best with,5,2,2.5
next best with datasets,5,2,2.5
best with datasets smote,2,2,1.0
with datasets smote and,2,2,1.0
datasets smote and had,2,2,1.0
smote and had the,2,2,1.0
had the highest in,2,1,2.0
the highest in dataset,1,1,1.0
highest in dataset for,1,1,1.0
in dataset for each,2,2,1.0
dataset for each method,2,2,1.0
for each method for,2,2,1.0
each method for the,2,2,1.0
method for the lr,2,2,1.0
for the lr classifier,4,2,2.0
the lr classifier table,2,2,1.0
lr classifier table the,2,2,1.0
table the results are,2,2,1.0
the results are quite,2,2,1.0
results are quite different,2,2,1.0
are quite different in,2,2,1.0
quite different in that,2,2,1.0
different in that they,2,2,1.0
in that they show,2,2,1.0
that they show that,2,2,1.0
they show that for,2,2,1.0
show that for metric,2,2,1.0
that for metric cfa,2,2,1.0
for metric cfa doing,2,2,1.0
metric cfa doing better,2,2,1.0
cfa doing better in,2,2,1.0
doing better in only,4,2,2.0
better in only out,2,2,1.0
in only out of,2,2,1.0
only out of datasets,2,2,1.0
of datasets with the,2,2,1.0
datasets with the being,1,1,1.0
with the being the,1,1,1.0
the being the next,1,1,1.0
best with datasets whereas,2,2,1.0
with datasets whereas baseline,2,2,1.0
datasets whereas baseline adasyn,2,2,1.0
whereas baseline adasyn and,2,2,1.0
baseline adasyn and doing,2,2,1.0
adasyn and doing better,2,2,1.0
and doing better in,2,2,1.0
doing better in datasets,2,2,1.0
better in datasets finally,2,2,1.0
in datasets finally for,2,2,1.0
datasets finally for the,2,2,1.0
finally for the mlp,2,2,1.0
for the mlp classifier,4,2,2.0
the mlp classifier table,2,2,1.0
mlp classifier table again,2,2,1.0
classifier table again the,2,2,1.0
table again the results,2,2,1.0
again the results showed,2,2,1.0
the results showed that,2,2,1.0
results showed that cfa,2,2,1.0
showed that cfa does,2,2,1.0
out of datasets whereas,2,2,1.0
of datasets whereas doing,2,2,1.0
datasets whereas doing better,2,2,1.0
whereas doing better in,2,2,1.0
better in only datasets,2,2,1.0
in only datasets with,2,2,1.0
only datasets with being,1,1,1.0
datasets with being the,1,1,1.0
best with datasets baseline,2,2,1.0
with datasets baseline and,2,2,1.0
datasets baseline and adasyn,2,2,1.0
baseline and adasyn had,2,2,1.0
and adasyn had the,2,2,1.0
adasyn had the highest,1,1,1.0
the highest in temraz,1,1,1.0
highest in temraz keane,1,1,1.0
in temraz keane counterfactual,2,2,1.0
data augmentation datasets for,2,2,1.0
augmentation datasets for each,2,2,1.0
datasets for each method,4,2,2.0
for each method notably,2,2,1.0
each method notably these,2,2,1.0
method notably these results,2,2,1.0
notably these results show,2,2,1.0
these results show for,2,2,1.0
results show for certain,2,2,1.0
show for certain datasets,2,2,1.0
for certain datasets and,2,2,1.0
certain datasets and classifiers,2,2,1.0
datasets and classifiers the,2,2,1.0
and classifiers the does,2,2,1.0
classifiers the does quite,2,2,1.0
the does quite well,2,2,1.0
does quite well however,2,2,1.0
quite well however when,2,2,1.0
well however when data,2,2,1.0
however when data augmentation,2,2,1.0
when data augmentation can,2,2,1.0
data augmentation can make,2,2,1.0
augmentation can make a,2,2,1.0
can make a contribution,2,2,1.0
make a contribution it,1,1,1.0
a contribution it seems,1,1,1.0
contribution it seems to,1,1,1.0
it seems to be,1,1,1.0
seems to be cfa,2,2,1.0
to be cfa that,2,2,1.0
be cfa that contributes,2,2,1.0
cfa that contributes the,2,2,1.0
that contributes the most,2,2,1.0
contributes the most to,2,2,1.0
the most to performance,2,2,1.0
most to performance improvements,2,2,1.0
to performance improvements table,2,2,1.0
performance improvements table auc,2,2,1.0
improvements table auc values,2,2,1.0
table auc values for,8,2,4.0
auc values for the,8,2,4.0
values for the rf,2,2,1.0
the rf classifier for,2,2,1.0
rf classifier for each,2,2,1.0
classifier for each data,8,2,4.0
for each data augmentation,8,2,4.0
each data augmentation method,8,2,4.0
data augmentation method dataset,8,2,4.0
augmentation method dataset baseline,8,2,4.0
method dataset baseline smote,8,2,4.0
dataset baseline smote adasyn,4,1,4.0
baseline smote adasyn smote,8,2,4.0
smote adasyn smote smote,10,2,5.0
adasyn smote smote smote,10,2,5.0
smote smote smote cfa,10,2,5.0
smote smote cfa total,8,2,4.0
smote cfa total temraz,8,2,4.0
cfa total temraz keane,8,2,4.0
total temraz keane counterfactual,10,2,5.0
data augmentation table auc,6,2,3.0
augmentation table auc values,6,2,3.0
values for the classifier,2,2,1.0
for the classifier for,2,2,1.0
the classifier for each,2,2,1.0
values for the lr,2,2,1.0
the lr classifier for,2,2,1.0
lr classifier for each,2,2,1.0
values for the mlp,2,2,1.0
the mlp classifier for,2,2,1.0
mlp classifier for each,2,2,1.0
counterfactual data augmentation notably,2,2,1.0
data augmentation notably if,2,2,1.0
augmentation notably if we,2,2,1.0
notably if we assess,2,2,1.0
if we assess overall,2,2,1.0
we assess overall performance,2,2,1.0
assess overall performance by,2,2,1.0
overall performance by noting,2,2,1.0
performance by noting occasions,2,2,1.0
by noting occasions for,2,2,1.0
noting occasions for a,2,2,1.0
occasions for a given,2,2,1.0
for a given method,4,2,2.0
a given method when,2,2,1.0
given method when the,2,2,1.0
method when the score,1,1,1.0
when the score is,1,1,1.0
the score is highest,1,1,1.0
score is highest we,2,2,1.0
is highest we see,2,2,1.0
highest we see that,2,2,1.0
we see that cfa,2,2,1.0
see that cfa scores,2,2,1.0
that cfa scores best,4,2,2.0
cfa scores best see,2,2,1.0
scores best see table,2,2,1.0
best see table in,1,1,1.0
see table in of,1,1,1.0
table in of cases,1,1,1.0
in of cases it,3,2,1.5
of cases it has,3,2,1.5
cases it has the,3,2,1.5
it has the highest,3,2,1.5
has the highest precision,2,2,1.0
the highest precision score,2,2,1.0
highest precision score as,2,2,1.0
precision score as opposed,2,2,1.0
score as opposed to,4,2,2.0
as opposed to for,4,2,2.0
opposed to for for,2,2,1.0
to for for for,1,1,1.0
for for for for,1,1,1.0
for for for adasyn,1,1,1.0
for for adasyn for,2,2,1.0
for adasyn for smote,2,2,1.0
adasyn for smote and,1,1,1.0
for smote and it,1,1,1.0
smote and it is,2,2,1.0
and it is also,2,2,1.0
it is also observed,2,2,1.0
is also observed that,2,2,1.0
also observed that cfa,2,2,1.0
observed that cfa scores,2,2,1.0
cfa scores best in,2,2,1.0
scores best in of,2,2,1.0
best in of cases,2,2,1.0
has the highest recall,1,1,1.0
the highest recall score,2,2,1.0
highest recall score as,2,2,1.0
recall score as opposed,2,2,1.0
opposed to for adasyn,2,2,1.0
to for adasyn for,2,2,1.0
for adasyn for both,2,2,1.0
adasyn for both smote,2,2,1.0
for both smote and,2,2,1.0
both smote and for,2,2,1.0
smote and for for,2,2,1.0
and for for for,2,2,1.0
for for for and,2,2,1.0
for for and for,2,2,1.0
for and for baseline,2,2,1.0
and for baseline table,2,2,1.0
for baseline table the,2,2,1.0
baseline table the number,2,2,1.0
table the number of,2,2,1.0
the number of datasets,6,4,1.5
number of datasets for,2,2,1.0
of datasets for each,2,2,1.0
for each method showing,2,2,1.0
each method showing the,2,2,1.0
method showing the highest,2,2,1.0
showing the highest precision,2,2,1.0
the highest precision and,2,2,1.0
highest precision and recall,2,2,1.0
precision and recall scores,2,2,1.0
and recall scores for,2,2,1.0
recall scores for a,2,2,1.0
scores for a given,2,2,1.0
a given method smote,2,2,1.0
given method smote adasyn,2,2,1.0
method smote adasyn and,2,2,1.0
adasyn and cfa on,4,2,2.0
and cfa on a,2,2,1.0
cfa on a selected,2,2,1.0
on a selected classifier,2,2,1.0
a selected classifier precision,2,2,1.0
selected classifier precision classifier,2,2,1.0
classifier precision classifier baseline,2,2,1.0
precision classifier baseline smote,2,2,1.0
classifier baseline smote adasyn,4,2,2.0
smote adasyn smote cfa,2,1,2.0
adasyn smote cfa rf,2,1,2.0
smote cfa rf lr,4,2,2.0
cfa rf lr mlp,4,2,2.0
rf lr mlp total,4,2,2.0
lr mlp total recall,2,2,1.0
mlp total recall classifier,2,2,1.0
total recall classifier baseline,2,2,1.0
recall classifier baseline smote,2,2,1.0
lr mlp total temraz,2,2,1.0
mlp total temraz keane,2,2,1.0
data augmentation a rf,2,2,1.0
augmentation a rf classifier,2,2,1.0
a rf classifier b,4,2,2.0
rf classifier b classifier,2,2,1.0
classifier b classifier c,2,2,1.0
b classifier c lr,2,2,1.0
classifier c lr classifier,2,2,1.0
c lr classifier d,2,2,1.0
lr classifier d mlp,2,2,1.0
classifier d mlp classifier,2,2,1.0
d mlp classifier figure,2,2,1.0
mlp classifier figure values,2,2,1.0
classifier figure values for,2,2,1.0
figure values for the,2,2,1.0
values for the different,2,2,1.0
for the different conditions,2,2,1.0
the different conditions across,2,2,1.0
different conditions across datasets,2,2,1.0
conditions across datasets for,2,2,1.0
across datasets for the,2,2,1.0
datasets for the four,2,2,1.0
for the four classifiers,2,2,1.0
the four classifiers a,2,2,1.0
four classifiers a rf,2,2,1.0
classifiers a rf classifier,2,2,1.0
rf classifier b c,2,2,1.0
classifier b c lr,2,2,1.0
b c lr and,2,2,1.0
c lr and d,2,2,1.0
lr and d mlp,2,2,1.0
and d mlp figure,2,2,1.0
d mlp figure shows,2,2,1.0
mlp figure shows the,2,2,1.0
figure shows the comparisons,2,2,1.0
shows the comparisons for,2,2,1.0
the comparisons for each,2,2,1.0
comparisons for each of,2,2,1.0
data augmentation methods smote,2,1,2.0
augmentation methods smote adasyn,2,1,2.0
methods smote adasyn and,1,1,1.0
and cfa on datasets,2,2,1.0
cfa on datasets using,2,2,1.0
on datasets using the,2,2,1.0
datasets using the four,2,2,1.0
using the four classifiers,8,2,4.0
the four classifiers rf,2,2,1.0
rf lr mlp in,2,2,1.0
lr mlp in general,2,2,1.0
mlp in general higher,2,2,1.0
in general higher values,2,2,1.0
general higher values indicate,2,2,1.0
higher values indicate better,2,2,1.0
values indicate better classifier,2,2,1.0
indicate better classifier performance,2,2,1.0
better classifier performance overall,2,2,1.0
classifier performance overall there,2,2,1.0
performance overall there is,2,2,1.0
overall there is no,2,2,1.0
there is no significant,2,2,1.0
is no significant difference,2,2,1.0
no significant difference in,2,2,1.0
significant difference in the,2,2,1.0
difference in the values,2,2,1.0
in the values between,2,2,1.0
the values between smote,2,2,1.0
values between smote variants,2,2,1.0
between smote variants keane,2,2,1.0
smote variants keane counterfactual,2,2,1.0
variants keane counterfactual data,2,2,1.0
smote adasyn and and,2,2,1.0
adasyn and and these,2,2,1.0
and and these achieved,1,1,1.0
and these achieved lower,1,1,1.0
these achieved lower values,1,1,1.0
achieved lower values than,2,2,1.0
lower values than cfa,2,2,1.0
values than cfa in,2,2,1.0
than cfa in most,2,2,1.0
cfa in most cases,2,2,1.0
in most cases perhaps,2,2,1.0
most cases perhaps the,2,2,1.0
cases perhaps the most,2,2,1.0
perhaps the most interesting,1,1,1.0
the most interesting result,1,1,1.0
most interesting result is,1,1,1.0
interesting result is that,1,1,1.0
result is that cfa,2,2,1.0
is that cfa achieved,2,2,1.0
that cfa achieved higher,2,2,1.0
cfa achieved higher values,2,2,1.0
achieved higher values in,2,2,1.0
higher values in out,2,2,1.0
values in out of,2,2,1.0
out of datasets when,4,2,2.0
of datasets when using,4,2,2.0
datasets when using the,2,2,1.0
when using the rf,2,2,1.0
using the rf classifier,2,2,1.0
the rf classifier and,2,2,1.0
rf classifier and in,2,2,1.0
classifier and in out,2,2,1.0
and in out of,2,2,1.0
datasets when using classifier,2,2,1.0
when using classifier it,2,2,1.0
using classifier it outperformed,2,2,1.0
classifier it outperformed the,2,2,1.0
it outperformed the baseline,2,2,1.0
outperformed the baseline with,2,2,1.0
the baseline with no,2,2,1.0
baseline with no data,2,2,1.0
with no data and,1,1,1.0
no data and smote,1,1,1.0
data and smote variants,1,1,1.0
and smote variants similarly,2,2,1.0
smote variants similarly when,2,2,1.0
variants similarly when applying,2,2,1.0
similarly when applying lr,2,2,1.0
when applying lr classifier,2,2,1.0
applying lr classifier cfa,2,2,1.0
lr classifier cfa achieved,2,2,1.0
classifier cfa achieved the,2,2,1.0
cfa achieved the best,2,2,1.0
achieved the best in,1,1,1.0
the best in out,1,1,1.0
best in out of,1,1,1.0
out of datasets although,2,2,1.0
of datasets although the,2,2,1.0
datasets although the improvement,2,2,1.0
although the improvement varied,2,2,1.0
the improvement varied with,2,2,1.0
improvement varied with different,2,2,1.0
varied with different data,2,2,1.0
with different data sets,1,1,1.0
different data sets finally,1,1,1.0
data sets finally in,1,1,1.0
sets finally in the,1,1,1.0
finally in the mlp,2,2,1.0
in the mlp classifier,2,2,1.0
the mlp classifier cfa,2,2,1.0
mlp classifier cfa still,2,2,1.0
classifier cfa still achieved,2,2,1.0
cfa still achieved highest,2,2,1.0
still achieved highest auc,2,2,1.0
achieved highest auc in,2,2,1.0
highest auc in out,2,2,1.0
auc in out of,2,2,1.0
out of datasets finally,2,2,1.0
of datasets finally the,2,2,1.0
datasets finally the roc,2,2,1.0
finally the roc curves,2,2,1.0
the roc curves which,2,2,1.0
roc curves which show,2,2,1.0
curves which show the,2,2,1.0
which show the between,2,2,1.0
show the between sensitivity,2,2,1.0
the between sensitivity and,2,2,1.0
between sensitivity and specificity,1,1,1.0
sensitivity and specificity are,1,1,1.0
and specificity are presented,1,1,1.0
specificity are presented in,1,1,1.0
are presented in figure,2,2,1.0
presented in figure and,2,2,1.0
in figure and figure,2,2,1.0
figure and figure shows,2,2,1.0
and figure shows selected,2,2,1.0
figure shows selected examples,2,2,1.0
shows selected examples of,2,2,1.0
selected examples of roc,2,2,1.0
examples of roc curves,4,2,2.0
of roc curves where,4,2,2.0
roc curves where cfa,4,2,2.0
curves where cfa methods,1,1,1.0
where cfa methods obtained,1,1,1.0
cfa methods obtained for,1,1,1.0
methods obtained for the,2,2,1.0
obtained for the four,2,2,1.0
for the four methods,2,2,1.0
the four methods using,2,2,1.0
four methods using the,2,2,1.0
methods using the four,6,2,3.0
the four classifiers on,6,2,3.0
four classifiers on data,1,1,1.0
classifiers on data sets,1,1,1.0
on data sets according,1,1,1.0
data sets according to,2,2,1.0
sets according to figure,2,2,1.0
according to figure cfa,2,2,1.0
to figure cfa clearly,2,2,1.0
figure cfa clearly outperformed,2,2,1.0
cfa clearly outperformed other,2,2,1.0
clearly outperformed other data,2,2,1.0
outperformed other data augmentation,1,1,1.0
other data augmentation methods,1,1,1.0
methods smote adasyn for,2,2,1.0
smote adasyn for example,2,2,1.0
adasyn for example when,2,2,1.0
for example when running,4,2,2.0
example when running a,4,2,2.0
when running a rf,2,2,1.0
running a rf classifier,2,2,1.0
a rf classifier on,2,2,1.0
rf classifier on the,2,2,1.0
classifier on the dataset,2,2,1.0
on the dataset we,2,2,1.0
the dataset we can,2,2,1.0
dataset we can see,4,2,2.0
we can see that,4,2,2.0
can see that cfa,2,2,1.0
see that cfa had,2,2,1.0
that cfa had better,2,2,1.0
cfa had better performance,2,2,1.0
had better performance than,4,2,2.0
better performance than methods,2,2,1.0
performance than methods with,2,2,1.0
than methods with respect,2,2,1.0
methods with respect to,2,2,1.0
with respect to roc,2,2,1.0
respect to roc curve,2,2,1.0
to roc curve figure,2,2,1.0
roc curve figure these,2,2,1.0
curve figure these results,1,1,1.0
figure these results support,1,1,1.0
these results support our,1,1,1.0
results support our previous,1,1,1.0
support our previous results,2,2,1.0
our previous results which,2,2,1.0
previous results which were,2,2,1.0
results which were showing,2,2,1.0
which were showing that,2,2,1.0
were showing that cfa,2,2,1.0
showing that cfa can,2,2,1.0
that cfa can work,2,2,1.0
cfa can work as,2,2,1.0
can work as a,2,2,1.0
work as a successful,2,2,1.0
as a successful technique,2,2,1.0
a successful technique for,2,2,1.0
successful technique for data,2,2,1.0
technique for data augmentation,2,2,1.0
for data augmentation to,2,2,1.0
data augmentation to handling,2,2,1.0
augmentation to handling the,2,2,1.0
to handling the class,2,2,1.0
handling the class imbalanced,3,3,1.0
the class imbalanced problem,5,3,1.6666666666666667
class imbalanced problem on,2,2,1.0
imbalanced problem on the,2,2,1.0
problem on the other,2,2,1.0
the other hand figure,1,1,1.0
other hand figure shows,1,1,1.0
hand figure shows several,1,1,1.0
figure shows several examples,1,1,1.0
shows several examples of,2,2,1.0
several examples of roc,2,2,1.0
roc curves where smote,4,2,2.0
curves where smote variants,4,2,2.0
where smote variants smote,2,2,1.0
smote variants smote adasyn,2,2,1.0
variants smote adasyn do,2,2,1.0
smote adasyn do better,2,2,1.0
adasyn do better than,2,2,1.0
do better than cfa,2,2,1.0
better than cfa for,2,2,1.0
than cfa for example,2,2,1.0
cfa for example when,2,2,1.0
when running a lr,2,2,1.0
running a lr classifier,2,2,1.0
a lr classifier on,2,2,1.0
lr classifier on the,2,2,1.0
classifier on the pima,2,2,1.0
on the pima dataset,2,2,1.0
the pima dataset we,2,2,1.0
pima dataset we can,2,2,1.0
can see that methods,2,2,1.0
see that methods had,2,2,1.0
that methods had better,2,2,1.0
methods had better performance,2,2,1.0
better performance than cfa,2,2,1.0
performance than cfa figure,2,2,1.0
than cfa figure temraz,2,2,1.0
cfa figure temraz keane,2,2,1.0
figure temraz keane counterfactual,2,2,1.0
counterfactual data augmentation figure,4,2,2.0
data augmentation figure selected,4,2,2.0
augmentation figure selected examples,4,2,2.0
figure selected examples for,4,2,2.0
selected examples for roc,4,2,2.0
examples for roc curves,4,2,2.0
for roc curves where,4,2,2.0
curves where cfa outperformed,2,2,1.0
where cfa outperformed methods,2,2,1.0
cfa outperformed methods for,1,1,1.0
outperformed methods for the,1,1,1.0
methods for the six,1,1,1.0
for the six methods,4,2,2.0
the six methods using,4,2,2.0
six methods using the,4,2,2.0
four classifiers on different,4,2,2.0
classifiers on different data,4,2,2.0
on different data sets,4,2,2.0
different data sets temraz,4,2,2.0
data sets temraz keane,4,2,2.0
sets temraz keane counterfactual,4,2,2.0
where smote variants outperformed,2,2,1.0
smote variants outperformed cfa,2,2,1.0
variants outperformed cfa obtained,2,2,1.0
outperformed cfa obtained for,2,2,1.0
cfa obtained for the,2,2,1.0
obtained for the six,2,2,1.0
counterfactual data augmentation why,2,2,1.0
data augmentation why does,2,2,1.0
augmentation why does cfa,2,2,1.0
why does cfa work,2,2,1.0
does cfa work in,2,2,1.0
cfa work in xai,2,2,1.0
work in xai counterfactual,2,2,1.0
in xai counterfactual methods,2,2,1.0
xai counterfactual methods have,2,2,1.0
methods have been found,2,2,1.0
have been found to,2,2,1.0
been found to create,2,2,1.0
found to create plausible,2,2,1.0
to create plausible synthetic,2,2,1.0
create plausible synthetic datapoints,2,2,1.0
synthetic datapoints for purposes,1,1,1.0
datapoints for purposes indeed,1,1,1.0
for purposes indeed the,1,1,1.0
purposes indeed the evaluative,2,2,1.0
indeed the evaluative metrics,2,2,1.0
the evaluative metrics in,2,2,1.0
evaluative metrics in xai,2,2,1.0
metrics in xai show,2,2,1.0
in xai show that,2,2,1.0
xai show that these,2,2,1.0
show that these explanatory,2,2,1.0
that these explanatory are,1,1,1.0
these explanatory are generally,1,1,1.0
explanatory are generally valid,1,1,1.0
are generally valid and,2,2,1.0
close to existing this,2,2,1.0
to existing this experience,2,2,1.0
existing this experience in,2,2,1.0
this experience in xai,2,2,1.0
experience in xai is,2,2,1.0
in xai is the,2,2,1.0
xai is the backdrop,2,2,1.0
is the backdrop and,2,2,1.0
the backdrop and motivation,2,2,1.0
backdrop and motivation for,2,2,1.0
and motivation for applying,2,2,1.0
motivation for applying this,2,2,1.0
for applying this counterfactual,2,2,1.0
applying this counterfactual method,2,2,1.0
this counterfactual method to,2,2,1.0
method to the data,2,2,1.0
to the data as,1,1,1.0
the data as we,1,1,1.0
data as we saw,1,1,1.0
as we saw earlier,2,2,1.0
we saw earlier initial,2,2,1.0
saw earlier initial tests,2,2,1.0
earlier initial tests on,2,2,1.0
initial tests on a,2,2,1.0
tests on a prediction,2,2,1.0
on a prediction problem,2,2,1.0
a prediction problem showed,1,1,1.0
prediction problem showed that,1,1,1.0
problem showed that generated,1,1,1.0
showed that generated counterfactuals,1,1,1.0
that generated counterfactuals in,2,2,1.0
generated counterfactuals in the,2,2,1.0
the minority class improved,1,1,1.0
minority class improved performance,1,1,1.0
class improved performance specifically,1,1,1.0
improved performance specifically dealing,2,2,1.0
performance specifically dealing with,2,2,1.0
specifically dealing with the,2,2,1.0
dealing with the dataset,2,2,1.0
with the dataset drift,2,2,1.0
the dataset drift caused,2,2,1.0
dataset drift caused by,2,2,1.0
drift caused by climate,2,2,1.0
caused by climate change,2,2,1.0
by climate change see,2,2,1.0
climate change see this,2,2,1.0
change see this experience,2,2,1.0
see this experience led,2,2,1.0
this experience led to,2,2,1.0
experience led to the,2,2,1.0
led to the present,2,2,1.0
to the present tests,2,2,1.0
the present tests to,2,2,1.0
present tests to determine,2,2,1.0
tests to determine the,2,2,1.0
to determine the generality,2,2,1.0
determine the generality of,2,2,1.0
the generality of these,2,2,1.0
generality of these effects,2,2,1.0
of these effects as,2,2,1.0
these effects as we,2,2,1.0
effects as we can,2,2,1.0
as we can see,2,2,1.0
we can see the,2,2,1.0
can see the cfa,2,2,1.0
see the cfa method,2,2,1.0
the cfa method seems,2,2,1.0
cfa method seems to,2,2,1.0
method seems to work,2,2,1.0
seems to work well,2,2,1.0
to work well across,2,2,1.0
work well across wide,2,2,1.0
well across wide range,2,2,1.0
across wide range of,2,2,1.0
wide range of datasets,4,2,2.0
range of datasets ml,2,2,1.0
of datasets ml models,2,2,1.0
datasets ml models and,2,2,1.0
ml models and different,2,2,1.0
models and different imbalance,2,2,1.0
and different imbalance ratios,2,2,1.0
different imbalance ratios but,2,2,1.0
imbalance ratios but why,2,2,1.0
ratios but why does,2,2,1.0
but why does it,2,2,1.0
why does it work,2,2,1.0
does it work so,2,2,1.0
it work so well,2,2,1.0
work so well from,2,2,1.0
so well from the,2,2,1.0
well from the climate,2,2,1.0
from the climate example,2,2,1.0
the climate example our,2,2,1.0
climate example our initial,2,2,1.0
example our initial explanation,2,2,1.0
our initial explanation was,2,2,1.0
initial explanation was that,2,2,1.0
explanation was that cfa,2,2,1.0
was that cfa does,2,2,1.0
that cfa does well,2,2,1.0
cfa does well because,2,2,1.0
does well because it,2,2,1.0
well because it minority,1,1,1.0
because it minority instances,1,1,1.0
it minority instances that,1,1,1.0
instances that are counterfactual,2,2,1.0
that are counterfactual offsets,2,2,1.0
are counterfactual offsets from,2,2,1.0
counterfactual offsets from known,2,2,1.0
offsets from known but,1,1,1.0
from known but this,1,1,1.0
known but this account,1,1,1.0
but this account does,2,2,1.0
this account does not,2,2,1.0
account does not answer,2,2,1.0
does not answer why,2,2,1.0
not answer why the,2,2,1.0
answer why the offsets,2,2,1.0
why the offsets tend,2,2,1.0
the offsets tend to,2,2,1.0
offsets tend to be,2,2,1.0
tend to be useful,2,2,1.0
to be useful our,2,2,1.0
be useful our best,2,2,1.0
useful our best account,2,2,1.0
our best account hinges,2,2,1.0
best account hinges on,2,2,1.0
account hinges on ideas,2,2,1.0
hinges on ideas from,2,2,1.0
on ideas from how,2,2,1.0
ideas from how reasoning,2,2,1.0
from how reasoning cbr,2,2,1.0
how reasoning cbr systems,2,2,1.0
reasoning cbr systems operate,2,2,1.0
cbr systems operate in,2,2,1.0
systems operate in cbr,2,2,1.0
operate in cbr target,2,2,1.0
in cbr target problems,2,2,1.0
cbr target problems are,2,2,1.0
target problems are solved,2,2,1.0
problems are solved by,2,2,1.0
are solved by similar,1,1,1.0
solved by similar cases,1,1,1.0
by similar cases and,1,1,1.0
similar cases and sometimes,2,2,1.0
cases and sometimes adapting,2,2,1.0
and sometimes adapting them,2,2,1.0
sometimes adapting them to,2,2,1.0
adapting them to generate,2,2,1.0
them to generate predictions,2,2,1.0
to generate predictions so,2,2,1.0
generate predictions so if,2,2,1.0
predictions so if i,2,2,1.0
if i am trying,2,2,1.0
i am trying to,2,2,1.0
am trying to predict,2,2,1.0
trying to predict in,2,2,1.0
to predict in a,2,2,1.0
predict in a city,2,2,1.0
in a city and,2,2,1.0
a city and my,2,2,1.0
city and my cbr,2,2,1.0
and my cbr system,2,2,1.0
my cbr system is,2,2,1.0
cbr system is presented,2,2,1.0
system is presented with,2,2,1.0
is presented with a,2,2,1.0
presented with a apartment,1,1,1.0
with a apartment with,1,1,1.0
a apartment with and,1,1,1.0
apartment with and the,1,1,1.0
with and the closest,1,1,1.0
and the closest retrieved,2,2,1.0
the closest retrieved case,2,2,1.0
closest retrieved case is,2,2,1.0
retrieved case is a,2,2,1.0
case is a apartment,2,2,1.0
is a apartment with,2,2,1.0
a apartment with the,1,1,1.0
apartment with the system,1,1,1.0
with the system could,1,1,1.0
the system could have,2,2,1.0
system could have an,2,2,1.0
could have an adaptation,2,2,1.0
have an adaptation rule,2,2,1.0
an adaptation rule than,2,2,1.0
adaptation rule than can,2,2,1.0
rule than can bridges,2,2,1.0
than can bridges the,2,2,1.0
can bridges the gap,2,2,1.0
bridges the gap between,2,2,1.0
the gap between the,2,2,1.0
gap between the historical,2,2,1.0
between the historical case,1,1,1.0
the historical case and,1,1,1.0
historical case and the,1,1,1.0
case and the target,1,1,1.0
and the target case,2,2,1.0
the target case for,2,2,1.0
target case for instance,2,2,1.0
case for instance there,2,2,1.0
for instance there may,2,2,1.0
instance there may be,2,2,1.0
there may be an,2,2,1.0
may be an adaptation,2,2,1.0
be an adaptation rule,2,2,1.0
an adaptation rule that,2,2,1.0
adaptation rule that says,2,2,1.0
rule that says in,2,2,1.0
that says in general,2,2,1.0
says in general an,2,2,1.0
in general an additional,2,2,1.0
general an additional bathroom,2,2,1.0
an additional bathroom is,2,2,1.0
additional bathroom is worth,2,2,1.0
bathroom is worth more,2,2,1.0
is worth more so,2,2,1.0
worth more so in,2,2,1.0
more so in a,2,2,1.0
so in a typical,2,2,1.0
in a typical this,2,2,1.0
a typical this rule,2,2,1.0
typical this rule would,2,2,1.0
this rule would be,2,2,1.0
rule would be applied,2,2,1.0
would be applied to,2,2,1.0
be applied to the,2,2,1.0
applied to the retrieved,2,2,1.0
to the retrieved case,2,2,1.0
the retrieved case to,2,2,1.0
retrieved case to bring,2,2,1.0
case to bring it,2,2,1.0
to bring it closer,2,2,1.0
bring it closer to,2,2,1.0
it closer to the,2,2,1.0
closer to the target,2,2,1.0
to the target case,2,2,1.0
the target case and,2,2,1.0
target case and improve,2,2,1.0
case and improve the,2,2,1.0
and improve the prediction,2,2,1.0
improve the prediction in,2,2,1.0
the prediction in cbr,2,2,1.0
prediction in cbr adaptation,2,2,1.0
in cbr adaptation rules,2,2,1.0
cbr adaptation rules were,2,2,1.0
adaptation rules were often,2,2,1.0
rules were often but,2,2,1.0
were often but they,2,2,1.0
often but they may,2,2,1.0
but they may also,2,2,1.0
they may also be,2,2,1.0
may also be learned,2,2,1.0
also be learned from,2,2,1.0
be learned from analyses,2,2,1.0
learned from analyses of,2,2,1.0
from analyses of patterns,2,2,1.0
analyses of patterns between,2,2,1.0
of patterns between temraz,2,2,1.0
patterns between temraz keane,2,2,1.0
between temraz keane counterfactual,2,2,1.0
instances in the so,2,2,1.0
in the so the,2,2,1.0
the so the rule,2,2,1.0
so the rule could,2,2,1.0
the rule could be,2,2,1.0
rule could be learned,2,2,1.0
could be learned from,2,2,1.0
be learned from differences,1,1,1.0
learned from differences found,1,1,1.0
from differences found between,1,1,1.0
differences found between historical,2,2,1.0
found between historical instances,2,2,1.0
between historical instances showing,2,2,1.0
historical instances showing that,2,2,1.0
instances showing that they,2,2,1.0
showing that they often,2,2,1.0
that they often lead,2,2,1.0
they often lead to,2,2,1.0
often lead to a,2,2,1.0
lead to a uplift,2,2,1.0
to a uplift in,2,2,1.0
a uplift in price,2,2,1.0
uplift in price other,2,2,1.0
in price other features,2,2,1.0
price other features being,2,2,1.0
other features being equal,2,2,1.0
features being equal so,2,2,1.0
being equal so in,2,2,1.0
equal so in cbr,2,2,1.0
so in cbr these,2,2,1.0
in cbr these adaptation,2,2,1.0
cbr these adaptation rules,2,2,1.0
these adaptation rules help,2,2,1.0
adaptation rules help to,2,2,1.0
rules help to bridge,2,2,1.0
help to bridge holes,2,2,1.0
to bridge holes in,2,2,1.0
bridge holes in the,2,2,1.0
holes in the by,2,2,1.0
in the by providing,2,2,1.0
the by providing plausible,2,2,1.0
by providing plausible transformations,2,2,1.0
providing plausible transformations of,2,2,1.0
plausible transformations of known,2,2,1.0
transformations of known datapoints,2,2,1.0
of known datapoints counterfactuals,2,2,1.0
known datapoints counterfactuals are,2,2,1.0
datapoints counterfactuals are special,2,2,1.0
counterfactuals are special case,2,2,1.0
are special case of,2,2,1.0
special case of an,2,2,1.0
case of an adaption,2,2,1.0
of an adaption rule,2,2,1.0
an adaption rule they,2,2,1.0
adaption rule they capture,2,2,1.0
rule they capture the,2,2,1.0
they capture the key,1,1,1.0
capture the key that,1,1,1.0
the key that lead,1,1,1.0
key that lead to,1,1,1.0
that lead to class,2,2,1.0
lead to class changes,2,2,1.0
to class changes across,2,2,1.0
class changes across the,2,2,1.0
changes across the boundary,2,2,1.0
across the boundary between,2,2,1.0
the boundary between majority,2,2,1.0
boundary between majority and,2,2,1.0
and minority instances so,2,2,1.0
minority instances so when,2,2,1.0
instances so when we,2,2,1.0
so when we apply,2,2,1.0
when we apply them,2,2,1.0
we apply them to,2,2,1.0
apply them to majority,2,2,1.0
them to majority instances,2,2,1.0
to majority instances to,2,2,1.0
majority instances to create,2,2,1.0
instances to create synthetic,2,2,1.0
to create synthetic minority,2,2,1.0
create synthetic minority instances,1,1,1.0
synthetic minority instances they,1,1,1.0
minority instances they stand,1,1,1.0
instances they stand a,1,1,1.0
they stand a good,2,2,1.0
stand a good chance,2,2,1.0
a good chance of,2,2,1.0
good chance of being,2,2,1.0
chance of being plausible,2,2,1.0
of being plausible as,2,2,1.0
being plausible as they,2,2,1.0
plausible as they are,2,2,1.0
as they are based,2,2,1.0
they are based on,3,3,1.0
are based on prior,2,2,1.0
based on prior local,2,2,1.0
on prior local transformations,2,2,1.0
prior local transformations though,2,2,1.0
local transformations though they,1,1,1.0
transformations though they lack,1,1,1.0
though they lack generality,1,1,1.0
they lack generality they,1,1,1.0
lack generality they are,2,2,1.0
generality they are not,2,2,1.0
they are not created,2,2,1.0
are not created from,2,2,1.0
not created from multiple,2,2,1.0
created from multiple pairings,2,2,1.0
from multiple pairings of,2,2,1.0
multiple pairings of the,2,2,1.0
pairings of the same,2,2,1.0
of the same as,1,1,1.0
the same as is,1,1,1.0
same as is the,1,1,1.0
as is the case,2,2,1.0
is the case for,2,2,1.0
the case for learned,2,2,1.0
case for learned adaption,2,2,1.0
for learned adaption rules,2,2,1.0
learned adaption rules perhaps,2,2,1.0
adaption rules perhaps they,2,2,1.0
rules perhaps they may,2,2,1.0
perhaps they may work,2,2,1.0
they may work because,2,2,1.0
may work because they,2,2,1.0
work because they are,2,2,1.0
because they are so,2,2,1.0
they are so constrained,2,2,1.0
are so constrained and,2,2,1.0
so constrained and local,2,2,1.0
constrained and local cfa,1,1,1.0
and local cfa only,1,1,1.0
local cfa only considers,1,1,1.0
cfa only considers native,2,2,1.0
only considers native counterfactual,2,2,1.0
considers native counterfactual with,2,2,1.0
native counterfactual with feature,2,2,1.0
counterfactual with feature differences,2,2,1.0
with feature differences so,2,2,1.0
feature differences so the,2,2,1.0
differences so the is,1,1,1.0
so the is highly,1,1,1.0
the is highly constrained,1,1,1.0
is highly constrained and,2,2,1.0
highly constrained and specific,2,2,1.0
constrained and specific to,2,2,1.0
and specific to the,2,2,1.0
specific to the instances,2,2,1.0
to the instances that,2,2,1.0
the instances that are,2,2,1.0
instances that are already,2,2,1.0
that are already very,1,1,1.0
are already very similar,1,1,1.0
already very similar all,1,1,1.0
very similar all other,1,1,1.0
similar all other features,2,2,1.0
all other features are,2,2,1.0
other features are essentially,2,2,1.0
features are essentially identical,2,2,1.0
are essentially identical to,2,2,1.0
essentially identical to put,2,2,1.0
identical to put it,2,2,1.0
to put it simply,2,2,1.0
put it simply cfa,2,2,1.0
it simply cfa delivers,2,2,1.0
simply cfa delivers good,2,2,1.0
cfa delivers good counterfactual,2,2,1.0
delivers good counterfactual rules,2,2,1.0
good counterfactual rules that,2,2,1.0
counterfactual rules that work,2,2,1.0
rules that work locally,2,2,1.0
that work locally to,2,2,1.0
work locally to generate,2,2,1.0
locally to generate plausible,2,2,1.0
to generate plausible datapoints,2,2,1.0
generate plausible datapoints that,2,2,1.0
plausible datapoints that are,2,2,1.0
datapoints that are predictively,2,2,1.0
that are predictively useful,2,2,1.0
are predictively useful so,2,2,1.0
predictively useful so it,2,2,1.0
useful so it looks,2,2,1.0
so it looks like,2,2,1.0
it looks like the,2,2,1.0
looks like the nature,2,2,1.0
like the nature of,2,2,1.0
the nature of the,5,5,1.0
nature of the is,2,2,1.0
of the is important,3,3,1.0
the is important to,2,2,1.0
is important to the,2,2,1.0
important to the success,2,2,1.0
to the success of,2,2,1.0
success of the method,2,2,1.0
of the method what,2,2,1.0
the method what are,2,2,1.0
method what are cfa,2,2,1.0
what are cfa s,2,2,1.0
are cfa s limitations,2,2,1.0
cfa s limitations the,2,2,1.0
s limitations the to,2,2,1.0
limitations the to the,2,2,1.0
the to the success,2,2,1.0
to the success question,2,2,1.0
the success question is,2,2,1.0
success question is the,2,2,1.0
question is the failure,2,2,1.0
is the failure one,2,2,1.0
the failure one namely,2,2,1.0
failure one namely when,2,2,1.0
one namely when do,2,2,1.0
namely when do we,2,2,1.0
when do we think,2,2,1.0
do we think cfa,2,2,1.0
we think cfa will,2,2,1.0
think cfa will fail,2,2,1.0
cfa will fail and,2,2,1.0
will fail and what,2,2,1.0
fail and what limitations,2,2,1.0
and what limitations might,2,2,1.0
what limitations might it,2,2,1.0
limitations might it encounter,2,2,1.0
might it encounter the,2,2,1.0
it encounter the current,2,2,1.0
encounter the current experiments,2,2,1.0
the current experiments a,2,2,1.0
current experiments a version,2,2,1.0
experiments a version of,2,2,1.0
a version of cfa,2,2,1.0
version of cfa that,2,2,1.0
of cfa that performs,2,2,1.0
cfa that performs well,2,2,1.0
that performs well so,2,2,1.0
performs well so it,2,2,1.0
well so it is,2,2,1.0
so it is not,3,3,1.0
it is not immediately,2,2,1.0
is not immediately clear,2,2,1.0
not immediately clear what,2,2,1.0
immediately clear what would,2,2,1.0
clear what would lead,2,2,1.0
what would lead cfa,2,2,1.0
would lead cfa to,2,2,1.0
lead cfa to fail,2,2,1.0
cfa to fail however,1,1,1.0
to fail however there,1,1,1.0
fail however there are,1,1,1.0
however there are several,1,1,1.0
there are several conditions,2,2,1.0
are several conditions under,2,2,1.0
several conditions under which,2,2,1.0
conditions under which cfa,2,2,1.0
under which cfa is,2,2,1.0
which cfa is likely,2,2,1.0
cfa is likely to,3,2,1.5
likely to be less,2,2,1.0
to be less good,2,2,1.0
be less good with,2,2,1.0
less good with respect,2,2,1.0
good with respect to,2,2,1.0
with respect to i,2,2,1.0
respect to i the,1,1,1.0
to i the quality,1,1,1.0
i the quality of,1,1,1.0
the quality of dataset,1,1,1.0
quality of dataset differences,1,1,1.0
of dataset differences ii,1,1,1.0
dataset differences ii the,1,1,1.0
differences ii the use,2,2,1.0
ii the use of,2,2,1.0
the use of the,4,4,1.0
use of the constraint,2,2,1.0
of the constraint and,2,2,1.0
the constraint and iii,2,2,1.0
constraint and iii the,2,2,1.0
and iii the use,2,2,1.0
iii the use of,2,2,1.0
the use of different,2,2,1.0
use of different tolerances,2,2,1.0
of different tolerances temraz,2,2,1.0
different tolerances temraz keane,2,2,1.0
tolerances temraz keane counterfactual,2,2,1.0
counterfactual data augmentation quality,2,2,1.0
data augmentation quality of,2,2,1.0
augmentation quality of the,2,2,1.0
quality of the dataset,2,2,1.0
of the dataset fundamentally,2,2,1.0
the dataset fundamentally cfa,2,2,1.0
dataset fundamentally cfa depends,2,2,1.0
fundamentally cfa depends on,2,2,1.0
cfa depends on the,2,2,1.0
depends on the set,2,2,1.0
on the set of,2,2,1.0
the set of native,2,2,1.0
set of native counterfactuals,2,2,1.0
of native counterfactuals in,2,2,1.0
in the dataset for,2,2,1.0
the dataset for its,2,2,1.0
dataset for its success,2,2,1.0
for its success to,2,2,1.0
its success to put,2,2,1.0
success to put this,2,2,1.0
to put this another,2,2,1.0
put this another way,2,2,1.0
this another way there,2,2,1.0
another way there needs,2,2,1.0
way there needs to,2,2,1.0
there needs to be,2,2,1.0
needs to be a,2,2,1.0
to be a rich,2,2,1.0
be a rich and,2,2,1.0
a rich and diverse,2,2,1.0
rich and diverse set,2,2,1.0
and diverse set of,2,2,1.0
diverse set of good,2,2,1.0
set of good counterfactual,2,2,1.0
of good counterfactual pairings,2,2,1.0
good counterfactual pairings between,2,2,1.0
counterfactual pairings between majority,2,2,1.0
pairings between majority and,2,2,1.0
and minority instances either,2,2,1.0
minority instances either side,2,2,1.0
side of a clear,2,2,1.0
of a clear decision,2,2,1.0
a clear decision boundary,2,2,1.0
clear decision boundary without,2,2,1.0
decision boundary without these,2,2,1.0
boundary without these counterfactuals,2,2,1.0
without these counterfactuals the,2,2,1.0
these counterfactuals the ability,2,2,1.0
counterfactuals the ability to,2,2,1.0
the ability to generate,2,2,1.0
ability to generate synthetic,2,2,1.0
to generate synthetic datapoints,1,1,1.0
generate synthetic datapoints will,1,1,1.0
synthetic datapoints will be,1,1,1.0
datapoints will be hampered,1,1,1.0
will be hampered current,1,1,1.0
be hampered current indications,1,1,1.0
hampered current indications are,2,2,1.0
current indications are that,2,2,1.0
indications are that at,2,2,1.0
are that at least,2,2,1.0
that at least of,2,2,1.0
at least of the,2,2,1.0
least of the majority,2,2,1.0
the majority class need,2,2,1.0
majority class need to,2,2,1.0
class need to be,2,2,1.0
need to be involved,2,2,1.0
to be involved in,2,2,1.0
be involved in these,2,2,1.0
involved in these native,2,2,1.0
in these native counterfactuals,2,2,1.0
these native counterfactuals in,2,2,1.0
native counterfactuals in order,2,2,1.0
counterfactuals in order to,2,2,1.0
in order to provide,3,3,1.0
order to provide a,2,2,1.0
to provide a basis,2,2,1.0
provide a basis for,2,2,1.0
a basis for generating,2,2,1.0
basis for generating minority,2,2,1.0
for generating minority instances,2,2,1.0
generating minority instances from,2,2,1.0
minority instances from the,2,2,1.0
instances from the of,2,2,1.0
from the of the,2,2,1.0
the of the majority,2,2,1.0
majority class however we,2,2,1.0
class however we have,2,2,1.0
however we have not,2,2,1.0
we have not systematically,2,2,1.0
have not systematically tested,1,1,1.0
not systematically tested how,1,1,1.0
systematically tested how changes,1,1,1.0
tested how changes in,1,1,1.0
how changes in this,2,2,1.0
changes in this percentage,2,2,1.0
in this percentage affect,2,2,1.0
this percentage affect performance,2,2,1.0
percentage affect performance what,2,2,1.0
affect performance what we,2,2,1.0
performance what we do,2,2,1.0
what we do know,2,2,1.0
we do know is,2,2,1.0
do know is that,2,2,1.0
know is that for,2,2,1.0
is that for many,2,2,1.0
that for many datasets,2,2,1.0
for many datasets the,2,2,1.0
many datasets the current,2,2,1.0
datasets the current parameters,2,2,1.0
the current parameters for,2,2,1.0
current parameters for cfa,2,2,1.0
parameters for cfa deliver,2,2,1.0
for cfa deliver good,2,2,1.0
cfa deliver good performance,2,2,1.0
deliver good performance so,2,2,1.0
good performance so this,2,2,1.0
performance so this factor,2,2,1.0
so this factor might,2,2,1.0
this factor might be,2,2,1.0
factor might be quite,2,2,1.0
might be quite robust,2,2,1.0
be quite robust to,2,2,1.0
quite robust to disruption,2,2,1.0
robust to disruption the,2,2,1.0
to disruption the constraint,2,2,1.0
disruption the constraint a,2,2,1.0
the constraint a key,2,2,1.0
constraint a key hyperparameter,2,2,1.0
a key hyperparameter for,2,2,1.0
key hyperparameter for cfa,2,2,1.0
hyperparameter for cfa is,2,2,1.0
for cfa is the,2,2,1.0
cfa is the constraint,2,2,1.0
is the constraint that,2,2,1.0
constraint that the native,2,2,1.0
that the native counterfactuals,2,2,1.0
the native counterfactuals build,2,2,1.0
native counterfactuals build from,2,2,1.0
counterfactuals build from the,2,2,1.0
build from the dataset,2,2,1.0
from the dataset involve,2,2,1.0
the dataset involve no,2,2,1.0
dataset involve no more,2,2,1.0
involve no more than,2,2,1.0
more than two this,2,2,1.0
than two this is,2,2,1.0
two this is a,2,2,1.0
this is a strong,2,2,1.0
is a strong constraint,2,2,1.0
a strong constraint that,2,2,1.0
strong constraint that was,2,2,1.0
constraint that was made,2,2,1.0
that was made originally,2,2,1.0
was made originally on,2,2,1.0
made originally on psychological,2,2,1.0
originally on psychological grounds,2,2,1.0
on psychological grounds in,2,2,1.0
psychological grounds in xai,2,2,1.0
grounds in xai that,2,2,1.0
in xai that is,2,2,1.0
xai that is it,2,2,1.0
that is it has,2,2,1.0
is it has been,2,2,1.0
it has been shown,3,3,1.0
has been shown that,3,3,1.0
been shown that people,2,2,1.0
shown that people prefer,2,2,1.0
that people prefer sparse,2,2,1.0
people prefer sparse counterfactuals,2,2,1.0
prefer sparse counterfactuals with,2,2,1.0
sparse counterfactuals with feature,2,2,1.0
counterfactuals with feature differences,2,2,1.0
with feature differences as,2,2,1.0
feature differences as they,2,2,1.0
differences as they are,2,2,1.0
as they are easier,2,2,1.0
they are easier to,2,2,1.0
are easier to comprehend,2,2,1.0
easier to comprehend however,2,2,1.0
to comprehend however this,2,2,1.0
comprehend however this rationale,2,2,1.0
however this rationale from,2,2,1.0
this rationale from xai,2,2,1.0
rationale from xai does,2,2,1.0
from xai does not,2,2,1.0
xai does not apply,2,2,1.0
does not apply to,2,2,1.0
not apply to data,2,2,1.0
apply to data augmentation,1,1,1.0
to data augmentation in,1,1,1.0
data augmentation in data,1,1,1.0
augmentation in data augmentation,1,1,1.0
data augmentation the may,2,2,1.0
augmentation the may work,2,2,1.0
the may work because,2,2,1.0
may work because it,2,2,1.0
work because it produces,2,2,1.0
because it produces very,2,2,1.0
it produces very counterfactual,1,1,1.0
produces very counterfactual pairs,1,1,1.0
very counterfactual pairs so,1,1,1.0
counterfactual pairs so they,2,2,1.0
pairs so they produce,2,2,1.0
so they produce very,2,2,1.0
they produce very simple,2,2,1.0
produce very simple adaptation,2,2,1.0
very simple adaptation rules,2,2,1.0
simple adaptation rules in,2,2,1.0
adaptation rules in which,2,2,1.0
rules in which most,2,2,1.0
in which most features,2,2,1.0
which most features remain,2,2,1.0
most features remain the,2,2,1.0
features remain the same,2,2,1.0
remain the same between,2,2,1.0
the same between instances,2,2,1.0
same between instances and,2,2,1.0
between instances and a,2,2,1.0
instances and a small,2,2,1.0
and a small number,2,2,1.0
a small number of,3,3,1.0
small number of features,2,2,1.0
number of features differ,2,2,1.0
of features differ these,2,2,1.0
features differ these difference,2,2,1.0
differ these difference patterns,2,2,1.0
these difference patterns may,2,2,1.0
difference patterns may be,2,2,1.0
patterns may be more,2,2,1.0
may be more representative,2,2,1.0
be more representative of,2,2,1.0
more representative of valid,2,2,1.0
representative of valid in,2,2,1.0
of valid in the,2,2,1.0
valid in the dataset,2,2,1.0
in the dataset and,4,2,2.0
the dataset and hence,2,2,1.0
dataset and hence be,2,2,1.0
and hence be more,2,2,1.0
hence be more likely,2,2,1.0
more likely to produce,2,2,1.0
likely to produce good,2,2,1.0
to produce good synthetic,2,2,1.0
produce good synthetic datapoints,2,2,1.0
good synthetic datapoints these,2,2,1.0
synthetic datapoints these is,2,2,1.0
datapoints these is some,2,2,1.0
these is some evidence,2,2,1.0
is some evidence to,2,2,1.0
some evidence to support,2,2,1.0
evidence to support this,2,2,1.0
to support this proposition,2,2,1.0
support this proposition in,2,2,1.0
this proposition in prior,2,2,1.0
proposition in prior work,2,2,1.0
in prior work temraz,2,2,1.0
prior work temraz et,2,2,1.0
work temraz et al,2,2,1.0
temraz et al report,2,2,1.0
et al report that,2,2,1.0
al report that in,2,2,1.0
report that in pilot,2,2,1.0
that in pilot runs,2,2,1.0
in pilot runs of,2,2,1.0
pilot runs of their,2,2,1.0
runs of their experiments,2,2,1.0
of their experiments they,2,2,1.0
their experiments they explored,2,2,1.0
experiments they explored using,1,1,1.0
they explored using and,1,1,1.0
explored using and counterfactuals,1,1,1.0
using and counterfactuals but,1,1,1.0
and counterfactuals but found,2,2,1.0
counterfactuals but found they,2,2,1.0
but found they did,2,2,1.0
found they did not,2,2,1.0
they did not significantly,2,2,1.0
did not significantly improve,2,2,1.0
not significantly improve importance,1,1,1.0
significantly improve importance that,1,1,1.0
improve importance that is,1,1,1.0
importance that is they,2,2,1.0
that is they were,2,2,1.0
is they were less,2,2,1.0
they were less likely,2,2,1.0
were less likely to,2,2,1.0
likely to generate useful,2,2,1.0
to generate useful minority,1,1,1.0
generate useful minority instances,1,1,1.0
useful minority instances we,1,1,1.0
minority instances we do,1,1,1.0
instances we do not,2,2,1.0
we do not know,2,2,1.0
do not know whether,2,2,1.0
not know whether similar,2,2,1.0
know whether similar results,2,2,1.0
whether similar results would,2,2,1.0
similar results would be,2,2,1.0
results would be found,2,2,1.0
would be found for,2,2,1.0
be found for other,2,2,1.0
found for other datasets,2,2,1.0
for other datasets though,2,2,1.0
other datasets though the,2,2,1.0
datasets though the fact,2,2,1.0
though the fact that,2,2,1.0
the fact that the,4,4,1.0
fact that the keane,2,2,1.0
that the keane counterfactual,2,2,1.0
the keane counterfactual data,2,2,1.0
counterfactual data augmentation difference,2,2,1.0
data augmentation difference constraint,2,2,1.0
augmentation difference constraint works,2,2,1.0
difference constraint works here,2,2,1.0
constraint works here for,2,2,1.0
works here for datasets,2,2,1.0
here for datasets with,2,2,1.0
for datasets with features,2,2,1.0
datasets with features suggests,2,2,1.0
with features suggests that,2,2,1.0
features suggests that this,2,2,1.0
suggests that this constraint,2,2,1.0
that this constraint works,2,2,1.0
this constraint works quite,2,2,1.0
constraint works quite generally,2,2,1.0
works quite generally so,2,2,1.0
quite generally so again,2,2,1.0
generally so again we,2,2,1.0
so again we would,2,2,1.0
again we would expect,2,2,1.0
we would expect cfa,2,2,1.0
would expect cfa to,2,2,1.0
expect cfa to fail,2,2,1.0
cfa to fail if,2,2,1.0
to fail if higher,2,2,1.0
fail if higher numbers,2,2,1.0
if higher numbers of,2,2,1.0
higher numbers of were,1,1,1.0
numbers of were used,1,1,1.0
of were used in,1,1,1.0
were used in computing,2,2,1.0
used in computing the,2,2,1.0
in computing the native,2,2,1.0
computing the native counterfactuals,2,2,1.0
the native counterfactuals when,2,2,1.0
native counterfactuals when applying,2,2,1.0
counterfactuals when applying the,2,2,1.0
when applying the method,2,2,1.0
applying the method the,2,2,1.0
the method the importance,2,2,1.0
method the importance of,2,2,1.0
the importance of tolerance,2,2,1.0
importance of tolerance a,2,2,1.0
of tolerance a final,2,2,1.0
tolerance a final key,2,2,1.0
a final key parameter,2,2,1.0
final key parameter in,2,2,1.0
key parameter in cfa,2,2,1.0
parameter in cfa is,2,2,1.0
in cfa is that,2,2,1.0
cfa is that of,2,2,1.0
is that of tolerance,2,2,1.0
that of tolerance in,2,2,1.0
of tolerance in finding,2,2,1.0
tolerance in finding and,2,2,1.0
a native counterfactual we,2,2,1.0
native counterfactual we apply,2,2,1.0
counterfactual we apply a,2,2,1.0
we apply a tolerance,2,2,1.0
apply a tolerance to,2,2,1.0
a tolerance to the,2,2,1.0
tolerance to the feature,2,2,1.0
to the feature values,3,3,1.0
the feature values specifically,2,2,1.0
feature values specifically we,2,2,1.0
values specifically we allow,2,2,1.0
specifically we allow features,2,2,1.0
we allow features to,2,2,1.0
allow features to match,2,2,1.0
for that feature this,1,1,1.0
that feature this was,1,1,1.0
feature this was applied,1,1,1.0
this was applied uniformly,1,1,1.0
was applied uniformly across,2,2,1.0
applied uniformly across all,2,2,1.0
uniformly across all of,2,2,1.0
across all of our,2,2,1.0
all of our datasets,2,2,1.0
of our datasets keane,2,2,1.0
our datasets keane smyth,2,2,1.0
datasets keane smyth used,2,2,1.0
keane smyth used a,2,2,1.0
smyth used a more,2,2,1.0
used a more tolerance,1,1,1.0
a more tolerance scheme,1,1,1.0
more tolerance scheme that,1,1,1.0
tolerance scheme that tailored,2,2,1.0
scheme that tailored the,2,2,1.0
that tailored the tolerance,2,2,1.0
tailored the tolerance to,2,2,1.0
the tolerance to each,2,2,1.0
tolerance to each dataset,2,2,1.0
to each dataset they,2,2,1.0
each dataset they varied,2,2,1.0
dataset they varied the,2,2,1.0
they varied the tolerances,2,2,1.0
varied the tolerances for,2,2,1.0
the tolerances for each,2,2,1.0
tolerances for each feature,2,2,1.0
for each feature until,2,2,1.0
each feature until changes,2,2,1.0
feature until changes in,2,2,1.0
until changes in classification,2,2,1.0
changes in classification of,2,2,1.0
in classification of the,2,2,1.0
classification of the original,2,2,1.0
the original dataset arose,2,2,1.0
original dataset arose and,2,2,1.0
dataset arose and then,2,2,1.0
arose and then chose,2,2,1.0
and then chose a,2,2,1.0
then chose a relative,2,2,1.0
chose a relative tolerance,2,2,1.0
a relative tolerance that,2,2,1.0
relative tolerance that produced,2,2,1.0
tolerance that produced no,2,2,1.0
that produced no classification,2,2,1.0
produced no classification change,2,2,1.0
no classification change obviously,2,2,1.0
classification change obviously without,2,2,1.0
change obviously without tolerance,2,2,1.0
obviously without tolerance fewer,2,2,1.0
without tolerance fewer would,1,1,1.0
tolerance fewer would be,1,1,1.0
fewer would be found,1,1,1.0
would likely diminish the,2,2,1.0
likely diminish the importance,2,2,1.0
diminish the importance of,2,2,1.0
importance of the decision,2,2,1.0
of the decision boundary,3,3,1.0
the decision boundary fundamentally,2,2,1.0
decision boundary fundamentally like,2,2,1.0
boundary fundamentally like cfa,2,2,1.0
fundamentally like cfa works,2,2,1.0
like cfa works with,2,2,1.0
cfa works with instances,2,2,1.0
works with instances that,2,2,1.0
with instances that are,2,2,1.0
decision boundary so clearly,1,1,1.0
boundary so clearly the,1,1,1.0
so clearly the definition,1,1,1.0
clearly the definition and,1,1,1.0
the definition and nature,2,2,1.0
definition and nature of,2,2,1.0
and nature of that,2,2,1.0
nature of that decision,2,2,1.0
of that decision boundary,2,2,1.0
that decision boundary is,2,2,1.0
decision boundary is critical,2,2,1.0
boundary is critical to,2,2,1.0
is critical to its,2,2,1.0
critical to its successful,2,2,1.0
to its successful performance,2,2,1.0
its successful performance if,2,2,1.0
successful performance if the,2,2,1.0
performance if the instances,2,2,1.0
if the instances around,2,2,1.0
the instances around the,2,2,1.0
instances around the boundary,2,2,1.0
around the boundary are,2,2,1.0
the boundary are noisy,2,2,1.0
boundary are noisy and,2,2,1.0
are noisy and the,2,2,1.0
noisy and the boundary,2,2,1.0
and the boundary is,2,2,1.0
the boundary is less,2,2,1.0
boundary is less then,2,2,1.0
is less then cfa,2,2,1.0
less then cfa is,2,2,1.0
then cfa is likely,2,2,1.0
is likely to disimprove,1,1,1.0
likely to disimprove in,1,1,1.0
to disimprove in this,1,1,1.0
disimprove in this respect,2,2,1.0
in this respect it,2,2,1.0
this respect it is,2,2,1.0
respect it is interesting,2,2,1.0
it is interesting note,2,2,1.0
is interesting note that,2,2,1.0
interesting note that cfa,2,2,1.0
note that cfa does,2,2,1.0
that cfa does best,2,2,1.0
cfa does best using,2,2,1.0
does best using the,2,2,1.0
best using the and,2,2,1.0
using the and random,2,2,1.0
the and random forests,2,2,1.0
and random forests models,2,2,1.0
random forests models relative,2,2,1.0
forests models relative to,2,2,1.0
models relative to the,2,2,1.0
relative to the mlp,2,2,1.0
to the mlp and,2,2,1.0
the mlp and linear,2,2,1.0
mlp and linear regression,2,2,1.0
and linear regression models,2,2,1.0
linear regression models with,2,2,1.0
regression models with the,2,2,1.0
models with the latter,2,2,1.0
with the latter doing,2,2,1.0
the latter doing the,2,2,1.0
latter doing the worst,2,2,1.0
doing the worst on,2,2,1.0
the worst on auc,2,2,1.0
worst on auc conclusion,2,2,1.0
on auc conclusion in,2,2,1.0
auc conclusion in this,2,2,1.0
conclusion in this paper,2,2,1.0
in this paper a,2,2,1.0
this paper a novel,2,2,1.0
paper a novel oversampling,2,2,1.0
a novel oversampling method,2,2,1.0
novel oversampling method counterfactual,2,2,1.0
oversampling method counterfactual augmentation,2,2,1.0
counterfactual augmentation cfa was,2,2,1.0
augmentation cfa was proposed,2,2,1.0
cfa was proposed to,2,2,1.0
was proposed to handle,2,2,1.0
proposed to handle the,2,2,1.0
to handle the class,2,2,1.0
handle the class imbalanced,2,2,1.0
class imbalanced problem for,2,2,1.0
imbalanced problem for binary,2,2,1.0
problem for binary classification,2,2,1.0
for binary classification tasks,2,2,1.0
binary classification tasks cfa,2,2,1.0
classification tasks cfa uses,2,2,1.0
tasks cfa uses a,2,2,1.0
cfa uses a approach,1,1,1.0
uses a approach to,1,1,1.0
a approach to generating,1,1,1.0
approach to generating synthetic,2,2,1.0
minority class the essence,2,2,1.0
class the essence of,2,2,1.0
the essence of this,2,2,1.0
essence of this temraz,2,2,1.0
of this temraz keane,2,2,1.0
this temraz keane counterfactual,2,2,1.0
data augmentation method is,2,2,1.0
augmentation method is that,2,2,1.0
method is that it,2,2,1.0
is that it oversamples,2,2,1.0
that it oversamples by,2,2,1.0
it oversamples by adaptively,2,2,1.0
oversamples by adaptively combining,2,2,1.0
by adaptively combining actual,2,2,1.0
adaptively combining actual from,2,2,1.0
combining actual from dataset,2,2,1.0
actual from dataset instances,2,2,1.0
from dataset instances rather,2,2,1.0
dataset instances rather than,2,2,1.0
instances rather than values,2,2,1.0
rather than values between,2,2,1.0
than values between instances,2,2,1.0
values between instances the,2,2,1.0
between instances the key,2,2,1.0
instances the key discoveries,2,2,1.0
the key discoveries made,2,2,1.0
key discoveries made are,2,2,1.0
discoveries made are i,2,2,1.0
made are i counterfactual,2,2,1.0
are i counterfactual methods,2,2,1.0
i counterfactual methods developed,2,2,1.0
counterfactual methods developed for,2,2,1.0
methods developed for xai,2,2,1.0
developed for xai can,2,2,1.0
for xai can be,2,2,1.0
xai can be usefully,2,2,1.0
can be usefully deployed,2,2,1.0
be usefully deployed to,2,2,1.0
usefully deployed to augment,2,2,1.0
deployed to augment datasets,2,2,1.0
to augment datasets with,2,2,1.0
augment datasets with synthetic,2,2,1.0
datasets with synthetic cases,2,2,1.0
with synthetic cases in,2,2,1.0
synthetic cases in the,2,2,1.0
cases in the minority,2,2,1.0
the minority class that,2,2,1.0
minority class that improve,2,2,1.0
class that improve the,2,2,1.0
performance of the ml,1,1,1.0
of the ml models,1,1,1.0
the ml models ii,1,1,1.0
ml models ii this,1,1,1.0
models ii this method,2,2,1.0
ii this method can,2,2,1.0
this method can successfully,2,2,1.0
method can successfully introduce,2,2,1.0
can successfully introduce new,2,2,1.0
successfully introduce new synthetic,2,2,1.0
introduce new synthetic minority,2,2,1.0
new synthetic minority examples,2,2,1.0
synthetic minority examples by,2,2,1.0
minority examples by leveraging,2,2,1.0
examples by leveraging known,1,1,1.0
by leveraging known in,1,1,1.0
leveraging known in the,1,1,1.0
known in the dataset,1,1,1.0
the dataset and iii,2,2,1.0
dataset and iii this,2,2,1.0
and iii this method,2,2,1.0
iii this method can,2,2,1.0
this method can outperform,2,2,1.0
method can outperform many,2,2,1.0
can outperform many key,2,2,1.0
outperform many key benchmark,2,2,1.0
many key benchmark smote,2,2,1.0
key benchmark smote on,1,1,1.0
benchmark smote on a,1,1,1.0
smote on a wide,1,1,1.0
on a wide range,2,2,1.0
a wide range of,2,2,1.0
range of datasets with,2,2,1.0
of datasets with differing,2,2,1.0
datasets with differing imbalance,2,2,1.0
with differing imbalance ratios,2,2,1.0
differing imbalance ratios using,2,2,1.0
imbalance ratios using representative,2,2,1.0
ratios using representative ml,2,2,1.0
using representative ml models,2,2,1.0
representative ml models acknowledgements,2,2,1.0
ml models acknowledgements this,2,2,1.0
models acknowledgements this publication,2,2,1.0
acknowledgements this publication has,2,2,1.0
this publication has emanated,2,2,1.0
publication has emanated from,2,2,1.0
has emanated from research,2,2,1.0
emanated from research conducted,2,2,1.0
from research conducted with,2,2,1.0
research conducted with the,2,2,1.0
conducted with the financial,2,2,1.0
with the financial support,2,2,1.0
the financial support of,2,2,1.0
financial support of i,2,2,1.0
support of i science,2,2,1.0
of i science foundation,2,2,1.0
i science foundation ireland,2,2,1.0
science foundation ireland sfi,2,2,1.0
foundation ireland sfi to,2,2,1.0
ireland sfi to the,2,2,1.0
sfi to the insight,2,2,1.0
to the insight centre,2,2,1.0
the insight centre for,2,2,1.0
for data analytics under,2,2,1.0
data analytics under grant,2,2,1.0
analytics under grant number,2,2,1.0
under grant number and,2,2,1.0
grant number and ii,2,2,1.0
number and ii sfi,2,2,1.0
and ii sfi and,2,2,1.0
ii sfi and the,2,2,1.0
sfi and the department,2,2,1.0
and the department of,2,2,1.0
the department of agriculture,2,2,1.0
department of agriculture food,2,2,1.0
of agriculture food and,2,2,1.0
agriculture food and marine,2,2,1.0
food and marine on,2,2,1.0
and marine on behalf,2,2,1.0
marine on behalf of,2,2,1.0
on behalf of the,2,2,1.0
behalf of the government,2,2,1.0
of the government of,2,2,1.0
the government of ireland,2,2,1.0
government of ireland to,2,2,1.0
of ireland to the,2,2,1.0
ireland to the vistamilk,2,2,1.0
to the vistamilk sfi,2,2,1.0
the vistamilk sfi research,2,2,1.0
sfi research centre under,2,2,1.0
research centre under grant,2,2,1.0
centre under grant number,2,2,1.0
under grant number references,2,2,1.0
grant number references aggarwal,2,2,1.0
number references aggarwal chen,2,2,1.0
references aggarwal chen han,2,2,1.0
aggarwal chen han j,2,2,1.0
chen han j the,2,2,1.0
han j the inverse,2,2,1.0
j the inverse classification,2,2,1.0
the inverse classification problem,2,2,1.0
inverse classification problem journal,2,2,1.0
classification problem journal of,2,2,1.0
problem journal of computer,2,2,1.0
journal of computer science,2,2,1.0
of computer science and,5,4,1.25
computer science and technology,4,3,1.3333333333333333
science and technology fernandez,2,2,1.0
and technology fernandez luengo,2,2,1.0
technology fernandez luengo derrac,2,2,1.0
fernandez luengo derrac garcía,2,2,1.0
luengo derrac garcía sánchez,2,2,1.0
derrac garcía sánchez herrera,2,2,1.0
garcía sánchez herrera keel,2,2,1.0
sánchez herrera keel software,2,2,1.0
herrera keel software tool,2,2,1.0
keel software tool data,2,2,1.0
software tool data set,2,2,1.0
tool data set repository,2,2,1.0
data set repository integration,2,2,1.0
set repository integration of,2,2,1.0
repository integration of and,1,1,1.0
integration of and experimental,1,1,1.0
of and experimental analysis,1,1,1.0
and experimental analysis framework,2,2,1.0
experimental analysis framework journal,2,2,1.0
analysis framework journal of,2,2,1.0
framework journal of logic,2,2,1.0
journal of logic and,2,2,1.0
of logic and soft,2,2,1.0
logic and soft computing,2,2,1.0
and soft computing asuncion,2,2,1.0
soft computing asuncion newman,2,2,1.0
computing asuncion newman uci,2,2,1.0
asuncion newman uci machine,2,2,1.0
newman uci machine learning,3,3,1.0
uci machine learning repository,7,4,1.75
machine learning repository https,2,2,1.0
learning repository https bache,2,2,1.0
repository https bache lichman,2,2,1.0
https bache lichman uci,2,2,1.0
bache lichman uci machine,2,2,1.0
lichman uci machine learning,3,3,1.0
machine learning repository batista,2,2,1.0
learning repository batista prati,2,2,1.0
repository batista prati monard,2,2,1.0
batista prati monard a,3,3,1.0
prati monard a study,3,3,1.0
monard a study of,4,4,1.0
a study of the,4,4,1.0
study of the behavior,3,3,1.0
of the behavior of,3,3,1.0
the behavior of several,4,4,1.0
behavior of several methods,4,4,1.0
of several methods for,5,5,1.0
several methods for balancing,4,4,1.0
methods for balancing machine,4,4,1.0
for balancing machine learning,4,4,1.0
balancing machine learning training,4,4,1.0
machine learning training data,5,5,1.0
learning training data sigkdd,4,4,1.0
training data sigkdd explor,3,3,1.0
data sigkdd explor temraz,2,2,1.0
sigkdd explor temraz keane,2,2,1.0
explor temraz keane counterfactual,2,2,1.0
counterfactual data augmentation bellinger,2,2,1.0
data augmentation bellinger sharma,2,2,1.0
augmentation bellinger sharma japkowicz,2,2,1.0
bellinger sharma japkowicz zaïane,2,2,1.0
sharma japkowicz zaïane framework,2,2,1.0
japkowicz zaïane framework for,2,2,1.0
zaïane framework for extreme,2,2,1.0
framework for extreme imbalance,2,2,1.0
for extreme imbalance classification,2,2,1.0
extreme imbalance classification with,2,2,1.0
imbalance classification with the,2,2,1.0
classification with the majority,2,2,1.0
the majority class knowledge,2,2,1.0
majority class knowledge and,2,2,1.0
class knowledge and systems,1,1,1.0
knowledge and systems bishop,1,1,1.0
and systems bishop pattern,1,1,1.0
systems bishop pattern recognition,2,2,1.0
bishop pattern recognition and,2,2,1.0
pattern recognition and machine,3,3,1.0
recognition and machine learning,3,3,1.0
and machine learning springer,2,2,1.0
machine learning springer blake,2,2,1.0
learning springer blake merz,2,2,1.0
springer blake merz uci,2,2,1.0
blake merz uci repository,2,2,1.0
merz uci repository of,2,2,1.0
uci repository of machine,2,2,1.0
repository of machine learning,2,2,1.0
of machine learning databases,2,2,1.0
machine learning databases department,2,2,1.0
learning databases department of,2,2,1.0
databases department of computer,2,2,1.0
department of computer science,4,4,1.0
computer science university of,2,2,1.0
science university of california,2,2,1.0
university of california bunkhumpornpat,2,2,1.0
of california bunkhumpornpat sinapiromsaran,2,2,1.0
california bunkhumpornpat sinapiromsaran lursinsap,2,2,1.0
bunkhumpornpat sinapiromsaran lursinsap minority,1,1,1.0
sinapiromsaran lursinsap minority technique,1,1,1.0
lursinsap minority technique for,2,2,1.0
minority technique for handling,3,3,1.0
technique for handling the,3,3,1.0
for handling the class,3,3,1.0
handling the class problem,1,1,1.0
the class problem in,1,1,1.0
class problem in proceedings,1,1,1.0
problem in proceedings of,4,4,1.0
in proceedings of the,60,5,12.0
proceedings of the conference,6,2,3.0
of the conference on,6,2,3.0
the conference on advances,2,2,1.0
conference on advances in,4,3,1.3333333333333333
on advances in knowledge,4,3,1.3333333333333333
advances in knowledge discovery,4,3,1.3333333333333333
in knowledge discovery and,4,3,1.3333333333333333
knowledge discovery and data,9,5,1.8
discovery and data mining,9,5,1.8
and data mining bunkhumpornpat,2,2,1.0
data mining bunkhumpornpat sinapiromsaran,2,2,1.0
mining bunkhumpornpat sinapiromsaran lursinsap,2,2,1.0
bunkhumpornpat sinapiromsaran lursinsap dbsmote,2,2,1.0
sinapiromsaran lursinsap dbsmote synthetic,1,1,1.0
lursinsap dbsmote synthetic minority,1,1,1.0
dbsmote synthetic minority technique,1,1,1.0
synthetic minority technique applied,2,2,1.0
minority technique applied intelligence,2,2,1.0
technique applied intelligence chawla,2,2,1.0
applied intelligence chawla bowyer,2,2,1.0
intelligence chawla bowyer hall,2,2,1.0
chawla bowyer hall kegelmeyer,2,2,1.0
bowyer hall kegelmeyer smote,2,2,1.0
hall kegelmeyer smote synthetic,2,2,1.0
kegelmeyer smote synthetic minority,5,5,1.0
smote synthetic minority technique,4,4,1.0
synthetic minority technique journal,3,3,1.0
minority technique journal of,3,3,1.0
technique journal of artificial,3,3,1.0
journal of artificial intelligence,6,3,2.0
of artificial intelligence research,6,3,2.0
artificial intelligence research chawla,2,2,1.0
intelligence research chawla lazarevic,2,2,1.0
research chawla lazarevic hall,2,2,1.0
chawla lazarevic hall bowyer,2,2,1.0
lazarevic hall bowyer smoteboost,2,2,1.0
hall bowyer smoteboost improving,2,2,1.0
bowyer smoteboost improving prediction,3,3,1.0
smoteboost improving prediction of,3,3,1.0
improving prediction of the,4,4,1.0
prediction of the minority,4,4,1.0
the minority class in,9,5,1.8
minority class in boosting,4,4,1.0
class in boosting knowledge,2,2,1.0
in boosting knowledge discovery,2,2,1.0
boosting knowledge discovery in,2,2,1.0
knowledge discovery in databases,3,3,1.0
discovery in databases pkdd,2,2,1.0
in databases pkdd cristianini,2,2,1.0
databases pkdd cristianini j,2,2,1.0
pkdd cristianini j an,2,2,1.0
cristianini j an introduction,2,2,1.0
j an introduction to,2,2,1.0
an introduction to support,2,2,1.0
introduction to support vector,2,2,1.0
to support vector machines,2,2,1.0
support vector machines and,2,2,1.0
vector machines and other,2,2,1.0
machines and other learning,2,2,1.0
and other learning methods,2,2,1.0
other learning methods cambridge,2,2,1.0
learning methods cambridge uk,2,2,1.0
methods cambridge uk cambridge,2,2,1.0
cambridge uk cambridge university,2,2,1.0
uk cambridge university press,2,2,1.0
cambridge university press dandl,2,2,1.0
university press dandl molnar,2,2,1.0
press dandl molnar binder,2,2,1.0
dandl molnar binder bischl,2,2,1.0
molnar binder bischl b,2,2,1.0
binder bischl b counterfactual,1,1,1.0
bischl b counterfactual explanations,1,1,1.0
b counterfactual explanations in,1,1,1.0
counterfactual explanations in international,1,1,1.0
explanations in international conference,2,2,1.0
in international conference on,14,2,7.0
international conference on parallel,2,2,1.0
conference on parallel problem,2,2,1.0
on parallel problem solving,2,2,1.0
parallel problem solving from,2,2,1.0
problem solving from nature,2,2,1.0
solving from nature d,2,2,1.0
from nature d aquin,2,2,1.0
nature d aquin badra,2,2,1.0
d aquin badra lafrogne,2,2,1.0
aquin badra lafrogne lieber,2,2,1.0
badra lafrogne lieber napoli,2,2,1.0
lafrogne lieber napoli szathmary,2,2,1.0
lieber napoli szathmary case,2,2,1.0
napoli szathmary case base,2,2,1.0
szathmary case base mining,2,2,1.0
case base mining for,2,2,1.0
base mining for adaptation,2,2,1.0
mining for adaptation knowledge,2,2,1.0
for adaptation knowledge acquisition,2,2,1.0
adaptation knowledge acquisition in,2,2,1.0
knowledge acquisition in proceedings,2,2,1.0
acquisition in proceedings of,2,2,1.0
proceedings of the twentieth,2,2,1.0
of the twentieth joint,1,1,1.0
the twentieth joint conference,1,1,1.0
twentieth joint conference on,1,1,1.0
joint conference on artificial,10,2,5.0
conference on artificial intelligence,11,3,3.6666666666666665
on artificial intelligence dasarathy,2,2,1.0
artificial intelligence dasarathy b,2,2,1.0
intelligence dasarathy b minimal,2,2,1.0
dasarathy b minimal consistent,2,2,1.0
b minimal consistent set,2,2,1.0
minimal consistent set mcs,2,2,1.0
consistent set mcs identification,2,2,1.0
set mcs identification for,2,2,1.0
mcs identification for optimal,2,2,1.0
identification for optimal nearest,1,1,1.0
for optimal nearest neighbor,1,1,1.0
optimal nearest neighbor decision,1,1,1.0
nearest neighbor decision systems,1,1,1.0
neighbor decision systems design,2,2,1.0
decision systems design ieee,2,2,1.0
systems design ieee transactions,2,2,1.0
design ieee transactions on,2,2,1.0
ieee transactions on systems,7,4,1.75
transactions on systems man,7,4,1.75
on systems man and,8,4,2.0
systems man and cybernetics,7,4,1.75
man and cybernetics delaney,2,2,1.0
and cybernetics delaney greene,2,2,1.0
cybernetics delaney greene keane,2,2,1.0
delaney greene keane counterfactual,2,2,1.0
greene keane counterfactual for,1,1,1.0
keane counterfactual for time,1,1,1.0
counterfactual for time series,1,1,1.0
for time series classification,2,2,1.0
time series classification in,2,2,1.0
series classification in international,2,2,1.0
classification in international conference,2,2,1.0
international conference on reasoning,7,2,3.5
conference on reasoning springer,7,2,3.5
on reasoning springer germany,7,2,3.5
reasoning springer germany temraz,2,2,1.0
springer germany temraz keane,2,2,1.0
germany temraz keane counterfactual,2,2,1.0
counterfactual data augmentation douzas,2,2,1.0
data augmentation douzas bacao,2,2,1.0
augmentation douzas bacao map,2,2,1.0
douzas bacao map oversampling,2,2,1.0
bacao map oversampling somo,2,2,1.0
map oversampling somo for,2,2,1.0
oversampling somo for data,1,1,1.0
somo for data set,1,1,1.0
for data set learning,1,1,1.0
data set learning expert,2,2,1.0
set learning expert systems,2,2,1.0
learning expert systems with,2,2,1.0
expert systems with applications,2,2,1.0
systems with applications douzas,2,2,1.0
with applications douzas bacao,2,2,1.0
applications douzas bacao last,2,2,1.0
douzas bacao last improving,2,2,1.0
bacao last improving imbalanced,2,2,1.0
last improving imbalanced learning,2,2,1.0
improving imbalanced learning through,2,2,1.0
imbalanced learning through a,2,2,1.0
learning through a oversampling,1,1,1.0
through a oversampling method,1,1,1.0
a oversampling method based,1,1,1.0
oversampling method based on,2,2,1.0
method based on and,2,2,1.0
based on and smote,2,2,1.0
on and smote information,2,2,1.0
and smote information sciences,2,2,1.0
smote information sciences elkan,2,2,1.0
information sciences elkan the,2,2,1.0
sciences elkan the foundations,2,2,1.0
elkan the foundations of,2,2,1.0
the foundations of learning,2,2,1.0
foundations of learning in,2,2,1.0
of learning in seventeenth,2,2,1.0
learning in seventeenth international,2,2,1.0
in seventeenth international joint,2,2,1.0
seventeenth international joint conference,2,2,1.0
international joint conference on,12,4,3.0
on artificial intelligence fernandez,2,2,1.0
artificial intelligence fernandez garcia,2,2,1.0
intelligence fernandez garcia herrera,2,2,1.0
fernandez garcia herrera chawla,2,2,1.0
garcia herrera chawla smote,2,2,1.0
herrera chawla smote for,2,2,1.0
chawla smote for learning,2,2,1.0
smote for learning from,2,2,1.0
for learning from imbalanced,3,3,1.0
learning from imbalanced data,29,6,4.833333333333333
from imbalanced data progress,2,2,1.0
imbalanced data progress and,2,2,1.0
data progress and challenges,2,2,1.0
progress and challenges marking,2,2,1.0
and challenges marking the,2,2,1.0
challenges marking the anniversary,2,2,1.0
marking the anniversary the,2,2,1.0
the anniversary the journal,2,2,1.0
anniversary the journal of,2,2,1.0
the journal of artificial,2,2,1.0
artificial intelligence research jair,2,2,1.0
intelligence research jair förster,2,2,1.0
research jair förster klier,2,2,1.0
jair förster klier kluge,2,2,1.0
förster klier kluge sigler,4,2,2.0
klier kluge sigler i,4,2,2.0
kluge sigler i evaluating,2,2,1.0
sigler i evaluating explainable,2,2,1.0
i evaluating explainable artifical,2,2,1.0
evaluating explainable artifical intelligence,2,2,1.0
explainable artifical intelligence what,2,2,1.0
artifical intelligence what users,2,2,1.0
intelligence what users really,2,2,1.0
what users really appreciate,2,2,1.0
users really appreciate in,2,2,1.0
really appreciate in european,2,2,1.0
appreciate in european conference,2,2,1.0
in european conference on,2,2,1.0
european conference on information,2,2,1.0
conference on information systems,2,2,1.0
on information systems förster,2,2,1.0
information systems förster klier,2,2,1.0
systems förster klier kluge,2,2,1.0
kluge sigler i fostering,2,2,1.0
sigler i fostering human,2,2,1.0
i fostering human agency,2,2,1.0
fostering human agency a,2,2,1.0
human agency a process,2,2,1.0
agency a process for,2,2,1.0
a process for the,2,2,1.0
process for the design,2,2,1.0
for the design of,2,2,1.0
the design of xai,2,2,1.0
design of xai systems,2,2,1.0
of xai systems in,2,2,1.0
xai systems in icis,2,2,1.0
systems in icis proceedings,2,2,1.0
in icis proceedings freund,2,2,1.0
icis proceedings freund schapire,2,2,1.0
proceedings freund schapire a,2,2,1.0
freund schapire a generalization,2,2,1.0
schapire a generalization of,4,4,1.0
a generalization of learning,4,4,1.0
generalization of learning and,4,4,1.0
of learning and an,4,4,1.0
learning and an application,4,4,1.0
and an application to,4,4,1.0
an application to boosting,4,4,1.0
application to boosting journal,3,3,1.0
to boosting journal of,3,3,1.0
boosting journal of computer,3,3,1.0
journal of computer and,3,3,1.0
of computer and system,3,3,1.0
computer and system sciences,3,3,1.0
and system sciences galar,2,2,1.0
system sciences galar fernandez,2,2,1.0
sciences galar fernandez barrenechea,2,2,1.0
galar fernandez barrenechea bustince,2,2,1.0
fernandez barrenechea bustince herrera,2,2,1.0
barrenechea bustince herrera a,2,2,1.0
bustince herrera a review,2,2,1.0
herrera a review on,3,3,1.0
a review on ensembles,3,3,1.0
review on ensembles for,3,3,1.0
on ensembles for the,3,3,1.0
ensembles for the class,3,3,1.0
imbalance problem and approaches,3,3,1.0
problem and approaches ieee,3,3,1.0
and approaches ieee transactions,3,3,1.0
approaches ieee transactions on,3,3,1.0
man and cybernetics han,2,2,1.0
and cybernetics han kamber,2,2,1.0
cybernetics han kamber data,2,2,1.0
han kamber data mining,2,2,1.0
kamber data mining concepts,2,2,1.0
data mining concepts and,2,2,1.0
mining concepts and technique,2,2,1.0
concepts and technique han,2,2,1.0
and technique han wang,2,2,1.0
technique han wang mao,2,2,1.0
han wang mao a,2,2,1.0
wang mao a new,2,2,1.0
mao a new method,2,2,1.0
a new method in,3,3,1.0
new method in imbalanced,3,3,1.0
method in imbalanced data,5,5,1.0
in imbalanced data sets,6,5,1.2
imbalanced data sets learning,6,5,1.2
data sets learning in,4,4,1.0
sets learning in international,2,2,1.0
learning in international conference,2,2,1.0
international conference on intelligent,3,3,1.0
conference on intelligent computing,3,3,1.0
on intelligent computing hanney,2,2,1.0
intelligent computing hanney keane,2,2,1.0
computing hanney keane learning,2,2,1.0
hanney keane learning adaptation,2,2,1.0
keane learning adaptation rules,2,2,1.0
learning adaptation rules from,2,2,1.0
adaptation rules from a,2,2,1.0
rules from a in,2,2,1.0
from a in of,1,1,1.0
a in of the,1,1,1.0
in of the third,1,1,1.0
of the third european,2,2,1.0
the third european workshop,2,2,1.0
third european workshop on,2,2,1.0
european workshop on reasoning,2,2,1.0
workshop on reasoning berlin,2,2,1.0
on reasoning berlin springer,2,2,1.0
reasoning berlin springer hasan,2,2,1.0
berlin springer hasan use,2,2,1.0
springer hasan use case,2,2,1.0
hasan use case of,2,2,1.0
use case of counterfactual,2,2,1.0
case of counterfactual examples,2,2,1.0
of counterfactual examples data,2,2,1.0
counterfactual examples data augmentation,2,2,1.0
examples data augmentation proceedings,2,2,1.0
data augmentation proceedings of,2,2,1.0
augmentation proceedings of student,2,2,1.0
proceedings of student research,2,2,1.0
of student research and,2,2,1.0
student research and creative,2,2,1.0
research and creative inquiry,2,2,1.0
and creative inquiry day,2,2,1.0
creative inquiry day he,2,2,1.0
inquiry day he bai,2,2,1.0
day he bai garcia,2,2,1.0
he bai garcia li,2,2,1.0
bai garcia li adasyn,2,2,1.0
garcia li adasyn adaptive,2,2,1.0
li adasyn adaptive synthetic,4,4,1.0
adasyn adaptive synthetic sampling,3,3,1.0
adaptive synthetic sampling approach,3,3,1.0
synthetic sampling approach for,3,3,1.0
sampling approach for imbalanced,3,3,1.0
approach for imbalanced learning,4,4,1.0
for imbalanced learning in,5,4,1.25
imbalanced learning in ieee,2,2,1.0
learning in ieee international,2,2,1.0
in ieee international joint,2,2,1.0
ieee international joint conference,2,2,1.0
joint conference on neural,4,3,1.3333333333333333
conference on neural networks,4,3,1.3333333333333333
on neural networks temraz,2,2,1.0
neural networks temraz keane,2,2,1.0
networks temraz keane counterfactual,2,2,1.0
counterfactual data augmentation he,2,2,1.0
data augmentation he garcia,2,2,1.0
augmentation he garcia learning,2,2,1.0
he garcia learning from,2,2,1.0
garcia learning from imbalanced,4,4,1.0
from imbalanced data ieee,5,5,1.0
imbalanced data ieee transactions,3,3,1.0
data ieee transactions on,3,3,1.0
ieee transactions on knowledge,5,3,1.6666666666666667
transactions on knowledge and,5,3,1.6666666666666667
on knowledge and data,5,3,1.6666666666666667
knowledge and data engineering,5,3,1.6666666666666667
and data engineering holte,2,2,1.0
data engineering holte acker,2,2,1.0
engineering holte acker porter,2,2,1.0
holte acker porter b,2,2,1.0
acker porter b concept,2,2,1.0
porter b concept learning,2,2,1.0
b concept learning and,2,2,1.0
concept learning and accuracy,2,2,1.0
learning and accuracy of,2,2,1.0
and accuracy of small,2,2,1.0
accuracy of small disjuncts,2,2,1.0
of small disjuncts in,2,2,1.0
small disjuncts in proceedings,2,2,1.0
disjuncts in proceedings of,2,2,1.0
proceedings of the llth,2,2,1.0
of the llth international,2,2,1.0
the llth international joint,2,2,1.0
llth international joint conference,2,2,1.0
on artificial intelligence hsu,2,2,1.0
artificial intelligence hsu lin,2,2,1.0
intelligence hsu lin a,2,2,1.0
hsu lin a comparison,2,2,1.0
lin a comparison of,2,2,1.0
a comparison of methods,2,2,1.0
comparison of methods for,2,2,1.0
of methods for multiclass,2,2,1.0
methods for multiclass support,2,2,1.0
for multiclass support vector,2,2,1.0
multiclass support vector ieee,1,1,1.0
support vector ieee transactions,1,1,1.0
vector ieee transactions on,1,1,1.0
ieee transactions on neural,19,4,4.75
transactions on neural networks,19,4,4.75
on neural networks hu,2,2,1.0
neural networks hu liang,2,2,1.0
networks hu liang ma,2,2,1.0
hu liang ma he,2,2,1.0
liang ma he y,2,2,1.0
ma he y msmote,2,2,1.0
he y msmote improving,2,2,1.0
y msmote improving classification,2,2,1.0
msmote improving classification when,1,1,1.0
improving classification when training,1,1,1.0
classification when training data,1,1,1.0
when training data is,2,2,1.0
training data is imbalanced,2,2,1.0
data is imbalanced in,2,2,1.0
is imbalanced in proceedings,2,2,1.0
imbalanced in proceedings of,2,2,1.0
proceedings of the second,2,2,1.0
of the second international,2,2,1.0
the second international workshop,2,2,1.0
second international workshop on,2,2,1.0
international workshop on computer,2,2,1.0
workshop on computer science,2,2,1.0
on computer science and,3,3,1.0
computer science and engineering,3,3,1.0
science and engineering jeni,2,2,1.0
and engineering jeni cohn,2,2,1.0
engineering jeni cohn torre,2,2,1.0
jeni cohn torre facing,2,2,1.0
cohn torre facing imbalanced,2,2,1.0
torre facing imbalanced data,2,2,1.0
facing imbalanced data recommendations,2,2,1.0
imbalanced data recommendations for,2,2,1.0
data recommendations for the,2,2,1.0
recommendations for the use,2,2,1.0
for the use of,2,2,1.0
the use of performance,2,2,1.0
use of performance metrics,2,2,1.0
of performance metrics in,2,2,1.0
performance metrics in proceedings,2,2,1.0
metrics in proceedings of,2,2,1.0
proceedings of the humaine,2,2,1.0
of the humaine association,2,2,1.0
the humaine association conference,2,2,1.0
humaine association conference on,2,2,1.0
association conference on affective,2,2,1.0
conference on affective computing,2,2,1.0
on affective computing and,2,2,1.0
affective computing and intelligent,2,2,1.0
computing and intelligent interaction,2,2,1.0
and intelligent interaction jiang,2,2,1.0
intelligent interaction jiang lu,2,2,1.0
interaction jiang lu xia,2,2,1.0
jiang lu xia a,2,2,1.0
lu xia a novel,2,2,1.0
xia a novel algorithm,2,2,1.0
a novel algorithm for,2,2,1.0
novel algorithm for imbalance,2,2,1.0
algorithm for imbalance data,2,2,1.0
for imbalance data classification,1,1,1.0
imbalance data classification based,1,1,1.0
data classification based on,1,1,1.0
classification based on genetic,1,1,1.0
based on genetic algorithm,3,3,1.0
on genetic algorithm improved,2,2,1.0
genetic algorithm improved smote,2,2,1.0
algorithm improved smote arabian,2,2,1.0
improved smote arabian journal,2,2,1.0
smote arabian journal for,2,2,1.0
arabian journal for science,2,2,1.0
journal for science and,2,2,1.0
for science and engineering,2,2,1.0
science and engineering karimi,2,2,1.0
and engineering karimi barthe,2,2,1.0
engineering karimi barthe schölkopf,2,2,1.0
karimi barthe schölkopf valera,2,2,1.0
barthe schölkopf valera i,2,2,1.0
schölkopf valera i a,2,2,1.0
valera i a survey,2,2,1.0
i a survey of,2,2,1.0
a survey of algorithmic,2,2,1.0
survey of algorithmic recourse,2,2,1.0
of algorithmic recourse definitions,2,2,1.0
algorithmic recourse definitions formulations,2,2,1.0
recourse definitions formulations solutions,2,2,1.0
definitions formulations solutions and,2,2,1.0
formulations solutions and prospects,2,2,1.0
solutions and prospects keane,2,2,1.0
and prospects keane kenny,2,2,1.0
prospects keane kenny delaney,2,2,1.0
keane kenny delaney smyth,2,2,1.0
kenny delaney smyth b,2,2,1.0
delaney smyth b if,2,2,1.0
smyth b if only,2,2,1.0
b if only we,2,2,1.0
if only we had,2,2,1.0
only we had better,2,2,1.0
we had better counterfactual,2,2,1.0
had better counterfactual explanations,2,2,1.0
better counterfactual explanations in,2,2,1.0
counterfactual explanations in proceedings,2,2,1.0
explanations in proceedings of,2,2,1.0
proceedings of the international,18,5,3.6
of the international joint,6,4,1.5
the international joint conference,6,4,1.5
on artificial intelligence montreal,2,2,1.0
artificial intelligence montreal canada,2,2,1.0
intelligence montreal canada keane,2,2,1.0
montreal canada keane smyth,2,2,1.0
canada keane smyth b,2,2,1.0
keane smyth b good,2,2,1.0
smyth b good counterfactuals,2,2,1.0
b good counterfactuals and,2,2,1.0
good counterfactuals and how,2,2,1.0
counterfactuals and how to,2,2,1.0
and how to find,2,2,1.0
how to find them,2,2,1.0
to find them in,2,2,1.0
find them in international,2,2,1.0
them in international conference,2,2,1.0
reasoning springer germany kotsiantis,2,2,1.0
springer germany kotsiantis supervised,2,2,1.0
germany kotsiantis supervised machine,2,2,1.0
kotsiantis supervised machine learning,2,2,1.0
supervised machine learning a,2,2,1.0
machine learning a review,2,2,1.0
learning a review of,2,2,1.0
a review of classification,2,2,1.0
review of classification techniques,2,2,1.0
of classification techniques informatica,2,2,1.0
classification techniques informatica krawczyk,2,2,1.0
techniques informatica krawczyk koziarski,2,2,1.0
informatica krawczyk koziarski woźniak,2,2,1.0
krawczyk koziarski woźniak oversampling,2,2,1.0
koziarski woźniak oversampling for,2,2,1.0
woźniak oversampling for multiclass,2,2,1.0
oversampling for multiclass imbalanced,2,2,1.0
for multiclass imbalanced data,2,2,1.0
multiclass imbalanced data classification,2,2,1.0
imbalanced data classification ieee,2,2,1.0
data classification ieee transactions,2,2,1.0
classification ieee transactions on,2,2,1.0
on neural networks and,4,4,1.0
neural networks and learning,3,3,1.0
networks and learning systems,3,3,1.0
and learning systems kubat,2,2,1.0
learning systems kubat holte,2,2,1.0
systems kubat holte matwin,2,2,1.0
kubat holte matwin machine,2,2,1.0
holte matwin machine learning,2,2,1.0
matwin machine learning for,3,3,1.0
machine learning for the,3,3,1.0
learning for the detection,3,3,1.0
for the detection of,3,3,1.0
the detection of oil,3,3,1.0
satellite radar images machine,2,2,1.0
radar images machine learning,2,2,1.0
images machine learning lash,2,2,1.0
machine learning lash lin,2,2,1.0
learning lash lin street,2,2,1.0
lash lin street robinson,2,2,1.0
lin street robinson ohlmann,2,2,1.0
street robinson ohlmann j,2,2,1.0
robinson ohlmann j generalized,2,2,1.0
ohlmann j generalized inverse,2,2,1.0
j generalized inverse classification,2,2,1.0
generalized inverse classification in,2,2,1.0
inverse classification in proceedings,2,2,1.0
classification in proceedings of,2,2,1.0
proceedings of the siam,2,2,1.0
of the siam international,2,2,1.0
the siam international conference,2,2,1.0
siam international conference on,2,2,1.0
international conference on data,5,3,1.6666666666666667
conference on data mining,5,3,1.6666666666666667
on data mining temraz,2,2,1.0
data mining temraz keane,2,2,1.0
mining temraz keane counterfactual,2,2,1.0
counterfactual data augmentation laugel,2,2,1.0
data augmentation laugel lesot,2,2,1.0
augmentation laugel lesot marsala,2,2,1.0
laugel lesot marsala renard,2,2,1.0
lesot marsala renard detyniecki,2,2,1.0
marsala renard detyniecki the,2,2,1.0
renard detyniecki the dangers,2,2,1.0
detyniecki the dangers of,2,2,1.0
the dangers of interpretability,2,2,1.0
dangers of interpretability unjustified,2,2,1.0
of interpretability unjustified counterfactual,2,2,1.0
interpretability unjustified counterfactual explanations,2,2,1.0
unjustified counterfactual explanations proceedings,2,2,1.0
counterfactual explanations proceedings of,2,2,1.0
explanations proceedings of the,2,2,1.0
on artificial intelligence lewis,2,2,1.0
artificial intelligence lewis counterfactuals,2,2,1.0
intelligence lewis counterfactuals john,2,2,1.0
lewis counterfactuals john wiley,2,2,1.0
counterfactuals john wiley sons,2,2,1.0
john wiley sons ling,2,2,1.0
wiley sons ling li,2,2,1.0
sons ling li data,2,2,1.0
ling li data mining,2,2,1.0
li data mining for,2,2,1.0
data mining for direct,2,2,1.0
mining for direct marketing,2,2,1.0
for direct marketing problems,2,2,1.0
direct marketing problems and,2,2,1.0
marketing problems and solutions,2,2,1.0
problems and solutions in,2,2,1.0
and solutions in proceedings,2,2,1.0
solutions in proceedings of,2,2,1.0
proceedings of the fourth,3,3,1.0
of the fourth international,3,3,1.0
the fourth international conference,3,3,1.0
fourth international conference on,6,3,2.0
international conference on knowledge,5,4,1.25
conference on knowledge discovery,5,4,1.25
on knowledge discovery and,5,4,1.25
and data mining kdd,2,2,1.0
data mining kdd luengo,2,2,1.0
mining kdd luengo fernandez,2,2,1.0
kdd luengo fernandez garcia,2,2,1.0
luengo fernandez garcia herrera,2,2,1.0
fernandez garcia herrera addressing,2,2,1.0
garcia herrera addressing data,2,2,1.0
herrera addressing data complexity,2,2,1.0
addressing data complexity for,2,2,1.0
data complexity for imbalanced,2,2,1.0
complexity for imbalanced data,2,2,1.0
imbalanced data sets analysis,2,2,1.0
data sets analysis of,2,2,1.0
sets analysis of oversampling,2,2,1.0
analysis of oversampling and,2,2,1.0
of oversampling and evolutionally,1,1,1.0
oversampling and evolutionally underdamping,1,1,1.0
and evolutionally underdamping soft,1,1,1.0
evolutionally underdamping soft computing,1,1,1.0
underdamping soft computing luo,2,2,1.0
soft computing luo liu,2,2,1.0
computing luo liu minority,2,2,1.0
luo liu minority oversampling,2,2,1.0
liu minority oversampling for,2,2,1.0
minority oversampling for imbalanced,2,2,1.0
oversampling for imbalanced classification,2,2,1.0
for imbalanced classification ma,2,2,1.0
imbalanced classification ma fan,2,2,1.0
classification ma fan algorithm,2,2,1.0
ma fan algorithm and,2,2,1.0
fan algorithm and hybrid,2,2,1.0
algorithm and hybrid algorithm,2,2,1.0
and hybrid algorithm for,2,2,1.0
hybrid algorithm for feature,2,2,1.0
algorithm for feature and,1,1,1.0
for feature and parameter,1,1,1.0
feature and parameter optimization,1,1,1.0
and parameter optimization based,2,2,1.0
parameter optimization based on,2,2,1.0
optimization based on random,2,2,1.0
based on random forests,2,2,1.0
on random forests bmc,2,2,1.0
random forests bmc bioinformatics,2,2,1.0
forests bmc bioinformatics maciejewski,2,2,1.0
bmc bioinformatics maciejewski stefanowski,2,2,1.0
bioinformatics maciejewski stefanowski j,2,2,1.0
maciejewski stefanowski j local,2,2,1.0
stefanowski j local neighbourhood,2,2,1.0
j local neighbourhood extension,2,2,1.0
local neighbourhood extension of,2,2,1.0
neighbourhood extension of smote,2,2,1.0
extension of smote for,2,2,1.0
of smote for mining,2,2,1.0
smote for mining imbalanced,2,2,1.0
for mining imbalanced data,2,2,1.0
mining imbalanced data ieee,2,2,1.0
imbalanced data ieee symposium,2,2,1.0
data ieee symposium on,2,2,1.0
ieee symposium on computational,2,2,1.0
symposium on computational intelligence,2,2,1.0
on computational intelligence and,2,2,1.0
computational intelligence and data,2,2,1.0
intelligence and data mining,2,2,1.0
and data mining cidm,2,2,1.0
data mining cidm mckenna,2,2,1.0
mining cidm mckenna smyth,2,2,1.0
cidm mckenna smyth b,2,2,1.0
mckenna smyth b editing,2,2,1.0
smyth b editing techniques,2,2,1.0
b editing techniques in,2,2,1.0
editing techniques in european,2,2,1.0
techniques in european workshop,2,2,1.0
in european workshop on,2,2,1.0
european workshop on advances,2,2,1.0
workshop on advances in,2,2,1.0
on advances in reasoning,2,2,1.0
advances in reasoning mothilal,2,2,1.0
in reasoning mothilal sharma,2,2,1.0
reasoning mothilal sharma tan,2,2,1.0
mothilal sharma tan explaining,2,2,1.0
sharma tan explaining machine,2,2,1.0
tan explaining machine learning,2,2,1.0
explaining machine learning classifiers,2,2,1.0
machine learning classifiers through,2,2,1.0
learning classifiers through diverse,2,2,1.0
classifiers through diverse counterfactual,2,2,1.0
through diverse counterfactual explanations,2,2,1.0
diverse counterfactual explanations conference,2,2,1.0
counterfactual explanations conference on,2,2,1.0
explanations conference on fairness,2,2,1.0
conference on fairness accountability,2,2,1.0
on fairness accountability and,2,2,1.0
fairness accountability and transparency,2,2,1.0
accountability and transparency fat,2,2,1.0
and transparency fat ng,2,2,1.0
transparency fat ng hu,2,2,1.0
fat ng hu yeung,2,2,1.0
ng hu yeung yin,2,2,1.0
hu yeung yin roli,2,2,1.0
yeung yin roli diversified,2,2,1.0
yin roli diversified undersampling,2,2,1.0
roli diversified undersampling for,2,2,1.0
diversified undersampling for imbalance,2,2,1.0
undersampling for imbalance classification,2,2,1.0
for imbalance classification problems,2,2,1.0
imbalance classification problems ieee,2,2,1.0
classification problems ieee transactions,2,2,1.0
problems ieee transactions on,2,2,1.0
ieee transactions on cybernetics,2,2,1.0
transactions on cybernetics nguyen,2,2,1.0
on cybernetics nguyen cooper,2,2,1.0
cybernetics nguyen cooper kamei,2,2,1.0
nguyen cooper kamei borderline,2,2,1.0
cooper kamei borderline for,2,2,1.0
kamei borderline for data,1,1,1.0
borderline for data classification,1,1,1.0
for data classification international,1,1,1.0
data classification international journal,2,2,1.0
classification international journal of,2,2,1.0
international journal of knowledge,3,3,1.0
journal of knowledge engineering,3,3,1.0
of knowledge engineering and,3,3,1.0
knowledge engineering and soft,3,3,1.0
engineering and soft data,3,3,1.0
and soft data paradigms,2,2,1.0
soft data paradigms nugent,2,2,1.0
data paradigms nugent doyle,2,2,1.0
paradigms nugent doyle cunningham,2,2,1.0
nugent doyle cunningham gaining,2,2,1.0
doyle cunningham gaining insight,2,2,1.0
cunningham gaining insight through,2,2,1.0
gaining insight through explanation,2,2,1.0
insight through explanation journal,2,2,1.0
through explanation journal of,2,2,1.0
explanation journal of intelligent,2,2,1.0
journal of intelligent information,2,2,1.0
of intelligent information systems,2,2,1.0
intelligent information systems pitis,2,2,1.0
information systems pitis creager,2,2,1.0
systems pitis creager garg,2,2,1.0
pitis creager garg a,2,2,1.0
creager garg a counterfactual,2,2,1.0
garg a counterfactual data,2,2,1.0
a counterfactual data augmentation,2,2,1.0
data augmentation using locally,2,2,1.0
augmentation using locally factored,2,2,1.0
using locally factored dynamics,2,2,1.0
locally factored dynamics advances,2,2,1.0
factored dynamics advances in,2,2,1.0
dynamics advances in neural,2,2,1.0
advances in neural information,3,3,1.0
in neural information processing,3,3,1.0
neural information processing systems,4,3,1.3333333333333333
information processing systems temraz,2,2,1.0
processing systems temraz keane,2,2,1.0
systems temraz keane counterfactual,2,2,1.0
counterfactual data augmentation provost,2,2,1.0
data augmentation provost machine,2,2,1.0
augmentation provost machine learning,2,2,1.0
provost machine learning from,2,2,1.0
machine learning from imbalanced,2,2,1.0
from imbalanced data sets,18,4,4.5
imbalanced data sets aaai,2,2,1.0
data sets aaai workshop,2,2,1.0
sets aaai workshop on,2,2,1.0
aaai workshop on imbalanced,2,2,1.0
workshop on imbalanced data,2,2,1.0
on imbalanced data sets,4,3,1.3333333333333333
imbalanced data sets prusty,2,2,1.0
data sets prusty jayanthi,2,2,1.0
sets prusty jayanthi velusamy,2,2,1.0
prusty jayanthi velusamy a,2,2,1.0
jayanthi velusamy a modification,2,2,1.0
velusamy a modification to,2,2,1.0
a modification to smote,2,2,1.0
modification to smote for,2,2,1.0
to smote for event,2,2,1.0
smote for event classification,2,2,1.0
for event classification in,2,2,1.0
event classification in sodium,2,2,1.0
classification in sodium cooled,2,2,1.0
in sodium cooled fast,2,2,1.0
sodium cooled fast reactors,2,2,1.0
cooled fast reactors progress,2,2,1.0
fast reactors progress in,2,2,1.0
reactors progress in nuclear,2,2,1.0
progress in nuclear energy,2,2,1.0
in nuclear energy ramentol,2,2,1.0
nuclear energy ramentol caballero,2,2,1.0
energy ramentol caballero bello,2,2,1.0
ramentol caballero bello herrera,2,2,1.0
caballero bello herrera a,2,2,1.0
bello herrera a hybrid,2,2,1.0
herrera a hybrid preprocessing,2,2,1.0
a hybrid preprocessing approach,2,2,1.0
hybrid preprocessing approach based,2,2,1.0
preprocessing approach based on,2,2,1.0
based on oversampling and,2,2,1.0
on oversampling and undersampling,2,2,1.0
oversampling and undersampling for,2,2,1.0
and undersampling for high,2,2,1.0
undersampling for high imbalanced,2,2,1.0
for high imbalanced using,2,2,1.0
high imbalanced using smote,2,2,1.0
imbalanced using smote and,2,2,1.0
using smote and rough,2,2,1.0
smote and rough sets,2,2,1.0
and rough sets theory,2,2,1.0
rough sets theory knowledge,2,2,1.0
sets theory knowledge and,2,2,1.0
theory knowledge and information,2,2,1.0
knowledge and information systems,2,2,1.0
and information systems ryan,2,2,1.0
information systems ryan gúeret,2,2,1.0
systems ryan gúeret berry,2,2,1.0
ryan gúeret berry corcoran,2,2,1.0
gúeret berry corcoran keane,2,2,1.0
berry corcoran keane mac,2,2,1.0
corcoran keane mac namee,2,2,1.0
keane mac namee b,2,2,1.0
mac namee b predicting,2,2,1.0
namee b predicting illness,2,2,1.0
b predicting illness for,2,2,1.0
predicting illness for a,2,2,1.0
illness for a sustainable,2,2,1.0
for a sustainable dairy,2,2,1.0
a sustainable dairy agriculture,2,2,1.0
sustainable dairy agriculture predicting,2,2,1.0
dairy agriculture predicting and,2,2,1.0
agriculture predicting and explaining,2,2,1.0
predicting and explaining the,2,2,1.0
and explaining the onset,2,2,1.0
explaining the onset of,2,2,1.0
the onset of mastitis,2,2,1.0
onset of mastitis in,2,2,1.0
of mastitis in dairy,2,2,1.0
mastitis in dairy cows,2,2,1.0
in dairy cows in,2,2,1.0
dairy cows in workshop,2,2,1.0
cows in workshop on,2,2,1.0
in workshop on explainable,2,2,1.0
workshop on explainable agency,2,2,1.0
on explainable agency in,2,2,1.0
explainable agency in ai,2,2,1.0
agency in ai xai,2,2,1.0
in ai xai sandhan,2,2,1.0
ai xai sandhan choi,2,2,1.0
xai sandhan choi y,2,2,1.0
sandhan choi y handling,2,2,1.0
choi y handling imbalanced,2,2,1.0
y handling imbalanced datasets,2,2,1.0
handling imbalanced datasets by,2,2,1.0
imbalanced datasets by partially,2,2,1.0
datasets by partially guided,2,2,1.0
by partially guided sampling,1,1,1.0
partially guided sampling for,1,1,1.0
guided sampling for pattern,1,1,1.0
sampling for pattern recognition,2,2,1.0
for pattern recognition international,2,2,1.0
pattern recognition international conference,2,2,1.0
recognition international conference on,2,2,1.0
international conference on pattern,2,2,1.0
conference on pattern recognition,2,2,1.0
on pattern recognition schleich,2,2,1.0
pattern recognition schleich geng,2,2,1.0
recognition schleich geng zhang,2,2,1.0
schleich geng zhang suciu,2,2,1.0
geng zhang suciu geco,2,2,1.0
zhang suciu geco quality,2,2,1.0
suciu geco quality counterfactual,2,2,1.0
geco quality counterfactual explanations,2,2,1.0
quality counterfactual explanations in,2,2,1.0
counterfactual explanations in real,2,2,1.0
explanations in real time,2,2,1.0
in real time arxiv,2,2,1.0
real time arxiv preprint,2,2,1.0
time arxiv preprint shorten,2,2,1.0
arxiv preprint shorten khoshgoftaar,2,2,1.0
preprint shorten khoshgoftaar a,2,2,1.0
shorten khoshgoftaar a survey,2,2,1.0
khoshgoftaar a survey on,2,2,1.0
a survey on image,2,2,1.0
survey on image data,2,2,1.0
on image data augmentation,2,2,1.0
image data augmentation for,2,2,1.0
for deep learning journal,2,2,1.0
deep learning journal of,2,2,1.0
learning journal of big,2,2,1.0
journal of big data,2,2,1.0
of big data smyth,2,2,1.0
big data smyth keane,2,2,1.0
data smyth keane a,2,2,1.0
smyth keane a few,2,2,1.0
keane a few good,2,2,1.0
a few good counterfactuals,2,2,1.0
few good counterfactuals generating,2,2,1.0
good counterfactuals generating interpretable,2,2,1.0
counterfactuals generating interpretable plausible,2,2,1.0
generating interpretable plausible and,2,2,1.0
interpretable plausible and diverse,2,2,1.0
plausible and diverse counterfactual,2,2,1.0
and diverse counterfactual explanations,2,2,1.0
diverse counterfactual explanations arxiv,2,2,1.0
counterfactual explanations arxiv preprint,2,2,1.0
explanations arxiv preprint subbaswamy,2,2,1.0
arxiv preprint subbaswamy saria,2,2,1.0
preprint subbaswamy saria counterfactual,2,2,1.0
subbaswamy saria counterfactual normalization,2,2,1.0
saria counterfactual normalization proactively,2,2,1.0
counterfactual normalization proactively addressing,2,2,1.0
normalization proactively addressing dataset,2,2,1.0
proactively addressing dataset shift,2,2,1.0
addressing dataset shift using,2,2,1.0
dataset shift using causal,2,2,1.0
shift using causal mechanisms,2,2,1.0
using causal mechanisms in,2,2,1.0
causal mechanisms in proceedings,2,2,1.0
mechanisms in proceedings of,2,2,1.0
the conference on uncertainty,2,2,1.0
conference on uncertainty in,3,3,1.0
on uncertainty in artificial,3,3,1.0
uncertainty in artificial intelligence,3,3,1.0
in artificial intelligence uai,2,2,1.0
artificial intelligence uai tek,2,2,1.0
intelligence uai tek dempster,2,2,1.0
uai tek dempster kale,2,2,1.0
tek dempster kale parasite,2,2,1.0
dempster kale parasite detection,2,2,1.0
kale parasite detection and,2,2,1.0
parasite detection and identification,2,2,1.0
detection and identification for,2,2,1.0
and identification for automated,2,2,1.0
identification for automated thin,2,2,1.0
for automated thin blood,2,2,1.0
automated thin blood film,2,2,1.0
thin blood film malaria,2,2,1.0
blood film malaria diagnosis,2,2,1.0
film malaria diagnosis computer,2,2,1.0
malaria diagnosis computer vision,2,2,1.0
diagnosis computer vision and,2,2,1.0
computer vision and image,2,2,1.0
vision and image understanding,2,2,1.0
and image understanding temraz,2,2,1.0
image understanding temraz kenny,2,2,1.0
understanding temraz kenny ruelle,2,2,1.0
temraz kenny ruelle shalloo,2,2,1.0
kenny ruelle shalloo smyth,2,2,1.0
ruelle shalloo smyth keane,2,2,1.0
shalloo smyth keane handling,2,2,1.0
smyth keane handling climate,2,2,1.0
keane handling climate change,2,2,1.0
handling climate change using,2,2,1.0
climate change using counterfactuals,2,2,1.0
change using counterfactuals using,2,2,1.0
using counterfactuals using counterfactuals,2,2,1.0
counterfactuals using counterfactuals in,2,2,1.0
using counterfactuals in data,2,2,1.0
counterfactuals in data augmentation,1,1,1.0
in data augmentation to,1,1,1.0
data augmentation to predict,1,1,1.0
augmentation to predict crop,1,1,1.0
to predict crop growth,2,2,1.0
predict crop growth in,2,2,1.0
crop growth in an,2,2,1.0
growth in an uncertain,2,2,1.0
in an uncertain climate,2,2,1.0
an uncertain climate future,2,2,1.0
uncertain climate future in,2,2,1.0
climate future in international,2,2,1.0
future in international conference,2,2,1.0
reasoning springer germany torres,2,2,1.0
springer germany torres a,2,2,1.0
germany torres a deterministic,2,2,1.0
torres a deterministic version,2,2,1.0
a deterministic version of,2,2,1.0
deterministic version of smote,2,2,1.0
version of smote pattern,2,2,1.0
of smote pattern recognition,2,2,1.0
smote pattern recognition temraz,2,2,1.0
pattern recognition temraz keane,2,2,1.0
recognition temraz keane counterfactual,2,2,1.0
counterfactual data augmentation vapnik,2,2,1.0
data augmentation vapnik statistical,2,2,1.0
augmentation vapnik statistical learning,2,2,1.0
vapnik statistical learning theory,2,2,1.0
statistical learning theory wiley,2,2,1.0
learning theory wiley wachter,2,2,1.0
theory wiley wachter mittelstadt,2,2,1.0
wiley wachter mittelstadt russell,2,2,1.0
wachter mittelstadt russell counterfactual,2,2,1.0
mittelstadt russell counterfactual explanations,2,2,1.0
russell counterfactual explanations without,2,2,1.0
counterfactual explanations without opening,2,2,1.0
explanations without opening the,2,2,1.0
without opening the black,2,2,1.0
opening the black box,2,2,1.0
the black box automated,2,2,1.0
black box automated decisions,2,2,1.0
box automated decisions and,2,2,1.0
automated decisions and the,2,2,1.0
decisions and the gdpr,2,2,1.0
and the gdpr harvard,2,2,1.0
the gdpr harvard journal,2,2,1.0
gdpr harvard journal of,2,2,1.0
harvard journal of law,2,2,1.0
journal of law technology,2,2,1.0
of law technology wang,2,2,1.0
law technology wang xu,2,2,1.0
technology wang xu wang,2,2,1.0
wang xu wang zhang,2,2,1.0
xu wang zhang j,2,2,1.0
wang zhang j classification,2,2,1.0
zhang j classification of,2,2,1.0
j classification of imbalanced,2,2,1.0
classification of imbalanced data,3,3,1.0
of imbalanced data by,2,2,1.0
imbalanced data by using,2,2,1.0
data by using the,2,2,1.0
by using the smote,2,2,1.0
using the smote algorithm,3,3,1.0
the smote algorithm and,2,2,1.0
smote algorithm and locally,2,2,1.0
algorithm and locally linear,2,2,1.0
and locally linear embedding,2,2,1.0
locally linear embedding international,2,2,1.0
linear embedding international conference,2,2,1.0
embedding international conference on,2,2,1.0
international conference on signal,2,2,1.0
conference on signal processing,2,2,1.0
on signal processing weiss,2,2,1.0
signal processing weiss mccarthy,2,2,1.0
processing weiss mccarthy zabar,2,2,1.0
weiss mccarthy zabar b,2,2,1.0
mccarthy zabar b learning,2,2,1.0
zabar b learning sampling,2,2,1.0
b learning sampling which,2,2,1.0
learning sampling which is,2,2,1.0
sampling which is best,2,2,1.0
which is best for,2,2,1.0
is best for handling,2,2,1.0
best for handling unbalanced,2,2,1.0
for handling unbalanced classes,2,2,1.0
handling unbalanced classes with,2,2,1.0
unbalanced classes with unequal,2,2,1.0
classes with unequal error,2,2,1.0
with unequal error costs,2,2,1.0
unequal error costs in,2,2,1.0
error costs in of,1,1,1.0
costs in of the,1,1,1.0
in of the international,1,1,1.0
of the international conference,15,5,3.0
the international conference on,15,5,3.0
on data mining dmin,2,2,1.0
data mining dmin wen,2,2,1.0
mining dmin wen sun,2,2,1.0
dmin wen sun yang,2,2,1.0
wen sun yang song,2,2,1.0
sun yang song gao,2,2,1.0
yang song gao xue,2,2,1.0
song gao xue huan,2,2,1.0
gao xue huan x,2,2,1.0
xue huan x time,2,2,1.0
huan x time series,2,2,1.0
x time series data,2,2,1.0
time series data augmentation,2,2,1.0
series data augmentation for,2,2,1.0
for deep learning a,2,2,1.0
deep learning a survey,2,2,1.0
learning a survey arxiv,2,2,1.0
a survey arxiv preprint,2,2,1.0
survey arxiv preprint wong,2,2,1.0
arxiv preprint wong gatt,2,2,1.0
preprint wong gatt stamatescu,2,2,1.0
wong gatt stamatescu mcdonnell,2,2,1.0
gatt stamatescu mcdonnell understanding,2,2,1.0
stamatescu mcdonnell understanding data,2,2,1.0
mcdonnell understanding data augmentation,2,2,1.0
understanding data augmentation for,2,2,1.0
data augmentation for classification,2,2,1.0
augmentation for classification when,2,2,1.0
for classification when to,2,2,1.0
classification when to warp,2,2,1.0
when to warp in,2,2,1.0
to warp in international,2,2,1.0
warp in international conference,2,2,1.0
international conference on digital,2,2,1.0
conference on digital image,2,2,1.0
on digital image computing,2,2,1.0
digital image computing techniques,2,2,1.0
image computing techniques and,2,2,1.0
computing techniques and applications,2,2,1.0
techniques and applications dicta,2,2,1.0
and applications dicta ye,2,2,1.0
applications dicta ye leake,2,2,1.0
dicta ye leake jalali,2,2,1.0
ye leake jalali crandall,2,2,1.0
leake jalali crandall learning,2,2,1.0
jalali crandall learning adaptations,2,2,1.0
crandall learning adaptations for,2,2,1.0
learning adaptations for classification,2,2,1.0
adaptations for classification a,2,2,1.0
for classification a neural,2,2,1.0
classification a neural network,2,2,1.0
a neural network approach,2,2,1.0
neural network approach in,2,2,1.0
network approach in international,2,2,1.0
approach in international conference,2,2,1.0
reasoning springer germany yun,2,2,1.0
springer germany yun ha,2,2,1.0
germany yun ha lee,2,2,1.0
yun ha lee j,2,2,1.0
ha lee j automatic,2,2,1.0
lee j automatic determination,2,2,1.0
j automatic determination of,2,2,1.0
automatic determination of neighborhood,2,2,1.0
determination of neighborhood size,2,2,1.0
of neighborhood size in,2,2,1.0
neighborhood size in smote,2,2,1.0
size in smote in,2,2,1.0
in smote in proceedings,2,2,1.0
smote in proceedings of,2,2,1.0
international conference on ubiquitous,2,2,1.0
conference on ubiquitous information,2,2,1.0
on ubiquitous information management,2,2,1.0
ubiquitous information management and,2,2,1.0
information management and communication,2,2,1.0
management and communication zeng,2,2,1.0
and communication zeng li,2,2,1.0
communication zeng li zhai,2,2,1.0
zeng li zhai zhang,2,2,1.0
li zhai zhang y,2,2,1.0
zhai zhang y counterfactual,2,2,1.0
zhang y counterfactual generator,2,2,1.0
y counterfactual generator in,2,2,1.0
counterfactual generator in proceedings,2,2,1.0
generator in proceedings of,2,2,1.0
the conference on empirical,2,2,1.0
conference on empirical methods,2,2,1.0
on empirical methods in,2,2,1.0
empirical methods in natural,2,2,1.0
methods in natural language,2,2,1.0
in natural language processing,2,2,1.0
natural language processing zheng,2,2,1.0
language processing zheng wu,2,2,1.0
processing zheng wu srihari,2,2,1.0
zheng wu srihari feature,2,2,1.0
wu srihari feature selection,2,2,1.0
srihari feature selection for,3,3,1.0
feature selection for text,3,3,1.0
selection for text categorization,3,3,1.0
for text categorization on,3,3,1.0
text categorization on imbalanced,3,3,1.0
categorization on imbalanced data,3,3,1.0
on imbalanced data acm,2,2,1.0
imbalanced data acm sigkdd,2,2,1.0
data acm sigkdd explorations,3,3,1.0
international journal of computational,8,1,8.0
journal of computational intelligence,8,1,8.0
of computational intelligence systems,8,1,8.0
computational intelligence systems vol,1,1,1.0
intelligence systems vol pp,1,1,1.0
systems vol pp doi,1,1,1.0
vol pp doi https,1,1,1.0
pp doi https issn,1,1,1.0
doi https issn eissn,1,1,1.0
https issn eissn https,1,1,1.0
issn eissn https distributed,1,1,1.0
eissn https distributed synthetic,1,1,1.0
https distributed synthetic minority,1,1,1.0
distributed synthetic minority oversampling,1,1,1.0
minority oversampling technique sakshi,1,1,1.0
oversampling technique sakshi suman,1,1,1.0
technique sakshi suman research,1,1,1.0
sakshi suman research scholar,1,1,1.0
suman research scholar ipu,1,1,1.0
research scholar ipu new,1,1,1.0
scholar ipu new delhi,1,1,1.0
ipu new delhi india,1,1,1.0
new delhi india associate,1,1,1.0
delhi india associate professor,1,1,1.0
india associate professor msit,1,1,1.0
associate professor msit new,1,1,1.0
professor msit new delhi,1,1,1.0
msit new delhi india,1,1,1.0
new delhi india a,1,1,1.0
delhi india a rt,1,1,1.0
india a rt i,1,1,1.0
a rt i c,1,1,1.0
rt i c l,1,1,1.0
i c l e,1,1,1.0
c l e i,1,1,1.0
l e i n,1,1,1.0
e i n f,1,1,1.0
i n f o,1,1,1.0
n f o article,1,1,1.0
f o article history,1,1,1.0
o article history received,1,1,1.0
article history received may,1,1,1.0
history received may accepted,1,1,1.0
received may accepted jul,1,1,1.0
may accepted jul keywords,1,1,1.0
accepted jul keywords smote,1,1,1.0
jul keywords smote apache,1,1,1.0
keywords smote apache spark,1,1,1.0
smote apache spark prediction,1,1,1.0
apache spark prediction machine,1,1,1.0
spark prediction machine learning,1,1,1.0
prediction machine learning imbalanced,1,1,1.0
machine learning imbalanced classification,1,1,1.0
learning imbalanced classification a,1,1,1.0
imbalanced classification a b,1,1,1.0
classification a b s,1,1,1.0
a b s t,1,1,1.0
b s t r,1,1,1.0
s t r ac,1,1,1.0
t r ac t,1,1,1.0
r ac t real,1,1,1.0
ac t real world,1,1,1.0
t real world problems,1,1,1.0
real world problems for,1,1,1.0
world problems for prediction,1,1,1.0
problems for prediction usually,1,1,1.0
for prediction usually try,1,1,1.0
prediction usually try to,1,1,1.0
usually try to predict,1,1,1.0
try to predict rare,1,1,1.0
to predict rare occurrences,1,1,1.0
predict rare occurrences application,1,1,1.0
rare occurrences application of,1,1,1.0
occurrences application of standard,1,1,1.0
application of standard classification,1,1,1.0
of standard classification algorithm,1,1,1.0
standard classification algorithm is,1,1,1.0
classification algorithm is biased,1,1,1.0
algorithm is biased toward,1,1,1.0
is biased toward against,1,1,1.0
biased toward against these,1,1,1.0
toward against these rare,1,1,1.0
against these rare events,1,1,1.0
these rare events due,1,1,1.0
rare events due to,1,1,1.0
events due to this,1,1,1.0
due to this data,1,1,1.0
to this data imbalance,1,1,1.0
this data imbalance typicalapproaches,1,1,1.0
data imbalance typicalapproaches to,1,1,1.0
imbalance typicalapproaches to solve,1,1,1.0
typicalapproaches to solve this,1,1,1.0
to solve this data,1,1,1.0
solve this data imbalance,1,1,1.0
this data imbalance involve,1,1,1.0
data imbalance involve oversampling,1,1,1.0
imbalance involve oversampling these,1,1,1.0
involve oversampling these rare,1,1,1.0
oversampling these rare events,1,1,1.0
these rare events or,1,1,1.0
rare events or under,1,1,1.0
events or under sampling,1,1,1.0
or under sampling the,1,1,1.0
under sampling the majority,1,1,1.0
sampling the majority occurring,1,1,1.0
the majority occurring events,1,1,1.0
majority occurring events synthetic,1,1,1.0
occurring events synthetic minority,1,1,1.0
events synthetic minority oversampling,1,1,1.0
minority oversampling technique is,1,1,1.0
oversampling technique is one,1,1,1.0
technique is one technique,1,1,1.0
is one technique that,1,1,1.0
one technique that addresses,1,1,1.0
technique that addresses this,1,1,1.0
that addresses this class,1,1,1.0
addresses this class imbalance,1,1,1.0
this class imbalance effectively,1,1,1.0
class imbalance effectively however,1,1,1.0
imbalance effectively however the,1,1,1.0
effectively however the existing,1,1,1.0
however the existing implementations,1,1,1.0
the existing implementations of,1,1,1.0
existing implementations of smote,1,1,1.0
implementations of smote fail,1,1,1.0
of smote fail when,1,1,1.0
smote fail when data,1,1,1.0
fail when data grows,1,1,1.0
when data grows and,1,1,1.0
data grows and can,1,1,1.0
grows and can t,1,1,1.0
and can t be,1,1,1.0
can t be stored,1,1,1.0
t be stored on,1,1,1.0
be stored on a,1,1,1.0
stored on a single,1,1,1.0
on a single machine,3,1,3.0
a single machine in,1,1,1.0
single machine in this,1,1,1.0
machine in this paper,1,1,1.0
in this paper present,1,1,1.0
this paper present our,1,1,1.0
paper present our solution,1,1,1.0
present our solution to,1,1,1.0
our solution to address,1,1,1.0
solution to address the,1,1,1.0
to address the big,1,1,1.0
address the big data,1,1,1.0
the big data challenge,1,1,1.0
big data challenge we,1,1,1.0
data challenge we provide,1,1,1.0
challenge we provide a,1,1,1.0
we provide a distributed,1,1,1.0
provide a distributed version,1,1,1.0
a distributed version of,1,1,1.0
distributed version of smote,2,1,2.0
version of smote by,1,1,1.0
of smote by using,1,1,1.0
smote by using scalable,1,1,1.0
by using scalable and,1,1,1.0
using scalable and with,1,1,1.0
scalable and with this,1,1,1.0
and with this implementation,1,1,1.0
with this implementation of,2,1,2.0
this implementation of smote,1,1,1.0
implementation of smote we,2,1,2.0
of smote we were,1,1,1.0
smote we were able,1,1,1.0
we were able to,1,1,1.0
were able to oversample,1,1,1.0
able to oversample the,1,1,1.0
to oversample the rare,1,1,1.0
oversample the rare events,1,1,1.0
the rare events and,1,1,1.0
rare events and achieve,1,1,1.0
events and achieve results,1,1,1.0
and achieve results which,1,1,1.0
achieve results which are,1,1,1.0
results which are better,1,1,1.0
which are better than,1,1,1.0
are better than the,1,1,1.0
better than the existing,1,1,1.0
than the existing python,1,1,1.0
the existing python version,1,1,1.0
existing python version of,1,1,1.0
python version of smote,5,1,5.0
version of smote the,1,1,1.0
of smote the authors,1,1,1.0
smote the authors published,1,1,1.0
the authors published by,1,1,1.0
authors published by atlantis,1,1,1.0
published by atlantis press,1,1,1.0
by atlantis press sarl,1,1,1.0
atlantis press sarl this,1,1,1.0
press sarl this is,1,1,1.0
sarl this is an,1,1,1.0
this is an open,1,1,1.0
is an open access,1,1,1.0
an open access article,1,1,1.0
open access article distributed,1,1,1.0
access article distributed under,1,1,1.0
article distributed under the,1,1,1.0
distributed under the cc,1,1,1.0
under the cc license,1,1,1.0
the cc license http,1,1,1.0
cc license http introduction,1,1,1.0
license http introduction in,1,1,1.0
http introduction in the,1,1,1.0
introduction in the aspect,1,1,1.0
in the aspect of,1,1,1.0
the aspect of machine,1,1,1.0
aspect of machine learning,1,1,1.0
of machine learning one,1,1,1.0
machine learning one of,1,1,1.0
learning one of the,1,1,1.0
of the most popular,1,1,1.0
the most popular issues,1,1,1.0
most popular issues involves,1,1,1.0
popular issues involves classification,1,1,1.0
issues involves classification in,1,1,1.0
involves classification in particular,1,1,1.0
classification in particular it,1,1,1.0
in particular it is,1,1,1.0
particular it is expected,1,1,1.0
it is expected that,1,1,1.0
is expected that the,1,1,1.0
expected that the data,1,1,1.0
that the data sets,1,1,1.0
the data sets containing,1,1,1.0
data sets containing information,1,1,1.0
sets containing information for,1,1,1.0
containing information for extraction,1,1,1.0
information for extraction constitute,1,1,1.0
for extraction constitute all,1,1,1.0
extraction constitute all the,1,1,1.0
constitute all the required,1,1,1.0
all the required content,1,1,1.0
the required content through,1,1,1.0
required content through which,1,1,1.0
content through which relevant,1,1,1.0
through which relevant concepts,1,1,1.0
which relevant concepts can,1,1,1.0
relevant concepts can be,1,1,1.0
concepts can be learned,1,1,1.0
can be learned especially,1,1,1.0
be learned especially in,1,1,1.0
learned especially in relation,1,1,1.0
especially in relation to,1,1,1.0
in relation to certain,1,1,1.0
relation to certain underlying,1,1,1.0
to certain underlying generating,1,1,1.0
certain underlying generating functions,1,1,1.0
underlying generating functions given,1,1,1.0
generating functions given a,1,1,1.0
functions given a sification,1,1,1.0
given a sification problem,1,1,1.0
a sification problem what,1,1,1.0
sification problem what remains,1,1,1.0
problem what remains notable,1,1,1.0
what remains notable is,1,1,1.0
remains notable is that,1,1,1.0
notable is that it,1,1,1.0
is that it is,1,1,1.0
that it is challenging,1,1,1.0
it is challenging to,1,1,1.0
is challenging to predict,1,1,1.0
challenging to predict the,1,1,1.0
to predict the outcomes,1,1,1.0
predict the outcomes some,1,1,1.0
the outcomes some of,1,1,1.0
outcomes some of these,1,1,1.0
some of these classification,1,1,1.0
of these classification problems,1,1,1.0
these classification problems include,1,1,1.0
classification problems include oil,1,1,1.0
problems include oil spills,1,1,1.0
include oil spills fraud,1,1,1.0
oil spills fraud detection,1,1,1.0
spills fraud detection and,1,1,1.0
fraud detection and intrusion,1,1,1.0
detection and intrusion detection,1,1,1.0
and intrusion detection a,1,1,1.0
intrusion detection a dilemma,1,1,1.0
detection a dilemma that,1,1,1.0
a dilemma that translates,1,1,1.0
dilemma that translates into,1,1,1.0
that translates into a,1,1,1.0
translates into a state,1,1,1.0
into a state of,1,1,1.0
a state of class,1,1,1.0
state of class imbalance,1,1,1.0
of class imbalance in,2,2,1.0
class imbalance in classes,1,1,1.0
imbalance in classes it,1,1,1.0
in classes it is,1,1,1.0
classes it is also,1,1,1.0
it is also worth,1,1,1.0
is also worth indicating,1,1,1.0
also worth indicating that,1,1,1.0
worth indicating that the,1,1,1.0
indicating that the resultant,1,1,1.0
that the resultant state,1,1,1.0
the resultant state of,1,1,1.0
resultant state of asymmetry,1,1,1.0
state of asymmetry could,1,1,1.0
of asymmetry could be,1,1,1.0
asymmetry could be puzzled,1,1,1.0
could be puzzled due,1,1,1.0
be puzzled due to,1,1,1.0
puzzled due to the,1,1,1.0
due to the infrequent,1,1,1.0
to the infrequent nature,1,1,1.0
the infrequent nature of,1,1,1.0
infrequent nature of such,1,1,1.0
nature of such rare,1,1,1.0
of such rare events,1,1,1.0
such rare events instead,1,1,1.0
rare events instead the,1,1,1.0
events instead the events,1,1,1.0
instead the events emerge,1,1,1.0
the events emerge as,1,1,1.0
events emerge as exceptional,1,1,1.0
emerge as exceptional situations,1,1,1.0
as exceptional situations that,1,1,1.0
exceptional situations that are,1,1,1.0
situations that are mostly,1,1,1.0
that are mostly neglected,1,1,1.0
are mostly neglected or,1,1,1.0
mostly neglected or unexplored,1,1,1.0
neglected or unexplored in,1,1,1.0
or unexplored in some,1,1,1.0
unexplored in some cases,1,1,1.0
in some cases these,1,1,1.0
some cases these events,1,1,1.0
cases these events have,1,1,1.0
these events have been,1,1,1.0
events have been treated,1,1,1.0
have been treated as,1,1,1.0
been treated as extreme,1,1,1.0
treated as extreme values,1,1,1.0
as extreme values including,1,1,1.0
extreme values including outliers,1,1,1.0
values including outliers or,1,1,1.0
including outliers or noisy,1,1,1.0
outliers or noisy disturbance,1,1,1.0
or noisy disturbance it,1,1,1.0
noisy disturbance it is,1,1,1.0
disturbance it is also,1,1,1.0
it is also ironical,1,1,1.0
is also ironical that,1,1,1.0
also ironical that most,1,1,1.0
ironical that most of,1,1,1.0
that most of the,1,1,1.0
most of the classes,1,1,1.0
of the classes attract,1,1,1.0
the classes attract greater,1,1,1.0
classes attract greater concern,1,1,1.0
attract greater concern and,1,1,1.0
greater concern and much,1,1,1.0
concern and much significance,1,1,1.0
and much significance and,1,1,1.0
much significance and acknowledgment,1,1,1.0
significance and acknowledgment are,1,1,1.0
and acknowledgment are extended,1,1,1.0
acknowledgment are extended to,1,1,1.0
are extended to them,1,1,1.0
extended to them including,1,1,1.0
to them including cases,1,1,1.0
them including cases such,1,1,1.0
including cases such as,1,1,1.0
cases such as those,1,1,1.0
such as those involving,1,1,1.0
as those involving the,1,1,1.0
those involving the identification,1,1,1.0
involving the identification of,1,1,1.0
the identification of credit,1,1,1.0
identification of credit card,1,1,1.0
of credit card data,1,1,1.0
credit card data breaches,1,1,1.0
card data breaches in,1,1,1.0
data breaches in online,1,1,1.0
breaches in online transactions,1,1,1.0
in online transactions as,1,1,1.0
online transactions as such,1,1,1.0
transactions as such most,1,1,1.0
as such most of,1,1,1.0
such most of the,1,1,1.0
most of the data,1,1,1.0
of the data experiences,1,1,1.0
the data experiences irregular,1,1,1.0
data experiences irregular or,1,1,1.0
experiences irregular or disproportionate,1,1,1.0
irregular or disproportionate observation,1,1,1.0
or disproportionate observation ratios,1,1,1.0
disproportionate observation ratios in,1,1,1.0
observation ratios in ferent,1,1,1.0
ratios in ferent classes,1,1,1.0
in ferent classes and,1,1,1.0
ferent classes and end,1,1,1.0
classes and end up,1,1,1.0
and end up yielding,1,1,1.0
end up yielding unacceptable,1,1,1.0
up yielding unacceptable classification,1,1,1.0
yielding unacceptable classification to,1,1,1.0
unacceptable classification to fraud,1,1,1.0
classification to fraud case,1,1,1.0
to fraud case identification,1,1,1.0
fraud case identification this,1,1,1.0
case identification this trend,1,1,1.0
identification this trend points,1,1,1.0
this trend points to,1,1,1.0
trend points to the,1,1,1.0
points to the criticality,1,1,1.0
to the criticality of,1,1,1.0
the criticality of a,1,1,1.0
criticality of a relevant,1,1,1.0
of a relevant model,1,1,1.0
a relevant model capable,1,1,1.0
relevant model capable of,1,1,1.0
model capable of identifying,1,1,1.0
capable of identifying the,1,1,1.0
of identifying the minority,1,1,1.0
identifying the minority classes,1,1,1.0
the minority classes or,1,1,1.0
minority classes or rare,1,1,1.0
classes or rare occurrences,1,1,1.0
or rare occurrences in,1,1,1.0
rare occurrences in various,1,1,1.0
occurrences in various sets,1,1,1.0
in various sets of,1,1,1.0
various sets of data,1,1,1.0
sets of data associated,1,1,1.0
of data associated with,1,1,1.0
data associated with higher,1,1,1.0
associated with higher accuracy,1,1,1.0
with higher accuracy as,1,1,1.0
higher accuracy as mentioned,1,1,1.0
accuracy as mentioned earlier,1,1,1.0
as mentioned earlier most,1,1,1.0
mentioned earlier most of,1,1,1.0
earlier most of the,1,1,1.0
most of the minority,1,1,1.0
of the minority ples,1,1,1.0
the minority ples have,1,1,1.0
minority ples have continually,1,1,1.0
ples have continually been,1,1,1.0
have continually been treated,1,1,1.0
continually been treated as,1,1,1.0
been treated as noise,1,1,1.0
treated as noise and,1,1,1.0
as noise and ended,1,1,1.0
noise and ended up,1,1,1.0
and ended up being,1,1,1.0
ended up being ignored,1,1,1.0
up being ignored the,1,1,1.0
being ignored the eventuality,1,1,1.0
ignored the eventuality is,1,1,1.0
the eventuality is that,2,1,2.0
eventuality is that valuable,1,1,1.0
is that valuable data,1,1,1.0
that valuable data has,1,1,1.0
valuable data has been,1,1,1.0
data has been lost,1,1,1.0
has been lost relative,1,1,1.0
been lost relative corresponding,1,1,1.0
lost relative corresponding author,1,1,1.0
relative corresponding author email,1,1,1.0
corresponding author email sakshihoodars,1,1,1.0
author email sakshihoodars to,1,1,1.0
email sakshihoodars to the,1,1,1.0
sakshihoodars to the affected,1,1,1.0
to the affected samples,1,1,1.0
the affected samples an,1,1,1.0
affected samples an example,1,1,1.0
samples an example is,1,1,1.0
an example is a,2,2,1.0
example is a case,1,1,1.0
is a case in,1,1,1.0
a case in which,2,1,2.0
case in which a,1,1,1.0
in which a given,1,1,1.0
which a given set,1,1,1.0
a given set of,2,1,2.0
given set of data,2,1,2.0
set of data exhibits,1,1,1.0
of data exhibits an,1,1,1.0
data exhibits an imbalanced,1,1,1.0
exhibits an imbalanced ratio,1,1,1.0
an imbalanced ratio in,1,1,1.0
imbalanced ratio in the,1,1,1.0
ratio in the form,1,1,1.0
in the form this,1,1,1.0
the form this ple,1,1,1.0
form this ple ratio,1,1,1.0
this ple ratio implies,1,1,1.0
ple ratio implies that,1,1,1.0
ratio implies that training,1,1,1.0
implies that training examples,1,1,1.0
that training examples of,1,1,1.0
training examples of a,1,1,1.0
examples of a negative,1,1,1.0
of a negative class,1,1,1.0
a negative class cide,1,1,1.0
negative class cide with,1,1,1.0
class cide with one,1,1,1.0
cide with one positive,1,1,1.0
with one positive class,1,1,1.0
one positive class training,1,1,1.0
positive class training sample,1,1,1.0
class training sample when,1,1,1.0
training sample when a,1,1,1.0
sample when a classifier,1,1,1.0
when a classifier is,1,1,1.0
a classifier is employed,1,1,1.0
classifier is employed it,1,1,1.0
is employed it could,1,1,1.0
employed it could generate,1,1,1.0
it could generate the,1,1,1.0
could generate the instances,1,1,1.0
generate the instances into,1,1,1.0
the instances into negative,1,1,1.0
instances into negative forms,1,1,1.0
into negative forms to,1,1,1.0
negative forms to ensure,1,1,1.0
forms to ensure that,1,1,1.0
to ensure that the,3,1,3.0
ensure that the classification,1,1,1.0
that the classification rule,1,1,1.0
the classification rule accuracy,1,1,1.0
classification rule accuracy is,1,1,1.0
rule accuracy is maximized,1,1,1.0
accuracy is maximized yielding,1,1,1.0
is maximized yielding accuracy,1,1,1.0
maximized yielding accuracy notably,1,1,1.0
yielding accuracy notably there,1,1,1.0
accuracy notably there exist,1,1,1.0
notably there exist unique,1,1,1.0
there exist unique categorizations,1,1,1.0
exist unique categorizations of,1,1,1.0
unique categorizations of solutions,1,1,1.0
categorizations of solutions through,1,1,1.0
of solutions through which,1,1,1.0
solutions through which problems,1,1,1.0
through which problems linked,1,1,1.0
which problems linked to,1,1,1.0
problems linked to imbalanced,1,1,1.0
linked to imbalanced classification,1,1,1.0
to imbalanced classification could,1,1,1.0
imbalanced classification could be,1,1,1.0
classification could be solved,1,1,1.0
could be solved one,1,1,1.0
be solved one of,1,1,1.0
solved one of such,1,1,1.0
one of such approaches,1,1,1.0
of such approaches entails,1,1,1.0
such approaches entails the,1,1,1.0
approaches entails the learning,1,1,1.0
entails the learning process,1,1,1.0
the learning process ification,1,1,1.0
learning process ification with,1,1,1.0
process ification with class,1,1,1.0
ification with class dispersion,1,1,1.0
with class dispersion on,1,1,1.0
class dispersion on focus,1,1,1.0
dispersion on focus this,1,1,1.0
on focus this process,1,1,1.0
focus this process is,1,1,1.0
this process is achieved,1,1,1.0
process is achieved by,1,1,1.0
is achieved by focusing,1,1,1.0
achieved by focusing on,1,1,1.0
by focusing on the,1,1,1.0
focusing on the undersampling,1,1,1.0
on the undersampling majority,1,1,1.0
the undersampling majority class,1,1,1.0
undersampling majority class oversampling,1,1,1.0
majority class oversampling minority,1,1,1.0
class oversampling minority class,1,1,1.0
oversampling minority class or,1,1,1.0
minority class or both,1,1,1.0
class or both in,1,1,1.0
or both in other,1,1,1.0
both in other situations,1,1,1.0
in other situations approaches,1,1,1.0
other situations approaches can,1,1,1.0
situations approaches can be,1,1,1.0
approaches can be employed,1,1,1.0
can be employed to,1,1,1.0
be employed to address,1,1,1.0
employed to address the,1,1,1.0
to address the respective,1,1,1.0
address the respective classes,1,1,1.0
the respective classes states,1,1,1.0
respective classes states of,1,1,1.0
classes states of misclassification,1,1,1.0
states of misclassification with,1,1,1.0
of misclassification with growth,1,1,1.0
misclassification with growth in,1,1,1.0
with growth in the,1,1,1.0
growth in the volume,1,1,1.0
in the volume of,1,1,1.0
the volume of data,1,1,1.0
volume of data it,1,1,1.0
of data it is,1,1,1.0
data it is also,1,1,1.0
it is also important,2,1,2.0
is also important to,2,1,2.0
also important to modify,1,1,1.0
important to modify the,1,1,1.0
to modify the traditional,1,1,1.0
modify the traditional methods,1,1,1.0
the traditional methods of,1,1,1.0
traditional methods of data,1,1,1.0
methods of data preprocessing,1,1,1.0
of data preprocessing indeed,1,1,1.0
data preprocessing indeed this,1,1,1.0
preprocessing indeed this approach,1,1,1.0
indeed this approach also,1,1,1.0
this approach also applies,1,1,1.0
approach also applies to,1,1,1.0
also applies to situations,1,1,1.0
applies to situations where,1,1,1.0
to situations where a,1,1,1.0
situations where a single,1,1,1.0
where a single machine,1,1,1.0
a single machine might,1,1,1.0
single machine might not,1,1,1.0
machine might not be,1,1,1.0
might not be better,1,1,1.0
not be better placed,1,1,1.0
be better placed to,1,1,1.0
better placed to process,1,1,1.0
placed to process the,1,1,1.0
to process the data,1,1,1.0
process the data for,1,1,1.0
the data for big,1,1,1.0
data for big data,2,1,2.0
for big data case,1,1,1.0
big data case studies,1,1,1.0
data case studies the,1,1,1.0
case studies the scalability,1,1,1.0
studies the scalability issues,1,1,1.0
the scalability issues related,1,1,1.0
scalability issues related with,1,1,1.0
issues related with data,1,1,1.0
related with data preprocessing,1,1,1.0
with data preprocessing need,1,1,1.0
data preprocessing need to,1,1,1.0
preprocessing need to be,1,1,1.0
need to be tackled,1,1,1.0
to be tackled appropriately,1,1,1.0
be tackled appropriately in,1,1,1.0
tackled appropriately in order,1,1,1.0
appropriately in order to,1,1,1.0
in order to formulate,1,1,1.0
order to formulate novel,1,1,1.0
to formulate novel solutions,1,1,1.0
formulate novel solutions or,1,1,1.0
novel solutions or accommodate,1,1,1.0
solutions or accommodate existing,1,1,1.0
or accommodate existing ones,1,1,1.0
accommodate existing ones data,1,1,1.0
existing ones data for,1,1,1.0
ones data for smaller,1,1,1.0
data for smaller class,1,1,1.0
for smaller class minority,1,1,1.0
smaller class minority that,1,1,1.0
class minority that can,1,1,1.0
minority that can be,1,1,1.0
that can be produced,1,1,1.0
can be produced artificially,1,1,1.0
be produced artificially with,1,1,1.0
produced artificially with the,1,1,1.0
artificially with the assistance,1,1,1.0
with the assistance of,1,1,1.0
the assistance of the,1,1,1.0
assistance of the thetic,1,1,1.0
of the thetic minority,1,1,1.0
the thetic minority oversampling,1,1,1.0
thetic minority oversampling technique,1,1,1.0
oversampling technique smote indeed,1,1,1.0
technique smote indeed this,1,1,1.0
smote indeed this approach,1,1,1.0
indeed this approach refers,1,1,1.0
this approach refers plays,1,1,1.0
approach refers plays a,1,1,1.0
refers plays a role,1,1,1.0
plays a role of,1,1,1.0
a role of minimizing,1,1,1.0
role of minimizing class,1,1,1.0
of minimizing class imbalance,1,1,1.0
minimizing class imbalance in,1,1,1.0
class imbalance in a,1,1,1.0
imbalance in a given,1,1,1.0
in a given set,1,1,1.0
set of data or,1,1,1.0
of data or data,1,1,1.0
data or data sample,1,1,1.0
or data sample at,1,1,1.0
data sample at present,1,1,1.0
sample at present nondistributed,1,1,1.0
at present nondistributed ronment,1,1,1.0
present nondistributed ronment is,1,1,1.0
nondistributed ronment is making,1,1,1.0
ronment is making use,1,1,1.0
is making use of,1,1,1.0
making use of availability,1,1,1.0
use of availability of,1,1,1.0
of availability of smote,1,1,1.0
availability of smote although,1,1,1.0
of smote although the,1,1,1.0
smote although the main,1,1,1.0
although the main confront,1,1,1.0
the main confront arises,1,1,1.0
main confront arises when,1,1,1.0
confront arises when we,1,1,1.0
arises when we have,1,1,1.0
when we have necessity,1,1,1.0
we have necessity to,1,1,1.0
have necessity to yield,1,1,1.0
necessity to yield the,1,1,1.0
to yield the data,1,1,1.0
yield the data for,1,1,1.0
the data for large,1,1,1.0
data for large minority,1,1,1.0
for large minority class,1,1,1.0
large minority class artificially,1,1,1.0
minority class artificially using,1,1,1.0
class artificially using smote,1,1,1.0
artificially using smote data,1,1,1.0
using smote data for,1,1,1.0
smote data for minority,1,1,1.0
data for minority class,1,1,1.0
for minority class is,1,1,1.0
minority class is generated,1,1,1.0
class is generated through,1,1,1.0
is generated through smote,1,1,1.0
generated through smote with,1,1,1.0
through smote with the,1,1,1.0
smote with the help,1,1,1.0
with the help of,4,1,4.0
the help of interpolation,1,1,1.0
help of interpolation of,1,1,1.0
of interpolation of data,1,1,1.0
interpolation of data hooda,1,1,1.0
of data hooda and,1,1,1.0
data hooda and mann,1,1,1.0
hooda and mann international,7,1,7.0
and mann international journal,7,1,7.0
mann international journal of,7,1,7.0
computational intelligence systems instances,1,1,1.0
intelligence systems instances that,1,1,1.0
systems instances that are,1,1,1.0
instances that are related,1,1,1.0
that are related to,1,1,1.0
are related to minority,1,1,1.0
related to minority as,1,1,1.0
to minority as well,1,1,1.0
minority as well as,1,1,1.0
as well as which,1,1,1.0
well as which are,1,1,1.0
as which are closure,1,1,1.0
which are closure with,1,1,1.0
are closure with each,1,1,1.0
closure with each other,1,1,1.0
with each other there,1,1,1.0
each other there are,1,1,1.0
other there are issues,1,1,1.0
there are issues related,1,1,1.0
are issues related to,1,1,1.0
issues related to distributed,1,1,1.0
related to distributed smote,1,1,1.0
to distributed smote as,1,1,1.0
distributed smote as well,1,1,1.0
smote as well one,1,1,1.0
as well one of,1,1,1.0
well one of these,1,1,1.0
one of these issues,1,1,1.0
of these issues is,1,1,1.0
these issues is the,1,1,1.0
issues is the appropriate,1,1,1.0
is the appropriate management,1,1,1.0
the appropriate management of,1,1,1.0
appropriate management of data,1,1,1.0
management of data that,1,1,1.0
of data that is,1,1,1.0
data that is to,1,1,1.0
that is to be,1,1,1.0
is to be distributed,1,1,1.0
to be distributed among,1,1,1.0
be distributed among a,1,1,1.0
distributed among a cluster,1,1,1.0
among a cluster of,1,1,1.0
a cluster of machines,2,1,2.0
cluster of machines for,1,1,1.0
of machines for ing,1,1,1.0
machines for ing to,1,1,1.0
for ing to ensure,1,1,1.0
ing to ensure effective,1,1,1.0
to ensure effective data,1,1,1.0
ensure effective data generation,1,1,1.0
effective data generation via,1,1,1.0
data generation via the,1,1,1.0
generation via the smote,1,1,1.0
via the smote technique,1,1,1.0
the smote technique it,1,1,1.0
smote technique it becomes,1,1,1.0
technique it becomes imperative,1,1,1.0
it becomes imperative to,1,1,1.0
becomes imperative to preserve,1,1,1.0
imperative to preserve sample,1,1,1.0
to preserve sample spatial,1,1,1.0
preserve sample spatial arrangement,1,1,1.0
sample spatial arrangement propriate,1,1,1.0
spatial arrangement propriate management,1,1,1.0
arrangement propriate management may,1,1,1.0
propriate management may result,1,1,1.0
management may result in,1,1,1.0
may result in inconsistent,1,1,1.0
result in inconsistent dissemination,1,1,1.0
in inconsistent dissemination of,1,1,1.0
inconsistent dissemination of data,1,1,1.0
dissemination of data among,1,1,1.0
of data among the,1,1,1.0
data among the nodes,1,1,1.0
among the nodes cluster,1,1,1.0
the nodes cluster therefore,1,1,1.0
nodes cluster therefore may,1,1,1.0
cluster therefore may affect,1,1,1.0
therefore may affect the,1,1,1.0
may affect the sampling,1,1,1.0
affect the sampling and,1,1,1.0
the sampling and consequently,1,1,1.0
sampling and consequently affecting,1,1,1.0
and consequently affecting the,1,1,1.0
consequently affecting the efficiency,1,1,1.0
affecting the efficiency of,1,1,1.0
the efficiency of the,1,1,1.0
efficiency of the algorithm,1,1,1.0
of the algorithm to,1,1,1.0
the algorithm to tackle,1,1,1.0
algorithm to tackle the,1,1,1.0
to tackle the problem,1,1,1.0
tackle the problem the,1,1,1.0
the problem the solution,1,1,1.0
problem the solution should,1,1,1.0
the solution should be,1,1,1.0
solution should be efficient,1,1,1.0
should be efficient scalable,1,1,1.0
be efficient scalable and,1,1,1.0
efficient scalable and parallel,1,1,1.0
scalable and parallel we,1,1,1.0
and parallel we introduce,1,1,1.0
parallel we introduce an,1,1,1.0
we introduce an algorithm,1,1,1.0
introduce an algorithm that,1,1,1.0
an algorithm that utilizes,1,1,1.0
algorithm that utilizes scalable,1,1,1.0
that utilizes scalable and,1,1,1.0
utilizes scalable and to,1,1,1.0
scalable and to synthetically,1,1,1.0
and to synthetically generate,1,1,1.0
to synthetically generate the,1,1,1.0
synthetically generate the minority,1,1,1.0
generate the minority data,1,1,1.0
the minority data in,1,1,1.0
minority data in this,1,1,1.0
data in this paper,1,1,1.0
this paper we use,1,1,1.0
paper we use apache,1,1,1.0
we use apache spark,1,1,1.0
use apache spark for,1,1,1.0
apache spark for implementing,1,1,1.0
spark for implementing the,1,1,1.0
for implementing the distributed,1,1,1.0
implementing the distributed version,1,1,1.0
the distributed version of,1,1,1.0
version of smote we,1,1,1.0
of smote we believe,1,1,1.0
smote we believe that,1,1,1.0
we believe that distributed,1,1,1.0
believe that distributed smote,1,1,1.0
that distributed smote is,1,1,1.0
distributed smote is an,1,1,1.0
smote is an active,1,1,1.0
is an active area,1,1,1.0
an active area of,1,1,1.0
active area of research,1,1,1.0
area of research our,1,1,1.0
of research our research,1,1,1.0
research our research can,1,1,1.0
our research can be,1,1,1.0
research can be extended,1,1,1.0
can be extended further,1,1,1.0
be extended further toward,1,1,1.0
extended further toward developing,1,1,1.0
further toward developing an,1,1,1.0
toward developing an effective,1,1,1.0
developing an effective and,1,1,1.0
an effective and efficient,1,1,1.0
effective and efficient algorithm,1,1,1.0
and efficient algorithm background,1,1,1.0
efficient algorithm background the,1,1,1.0
algorithm background the problem,1,1,1.0
background the problem of,1,1,1.0
the problem of classification,1,1,1.0
problem of classification is,1,1,1.0
of classification is termed,1,1,1.0
classification is termed imbalanced,1,1,1.0
is termed imbalanced classification,1,1,1.0
termed imbalanced classification when,1,1,1.0
imbalanced classification when the,1,1,1.0
classification when the positive,1,1,1.0
when the positive class,1,1,1.0
the positive class samples,1,1,1.0
positive class samples are,1,1,1.0
class samples are far,1,1,1.0
samples are far too,1,1,1.0
are far too few,1,1,1.0
far too few compared,1,1,1.0
too few compared to,1,1,1.0
few compared to the,1,1,1.0
compared to the negative,1,1,1.0
to the negative samples,1,1,1.0
the negative samples given,1,1,1.0
negative samples given negative,1,1,1.0
samples given negative and,1,1,1.0
given negative and positive,1,1,1.0
negative and positive samples,1,1,1.0
and positive samples the,1,1,1.0
positive samples the mance,1,1,1.0
samples the mance on,1,1,1.0
the mance on the,1,1,1.0
mance on the latter,1,1,1.0
on the latter overwhelms,1,1,1.0
the latter overwhelms that,1,1,1.0
latter overwhelms that which,1,1,1.0
overwhelms that which is,1,1,1.0
that which is felt,1,1,1.0
which is felt on,1,1,1.0
is felt on the,1,1,1.0
felt on the former,1,1,1.0
on the former type,1,1,1.0
the former type of,1,1,1.0
former type of sample,1,1,1.0
type of sample in,1,1,1.0
of sample in imbalance,1,1,1.0
sample in imbalance data,1,1,1.0
in imbalance data classification,1,1,1.0
imbalance data classification issues,1,1,1.0
data classification issues we,1,1,1.0
classification issues we may,1,1,1.0
issues we may neglect,1,1,1.0
we may neglect the,1,1,1.0
may neglect the asymmetry,1,1,1.0
neglect the asymmetry of,1,1,1.0
the asymmetry of data,1,1,1.0
asymmetry of data across,1,1,1.0
of data across a,1,1,1.0
data across a single,1,1,1.0
across a single class,1,1,1.0
a single class occasionally,1,1,1.0
single class occasionally and,1,1,1.0
class occasionally and this,1,1,1.0
occasionally and this asymmetry,1,1,1.0
and this asymmetry within,1,1,1.0
this asymmetry within a,1,1,1.0
asymmetry within a class,1,1,1.0
within a class is,2,2,1.0
a class is cited,1,1,1.0
class is cited for,1,1,1.0
is cited for small,1,1,1.0
cited for small disjunct,1,1,1.0
for small disjunct these,1,1,1.0
small disjunct these small,1,1,1.0
disjunct these small disjuncts,1,1,1.0
these small disjuncts can,1,1,1.0
small disjuncts can be,1,1,1.0
disjuncts can be influenced,1,1,1.0
can be influenced through,1,1,1.0
be influenced through the,1,1,1.0
influenced through the undersampling,1,1,1.0
through the undersampling of,1,1,1.0
the undersampling of the,1,1,1.0
undersampling of the majority,1,1,1.0
the majority class we,1,1,1.0
majority class we may,1,1,1.0
class we may have,1,1,1.0
we may have to,2,1,2.0
may have to confront,1,1,1.0
have to confront the,1,1,1.0
to confront the possible,1,1,1.0
confront the possible downfall,1,1,1.0
the possible downfall of,1,1,1.0
possible downfall of essential,1,1,1.0
downfall of essential information,1,1,1.0
of essential information through,1,1,1.0
essential information through this,1,1,1.0
information through this undersampling,1,1,1.0
through this undersampling of,1,1,1.0
this undersampling of majority,1,1,1.0
undersampling of majority class,1,1,1.0
of majority class analogously,1,1,1.0
majority class analogously we,1,1,1.0
class analogously we may,1,1,1.0
analogously we may have,1,1,1.0
may have to face,1,1,1.0
have to face the,1,1,1.0
to face the issues,1,1,1.0
face the issues of,1,1,1.0
the issues of data,1,1,1.0
issues of data tion,1,1,1.0
of data tion in,1,1,1.0
data tion in oversampling,1,1,1.0
tion in oversampling which,1,1,1.0
in oversampling which will,1,1,1.0
oversampling which will enhance,1,1,1.0
which will enhance the,1,1,1.0
will enhance the count,1,1,1.0
enhance the count of,1,1,1.0
the count of illustrations,1,1,1.0
count of illustrations excluding,1,1,1.0
of illustrations excluding the,1,1,1.0
illustrations excluding the addition,1,1,1.0
excluding the addition of,1,1,1.0
the addition of any,1,1,1.0
addition of any value,1,1,1.0
of any value to,1,1,1.0
any value to data,1,1,1.0
value to data as,1,1,1.0
to data as well,1,1,1.0
data as well as,1,1,1.0
as well as any,1,1,1.0
well as any tion,1,1,1.0
as any tion related,1,1,1.0
any tion related to,1,1,1.0
tion related to class,1,1,1.0
related to class the,1,1,1.0
to class the chances,1,1,1.0
class the chances of,1,1,1.0
the chances of over,1,1,1.0
chances of over fitting,1,1,1.0
of over fitting can,1,1,1.0
over fitting can additionally,1,1,1.0
fitting can additionally be,1,1,1.0
can additionally be enhanced,1,1,1.0
additionally be enhanced by,1,1,1.0
be enhanced by oversampling,1,1,1.0
enhanced by oversampling which,1,1,1.0
by oversampling which can,1,1,1.0
oversampling which can reinforce,1,1,1.0
which can reinforce all,1,1,1.0
can reinforce all minority,1,1,1.0
reinforce all minority ters,1,1,1.0
all minority ters without,1,1,1.0
minority ters without taking,1,1,1.0
ters without taking into,1,1,1.0
without taking into account,1,1,1.0
taking into account their,1,1,1.0
into account their tangible,1,1,1.0
account their tangible participation,1,1,1.0
their tangible participation to,1,1,1.0
tangible participation to the,1,1,1.0
participation to the problem,1,1,1.0
to the problem lot,1,1,1.0
the problem lot of,1,1,1.0
problem lot of exploration,1,1,1.0
lot of exploration has,1,1,1.0
of exploration has been,1,1,1.0
exploration has been going,1,1,1.0
has been going to,1,1,1.0
been going to find,1,1,1.0
going to find the,1,1,1.0
to find the solution,2,1,2.0
find the solution for,2,1,2.0
the solution for the,1,1,1.0
solution for the issues,1,1,1.0
for the issues of,1,1,1.0
the issues of classification,1,1,1.0
issues of classification in,1,1,1.0
of classification in a,1,1,1.0
classification in a disseminated,1,1,1.0
in a disseminated distributed,1,1,1.0
a disseminated distributed environment,1,1,1.0
disseminated distributed environment the,1,1,1.0
distributed environment the difficulties,1,1,1.0
environment the difficulties faced,1,1,1.0
the difficulties faced in,1,1,1.0
difficulties faced in classifying,1,1,1.0
faced in classifying imbalanced,1,1,1.0
in classifying imbalanced data,1,1,1.0
classifying imbalanced data and,1,1,1.0
imbalanced data and the,1,1,1.0
data and the concerns,1,1,1.0
and the concerns related,1,1,1.0
the concerns related to,1,1,1.0
concerns related to the,1,1,1.0
related to the generation,1,1,1.0
to the generation of,1,1,1.0
the generation of imitated,1,1,1.0
generation of imitated or,1,1,1.0
of imitated or we,1,1,1.0
imitated or we can,1,1,1.0
or we can say,2,1,2.0
we can say synthetic,1,1,1.0
can say synthetic data,1,1,1.0
say synthetic data for,1,1,1.0
synthetic data for big,1,1,1.0
for big data classification,1,1,1.0
big data classification problems,1,1,1.0
data classification problems which,1,1,1.0
classification problems which are,1,1,1.0
problems which are extremely,1,1,1.0
which are extremely imbalanced,1,1,1.0
are extremely imbalanced is,1,1,1.0
extremely imbalanced is examined,1,1,1.0
imbalanced is examined by,1,1,1.0
is examined by chawla,1,1,1.0
examined by chawla applying,1,1,1.0
by chawla applying smote,1,1,1.0
chawla applying smote requires,1,1,1.0
applying smote requires us,1,1,1.0
smote requires us to,1,1,1.0
requires us to find,1,1,1.0
us to find neighbors,1,1,1.0
to find neighbors when,1,1,1.0
find neighbors when the,1,1,1.0
neighbors when the dataset,1,1,1.0
when the dataset is,1,1,1.0
the dataset is big,1,1,1.0
dataset is big the,1,1,1.0
is big the cost,1,1,1.0
big the cost of,1,1,1.0
the cost of computing,1,1,1.0
cost of computing the,1,1,1.0
of computing the neighbors,1,1,1.0
computing the neighbors increases,1,1,1.0
the neighbors increases for,1,1,1.0
neighbors increases for problems,1,1,1.0
increases for problems where,1,1,1.0
for problems where the,2,1,2.0
problems where the data,1,1,1.0
where the data set,1,1,1.0
the data set is,1,1,1.0
data set is huge,1,1,1.0
set is huge and,1,1,1.0
is huge and distributed,1,1,1.0
huge and distributed one,1,1,1.0
and distributed one approach,1,1,1.0
distributed one approach would,1,1,1.0
one approach would be,1,1,1.0
approach would be to,1,1,1.0
would be to group,1,1,1.0
be to group the,1,1,1.0
to group the samples,1,1,1.0
group the samples into,1,1,1.0
the samples into groups,1,1,1.0
samples into groups and,1,1,1.0
into groups and move,1,1,1.0
groups and move these,1,1,1.0
and move these groups,1,1,1.0
move these groups on,1,1,1.0
these groups on different,1,1,1.0
groups on different machines,1,1,1.0
on different machines where,1,1,1.0
different machines where can,1,1,1.0
machines where can be,1,1,1.0
where can be achieved,1,1,1.0
can be achieved lsh,1,1,1.0
be achieved lsh is,1,1,1.0
achieved lsh is another,1,1,1.0
lsh is another technique,1,1,1.0
is another technique for,1,1,1.0
another technique for distributed,1,1,1.0
technique for distributed knn,1,1,1.0
for distributed knn search,1,1,1.0
distributed knn search in,2,1,2.0
knn search in high,2,1,2.0
search in high dimensions,2,1,2.0
in high dimensions clustering,1,1,1.0
high dimensions clustering problems,1,1,1.0
dimensions clustering problems have,1,1,1.0
clustering problems have been,1,1,1.0
problems have been one,1,1,1.0
have been one of,1,1,1.0
been one of the,1,1,1.0
of the most commonly,1,1,1.0
the most commonly researched,1,1,1.0
most commonly researched problems,1,1,1.0
commonly researched problems more,1,1,1.0
researched problems more details,1,1,1.0
problems more details on,1,1,1.0
more details on various,1,1,1.0
details on various clustering,1,1,1.0
on various clustering algorithms,1,1,1.0
various clustering algorithms can,1,1,1.0
clustering algorithms can be,1,1,1.0
algorithms can be found,1,1,1.0
can be found in,4,3,1.3333333333333333
be found in one,1,1,1.0
found in one of,1,1,1.0
in one of the,2,1,2.0
one of the common,1,1,1.0
of the common clustering,1,1,1.0
the common clustering algorithms,1,1,1.0
common clustering algorithms involves,1,1,1.0
clustering algorithms involves the,1,1,1.0
algorithms involves the simple,1,1,1.0
involves the simple nature,1,1,1.0
the simple nature of,1,1,1.0
simple nature of the,1,1,1.0
nature of the algorithm,1,1,1.0
of the algorithm accounts,1,1,1.0
the algorithm accounts for,1,1,1.0
algorithm accounts for its,1,1,1.0
accounts for its popularity,1,1,1.0
for its popularity in,1,1,1.0
its popularity in recent,1,1,1.0
popularity in recent scholarly,1,1,1.0
in recent scholarly investigations,1,1,1.0
recent scholarly investigations much,1,1,1.0
scholarly investigations much tion,1,1,1.0
investigations much tion has,1,1,1.0
much tion has been,1,1,1.0
tion has been extended,1,1,1.0
has been extended to,1,1,1.0
been extended to initialization,1,1,1.0
extended to initialization improvement,1,1,1.0
to initialization improvement the,1,1,1.0
initialization improvement the need,1,1,1.0
improvement the need for,1,1,1.0
the need for better,1,1,1.0
need for better convergence,1,1,1.0
for better convergence has,1,1,1.0
better convergence has been,1,1,1.0
convergence has been attributed,1,1,1.0
has been attributed to,1,1,1.0
been attributed to this,1,1,1.0
attributed to this trend,1,1,1.0
to this trend given,1,1,1.0
this trend given better,1,1,1.0
trend given better initialization,1,1,1.0
given better initialization a,1,1,1.0
better initialization a resultant,1,1,1.0
initialization a resultant merit,1,1,1.0
a resultant merit is,1,1,1.0
resultant merit is that,1,1,1.0
merit is that it,1,1,1.0
is that it yields,1,1,1.0
that it yields cant,1,1,1.0
it yields cant improvements,1,1,1.0
yields cant improvements in,1,1,1.0
cant improvements in an,1,1,1.0
improvements in an algorithm,1,1,1.0
in an algorithm s,1,1,1.0
an algorithm s running,1,1,1.0
algorithm s running time,1,1,1.0
s running time in,1,1,1.0
running time in a,1,1,1.0
time in a distributed,1,1,1.0
in a distributed setup,1,1,1.0
a distributed setup it,1,1,1.0
distributed setup it remains,1,1,1.0
setup it remains notable,1,1,1.0
it remains notable that,2,1,2.0
remains notable that there,1,1,1.0
notable that there is,1,1,1.0
that there is also,1,1,1.0
there is also increasing,1,1,1.0
is also increasing attention,1,1,1.0
also increasing attention and,1,1,1.0
increasing attention and effort,1,1,1.0
attention and effort toward,1,1,1.0
and effort toward understanding,1,1,1.0
effort toward understanding the,1,1,1.0
toward understanding the running,1,1,1.0
understanding the running of,1,1,1.0
the running of the,1,1,1.0
running of the algorithm,1,1,1.0
of the algorithm the,1,1,1.0
the algorithm the role,1,1,1.0
algorithm the role of,1,1,1.0
the role of the,1,1,1.0
role of the scalable,1,1,1.0
of the scalable lies,1,1,1.0
the scalable lies in,1,1,1.0
scalable lies in center,1,1,1.0
lies in center ization,1,1,1.0
in center ization having,1,1,1.0
center ization having evolved,1,1,1.0
ization having evolved in,1,1,1.0
having evolved in a,1,1,1.0
evolved in a form,1,1,1.0
in a form that,1,1,1.0
a form that supports,1,1,1.0
form that supports parallel,1,1,1.0
that supports parallel mentation,1,1,1.0
supports parallel mentation given,1,1,1.0
parallel mentation given which,1,1,1.0
mentation given which is,1,1,1.0
given which is inherently,1,1,1.0
which is inherently sequential,1,1,1.0
is inherently sequential its,1,1,1.0
inherently sequential its efficient,1,1,1.0
sequential its efficient and,1,1,1.0
its efficient and parallel,1,1,1.0
efficient and parallel version,1,1,1.0
and parallel version involves,1,1,1.0
parallel version involves the,1,1,1.0
version involves the scalable,1,1,1.0
involves the scalable able,1,1,1.0
the scalable able sampleso,1,1,1.0
scalable able sampleso m,1,1,1.0
able sampleso m points,1,1,1.0
sampleso m points in,1,1,1.0
m points in each,1,1,1.0
points in each round,1,1,1.0
in each round and,1,1,1.0
each round and repeats,1,1,1.0
round and repeats the,1,1,1.0
and repeats the process,1,1,1.0
repeats the process for,1,1,1.0
the process for approximatelyo,1,1,1.0
process for approximatelyo logn,1,1,1.0
for approximatelyo logn rounds,1,1,1.0
approximatelyo logn rounds instead,1,1,1.0
logn rounds instead of,1,1,1.0
rounds instead of sampling,1,1,1.0
instead of sampling a,1,1,1.0
of sampling a single,1,1,1.0
sampling a single point,1,1,1.0
a single point in,1,1,1.0
single point in each,1,1,1.0
point in each pass,1,1,1.0
in each pass as,1,1,1.0
each pass as used,1,1,1.0
pass as used to,1,1,1.0
as used to be,1,1,1.0
used to be in,1,1,1.0
to be in algorithm,1,1,1.0
be in algorithm so,1,1,1.0
in algorithm so o,1,1,1.0
algorithm so o mlogn,1,1,1.0
so o mlogn are,1,1,1.0
o mlogn are sampled,1,1,1.0
mlogn are sampled which,1,1,1.0
are sampled which is,1,1,1.0
sampled which is typically,1,1,1.0
which is typically more,1,1,1.0
is typically more than,1,1,1.0
typically more than the,1,1,1.0
more than the m,1,1,1.0
than the m clusters,1,1,1.0
the m clusters which,1,1,1.0
m clusters which are,1,1,1.0
clusters which are then,1,1,1.0
which are then clustered,1,1,1.0
are then clustered into,1,1,1.0
then clustered into m,1,1,1.0
clustered into m initial,1,1,1.0
into m initial centers,1,1,1.0
m initial centers for,1,1,1.0
initial centers for llyod,1,1,1.0
centers for llyod s,1,1,1.0
for llyod s iteration,1,1,1.0
llyod s iteration since,1,1,1.0
s iteration since the,1,1,1.0
iteration since the size,1,1,1.0
since the size of,1,1,1.0
the size of sample,1,1,1.0
size of sample is,1,1,1.0
of sample is smaller,1,1,1.0
sample is smaller than,1,1,1.0
is smaller than the,1,1,1.0
smaller than the input,1,1,1.0
than the input size,1,1,1.0
the input size the,1,1,1.0
input size the can,1,1,1.0
size the can be,1,1,1.0
the can be done,1,1,1.0
can be done quickly,1,1,1.0
be done quickly once,1,1,1.0
done quickly once the,1,1,1.0
quickly once the samples,1,1,1.0
once the samples are,1,1,1.0
the samples are tered,1,1,1.0
samples are tered into,1,1,1.0
are tered into m,1,1,1.0
tered into m clusters,1,1,1.0
into m clusters each,1,1,1.0
m clusters each of,1,1,1.0
clusters each of these,1,1,1.0
each of these clusters,1,1,1.0
of these clusters can,1,1,1.0
these clusters can use,1,1,1.0
clusters can use any,1,1,1.0
can use any of,1,1,1.0
use any of the,1,1,1.0
any of the widely,1,1,1.0
of the widely available,1,1,1.0
the widely available algorithms,1,1,1.0
widely available algorithms to,1,1,1.0
available algorithms to find,1,1,1.0
algorithms to find neighbors,1,1,1.0
to find neighbors we,1,1,1.0
find neighbors we have,1,1,1.0
neighbors we have used,1,1,1.0
we have used space,1,1,1.0
have used space indexing,1,1,1.0
used space indexing algorithms,1,1,1.0
space indexing algorithms to,1,1,1.0
indexing algorithms to index,1,1,1.0
algorithms to index the,1,1,1.0
to index the samples,2,1,2.0
index the samples and,1,1,1.0
the samples and then,1,1,1.0
samples and then search,1,1,1.0
and then search for,1,1,1.0
then search for neighbors,1,1,1.0
search for neighbors whenever,1,1,1.0
for neighbors whenever the,1,1,1.0
neighbors whenever the dimensionality,1,1,1.0
whenever the dimensionality is,1,1,1.0
the dimensionality is small,1,1,1.0
dimensionality is small we,1,1,1.0
is small we take,1,1,1.0
small we take advantage,1,1,1.0
we take advantage of,1,1,1.0
take advantage of space,1,1,1.0
advantage of space partitioning,1,1,1.0
of space partitioning approaches,1,1,1.0
space partitioning approaches that,1,1,1.0
partitioning approaches that includes,1,1,1.0
approaches that includes methods,1,1,1.0
that includes methods alike,1,1,1.0
includes methods alike trees,1,1,1.0
methods alike trees which,1,1,1.0
alike trees which is,1,1,1.0
trees which is capable,1,1,1.0
which is capable of,1,1,1.0
is capable of performing,1,1,1.0
capable of performing extremely,1,1,1.0
of performing extremely better,1,1,1.0
performing extremely better in,1,1,1.0
extremely better in that,1,1,1.0
better in that case,1,1,1.0
in that case the,1,1,1.0
that case the apportioning,1,1,1.0
case the apportioning of,1,1,1.0
the apportioning of the,1,1,1.0
apportioning of the points,1,1,1.0
of the points results,1,1,1.0
the points results in,1,1,1.0
points results in disjoint,1,1,1.0
results in disjoint sets,1,1,1.0
in disjoint sets a,1,1,1.0
disjoint sets a process,1,1,1.0
sets a process achieved,1,1,1.0
a process achieved through,1,1,1.0
process achieved through titioning,1,1,1.0
achieved through titioning in,1,1,1.0
through titioning in the,1,1,1.0
titioning in the target,1,1,1.0
in the target cell,1,1,1.0
the target cell all,1,1,1.0
target cell all points,1,1,1.0
cell all points are,1,1,1.0
all points are considered,1,1,1.0
points are considered nearest,1,1,1.0
are considered nearest to,1,1,1.0
considered nearest to its,1,1,1.0
nearest to its to,1,1,1.0
to its to other,1,1,1.0
its to other pivots,1,1,1.0
to other pivots regarding,1,1,1.0
other pivots regarding the,1,1,1.0
pivots regarding the indexing,1,1,1.0
regarding the indexing and,1,1,1.0
the indexing and searching,1,1,1.0
indexing and searching of,1,1,1.0
and searching of metric,1,1,1.0
searching of metric spaces,1,1,1.0
of metric spaces to,1,1,1.0
metric spaces to discern,1,1,1.0
spaces to discern similarity,1,1,1.0
to discern similarity another,1,1,1.0
discern similarity another efficient,1,1,1.0
similarity another efficient method,1,1,1.0
another efficient method entails,1,1,1.0
efficient method entails this,1,1,1.0
method entails this technique,1,1,1.0
entails this technique has,1,1,1.0
this technique has been,1,1,1.0
technique has been mented,1,1,1.0
has been mented to,1,1,1.0
been mented to be,1,1,1.0
mented to be the,1,1,1.0
to be the most,1,1,1.0
be the most efficient,1,1,1.0
the most efficient if,1,1,1.0
most efficient if the,1,1,1.0
efficient if the situation,1,1,1.0
if the situation on,1,1,1.0
the situation on focus,1,1,1.0
situation on focus entails,1,1,1.0
on focus entails a,1,1,1.0
focus entails a large,1,1,1.0
entails a large number,1,1,1.0
a large number of,3,2,1.5
large number of records,1,1,1.0
number of records or,1,1,1.0
of records or numbers,1,1,1.0
records or numbers of,1,1,1.0
or numbers of dimensions,1,1,1.0
numbers of dimensions indeed,1,1,1.0
of dimensions indeed as,1,1,1.0
dimensions indeed as a,1,1,1.0
indeed as a technique,1,1,1.0
as a technique of,1,1,1.0
a technique of object,1,1,1.0
technique of object indexing,1,1,1.0
of object indexing seeks,1,1,1.0
object indexing seeks to,1,1,1.0
indexing seeks to support,1,1,1.0
seeks to support similarity,1,1,1.0
to support similarity queries,1,1,1.0
support similarity queries by,1,1,1.0
similarity queries by comparing,1,1,1.0
queries by comparing features,1,1,1.0
by comparing features based,1,1,1.0
comparing features based on,1,1,1.0
features based on the,1,1,1.0
based on the parameter,1,1,1.0
on the parameter of,1,1,1.0
the parameter of distance,1,1,1.0
parameter of distance given,1,1,1.0
of distance given m,1,1,1.0
distance given m the,1,1,1.0
given m the metric,1,1,1.0
m the metric space,1,1,1.0
the metric space is,1,1,1.0
metric space is established,1,1,1.0
space is established in,1,1,1.0
is established in the,1,1,1.0
established in the form,1,1,1.0
in the form m,1,1,1.0
the form m o,1,1,1.0
form m o d,1,1,1.0
m o d in,1,1,1.0
o d in this,1,1,1.0
d in this case,1,1,1.0
in this case d,1,1,1.0
this case d represents,1,1,1.0
case d represents the,1,1,1.0
d represents the distance,1,1,1.0
represents the distance function,1,1,1.0
the distance function while,1,1,1.0
distance function while o,1,1,1.0
function while o refers,1,1,1.0
while o refers to,1,1,1.0
o refers to the,1,1,1.0
refers to the feature,1,1,1.0
the feature values domain,1,1,1.0
feature values domain d,1,1,1.0
values domain d ox,1,1,1.0
domain d ox oy,1,1,1.0
d ox oy d,2,1,2.0
ox oy d oy,1,1,1.0
oy d oy ox,1,1,1.0
d oy ox d,1,1,1.0
oy ox d ox,1,1,1.0
ox d ox oy,2,1,2.0
d ox oy ox,1,1,1.0
ox oy ox oy,1,1,1.0
oy ox oy d,1,1,1.0
ox oy d ox,2,1,2.0
oy d ox ox,1,1,1.0
d ox ox d,1,1,1.0
ox ox d ox,1,1,1.0
oy d ox oz,1,1,1.0
d ox oz d,1,1,1.0
ox oz d oz,1,1,1.0
oz d oz oy,1,1,1.0
d oz oy in,1,1,1.0
oz oy in o,1,1,1.0
oy in o the,1,1,1.0
in o the objects,1,1,1.0
o the objects are,1,1,1.0
the objects are represented,1,1,1.0
objects are represented by,1,1,1.0
are represented by oz,1,1,1.0
represented by oz oy,1,1,1.0
by oz oy and,1,1,1.0
oz oy and ox,1,1,1.0
oy and ox to,1,1,1.0
and ox to determine,1,1,1.0
ox to determine any,1,1,1.0
to determine any similarity,1,1,1.0
determine any similarity or,1,1,1.0
any similarity or dissimilarity,1,1,1.0
similarity or dissimilarity between,1,1,1.0
or dissimilarity between objects,1,1,1.0
dissimilarity between objects d,1,1,1.0
between objects d is,1,1,1.0
objects d is considered,1,1,1.0
d is considered as,1,1,1.0
is considered as the,1,1,1.0
considered as the informative,1,1,1.0
as the informative parameter,1,1,1.0
the informative parameter for,1,1,1.0
informative parameter for the,1,1,1.0
parameter for the entries,1,1,1.0
for the entries that,1,1,1.0
the entries that can,1,1,1.0
entries that can be,1,1,1.0
that can be stored,1,1,1.0
can be stored can,1,1,1.0
be stored can go,1,1,1.0
stored can go up,1,1,1.0
can go up to,1,1,1.0
go up to l,1,1,1.0
up to l hence,1,1,1.0
to l hence l,1,1,1.0
l hence l represents,1,1,1.0
hence l represents the,1,1,1.0
l represents the node,1,1,1.0
represents the node capacity,1,1,1.0
the node capacity it,1,1,1.0
node capacity it is,1,1,1.0
capacity it is also,1,1,1.0
also important to highlight,1,1,1.0
important to highlight that,1,1,1.0
to highlight that exhibits,1,1,1.0
highlight that exhibits two,1,1,1.0
that exhibits two internal,1,1,1.0
exhibits two internal node,1,1,1.0
two internal node types,1,1,1.0
internal node types these,1,1,1.0
node types these nodes,1,1,1.0
types these nodes include,1,1,1.0
these nodes include the,1,1,1.0
nodes include the leaf,1,1,1.0
include the leaf nodes,1,1,1.0
the leaf nodes and,1,1,1.0
leaf nodes and the,1,1,1.0
nodes and the routing,1,1,1.0
and the routing nodes,1,1,1.0
the routing nodes poised,1,1,1.0
routing nodes poised to,1,1,1.0
nodes poised to represent,1,1,1.0
poised to represent data,1,1,1.0
to represent data or,1,1,1.0
represent data or samples,1,1,1.0
data or samples indeed,1,1,1.0
or samples indeed tions,1,1,1.0
samples indeed tions through,1,1,1.0
indeed tions through sample,1,1,1.0
tions through sample organization,1,1,1.0
through sample organization to,1,1,1.0
sample organization to ensure,1,1,1.0
organization to ensure that,1,1,1.0
to ensure that nodes,1,1,1.0
ensure that nodes are,1,1,1.0
that nodes are achieved,1,1,1.0
nodes are achieved notably,1,1,1.0
are achieved notably the,1,1,1.0
achieved notably the resultant,1,1,1.0
notably the resultant nodes,1,1,1.0
the resultant nodes correspond,1,1,1.0
resultant nodes correspond to,1,1,1.0
nodes correspond to the,1,1,1.0
correspond to the metric,1,1,1.0
to the metric space,2,1,2.0
the metric space region,1,1,1.0
metric space region grows,1,1,1.0
space region grows in,1,1,1.0
region grows in a,1,1,1.0
grows in a bottom,1,1,1.0
in a bottom up,1,1,1.0
a bottom up fashion,1,1,1.0
bottom up fashion when,1,1,1.0
up fashion when the,1,1,1.0
fashion when the number,1,1,1.0
when the number of,2,2,1.0
the number of leaf,1,1,1.0
number of leaf nodes,1,1,1.0
of leaf nodes grows,1,1,1.0
leaf nodes grows beyond,1,1,1.0
nodes grows beyond the,1,1,1.0
grows beyond the capacity,1,1,1.0
beyond the capacity of,1,1,1.0
the capacity of the,3,1,3.0
capacity of the routing,1,1,1.0
of the routing node,1,1,1.0
the routing node a,2,1,2.0
routing node a new,1,1,1.0
node a new node,1,1,1.0
a new node from,1,1,1.0
new node from the,1,1,1.0
node from the pool,1,1,1.0
from the pool is,1,1,1.0
the pool is promoted,1,1,1.0
pool is promoted as,1,1,1.0
is promoted as a,1,1,1.0
promoted as a routing,1,1,1.0
as a routing node,1,1,1.0
a routing node and,1,1,1.0
routing node and the,1,1,1.0
node and the tree,1,1,1.0
and the tree is,1,1,1.0
the tree is split,1,1,1.0
tree is split and,1,1,1.0
is split and nodes,1,1,1.0
split and nodes partitioned,1,1,1.0
and nodes partitioned between,1,1,1.0
nodes partitioned between the,1,1,1.0
partitioned between the two,1,1,1.0
between the two routing,1,1,1.0
the two routing nodes,1,1,1.0
two routing nodes based,1,1,1.0
routing nodes based on,1,1,1.0
nodes based on the,1,1,1.0
based on the partitioning,1,1,1.0
on the partitioning policy,1,1,1.0
the partitioning policy hooda,1,1,1.0
partitioning policy hooda and,1,1,1.0
policy hooda and mann,1,1,1.0
computational intelligence systems provides,1,1,1.0
intelligence systems provides a,1,1,1.0
systems provides a very,1,1,1.0
provides a very fast,1,1,1.0
a very fast and,1,1,1.0
very fast and efficient,1,1,1.0
fast and efficient nearest,1,1,1.0
and efficient nearest neighbor,1,1,1.0
efficient nearest neighbor search,1,1,1.0
nearest neighbor search search,1,1,1.0
neighbor search search is,1,1,1.0
search search is based,1,1,1.0
search is based on,1,1,1.0
based on the concept,1,1,1.0
on the concept that,1,1,1.0
the concept that if,1,1,1.0
concept that if a,1,1,1.0
that if a sample,1,1,1.0
if a sample q,1,1,1.0
a sample q is,1,1,1.0
sample q is at,1,1,1.0
q is at a,1,1,1.0
is at a distance,2,1,2.0
at a distance of,2,1,2.0
a distance of d,2,1,2.0
distance of d or,2,1,2.0
of d or q,1,1,1.0
d or q and,1,1,1.0
or q and another,1,1,1.0
q and another sample,1,1,1.0
and another sample p,1,1,1.0
another sample p is,1,1,1.0
sample p is at,1,1,1.0
p is at a,1,1,1.0
of d or p,1,1,1.0
d or p then,1,1,1.0
or p then the,1,1,1.0
p then the distance,1,1,1.0
then the distance between,1,1,1.0
the distance between d,1,1,1.0
distance between d q,1,1,1.0
between d q p,1,1,1.0
d q p will,1,1,1.0
q p will always,1,1,1.0
p will always be,1,1,1.0
will always be greater,1,1,1.0
always be greater than,1,1,1.0
be greater than d,1,1,1.0
greater than d or,1,1,1.0
than d or q,1,1,1.0
d or q d,2,1,2.0
or q d or,1,1,1.0
q d or p,1,1,1.0
d or p this,2,1,2.0
or p this follows,1,1,1.0
p this follows from,1,1,1.0
this follows from the,1,1,1.0
follows from the triangle,1,1,1.0
from the triangle inequality,1,1,1.0
the triangle inequality which,1,1,1.0
triangle inequality which states,1,1,1.0
inequality which states that,1,1,1.0
which states that d,1,1,1.0
states that d or,1,1,1.0
that d or q,1,1,1.0
or q d q,1,1,1.0
q d q p,1,1,1.0
d q p d,1,1,1.0
q p d or,1,1,1.0
p d or p,1,1,1.0
or p this simple,1,1,1.0
p this simple inequality,1,1,1.0
this simple inequality optimizes,1,1,1.0
simple inequality optimizes the,1,1,1.0
inequality optimizes the search,1,1,1.0
optimizes the search for,1,1,1.0
the search for the,1,1,1.0
search for the nearest,2,2,1.0
for the nearest bors,1,1,1.0
the nearest bors of,1,1,1.0
nearest bors of each,1,1,1.0
bors of each sample,1,1,1.0
of each sample as,1,1,1.0
each sample as all,1,1,1.0
sample as all these,1,1,1.0
as all these points,1,1,1.0
all these points are,1,1,1.0
these points are organized,1,1,1.0
points are organized in,1,1,1.0
are organized in a,1,1,1.0
organized in a metric,1,1,1.0
in a metric space,1,1,1.0
a metric space imperatively,1,1,1.0
metric space imperatively these,1,1,1.0
space imperatively these methods,1,1,1.0
imperatively these methods are,1,1,1.0
these methods are meant,1,1,1.0
methods are meant to,1,1,1.0
are meant to operate,1,1,1.0
meant to operate on,1,1,1.0
to operate on single,1,1,1.0
operate on single machines,1,1,1.0
on single machines as,1,1,1.0
single machines as such,1,1,1.0
machines as such the,1,1,1.0
as such the techniques,1,1,1.0
such the techniques become,1,1,1.0
the techniques become impractical,1,1,1.0
techniques become impractical and,1,1,1.0
become impractical and cient,1,1,1.0
impractical and cient when,1,1,1.0
and cient when a,1,1,1.0
cient when a given,1,1,1.0
when a given sample,1,1,1.0
a given sample involves,1,1,1.0
given sample involves big,1,1,1.0
sample involves big data,1,1,1.0
involves big data in,1,1,1.0
big data in one,1,1,1.0
data in one of,1,1,1.0
one of the ous,1,1,1.0
of the ous investigations,1,1,1.0
the ous investigations it,1,1,1.0
ous investigations it was,1,1,1.0
investigations it was documented,1,1,1.0
it was documented that,1,1,1.0
was documented that a,1,1,1.0
documented that a combination,1,1,1.0
that a combination of,1,1,1.0
a combination of spill,1,1,1.0
combination of spill trees,1,1,1.0
of spill trees and,1,1,1.0
spill trees and scales,1,1,1.0
trees and scales in,1,1,1.0
and scales in a,1,1,1.0
scales in a case,1,1,1.0
in a case involving,1,1,1.0
a case involving distributed,1,1,1.0
case involving distributed sample,1,1,1.0
involving distributed sample space,1,1,1.0
distributed sample space ensuring,1,1,1.0
sample space ensuring further,1,1,1.0
space ensuring further that,1,1,1.0
ensuring further that similar,1,1,1.0
further that similar samples,1,1,1.0
that similar samples are,1,1,1.0
similar samples are searched,1,1,1.0
samples are searched efficiently,1,1,1.0
are searched efficiently in,1,1,1.0
searched efficiently in the,1,1,1.0
efficiently in the current,1,1,1.0
in the current investigation,1,1,1.0
the current investigation the,1,1,1.0
current investigation the central,1,1,1.0
investigation the central purpose,1,1,1.0
the central purpose is,1,1,1.0
central purpose is to,1,1,1.0
purpose is to present,1,1,1.0
is to present a,1,1,1.0
to present a hybrid,1,1,1.0
present a hybrid technique,1,1,1.0
a hybrid technique through,1,1,1.0
hybrid technique through which,1,1,1.0
technique through which artificial,1,1,1.0
through which artificial minority,1,1,1.0
which artificial minority data,1,1,1.0
artificial minority data could,1,1,1.0
minority data could be,1,1,1.0
data could be generated,1,1,1.0
could be generated using,1,1,1.0
be generated using scalable,1,1,1.0
generated using scalable similar,1,1,1.0
using scalable similar samples,1,1,1.0
scalable similar samples are,1,1,1.0
similar samples are clustered,1,1,1.0
samples are clustered together,1,1,1.0
are clustered together also,1,1,1.0
clustered together also are,1,1,1.0
together also are used,1,1,1.0
also are used to,1,1,1.0
are used to index,1,1,1.0
used to index the,1,1,1.0
index the samples before,1,1,1.0
the samples before ing,1,1,1.0
samples before ing for,1,1,1.0
before ing for the,1,1,1.0
ing for the perceived,1,1,1.0
for the perceived samples,1,1,1.0
the perceived samples performance,1,1,1.0
perceived samples performance measures,1,1,1.0
samples performance measures as,1,1,1.0
performance measures as the,1,1,1.0
measures as the minority,1,1,1.0
as the minority smaller,1,1,1.0
the minority smaller data,1,1,1.0
minority smaller data is,1,1,1.0
smaller data is devastated,1,1,1.0
data is devastated by,1,1,1.0
is devastated by the,1,1,1.0
devastated by the majority,1,1,1.0
by the majority data,1,1,1.0
the majority data present,1,1,1.0
majority data present for,1,1,1.0
data present for this,1,1,1.0
present for this reason,1,1,1.0
for this reason the,1,1,1.0
this reason the problem,1,1,1.0
reason the problem of,1,1,1.0
the problem of imbalanced,1,1,1.0
problem of imbalanced classification,1,1,1.0
of imbalanced classification which,1,1,1.0
imbalanced classification which was,1,1,1.0
classification which was described,1,1,1.0
which was described previously,1,1,1.0
was described previously can,1,1,1.0
described previously can t,1,1,1.0
previously can t be,1,1,1.0
can t be contrasted,1,1,1.0
t be contrasted through,1,1,1.0
be contrasted through the,1,1,1.0
contrasted through the use,1,1,1.0
through the use of,1,1,1.0
the use of accuracy,1,1,1.0
use of accuracy metrics,1,1,1.0
of accuracy metrics for,1,1,1.0
accuracy metrics for illustration,1,1,1.0
metrics for illustration the,1,1,1.0
for illustration the accuracy,1,1,1.0
illustration the accuracy of,1,1,1.0
the accuracy of the,3,2,1.5
accuracy of the prediction,1,1,1.0
of the prediction can,1,1,1.0
the prediction can simply,1,1,1.0
prediction can simply be,1,1,1.0
can simply be taken,1,1,1.0
simply be taken as,1,1,1.0
be taken as accurate,1,1,1.0
taken as accurate in,1,1,1.0
as accurate in case,1,1,1.0
accurate in case a,1,1,1.0
in case a provided,1,1,1.0
case a provided dataset,1,1,1.0
a provided dataset has,1,1,1.0
provided dataset has a,1,1,1.0
dataset has a distribution,1,1,1.0
has a distribution by,1,1,1.0
a distribution by means,1,1,1.0
distribution by means of,1,1,1.0
by means of merely,1,1,1.0
means of merely predicting,1,1,1.0
of merely predicting entire,1,1,1.0
merely predicting entire data,1,1,1.0
predicting entire data as,1,1,1.0
entire data as the,1,1,1.0
data as the majority,1,1,1.0
as the majority class,1,1,1.0
the majority class table,1,1,1.0
majority class table is,1,1,1.0
class table is being,1,1,1.0
table is being implemented,1,1,1.0
is being implemented based,1,1,1.0
being implemented based upon,1,1,1.0
implemented based upon plete,1,1,1.0
based upon plete confusion,1,1,1.0
upon plete confusion matrix,1,1,1.0
plete confusion matrix and,1,1,1.0
confusion matrix and that,1,1,1.0
matrix and that confusion,1,1,1.0
and that confusion matrix,1,1,1.0
that confusion matrix is,1,1,1.0
confusion matrix is essential,1,1,1.0
matrix is essential for,1,1,1.0
is essential for the,1,1,1.0
essential for the issues,1,1,1.0
for the issues related,1,1,1.0
the issues related to,1,1,1.0
issues related to imbalanced,1,1,1.0
related to imbalanced classification,1,1,1.0
to imbalanced classification relative,1,1,1.0
imbalanced classification relative to,1,1,1.0
classification relative to the,1,1,1.0
relative to the case,1,1,1.0
to the case of,1,1,1.0
the case of the,8,2,4.0
case of the confusion,1,1,1.0
of the confusion matrix,1,1,1.0
the confusion matrix it,1,1,1.0
confusion matrix it is,1,1,1.0
matrix it is evident,1,1,1.0
it is evident that,1,1,1.0
is evident that the,1,1,1.0
evident that the performance,1,1,1.0
that the performance categorization,1,1,1.0
the performance categorization for,1,1,1.0
performance categorization for negative,1,1,1.0
categorization for negative and,1,1,1.0
for negative and positive,2,2,1.0
negative and positive classes,2,2,1.0
and positive classes can,1,1,1.0
positive classes can be,1,1,1.0
classes can be discerned,1,1,1.0
can be discerned the,1,1,1.0
be discerned the tpr,1,1,1.0
discerned the tpr recall,1,1,1.0
the tpr recall or,1,1,1.0
tpr recall or true,1,1,1.0
recall or true positive,1,1,1.0
or true positive becomes,1,1,1.0
true positive becomes tpr,1,1,1.0
positive becomes tpr tp,1,1,1.0
becomes tpr tp fn,1,1,1.0
tpr tp fn the,1,1,1.0
tp fn the resultant,1,1,1.0
fn the resultant value,1,1,1.0
the resultant value obtained,1,1,1.0
resultant value obtained from,1,1,1.0
value obtained from the,2,2,1.0
obtained from the equation,1,1,1.0
from the equation above,1,1,1.0
the equation above represents,1,1,1.0
equation above represents the,1,1,1.0
above represents the percentage,1,1,1.0
represents the percentage of,2,1,2.0
the percentage of positive,2,1,2.0
percentage of positive situations,2,1,2.0
of positive situations that,1,1,1.0
positive situations that have,1,1,1.0
situations that have been,1,1,1.0
that have been categorized,1,1,1.0
have been categorized accurately,1,1,1.0
been categorized accurately regarding,1,1,1.0
categorized accurately regarding fpr,1,1,1.0
accurately regarding fpr the,1,1,1.0
regarding fpr the rate,1,1,1.0
fpr the rate it,1,1,1.0
the rate it represents,1,1,1.0
rate it represents the,1,1,1.0
it represents the percentage,1,1,1.0
the percentage of negative,1,1,1.0
percentage of negative situations,1,1,1.0
of negative situations that,1,1,1.0
negative situations that have,1,1,1.0
situations that have not,1,1,1.0
that have not been,1,1,1.0
have not been categorized,1,1,1.0
not been categorized accurately,1,1,1.0
been categorized accurately it,1,1,1.0
categorized accurately it is,1,1,1.0
accurately it is expressed,1,1,1.0
it is expressed as,1,1,1.0
is expressed as fnr,2,1,2.0
expressed as fnr tp,2,1,2.0
as fnr tp fn,2,1,2.0
fnr tp fn table,1,1,1.0
tp fn table confusion,1,1,1.0
fn table confusion matrix,1,1,1.0
confusion matrix for a,1,1,1.0
matrix for a sample,1,1,1.0
for a sample problem,1,1,1.0
a sample problem actual,1,1,1.0
sample problem actual class,1,1,1.0
problem actual class predicted,1,1,1.0
actual class predicted class,1,1,1.0
class predicted class positive,1,1,1.0
predicted class positive negative,1,1,1.0
class positive negative negative,1,1,1.0
positive negative negative class,1,1,1.0
negative negative class fp,1,1,1.0
negative class fp false,1,1,1.0
class fp false positive,1,1,1.0
fp false positive tn,2,2,1.0
false positive tn true,2,2,1.0
positive tn true negative,2,2,1.0
tn true negative positive,1,1,1.0
true negative positive class,1,1,1.0
negative positive class tp,1,1,1.0
positive class tp true,1,1,1.0
class tp true positive,1,1,1.0
tp true positive fn,2,2,1.0
true positive fn false,2,2,1.0
positive fn false negative,2,2,1.0
fn false negative lastly,1,1,1.0
false negative lastly the,1,1,1.0
negative lastly the fnr,1,1,1.0
lastly the fnr which,1,1,1.0
the fnr which indicates,1,1,1.0
fnr which indicates the,1,1,1.0
which indicates the rate,1,1,1.0
indicates the rate it,1,1,1.0
the rate it refers,1,1,1.0
rate it refers to,1,1,1.0
it refers to the,1,1,1.0
refers to the percentage,1,1,1.0
to the percentage of,1,1,1.0
of positive situations where,1,1,1.0
positive situations where there,1,1,1.0
situations where there has,1,1,1.0
where there has been,1,1,1.0
there has been priate,1,1,1.0
has been priate categorization,1,1,1.0
been priate categorization the,1,1,1.0
priate categorization the parameter,1,1,1.0
categorization the parameter is,1,1,1.0
the parameter is expressed,1,1,1.0
parameter is expressed as,1,1,1.0
fnr tp fn combining,1,1,1.0
tp fn combining the,1,1,1.0
fn combining the fpr,1,1,1.0
combining the fpr and,1,1,1.0
the fpr and the,1,1,1.0
fpr and the tpr,1,1,1.0
and the tpr in,1,1,1.0
the tpr in one,1,1,1.0
tpr in one metric,1,1,1.0
in one metric calls,1,1,1.0
one metric calls for,1,1,1.0
metric calls for the,1,1,1.0
calls for the ting,1,1,1.0
for the ting of,1,1,1.0
the ting of tpr,1,1,1.0
ting of tpr against,1,1,1.0
of tpr against fpr,1,1,1.0
tpr against fpr the,1,1,1.0
against fpr the resultant,1,1,1.0
fpr the resultant curve,1,1,1.0
the resultant curve becomes,1,1,1.0
resultant curve becomes the,1,1,1.0
curve becomes the roc,1,1,1.0
becomes the roc receiver,1,1,1.0
the roc receiver operating,1,1,1.0
roc receiver operating characteristic,1,1,1.0
receiver operating characteristic the,1,1,1.0
operating characteristic the area,1,1,1.0
characteristic the area under,1,1,1.0
the area under the,4,3,1.3333333333333333
area under the curve,1,1,1.0
under the curve the,1,1,1.0
the curve the roc,1,1,1.0
curve the roc curve,1,1,1.0
the roc curve becomes,1,1,1.0
roc curve becomes the,1,1,1.0
curve becomes the auc,1,1,1.0
becomes the auc other,1,1,1.0
the auc other common,1,1,1.0
auc other common metrics,1,1,1.0
other common metrics through,1,1,1.0
common metrics through which,1,1,1.0
metrics through which the,1,1,1.0
through which the classes,1,1,1.0
which the classes joint,1,1,1.0
the classes joint mance,1,1,1.0
classes joint mance has,1,1,1.0
joint mance has been,1,1,1.0
mance has been maximized,1,1,1.0
has been maximized include,1,1,1.0
been maximized include the,1,1,1.0
maximized include the true,1,1,1.0
include the true rates,1,1,1.0
the true rates geometric,1,1,1.0
true rates geometric mean,1,1,1.0
rates geometric mean gm,1,1,1.0
geometric mean gm and,1,1,1.0
mean gm and the,1,1,1.0
gm and the auc,1,1,1.0
and the auc whereas,1,1,1.0
the auc whereas the,1,1,1.0
auc whereas the aim,1,1,1.0
whereas the aim of,1,1,1.0
the aim of the,1,1,1.0
aim of the gm,1,1,1.0
of the gm lies,1,1,1.0
the gm lies in,1,1,1.0
gm lies in racy,1,1,1.0
lies in racy maximization,1,1,1.0
in racy maximization relative,1,1,1.0
racy maximization relative to,1,1,1.0
maximization relative to the,1,1,1.0
relative to the respective,1,1,1.0
to the respective classes,2,2,1.0
the respective classes the,1,1,1.0
respective classes the auc,1,1,1.0
classes the auc ric,1,1,1.0
the auc ric seeks,1,1,1.0
auc ric seeks to,1,1,1.0
ric seeks to manifest,1,1,1.0
seeks to manifest between,1,1,1.0
to manifest between costs,1,1,1.0
manifest between costs and,1,1,1.0
between costs and benefits,1,1,1.0
costs and benefits gm,1,1,1.0
and benefits gm is,1,1,1.0
benefits gm is expressed,1,1,1.0
gm is expressed as,1,1,1.0
is expressed as gm,1,1,1.0
expressed as gm tprate,1,1,1.0
as gm tprate tnrate,1,1,1.0
gm tprate tnrate algorithm,1,1,1.0
tprate tnrate algorithm in,1,1,1.0
tnrate algorithm in situations,1,1,1.0
algorithm in situations where,1,1,1.0
in situations where distributed,1,1,1.0
situations where distributed environments,1,1,1.0
where distributed environments have,1,1,1.0
distributed environments have seen,1,1,1.0
environments have seen based,1,1,1.0
have seen based oversampling,1,1,1.0
seen based oversampling procedures,1,1,1.0
based oversampling procedures applied,1,1,1.0
oversampling procedures applied most,1,1,1.0
procedures applied most of,1,1,1.0
applied most of the,1,1,1.0
most of the results,1,1,1.0
of the results have,1,1,1.0
the results have proved,1,1,1.0
results have proved less,1,1,1.0
have proved less successful,1,1,1.0
proved less successful this,1,1,1.0
less successful this failure,1,1,1.0
successful this failure continues,1,1,1.0
this failure continues to,1,1,1.0
failure continues to be,1,1,1.0
continues to be attributed,1,1,1.0
to be attributed to,1,1,1.0
be attributed to the,1,1,1.0
attributed to the data,1,1,1.0
to the data s,1,1,1.0
the data s random,1,1,1.0
data s random partitioning,1,1,1.0
s random partitioning as,1,1,1.0
random partitioning as well,1,1,1.0
partitioning as well as,1,1,1.0
as well as artificial,1,1,1.0
well as artificial sample,1,1,1.0
as artificial sample eration,1,1,1.0
artificial sample eration with,1,1,1.0
sample eration with these,1,1,1.0
eration with these parameters,1,1,1.0
with these parameters lacking,1,1,1.0
these parameters lacking spatial,1,1,1.0
parameters lacking spatial relationships,1,1,1.0
lacking spatial relationships in,1,1,1.0
spatial relationships in this,1,1,1.0
relationships in this study,1,1,1.0
in this study we,1,1,1.0
this study we try,1,1,1.0
study we try to,1,1,1.0
we try to resolve,1,1,1.0
try to resolve that,1,1,1.0
to resolve that issue,1,1,1.0
resolve that issue with,1,1,1.0
that issue with the,1,1,1.0
issue with the help,1,1,1.0
the help of effectually,1,1,1.0
help of effectually segmenting,1,1,1.0
of effectually segmenting and,1,1,1.0
effectually segmenting and dispensing,1,1,1.0
segmenting and dispensing the,1,1,1.0
and dispensing the dataset,1,1,1.0
dispensing the dataset in,1,1,1.0
the dataset in space,1,1,1.0
dataset in space apache,1,1,1.0
in space apache based,1,1,1.0
space apache based smote,1,1,1.0
apache based smote implementation,1,1,1.0
based smote implementation is,1,1,1.0
smote implementation is being,1,1,1.0
implementation is being represented,1,1,1.0
is being represented in,1,1,1.0
being represented in our,1,1,1.0
represented in our work,1,1,1.0
in our work for,1,1,1.0
our work for big,1,1,1.0
work for big data,1,1,1.0
for big data smaller,1,1,1.0
big data smaller minority,1,1,1.0
data smaller minority dataset,1,1,1.0
smaller minority dataset we,1,1,1.0
minority dataset we need,1,1,1.0
dataset we need to,1,1,1.0
we need to determine,1,1,1.0
need to determine est,1,1,1.0
to determine est neighbors,1,1,1.0
determine est neighbors for,1,1,1.0
est neighbors for a,1,1,1.0
neighbors for a point,1,1,1.0
for a point in,1,1,1.0
a point in d,1,1,1.0
point in d dimensional,1,1,1.0
in d dimensional space,1,1,1.0
d dimensional space in,1,1,1.0
dimensional space in order,1,1,1.0
space in order to,2,2,1.0
in order to inate,1,1,1.0
order to inate samples,1,1,1.0
to inate samples for,1,1,1.0
inate samples for minority,1,1,1.0
samples for minority class,1,1,1.0
for minority class synthetically,1,1,1.0
minority class synthetically for,1,1,1.0
class synthetically for the,1,1,1.0
synthetically for the context,1,1,1.0
for the context of,1,1,1.0
the context of issue,1,1,1.0
context of issue or,1,1,1.0
of issue or problem,1,1,1.0
issue or problem which,1,1,1.0
or problem which we,1,1,1.0
problem which we are,1,1,1.0
which we are tackling,1,1,1.0
we are tackling the,1,1,1.0
are tackling the data,1,1,1.0
tackling the data distribution,1,1,1.0
the data distribution to,1,1,1.0
data distribution to several,1,1,1.0
distribution to several different,1,1,1.0
to several different nodes,1,1,1.0
several different nodes which,1,1,1.0
different nodes which is,1,1,1.0
nodes which is done,1,1,1.0
which is done randomly,1,1,1.0
is done randomly in,1,1,1.0
done randomly in a,1,1,1.0
randomly in a distributed,1,1,1.0
in a distributed cluster,1,1,1.0
a distributed cluster might,1,1,1.0
distributed cluster might result,1,1,1.0
cluster might result in,1,1,1.0
might result in distribution,1,1,1.0
result in distribution of,1,1,1.0
in distribution of points,1,1,1.0
distribution of points to,1,1,1.0
of points to different,1,1,1.0
points to different nodes,1,1,1.0
to different nodes rather,1,1,1.0
different nodes rather than,1,1,1.0
nodes rather than their,1,1,1.0
rather than their nearest,1,1,1.0
than their nearest nodes,1,1,1.0
their nearest nodes hence,1,1,1.0
nearest nodes hence it,1,1,1.0
nodes hence it will,1,1,1.0
hence it will become,1,1,1.0
it will become unfeasible,1,1,1.0
will become unfeasible for,1,1,1.0
become unfeasible for different,1,1,1.0
unfeasible for different individual,1,1,1.0
for different individual nodes,1,1,1.0
different individual nodes to,1,1,1.0
individual nodes to identify,1,1,1.0
nodes to identify their,1,1,1.0
to identify their closest,1,1,1.0
identify their closest neighbors,1,1,1.0
their closest neighbors therefore,1,1,1.0
closest neighbors therefore for,1,1,1.0
neighbors therefore for problems,1,1,1.0
therefore for problems where,1,1,1.0
problems where the dataset,1,1,1.0
where the dataset is,1,1,1.0
the dataset is very,2,2,1.0
dataset is very big,1,1,1.0
is very big and,1,1,1.0
very big and most,1,1,1.0
big and most probably,1,1,1.0
and most probably distributed,1,1,1.0
most probably distributed across,1,1,1.0
probably distributed across a,1,1,1.0
distributed across a cluster,1,1,1.0
across a cluster of,1,1,1.0
cluster of machines it,1,1,1.0
of machines it is,1,1,1.0
machines it is significant,1,1,1.0
it is significant that,1,1,1.0
is significant that the,1,1,1.0
significant that the identical,1,1,1.0
that the identical samples,1,1,1.0
the identical samples are,1,1,1.0
identical samples are clustered,1,1,1.0
samples are clustered and,1,1,1.0
are clustered and samples,1,1,1.0
clustered and samples belonging,1,1,1.0
and samples belonging to,1,1,1.0
samples belonging to a,2,1,2.0
belonging to a cluster,2,1,2.0
to a cluster brought,1,1,1.0
a cluster brought to,1,1,1.0
cluster brought to a,1,1,1.0
brought to a machine,1,1,1.0
to a machine so,1,1,1.0
a machine so that,1,1,1.0
machine so that all,1,1,1.0
so that all the,1,1,1.0
that all the samples,2,1,2.0
all the samples ing,1,1,1.0
the samples ing to,1,1,1.0
samples ing to a,1,1,1.0
ing to a cluster,1,1,1.0
to a cluster now,1,1,1.0
a cluster now are,1,1,1.0
cluster now are present,1,1,1.0
now are present on,1,1,1.0
are present on a,2,1,2.0
present on a single,1,1,1.0
a single machine that,1,1,1.0
single machine that is,1,1,1.0
machine that is we,1,1,1.0
that is we the,1,1,1.0
is we the data,1,1,1.0
we the data in,1,1,1.0
the data in such,1,1,1.0
data in such a,1,1,1.0
in such a way,3,1,3.0
such a way that,3,1,3.0
a way that all,1,1,1.0
way that all the,1,1,1.0
all the samples which,1,1,1.0
the samples which belong,1,1,1.0
samples which belong to,1,1,1.0
which belong to a,1,1,1.0
belong to a cluster,1,1,1.0
to a cluster are,1,1,1.0
a cluster are present,1,1,1.0
cluster are present on,1,1,1.0
present on a machine,1,1,1.0
on a machine these,1,1,1.0
a machine these different,1,1,1.0
machine these different ters,1,1,1.0
these different ters on,1,1,1.0
different ters on different,1,1,1.0
ters on different machines,1,1,1.0
on different machines can,1,1,1.0
different machines can then,1,1,1.0
machines can then be,1,1,1.0
can then be processed,1,1,1.0
then be processed in,1,1,1.0
be processed in parallel,1,1,1.0
processed in parallel to,1,1,1.0
in parallel to erate,1,1,1.0
parallel to erate synthetic,1,1,1.0
to erate synthetic data,1,1,1.0
erate synthetic data using,1,1,1.0
synthetic data using smote,1,1,1.0
data using smote though,1,1,1.0
using smote though the,1,1,1.0
smote though the dataset,1,1,1.0
though the dataset is,1,1,1.0
the dataset is huge,1,1,1.0
dataset is huge the,1,1,1.0
is huge the individual,1,1,1.0
huge the individual groups,1,1,1.0
the individual groups that,1,1,1.0
individual groups that we,1,1,1.0
groups that we create,1,1,1.0
that we create can,1,1,1.0
we create can fit,1,1,1.0
create can fit on,1,1,1.0
can fit on a,1,1,1.0
fit on a single,1,1,1.0
a single machine and,1,1,1.0
single machine and hence,1,1,1.0
machine and hence can,1,1,1.0
and hence can be,1,1,1.0
hence can be processed,1,1,1.0
can be processed independently,1,1,1.0
be processed independently and,1,1,1.0
processed independently and in,1,1,1.0
independently and in parallel,1,1,1.0
and in parallel thus,1,1,1.0
in parallel thus ing,1,1,1.0
parallel thus ing motivation,1,1,1.0
thus ing motivation to,1,1,1.0
ing motivation to solve,1,1,1.0
motivation to solve this,1,1,1.0
to solve this complex,1,1,1.0
solve this complex problem,1,1,1.0
this complex problem during,1,1,1.0
complex problem during the,1,1,1.0
problem during the application,1,1,1.0
during the application of,1,1,1.0
the application of smote,1,1,1.0
application of smote the,1,1,1.0
of smote the main,1,1,1.0
smote the main objective,1,1,1.0
the main objective is,1,1,1.0
main objective is to,1,1,1.0
objective is to mine,1,1,1.0
is to mine the,1,1,1.0
to mine the most,1,1,1.0
mine the most efficient,1,1,1.0
the most efficient way,1,1,1.0
most efficient way through,1,1,1.0
efficient way through which,1,1,1.0
way through which the,1,1,1.0
through which the data,1,1,1.0
which the data could,1,1,1.0
the data could be,1,1,1.0
data could be tioned,1,1,1.0
could be tioned one,1,1,1.0
be tioned one of,1,1,1.0
tioned one of the,1,1,1.0
one of the perceived,1,1,1.0
of the perceived convenient,1,1,1.0
the perceived convenient ways,1,1,1.0
perceived convenient ways involves,1,1,1.0
convenient ways involves the,1,1,1.0
ways involves the random,1,1,1.0
involves the random grouping,1,1,1.0
the random grouping of,1,1,1.0
random grouping of data,1,1,1.0
grouping of data to,1,1,1.0
of data to obtain,1,1,1.0
data to obtain m,1,1,1.0
to obtain m clusters,1,1,1.0
obtain m clusters despite,1,1,1.0
m clusters despite the,1,1,1.0
clusters despite the contributory,1,1,1.0
despite the contributory role,1,1,1.0
the contributory role of,1,1,1.0
contributory role of this,1,1,1.0
role of this technique,1,1,1.0
of this technique it,1,1,1.0
this technique it remains,1,1,1.0
technique it remains notable,1,1,1.0
remains notable that it,1,1,1.0
notable that it requires,1,1,1.0
that it requires that,1,1,1.0
it requires that each,1,1,1.0
requires that each query,1,1,1.0
that each query is,1,1,1.0
each query is run,1,1,1.0
query is run through,1,1,1.0
is run through the,1,1,1.0
run through the clusters,1,1,1.0
through the clusters in,1,1,1.0
the clusters in the,1,1,1.0
clusters in the entirety,1,1,1.0
in the entirety algorithms,1,1,1.0
the entirety algorithms detail,1,1,1.0
entirety algorithms detail the,1,1,1.0
algorithms detail the techniques,1,1,1.0
detail the techniques for,1,1,1.0
the techniques for the,1,1,1.0
techniques for the searching,1,1,1.0
for the searching indexing,1,1,1.0
the searching indexing and,1,1,1.0
searching indexing and partitioning,1,1,1.0
indexing and partitioning hooda,1,1,1.0
and partitioning hooda and,1,1,1.0
partitioning hooda and mann,1,1,1.0
computational intelligence systems given,1,1,1.0
intelligence systems given larger,1,1,1.0
systems given larger sets,1,1,1.0
given larger sets of,1,1,1.0
larger sets of data,1,1,1.0
sets of data they,1,1,1.0
of data they could,1,1,1.0
data they could be,1,1,1.0
they could be clustered,1,1,1.0
could be clustered or,1,1,1.0
be clustered or segmented,1,1,1.0
clustered or segmented using,1,1,1.0
or segmented using different,1,1,1.0
segmented using different techniques,1,1,1.0
using different techniques an,1,1,1.0
different techniques an example,1,1,1.0
techniques an example of,1,1,1.0
an example of a,2,2,1.0
example of a fast,1,1,1.0
of a fast algorithm,1,1,1.0
a fast algorithm that,1,1,1.0
fast algorithm that could,1,1,1.0
algorithm that could achieve,1,1,1.0
that could achieve this,1,1,1.0
could achieve this role,1,1,1.0
achieve this role is,1,1,1.0
this role is scalable,1,1,1.0
role is scalable in,1,1,1.0
is scalable in this,1,1,1.0
scalable in this study,1,1,1.0
in this study the,1,1,1.0
this study the spark,1,1,1.0
study the spark implementation,1,1,1.0
the spark implementation of,1,1,1.0
spark implementation of scalable,1,1,1.0
implementation of scalable k,1,1,1.0
of scalable k is,1,1,1.0
scalable k is employed,1,1,1.0
k is employed the,1,1,1.0
is employed the use,1,1,1.0
employed the use of,1,1,1.0
the use of this,2,2,1.0
use of this approach,1,1,1.0
of this approach is,1,1,1.0
this approach is to,1,1,1.0
approach is to ensure,1,1,1.0
is to ensure that,1,1,1.0
to ensure that large,1,1,1.0
ensure that large sets,1,1,1.0
that large sets of,1,1,1.0
large sets of data,2,1,2.0
sets of data are,2,1,2.0
of data are segmented,1,1,1.0
data are segmented to,1,1,1.0
are segmented to yield,1,1,1.0
segmented to yield m,1,1,1.0
to yield m spaces,1,1,1.0
yield m spaces upon,1,1,1.0
m spaces upon data,1,1,1.0
spaces upon data clustering,1,1,1.0
upon data clustering are,1,1,1.0
data clustering are established,1,1,1.0
clustering are established for,1,1,1.0
are established for sample,1,1,1.0
established for sample indexing,1,1,1.0
for sample indexing before,1,1,1.0
sample indexing before establishing,1,1,1.0
indexing before establishing the,1,1,1.0
before establishing the nearest,1,1,1.0
establishing the nearest for,1,1,1.0
the nearest for the,1,1,1.0
nearest for the respective,1,1,1.0
for the respective samples,2,1,2.0
the respective samples indeed,1,1,1.0
respective samples indeed serves,1,1,1.0
samples indeed serves the,1,1,1.0
indeed serves the role,1,1,1.0
serves the role of,1,1,1.0
the role of partitioning,1,1,1.0
role of partitioning objects,1,1,1.0
of partitioning objects via,1,1,1.0
partitioning objects via relative,1,1,1.0
objects via relative distances,1,1,1.0
via relative distances in,1,1,1.0
relative distances in this,1,1,1.0
distances in this investigation,1,1,1.0
in this investigation the,1,1,1.0
this investigation the euclidean,1,1,1.0
investigation the euclidean approach,1,1,1.0
the euclidean approach is,1,1,1.0
euclidean approach is employed,1,1,1.0
approach is employed and,1,1,1.0
is employed and seeks,1,1,1.0
employed and seeks to,1,1,1.0
and seeks to ensure,1,1,1.0
seeks to ensure that,1,1,1.0
ensure that the objects,1,1,1.0
that the objects are,1,1,1.0
the objects are formed,1,1,1.0
objects are formed into,1,1,1.0
are formed into nodes,1,1,1.0
formed into nodes aligning,1,1,1.0
into nodes aligning them,1,1,1.0
nodes aligning them to,1,1,1.0
aligning them to the,1,1,1.0
them to the metric,1,1,1.0
the metric space constrained,1,1,1.0
metric space constrained regions,1,1,1.0
space constrained regions upon,1,1,1.0
constrained regions upon point,1,1,1.0
regions upon point clustering,1,1,1.0
upon point clustering each,1,1,1.0
point clustering each cluster,1,1,1.0
clustering each cluster of,1,1,1.0
each cluster of points,1,1,1.0
cluster of points is,1,1,1.0
of points is pushed,1,1,1.0
points is pushed to,1,1,1.0
is pushed to the,1,1,1.0
pushed to the tive,1,1,1.0
to the tive nodes,1,1,1.0
the tive nodes before,1,1,1.0
tive nodes before being,1,1,1.0
nodes before being arranged,1,1,1.0
before being arranged relative,1,1,1.0
being arranged relative to,1,1,1.0
arranged relative to distances,1,1,1.0
relative to distances given,1,1,1.0
to distances given the,1,1,1.0
distances given the respective,1,1,1.0
given the respective samples,1,1,1.0
the respective samples the,1,1,1.0
respective samples the nearest,1,1,1.0
samples the nearest neighbors,1,1,1.0
the nearest neighbors are,1,1,1.0
nearest neighbors are searched,1,1,1.0
neighbors are searched before,1,1,1.0
are searched before generating,1,1,1.0
searched before generating synthetic,1,1,1.0
before generating synthetic samples,1,1,1.0
generating synthetic samples dataset,1,1,1.0
synthetic samples dataset the,1,1,1.0
samples dataset the represents,1,1,1.0
dataset the represents the,1,1,1.0
the represents the data,1,1,1.0
represents the data set,1,1,1.0
the data set employed,1,1,1.0
data set employed in,1,1,1.0
set employed in this,1,1,1.0
employed in this tion,1,1,1.0
in this tion in,1,1,1.0
this tion in the,1,1,1.0
tion in the context,1,1,1.0
in the context of,4,3,1.3333333333333333
the context of vancouver,1,1,1.0
context of vancouver canada,1,1,1.0
of vancouver canada this,1,1,1.0
vancouver canada this set,1,1,1.0
canada this set was,1,1,1.0
this set was used,1,1,1.0
set was used at,1,1,1.0
was used at one,1,1,1.0
used at one of,1,1,1.0
at one of the,1,1,1.0
one of the data,1,1,1.0
of the data mining,1,1,1.0
the data mining competitions,1,1,1.0
data mining competitions the,1,1,1.0
mining competitions the set,1,1,1.0
competitions the set involves,1,1,1.0
the set involves a,1,1,1.0
set involves a big,1,1,1.0
involves a big data,1,1,1.0
a big data problem,1,1,1.0
big data problem with,1,1,1.0
data problem with imbalance,1,1,1.0
problem with imbalance bioinformatics,1,1,1.0
with imbalance bioinformatics whereas,1,1,1.0
imbalance bioinformatics whereas there,1,1,1.0
bioinformatics whereas there were,1,1,1.0
whereas there were attributes,1,1,1.0
there were attributes and,1,1,1.0
were attributes and million,1,1,1.0
attributes and million instances,1,1,1.0
and million instances positive,1,1,1.0
million instances positive instances,1,1,1.0
instances positive instances were,1,1,1.0
positive instances were found,1,1,1.0
instances were found to,1,1,1.0
were found to be,2,2,1.0
found to be this,1,1,1.0
to be this study,1,1,1.0
be this study relies,1,1,1.0
this study relies on,1,1,1.0
study relies on s,1,1,1.0
relies on s two,1,1,1.0
on s two imbalanced,1,1,1.0
s two imbalanced sets,1,1,1.0
two imbalanced sets of,1,1,1.0
imbalanced sets of data,1,1,1.0
sets of data the,1,1,1.0
of data the sets,1,1,1.0
data the sets are,1,1,1.0
the sets are generated,1,1,1.0
sets are generated in,1,1,1.0
are generated in such,1,1,1.0
generated in such a,1,1,1.0
a way that the,2,1,2.0
way that the same,1,1,1.0
that the same algorithm,1,1,1.0
the same algorithm cluster,1,1,1.0
same algorithm cluster the,1,1,1.0
algorithm cluster the dataset,1,1,1.0
cluster the dataset cluster,1,1,1.0
the dataset cluster the,1,1,1.0
dataset cluster the samples,1,1,1.0
cluster the samples using,1,1,1.0
the samples using the,1,1,1.0
samples using the scalable,1,1,1.0
using the scalable api,1,1,1.0
the scalable api provided,1,1,1.0
scalable api provided by,1,1,1.0
api provided by spark,1,1,1.0
provided by spark ml,1,1,1.0
by spark ml library,1,1,1.0
spark ml library number,1,1,1.0
ml library number of,1,1,1.0
library number of groups,1,1,1.0
number of groups given,1,1,1.0
of groups given as,1,1,1.0
groups given as input,1,1,1.0
given as input for,1,1,1.0
as input for clustering,1,1,1.0
input for clustering should,1,1,1.0
for clustering should be,1,1,1.0
clustering should be equal,1,1,1.0
should be equal to,1,1,1.0
be equal to the,1,1,1.0
equal to the number,2,2,1.0
the number of for,1,1,1.0
number of for each,1,1,1.0
of for each sample,1,1,1.0
for each sample do,2,1,2.0
each sample do id,1,1,1.0
sample do id closest,1,1,1.0
do id closest pivot,1,1,1.0
id closest pivot for,1,1,1.0
closest pivot for each,1,1,1.0
pivot for each point,1,1,1.0
for each point end,1,1,1.0
each point end for,1,1,1.0
point end for each,1,1,1.0
end for each sample,1,1,1.0
each sample do calculate,1,1,1.0
sample do calculate tance,1,1,1.0
do calculate tance with,1,1,1.0
calculate tance with pivots,1,1,1.0
tance with pivots parent,1,1,1.0
with pivots parent pivot,1,1,1.0
pivots parent pivot pivot,1,1,1.0
parent pivot pivot with,1,1,1.0
pivot pivot with shortest,1,1,1.0
pivot with shortest distance,1,1,1.0
with shortest distance done,1,1,1.0
shortest distance done for,1,1,1.0
distance done for each,1,1,1.0
done for each cluster,1,1,1.0
for each cluster do,1,1,1.0
each cluster do push,1,1,1.0
cluster do push each,1,1,1.0
do push each cluster,1,1,1.0
push each cluster to,1,1,1.0
each cluster to a,1,1,1.0
cluster to a different,1,1,1.0
to a different done,1,1,1.0
a different done up,1,1,1.0
different done up sampling,1,1,1.0
done up sampling ratio,1,1,1.0
up sampling ratio and,1,1,1.0
sampling ratio and nearest,1,1,1.0
ratio and nearest neighbors,1,1,1.0
and nearest neighbors k,1,1,1.0
nearest neighbors k provided,1,1,1.0
neighbors k provided as,1,1,1.0
k provided as input,1,1,1.0
provided as input number,1,1,1.0
as input number of,1,1,1.0
input number of synthetic,1,1,1.0
number of synthetic data,3,2,1.5
of synthetic data to,1,1,1.0
synthetic data to be,1,1,1.0
data to be generated,1,1,1.0
to be generated per,1,1,1.0
be generated per sample,1,1,1.0
generated per sample m,1,1,1.0
per sample m is,1,1,1.0
sample m is given,1,1,1.0
m is given by,1,1,1.0
is given by this,1,1,1.0
given by this part,1,1,1.0
by this part runs,1,1,1.0
this part runs on,1,1,1.0
part runs on individual,1,1,1.0
runs on individual given,1,1,1.0
on individual given the,1,1,1.0
individual given the respective,1,1,1.0
given the respective partition,1,1,1.0
the respective partition perform,1,1,1.0
respective partition perform the,1,1,1.0
partition perform the procedure,1,1,1.0
perform the procedure for,1,1,1.0
the procedure for ual,1,1,1.0
procedure for ual samples,1,1,1.0
for ual samples also,1,1,1.0
ual samples also establish,1,1,1.0
samples also establish an,1,1,1.0
also establish an the,1,1,1.0
establish an the second,1,1,1.0
an the second algorithm,1,1,1.0
the second algorithm end,1,1,1.0
second algorithm end for,1,1,1.0
algorithm end for the,1,1,1.0
end for the respective,1,1,1.0
the respective samples in,1,1,1.0
respective samples in the,1,1,1.0
samples in the selected,1,1,1.0
in the selected or,1,1,1.0
the selected or target,1,1,1.0
selected or target partitions,1,1,1.0
or target partitions before,1,1,1.0
target partitions before searching,1,1,1.0
partitions before searching k,1,1,1.0
before searching k nearest,1,1,1.0
searching k nearest neighbors,1,1,1.0
k nearest neighbors from,2,2,1.0
neighbors from the establish,1,1,1.0
from the establish the,1,1,1.0
the establish the third,1,1,1.0
establish the third algorithm,1,1,1.0
the third algorithm given,1,1,1.0
third algorithm given the,1,1,1.0
algorithm given the individual,1,1,1.0
given the individual k,1,1,1.0
the individual k neighbors,1,1,1.0
individual k neighbors establish,1,1,1.0
k neighbors establish m,1,1,1.0
neighbors establish m thetic,1,1,1.0
establish m thetic samples,1,1,1.0
m thetic samples to,1,1,1.0
thetic samples to achieve,1,1,1.0
samples to achieve the,1,1,1.0
to achieve the fourth,1,1,1.0
achieve the fourth algorithm,1,1,1.0
the fourth algorithm algorithm,1,1,1.0
fourth algorithm algorithm build,1,1,1.0
algorithm algorithm build m,1,1,1.0
algorithm build m tree,1,1,1.0
build m tree this,1,1,1.0
m tree this algorithm,1,1,1.0
tree this algorithm takes,1,1,1.0
this algorithm takes the,1,1,1.0
algorithm takes the capacity,1,1,1.0
takes the capacity of,1,1,1.0
capacity of the c,1,1,1.0
of the c as,1,1,1.0
the c as the,1,1,1.0
c as the input,1,1,1.0
as the input init,1,1,1.0
the input init add,1,1,1.0
input init add the,1,1,1.0
init add the first,1,1,1.0
add the first point,1,1,1.0
the first point as,1,1,1.0
first point as a,1,1,1.0
point as a router,1,1,1.0
as a router for,1,1,1.0
a router for p,1,1,1.0
router for p each,1,1,1.0
for p each sample,1,1,1.0
p each sample in,1,1,1.0
each sample in the,1,1,1.0
sample in the partition,1,1,1.0
in the partition do,2,1,2.0
the partition do calculate,1,1,1.0
partition do calculate the,1,1,1.0
do calculate the distances,1,1,1.0
calculate the distances from,1,1,1.0
the distances from each,1,1,1.0
distances from each router,1,1,1.0
from each router choose,1,1,1.0
each router choose the,1,1,1.0
router choose the router,1,1,1.0
choose the router with,1,1,1.0
the router with minimum,1,1,1.0
router with minimum distance,1,1,1.0
with minimum distance if,1,1,1.0
minimum distance if distance,1,1,1.0
distance if distance router,1,1,1.0
if distance router covering,1,1,1.0
distance router covering radius,1,1,1.0
router covering radius update,1,1,1.0
covering radius update covering,1,1,1.0
radius update covering radius,1,1,1.0
update covering radius add,1,1,1.0
covering radius add the,1,1,1.0
radius add the sample,1,1,1.0
add the sample as,1,1,1.0
the sample as leaf,1,1,1.0
sample as leaf node,1,1,1.0
as leaf node to,1,1,1.0
leaf node to the,1,1,1.0
node to the router,1,1,1.0
to the router increment,1,1,1.0
the router increment leaf,1,1,1.0
router increment leaf count,1,1,1.0
increment leaf count if,1,1,1.0
leaf count if leaf,1,1,1.0
count if leaf count,1,1,1.0
if leaf count maxleafsize,1,1,1.0
leaf count maxleafsize promote,1,1,1.0
count maxleafsize promote another,1,1,1.0
maxleafsize promote another leaf,1,1,1.0
promote another leaf node,1,1,1.0
another leaf node to,1,1,1.0
leaf node to router,1,1,1.0
node to router split,1,1,1.0
to router split the,1,1,1.0
router split the leaf,1,1,1.0
split the leaf nodes,1,1,1.0
the leaf nodes between,1,1,1.0
leaf nodes between the,1,1,1.0
nodes between the two,1,1,1.0
between the two routers,1,1,1.0
the two routers done,1,1,1.0
two routers done algorithm,1,1,1.0
routers done algorithm finding,1,1,1.0
done algorithm finding nearest,1,1,1.0
algorithm finding nearest neighbors,1,1,1.0
finding nearest neighbors for,1,1,1.0
nearest neighbors for each,1,1,1.0
neighbors for each sample,1,1,1.0
for each sample p,2,1,2.0
each sample p in,2,1,2.0
sample p in the,1,1,1.0
p in the partition,1,1,1.0
the partition do store,1,1,1.0
partition do store the,1,1,1.0
do store the first,1,1,1.0
store the first k,1,1,1.0
the first k points,1,1,1.0
first k points in,1,1,1.0
k points in a,1,1,1.0
points in a priority,1,1,1.0
in a priority queue,1,1,1.0
a priority queue of,1,1,1.0
priority queue of size,1,1,1.0
queue of size k,1,1,1.0
of size k priority,1,1,1.0
size k priority distance,1,1,1.0
k priority distance from,1,1,1.0
priority distance from point,1,1,1.0
distance from point p,1,1,1.0
from point p smaller,1,1,1.0
point p smaller distance,1,1,1.0
p smaller distance means,1,1,1.0
smaller distance means higher,1,1,1.0
distance means higher priority,1,1,1.0
means higher priority if,1,1,1.0
higher priority if distance,1,1,1.0
priority if distance of,1,1,1.0
if distance of sample,1,1,1.0
distance of sample p,1,1,1.0
of sample p from,1,1,1.0
sample p from router,1,1,1.0
p from router distance,1,1,1.0
from router distance of,1,1,1.0
router distance of another,1,1,1.0
distance of another sample,1,1,1.0
of another sample belonging,1,1,1.0
another sample belonging to,1,1,1.0
sample belonging to the,1,1,1.0
belonging to the router,1,1,1.0
to the router the,1,1,1.0
the router the max,1,1,1.0
router the max distance,1,1,1.0
the max distance of,1,1,1.0
max distance of the,1,1,1.0
distance of the samples,1,1,1.0
of the samples stored,1,1,1.0
the samples stored in,1,1,1.0
samples stored in the,1,1,1.0
stored in the priority,1,1,1.0
in the priority q,2,1,2.0
the priority q ignore,1,1,1.0
priority q ignore the,1,1,1.0
q ignore the sample,1,1,1.0
ignore the sample else,1,1,1.0
the sample else calculate,1,1,1.0
sample else calculate the,1,1,1.0
else calculate the distance,1,1,1.0
calculate the distance from,1,1,1.0
the distance from sample,1,1,1.0
distance from sample p,1,1,1.0
from sample p and,1,1,1.0
sample p and store,1,1,1.0
p and store the,1,1,1.0
and store the sample,1,1,1.0
store the sample in,1,1,1.0
the sample in the,1,1,1.0
sample in the priority,1,1,1.0
the priority q end,1,1,1.0
priority q end if,1,1,1.0
q end if done,1,1,1.0
end if done class,1,1,1.0
if done class distribution,1,1,1.0
done class distribution is,1,1,1.0
class distribution is considered,1,1,1.0
distribution is considered for,1,1,1.0
is considered for training,1,1,1.0
considered for training purposes,1,1,1.0
for training purposes the,1,1,1.0
training purposes the tigation,1,1,1.0
purposes the tigation also,1,1,1.0
the tigation also considers,1,1,1.0
tigation also considers a,1,1,1.0
also considers a larger,1,1,1.0
considers a larger dataset,1,1,1.0
a larger dataset however,1,1,1.0
larger dataset however when,1,1,1.0
dataset however when it,1,1,1.0
however when it comes,1,1,1.0
when it comes to,1,1,1.0
it comes to testing,1,1,1.0
comes to testing procedure,1,1,1.0
to testing procedure a,1,1,1.0
testing procedure a smaller,1,1,1.0
procedure a smaller set,1,1,1.0
a smaller set is,1,1,1.0
smaller set is used,1,1,1.0
set is used with,1,1,1.0
is used with senting,1,1,1.0
used with senting a,1,1,1.0
with senting a large,1,1,1.0
senting a large number,1,1,1.0
large number of features,1,1,1.0
number of features the,2,2,1.0
of features the feature,1,1,1.0
features the feature reduction,1,1,1.0
the feature reduction procedure,1,1,1.0
feature reduction procedure hooda,1,1,1.0
reduction procedure hooda and,1,1,1.0
procedure hooda and mann,1,1,1.0
computational intelligence systems algorithm,1,1,1.0
intelligence systems algorithm smote,1,1,1.0
systems algorithm smote for,1,1,1.0
algorithm smote for all,1,1,1.0
smote for all the,1,1,1.0
for all the of,1,1,1.0
all the of samples,1,1,1.0
the of samples for,1,1,1.0
of samples for each,1,1,1.0
samples for each sample,1,1,1.0
sample p in partition,1,1,1.0
p in partition for,1,1,1.0
in partition for each,1,1,1.0
partition for each of,1,1,1.0
each of the k,1,1,1.0
of the k neighbors,1,1,1.0
the k neighbors gap,1,1,1.0
k neighbors gap random,1,1,1.0
neighbors gap random number,1,1,1.0
gap random number between,1,1,1.0
number between and synthetic,1,1,1.0
between and synthetic attribute,1,1,1.0
and synthetic attribute p,1,1,1.0
synthetic attribute p attribute,1,1,1.0
attribute p attribute gap,1,1,1.0
p attribute gap difference,1,1,1.0
attribute gap difference in,1,1,1.0
gap difference in values,1,1,1.0
difference in values of,1,1,1.0
in values of this,1,1,1.0
values of this attribute,1,1,1.0
of this attribute between,1,1,1.0
this attribute between p,1,1,1.0
attribute between p and,1,1,1.0
between p and neighbor,1,1,1.0
p and neighbor done,1,1,1.0
and neighbor done the,1,1,1.0
neighbor done the choice,1,1,1.0
done the choice for,1,1,1.0
the choice for n,1,1,1.0
choice for n took,1,1,1.0
for n took some,1,1,1.0
n took some experimentation,1,1,1.0
took some experimentation we,1,1,1.0
some experimentation we tried,1,1,1.0
experimentation we tried for,1,1,1.0
we tried for various,1,1,1.0
tried for various distinct,1,1,1.0
for various distinct values,1,1,1.0
various distinct values of,1,1,1.0
distinct values of n,1,1,1.0
values of n and,1,1,1.0
of n and found,1,1,1.0
n and found that,1,1,1.0
and found that the,1,1,1.0
found that the optimum,1,1,1.0
that the optimum results,1,1,1.0
the optimum results were,1,1,1.0
optimum results were obtained,1,1,1.0
results were obtained for,1,1,1.0
were obtained for n,1,1,1.0
obtained for n has,1,1,1.0
for n has been,1,1,1.0
n has been employed,1,1,1.0
has been employed to,1,1,1.0
been employed to ensure,1,1,1.0
employed to ensure that,1,1,1.0
to ensure that relevant,1,1,1.0
ensure that relevant and,1,1,1.0
that relevant and utmost,1,1,1.0
relevant and utmost ate,1,1,1.0
and utmost ate features,1,1,1.0
utmost ate features are,1,1,1.0
ate features are acquired,1,1,1.0
features are acquired in,1,1,1.0
are acquired in particular,1,1,1.0
acquired in particular the,1,1,1.0
in particular the feature,1,1,1.0
particular the feature reduction,1,1,1.0
the feature reduction lates,1,1,1.0
feature reduction lates into,1,1,1.0
reduction lates into concentration,1,1,1.0
lates into concentration on,1,1,1.0
into concentration on out,1,1,1.0
concentration on out of,1,1,1.0
on out of the,1,1,1.0
out of the original,1,1,1.0
of the original features,1,1,1.0
the original features using,1,1,1.0
original features using the,1,1,1.0
features using the hadoop,1,1,1.0
using the hadoop map,1,1,1.0
the hadoop map reduce,1,1,1.0
hadoop map reduce approach,1,1,1.0
map reduce approach the,1,1,1.0
reduce approach the smote,1,1,1.0
approach the smote implementation,1,1,1.0
the smote implementation which,1,1,1.0
smote implementation which is,1,1,1.0
implementation which is distributed,1,1,1.0
which is distributed is,1,1,1.0
is distributed is benchmarked,1,1,1.0
distributed is benchmarked using,1,1,1.0
is benchmarked using the,1,1,1.0
benchmarked using the dataset,1,1,1.0
using the dataset to,1,1,1.0
the dataset to compare,1,1,1.0
dataset to compare the,1,1,1.0
to compare the performance,1,1,1.0
compare the performance of,1,1,1.0
performance of the algorithm,2,1,2.0
of the algorithm with,1,1,1.0
the algorithm with the,1,1,1.0
algorithm with the results,1,1,1.0
with the results obtained,1,1,1.0
the results obtained from,2,2,1.0
results obtained from previous,1,1,1.0
obtained from previous approaches,1,1,1.0
from previous approaches such,1,1,1.0
previous approaches such as,1,1,1.0
approaches such as python,1,1,1.0
such as python implementation,1,1,1.0
as python implementation of,1,1,1.0
python implementation of smote,1,1,1.0
implementation of smote two,1,1,1.0
of smote two additional,1,1,1.0
smote two additional sets,1,1,1.0
two additional sets of,1,1,1.0
additional sets of data,1,1,1.0
of data are used,1,1,1.0
data are used they,1,1,1.0
are used they include,1,1,1.0
used they include sat,1,1,1.0
they include sat image,1,1,1.0
include sat image and,1,1,1.0
sat image and yeast,1,1,1.0
image and yeast which,1,1,1.0
and yeast which originate,1,1,1.0
yeast which originate form,1,1,1.0
which originate form uci,1,1,1.0
originate form uci viz,1,1,1.0
form uci viz abalone,1,1,1.0
uci viz abalone the,1,1,1.0
viz abalone the framework,1,1,1.0
abalone the framework or,1,1,1.0
the framework or we,1,1,1.0
framework or we can,1,1,1.0
we can say infrastructure,1,1,1.0
can say infrastructure which,1,1,1.0
say infrastructure which was,1,1,1.0
infrastructure which was utilized,1,1,1.0
which was utilized for,1,1,1.0
was utilized for these,1,1,1.0
utilized for these experiments,1,1,1.0
for these experiments consist,1,1,1.0
these experiments consist of,1,1,1.0
experiments consist of a,1,1,1.0
consist of a cluster,1,1,1.0
of a cluster of,1,1,1.0
a cluster of four,1,1,1.0
cluster of four centos,1,1,1.0
of four centos linux,1,1,1.0
four centos linux machines,1,1,1.0
centos linux machines each,1,1,1.0
linux machines each having,1,1,1.0
machines each having eight,1,1,1.0
each having eight cores,1,1,1.0
having eight cores and,1,1,1.0
eight cores and gb,1,1,1.0
cores and gb ram,1,1,1.0
and gb ram spark,1,1,1.0
gb ram spark was,1,1,1.0
ram spark was used,1,1,1.0
spark was used to,1,1,1.0
was used to configure,1,1,1.0
used to configure the,1,1,1.0
to configure the cluster,1,1,1.0
configure the cluster and,1,1,1.0
the cluster and input,1,1,1.0
cluster and input as,1,1,1.0
and input as well,1,1,1.0
input as well as,1,1,1.0
as well as output,1,1,1.0
well as output data,1,1,1.0
as output data was,1,1,1.0
output data was stored,1,1,1.0
data was stored with,1,1,1.0
was stored with the,1,1,1.0
stored with the help,1,1,1.0
the help of hadoop,1,1,1.0
help of hadoop hdfs,1,1,1.0
of hadoop hdfs analysis,1,1,1.0
hadoop hdfs analysis the,1,1,1.0
hdfs analysis the input,1,1,1.0
analysis the input parameters,1,1,1.0
the input parameters to,1,1,1.0
input parameters to the,1,1,1.0
parameters to the algorithm,1,1,1.0
to the algorithm were,1,1,1.0
the algorithm were as,1,1,1.0
algorithm were as follows,1,1,1.0
were as follows the,1,1,1.0
as follows the number,2,2,1.0
follows the number of,2,2,1.0
the number of clusters,3,1,3.0
number of clusters to,1,1,1.0
of clusters to be,1,1,1.0
clusters to be created,1,1,1.0
to be created by,1,1,1.0
be created by scalable,1,1,1.0
created by scalable was,1,1,1.0
by scalable was taken,1,1,1.0
scalable was taken as,1,1,1.0
was taken as eight,1,1,1.0
taken as eight the,1,1,1.0
as eight the of,1,1,1.0
eight the of clusters,1,1,1.0
the of clusters must,1,1,1.0
of clusters must be,1,1,1.0
clusters must be tuned,1,1,1.0
must be tuned so,1,1,1.0
be tuned so that,1,1,1.0
tuned so that it,1,1,1.0
so that it is,1,1,1.0
that it is neither,1,1,1.0
it is neither too,1,1,1.0
is neither too high,1,1,1.0
neither too high nor,1,1,1.0
too high nor too,1,1,1.0
high nor too low,1,1,1.0
nor too low providing,1,1,1.0
too low providing a,1,1,1.0
low providing a small,1,1,1.0
providing a small number,1,1,1.0
small number of clusters,1,1,1.0
number of clusters may,1,1,1.0
of clusters may again,1,1,1.0
clusters may again lead,1,1,1.0
may again lead to,1,1,1.0
again lead to a,1,1,1.0
lead to a problem,1,1,1.0
to a problem where,1,1,1.0
a problem where all,1,1,1.0
problem where all the,1,1,1.0
where all the samples,1,1,1.0
all the samples belonging,1,1,1.0
the samples belonging to,1,1,1.0
to a cluster may,1,1,1.0
a cluster may not,1,1,1.0
cluster may not fit,1,1,1.0
may not fit into,1,1,1.0
not fit into a,1,1,1.0
fit into a single,1,1,1.0
into a single machine,1,1,1.0
a single machine disk,1,1,1.0
single machine disk memory,1,1,1.0
machine disk memory setting,1,1,1.0
disk memory setting the,1,1,1.0
memory setting the number,1,1,1.0
setting the number of,1,1,1.0
number of clusters too,1,1,1.0
of clusters too high,1,1,1.0
clusters too high though,1,1,1.0
too high though may,1,1,1.0
high though may increase,1,1,1.0
though may increase the,1,1,1.0
may increase the parallelism,1,1,1.0
increase the parallelism and,1,1,1.0
the parallelism and throughput,1,1,1.0
parallelism and throughput but,1,1,1.0
and throughput but may,1,1,1.0
throughput but may cause,1,1,1.0
but may cause the,1,1,1.0
may cause the samples,1,1,1.0
cause the samples which,1,1,1.0
the samples which are,1,1,1.0
samples which are similar,1,1,1.0
which are similar and,1,1,1.0
are similar and near,1,1,1.0
similar and near to,1,1,1.0
and near to one,1,1,1.0
near to one another,1,1,1.0
to one another to,1,1,1.0
one another to fall,1,1,1.0
another to fall into,1,1,1.0
to fall into different,1,1,1.0
fall into different clusters,1,1,1.0
into different clusters thereby,1,1,1.0
different clusters thereby decreasing,1,1,1.0
clusters thereby decreasing the,1,1,1.0
thereby decreasing the quality,1,1,1.0
decreasing the quality of,1,1,1.0
the quality of the,3,1,3.0
quality of the synthetic,1,1,1.0
of the synthetic data,1,1,1.0
the synthetic data produced,1,1,1.0
synthetic data produced as,1,1,1.0
data produced as the,1,1,1.0
produced as the smote,1,1,1.0
as the smote algorithm,1,1,1.0
the smote algorithm relies,1,1,1.0
smote algorithm relies heavily,1,1,1.0
algorithm relies heavily on,1,1,1.0
relies heavily on finding,1,1,1.0
heavily on finding the,1,1,1.0
on finding the nearest,1,1,1.0
finding the nearest neighbors,1,1,1.0
the nearest neighbors this,1,1,1.0
nearest neighbors this number,1,1,1.0
neighbors this number must,1,1,1.0
this number must be,1,1,1.0
number must be tuned,1,1,1.0
must be tuned based,1,1,1.0
be tuned based on,1,1,1.0
tuned based on the,1,1,1.0
based on the dimension,1,1,1.0
on the dimension of,1,1,1.0
the dimension of the,1,1,1.0
dimension of the dataset,1,1,1.0
of the dataset and,1,1,1.0
the dataset and the,2,2,1.0
dataset and the segment,1,1,1.0
and the segment cluster,1,1,1.0
the segment cluster specifications,1,1,1.0
segment cluster specifications with,1,1,1.0
cluster specifications with the,1,1,1.0
specifications with the emphasis,1,1,1.0
with the emphasis on,1,1,1.0
the emphasis on using,1,1,1.0
emphasis on using the,1,1,1.0
on using the least,1,1,1.0
using the least ble,1,1,1.0
the least ble for,1,1,1.0
least ble for the,1,1,1.0
ble for the cluster,1,1,1.0
for the cluster configuration,1,1,1.0
the cluster configuration another,1,1,1.0
cluster configuration another crucial,1,1,1.0
configuration another crucial input,1,1,1.0
another crucial input parameter,1,1,1.0
crucial input parameter that,1,1,1.0
input parameter that is,1,1,1.0
parameter that is worth,1,1,1.0
that is worth considering,1,1,1.0
is worth considering involves,1,1,1.0
worth considering involves the,1,1,1.0
considering involves the number,1,1,1.0
involves the number of,1,1,1.0
the number of nearest,3,1,3.0
number of nearest numbers,1,1,1.0
of nearest numbers that,1,1,1.0
nearest numbers that need,1,1,1.0
numbers that need to,1,1,1.0
that need to be,3,2,1.5
need to be used,1,1,1.0
to be used to,1,1,1.0
be used to ensure,1,1,1.0
used to ensure that,1,1,1.0
ensure that the synthetic,1,1,1.0
that the synthetic data,1,1,1.0
the synthetic data is,1,1,1.0
synthetic data is generated,1,1,1.0
data is generated particularly,1,1,1.0
is generated particularly the,1,1,1.0
generated particularly the stage,1,1,1.0
particularly the stage focuses,1,1,1.0
the stage focuses on,1,1,1.0
stage focuses on the,1,1,1.0
focuses on the number,1,1,1.0
on the number of,2,2,1.0
number of nearest neighbors,4,2,2.0
of nearest neighbors to,2,2,1.0
nearest neighbors to be,1,1,1.0
neighbors to be sought,1,1,1.0
to be sought from,1,1,1.0
be sought from the,1,1,1.0
sought from the respective,1,1,1.0
from the respective ples,1,1,1.0
the respective ples upon,1,1,1.0
respective ples upon which,1,1,1.0
ples upon which the,1,1,1.0
upon which the smote,1,1,1.0
which the smote algorithm,1,1,1.0
the smote algorithm could,1,1,1.0
smote algorithm could be,1,1,1.0
algorithm could be used,1,1,1.0
could be used to,1,1,1.0
be used to establish,1,1,1.0
used to establish synthetic,1,1,1.0
to establish synthetic data,1,1,1.0
establish synthetic data in,1,1,1.0
synthetic data in this,1,1,1.0
data in this investigation,1,1,1.0
in this investigation five,1,1,1.0
this investigation five is,1,1,1.0
investigation five is the,1,1,1.0
five is the number,1,1,1.0
is the number at,1,1,1.0
the number at which,1,1,1.0
number at which the,1,1,1.0
at which the number,1,1,1.0
which the number of,1,1,1.0
of nearest neighbors used,2,2,1.0
nearest neighbors used to,2,2,1.0
neighbors used to generate,2,2,1.0
generate synthetic data is,1,1,1.0
synthetic data is kept,1,1,1.0
data is kept in,1,1,1.0
is kept in the,1,1,1.0
kept in the study,1,1,1.0
in the study by,1,1,1.0
the study by chawla,1,1,1.0
study by chawla this,1,1,1.0
by chawla this approach,1,1,1.0
chawla this approach was,1,1,1.0
this approach was employed,1,1,1.0
approach was employed ratio,1,1,1.0
was employed ratio determines,1,1,1.0
employed ratio determines the,1,1,1.0
ratio determines the fraction,1,1,1.0
determines the fraction by,1,1,1.0
the fraction by which,1,1,1.0
fraction by which we,1,1,1.0
by which we need,1,1,1.0
which we need to,1,1,1.0
we need to the,1,1,1.0
need to the minority,1,1,1.0
to the minority dataset,1,1,1.0
the minority dataset a,1,1,1.0
minority dataset a ratio,1,1,1.0
dataset a ratio of,1,1,1.0
a ratio of one,1,1,1.0
ratio of one would,1,1,1.0
of one would generate,1,1,1.0
one would generate the,1,1,1.0
would generate the minority,1,1,1.0
generate the minority samples,1,1,1.0
the minority samples so,1,1,1.0
minority samples so that,1,1,1.0
samples so that they,1,1,1.0
so that they are,1,1,1.0
that they are equal,1,1,1.0
they are equal in,1,1,1.0
are equal in count,1,1,1.0
equal in count with,1,1,1.0
in count with the,1,1,1.0
count with the majority,1,1,1.0
with the majority samples,1,1,1.0
the majority samples providing,1,1,1.0
majority samples providing an,1,1,1.0
samples providing an ratio,1,1,1.0
providing an ratio of,1,1,1.0
an ratio of will,1,1,1.0
ratio of will create,1,1,1.0
of will create a,1,1,1.0
will create a final,1,1,1.0
create a final distribution,1,1,1.0
a final distribution of,1,1,1.0
final distribution of majority,1,1,1.0
distribution of majority samples,1,1,1.0
of majority samples vs,1,1,1.0
majority samples vs minority,1,1,1.0
samples vs minority samples,1,1,1.0
vs minority samples to,1,1,1.0
minority samples to be,1,1,1.0
samples to be in,1,1,1.0
be in the ratio,1,1,1.0
in the ratio of,1,1,1.0
the ratio of are,1,1,1.0
ratio of are balanced,1,1,1.0
of are balanced trees,1,1,1.0
are balanced trees and,1,1,1.0
balanced trees and each,1,1,1.0
trees and each routing,1,1,1.0
and each routing node,1,1,1.0
each routing node of,1,1,1.0
routing node of the,1,1,1.0
node of the tree,1,1,1.0
of the tree has,1,1,1.0
the tree has a,1,1,1.0
tree has a maximum,1,1,1.0
has a maximum size,1,1,1.0
a maximum size capacity,1,1,1.0
maximum size capacity of,1,1,1.0
size capacity of leaf,1,1,1.0
capacity of leaf nodes,1,1,1.0
of leaf nodes that,1,1,1.0
leaf nodes that it,1,1,1.0
nodes that it can,1,1,1.0
that it can store,1,1,1.0
it can store the,1,1,1.0
can store the knn,1,1,1.0
store the knn search,1,1,1.0
the knn search for,1,1,1.0
knn search for a,1,1,1.0
search for a sample,1,1,1.0
for a sample is,1,1,1.0
a sample is performed,1,1,1.0
sample is performed in,1,1,1.0
is performed in each,1,1,1.0
performed in each of,1,1,1.0
in each of these,2,2,1.0
each of these leaf,1,1,1.0
of these leaf nodes,1,1,1.0
these leaf nodes ing,1,1,1.0
leaf nodes ing to,1,1,1.0
nodes ing to the,1,1,1.0
ing to the routing,1,1,1.0
to the routing node,1,1,1.0
routing node a large,1,1,1.0
node a large capacity,1,1,1.0
a large capacity may,1,1,1.0
large capacity may improve,1,1,1.0
capacity may improve the,1,1,1.0
may improve the quality,1,1,1.0
improve the quality of,1,1,1.0
the quality of nearest,1,1,1.0
quality of nearest neighbor,1,1,1.0
of nearest neighbor search,1,1,1.0
nearest neighbor search but,1,1,1.0
neighbor search but may,1,1,1.0
search but may impact,1,1,1.0
but may impact the,1,1,1.0
may impact the throughput,1,1,1.0
impact the throughput as,1,1,1.0
the throughput as larger,1,1,1.0
throughput as larger number,1,1,1.0
as larger number of,1,1,1.0
larger number of samples,1,1,1.0
number of samples need,1,1,1.0
of samples need to,1,1,1.0
samples need to be,1,1,1.0
need to be searched,1,1,1.0
to be searched for,1,1,1.0
be searched for identifying,1,1,1.0
searched for identifying the,1,1,1.0
for identifying the nearest,1,1,1.0
identifying the nearest neighbors,1,1,1.0
the nearest neighbors while,1,1,1.0
nearest neighbors while a,1,1,1.0
neighbors while a smaller,1,1,1.0
while a smaller leaf,1,1,1.0
a smaller leaf capacity,1,1,1.0
smaller leaf capacity may,1,1,1.0
leaf capacity may split,1,1,1.0
capacity may split the,1,1,1.0
may split the similar,1,1,1.0
split the similar ples,1,1,1.0
the similar ples into,1,1,1.0
similar ples into two,1,1,1.0
ples into two different,1,1,1.0
into two different groups,1,1,1.0
two different groups thereby,1,1,1.0
different groups thereby reducing,1,1,1.0
groups thereby reducing the,1,1,1.0
thereby reducing the quality,1,1,1.0
reducing the quality of,1,1,1.0
quality of the samples,2,1,2.0
of the samples generated,2,1,2.0
the samples generated tuning,1,1,1.0
samples generated tuning the,1,1,1.0
generated tuning the capacity,1,1,1.0
tuning the capacity of,1,1,1.0
capacity of the is,1,1,1.0
the is important and,1,1,1.0
is important and for,1,1,1.0
important and for this,1,1,1.0
and for this experiment,1,1,1.0
for this experiment we,2,2,1.0
this experiment we arrived,1,1,1.0
experiment we arrived at,1,1,1.0
we arrived at a,1,1,1.0
arrived at a size,1,1,1.0
at a size of,1,1,1.0
a size of after,1,1,1.0
size of after few,1,1,1.0
of after few iments,1,1,1.0
after few iments minority,1,1,1.0
few iments minority class,1,1,1.0
iments minority class data,1,1,1.0
minority class data was,1,1,1.0
class data was originated,1,1,1.0
data was originated synthetically,1,1,1.0
was originated synthetically with,1,1,1.0
originated synthetically with the,1,1,1.0
synthetically with the help,1,1,1.0
the help of our,1,1,1.0
help of our algorithms,1,1,1.0
of our algorithms then,1,1,1.0
our algorithms then we,1,1,1.0
algorithms then we merged,1,1,1.0
then we merged that,1,1,1.0
we merged that data,1,1,1.0
merged that data with,1,1,1.0
that data with the,1,1,1.0
data with the data,1,1,1.0
with the data set,1,1,1.0
the data set which,1,1,1.0
data set which we,1,1,1.0
set which we have,1,1,1.0
which we have set,1,1,1.0
we have set alongside,1,1,1.0
have set alongside for,1,1,1.0
set alongside for the,1,1,1.0
alongside for the purpose,1,1,1.0
for the purpose of,3,2,1.5
the purpose of training,1,1,1.0
purpose of training given,1,1,1.0
of training given the,1,1,1.0
training given the ecbdl,1,1,1.0
given the ecbdl set,1,1,1.0
the ecbdl set of,2,1,2.0
ecbdl set of data,2,1,2.0
set of data the,1,1,1.0
of data the accuracy,1,1,1.0
data the accuracy of,1,1,1.0
accuracy of the model,1,1,1.0
of the model was,1,1,1.0
the model was assessed,1,1,1.0
model was assessed using,1,1,1.0
was assessed using classification,1,1,1.0
assessed using classification algorithms,1,1,1.0
using classification algorithms linked,1,1,1.0
classification algorithms linked to,1,1,1.0
algorithms linked to the,1,1,1.0
linked to the distributed,1,1,1.0
to the distributed random,1,1,1.0
the distributed random forest,1,1,1.0
distributed random forest drf,1,1,1.0
random forest drf this,1,1,1.0
forest drf this process,1,1,1.0
drf this process targeted,1,1,1.0
this process targeted the,1,1,1.0
process targeted the test,1,1,1.0
targeted the test data,1,1,1.0
the test data that,2,2,1.0
test data that was,1,1,1.0
data that was achieved,1,1,1.0
that was achieved after,1,1,1.0
was achieved after feature,1,1,1.0
achieved after feature reduction,1,1,1.0
after feature reduction the,1,1,1.0
feature reduction the model,1,1,1.0
reduction the model parameters,1,1,1.0
the model parameters employed,1,1,1.0
model parameters employed in,1,1,1.0
parameters employed in this,1,1,1.0
employed in this study,1,1,1.0
in this study included,1,1,1.0
this study included sat,1,1,1.0
study included sat image,1,1,1.0
included sat image datasets,1,1,1.0
sat image datasets and,1,1,1.0
image datasets and the,1,1,1.0
datasets and the scikit,1,1,1.0
and the scikit python,1,1,1.0
the scikit python implementation,1,1,1.0
scikit python implementation of,1,1,1.0
python implementation of random,1,1,1.0
implementation of random forest,1,1,1.0
of random forest for,1,1,1.0
random forest for the,1,1,1.0
forest for the abalone,1,1,1.0
for the abalone tables,1,1,1.0
the abalone tables and,1,1,1.0
abalone tables and from,1,1,1.0
tables and from the,1,1,1.0
and from the spark,1,1,1.0
from the spark implementation,1,1,1.0
the spark implementation the,1,1,1.0
spark implementation the synthetic,1,1,1.0
implementation the synthetic data,1,1,1.0
the synthetic data s,1,1,1.0
synthetic data s accuracy,1,1,1.0
data s accuracy was,1,1,1.0
s accuracy was benchmarked,1,1,1.0
accuracy was benchmarked via,1,1,1.0
was benchmarked via minority,1,1,1.0
benchmarked via minority data,1,1,1.0
via minority data generation,1,1,1.0
minority data generation facilitated,1,1,1.0
data generation facilitated through,1,1,1.0
generation facilitated through smote,1,1,1.0
facilitated through smote s,1,1,1.0
through smote s python,1,1,1.0
smote s python implementation,1,1,1.0
s python implementation also,1,1,1.0
python implementation also various,1,1,1.0
implementation also various input,1,1,1.0
also various input parameters,1,1,1.0
various input parameters aided,1,1,1.0
input parameters aided in,1,1,1.0
parameters aided in the,1,1,1.0
aided in the tuning,1,1,1.0
in the tuning of,1,1,1.0
the tuning of the,1,1,1.0
tuning of the performance,1,1,1.0
of the performance of,2,2,1.0
of the algorithm indeed,1,1,1.0
the algorithm indeed optimal,1,1,1.0
algorithm indeed optimal outcomes,1,1,1.0
indeed optimal outcomes were,1,1,1.0
optimal outcomes were realized,1,1,1.0
outcomes were realized at,1,1,1.0
were realized at a,1,1,1.0
realized at a point,1,1,1.0
at a point when,1,1,1.0
a point when four,1,1,1.0
point when four was,1,1,1.0
when four was set,1,1,1.0
four was set as,1,1,1.0
was set as the,1,1,1.0
set as the number,1,1,1.0
as the number of,1,1,1.0
number of clusters with,1,1,1.0
of clusters with the,1,1,1.0
clusters with the set,1,1,1.0
with the set s,1,1,1.0
the set s leaf,1,1,1.0
set s leaf size,1,1,1.0
s leaf size set,1,1,1.0
leaf size set at,1,1,1.0
size set at for,1,1,1.0
set at for the,1,1,1.0
at for the case,1,1,1.0
for the case of,4,2,2.0
case of the smaller,1,1,1.0
of the smaller set,1,1,1.0
the smaller set of,1,1,1.0
smaller set of was,1,1,1.0
set of was less,1,1,1.0
of was less than,1,1,1.0
was less than for,1,1,1.0
less than for the,1,1,1.0
than for the larger,1,1,1.0
for the larger data,1,1,1.0
the larger data set,1,1,1.0
larger data set optimal,1,1,1.0
data set optimal outcomes,1,1,1.0
set optimal outcomes were,1,1,1.0
optimal outcomes were also,1,1,1.0
outcomes were also table,1,1,1.0
were also table distribution,1,1,1.0
also table distribution for,1,1,1.0
table distribution for the,1,1,1.0
distribution for the dataset,1,1,1.0
for the dataset being,1,1,1.0
the dataset being used,1,1,1.0
dataset being used dataset,1,1,1.0
being used dataset attr,1,1,1.0
used dataset attr class,1,1,1.0
dataset attr class max,1,1,1.0
attr class max min,1,1,1.0
class max min class,2,1,2.0
max min class max,1,1,1.0
min class max min,1,1,1.0
max min class ecbdl,1,1,1.0
min class ecbdl million,1,1,1.0
class ecbdl million samples,1,1,1.0
ecbdl million samples keel,1,1,1.0
million samples keel samples,1,1,1.0
samples keel samples keel,1,1,1.0
keel samples keel samples,1,1,1.0
samples keel samples uci,1,1,1.0
keel samples uci satimage,1,1,1.0
samples uci satimage samples,1,1,1.0
uci satimage samples table,1,1,1.0
satimage samples table input,1,1,1.0
samples table input parameters,1,1,1.0
table input parameters parameter,1,1,1.0
input parameters parameter value,1,1,1.0
parameters parameter value of,1,1,1.0
parameter value of clusters,1,1,1.0
value of clusters nearest,1,1,1.0
of clusters nearest neighbor,1,1,1.0
clusters nearest neighbor upsampling,1,1,1.0
nearest neighbor upsampling ratio,1,1,1.0
neighbor upsampling ratio capacity,1,1,1.0
upsampling ratio capacity hooda,1,1,1.0
ratio capacity hooda and,1,1,1.0
capacity hooda and mann,1,1,1.0
computational intelligence systems achieved,1,1,1.0
intelligence systems achieved when,1,1,1.0
systems achieved when the,1,1,1.0
achieved when the study,1,1,1.0
when the study s,1,1,1.0
the study s conditions,1,1,1.0
study s conditions were,1,1,1.0
s conditions were set,1,1,1.0
conditions were set in,1,1,1.0
were set in such,1,1,1.0
set in such a,1,1,1.0
way that the leaf,1,1,1.0
that the leaf size,1,1,1.0
the leaf size was,1,1,1.0
leaf size was set,1,1,1.0
size was set at,1,1,1.0
was set at and,1,1,1.0
set at and the,1,1,1.0
at and the cluster,1,1,1.0
and the cluster number,1,1,1.0
the cluster number established,1,1,1.0
cluster number established at,1,1,1.0
number established at or,1,1,1.0
established at or tables,1,1,1.0
at or tables and,1,1,1.0
or tables and represent,1,1,1.0
tables and represent the,1,1,1.0
and represent the confusion,1,1,1.0
represent the confusion matrix,1,1,1.0
the confusion matrix gm,1,1,1.0
confusion matrix gm recall,1,1,1.0
matrix gm recall and,1,1,1.0
gm recall and auc,1,1,1.0
recall and auc for,1,1,1.0
and auc for the,1,1,1.0
auc for the implemented,1,1,1.0
for the implemented drf,1,1,1.0
the implemented drf and,1,1,1.0
implemented drf and random,1,1,1.0
drf and random forest,1,1,1.0
and random forest indeed,1,1,1.0
random forest indeed the,1,1,1.0
forest indeed the outcomes,1,1,1.0
indeed the outcomes demonstrate,1,1,1.0
the outcomes demonstrate that,1,1,1.0
outcomes demonstrate that there,1,1,1.0
demonstrate that there is,1,1,1.0
that there is data,1,1,1.0
there is data sampling,1,1,1.0
is data sampling in,1,1,1.0
data sampling in relation,1,1,1.0
sampling in relation to,1,1,1.0
in relation to the,3,2,1.5
relation to the minority,1,1,1.0
to the minority data,1,1,1.0
the minority data findings,1,1,1.0
minority data findings suggest,1,1,1.0
data findings suggest further,1,1,1.0
findings suggest further that,1,1,1.0
suggest further that the,1,1,1.0
further that the spark,1,1,1.0
that the spark tation,1,1,1.0
the spark tation tends,1,1,1.0
spark tation tends to,1,1,1.0
tation tends to exceed,1,1,1.0
tends to exceed or,1,1,1.0
to exceed or match,1,1,1.0
exceed or match compared,1,1,1.0
or match compared to,1,1,1.0
match compared to a,1,1,1.0
compared to a case,1,1,1.0
to a case in,1,1,1.0
case in which the,1,1,1.0
in which the python,1,1,1.0
which the python smote,1,1,1.0
the python smote implementation,1,1,1.0
python smote implementation is,1,1,1.0
smote implementation is applied,1,1,1.0
implementation is applied these,1,1,1.0
is applied these observations,1,1,1.0
applied these observations are,1,1,1.0
these observations are evidenced,1,1,1.0
observations are evidenced by,1,1,1.0
are evidenced by the,1,1,1.0
evidenced by the information,1,1,1.0
by the information in,1,1,1.0
the information in tables,1,1,1.0
information in tables therefore,1,1,1.0
in tables therefore the,1,1,1.0
tables therefore the tributed,1,1,1.0
therefore the tributed spark,1,1,1.0
the tributed spark smote,1,1,1.0
tributed spark smote exhibits,1,1,1.0
spark smote exhibits superior,1,1,1.0
smote exhibits superior performance,2,1,2.0
exhibits superior performance with,2,1,2.0
superior performance with a,1,1,1.0
performance with a lar,1,1,1.0
with a lar trend,1,1,1.0
a lar trend observed,1,1,1.0
lar trend observed even,1,1,1.0
trend observed even in,1,1,1.0
observed even in situations,1,1,1.0
even in situations involving,1,1,1.0
in situations involving an,1,1,1.0
situations involving an increased,1,1,1.0
involving an increased amount,1,1,1.0
an increased amount of,1,1,1.0
increased amount of dataset,1,1,1.0
amount of dataset given,1,1,1.0
of dataset given a,1,1,1.0
dataset given a large,1,1,1.0
given a large dataset,1,1,1.0
a large dataset an,1,1,1.0
large dataset an application,1,1,1.0
dataset an application of,1,1,1.0
an application of smote,1,1,1.0
application of smote on,1,1,1.0
of smote on the,1,1,1.0
smote on the ecbdl,1,1,1.0
on the ecbdl set,1,1,1.0
set of data in,1,1,1.0
of data in this,1,1,1.0
data in this case,1,1,1.0
in this case is,1,1,1.0
this case is seen,1,1,1.0
case is seen to,1,1,1.0
is seen to fail,1,1,1.0
seen to fail to,1,1,1.0
to fail to steer,1,1,1.0
fail to steer improvements,1,1,1.0
to steer improvements in,1,1,1.0
steer improvements in minority,1,1,1.0
improvements in minority class,1,1,1.0
in minority class to,1,1,1.0
minority class to situations,1,1,1.0
class to situations where,1,1,1.0
to situations where smote,1,1,1.0
situations where smote is,1,1,1.0
where smote is not,1,1,1.0
smote is not employed,1,1,1.0
is not employed the,1,1,1.0
not employed the implication,1,1,1.0
employed the implication is,1,1,1.0
the implication is that,1,1,1.0
implication is that smote,1,1,1.0
is that smote is,1,1,1.0
that smote is unlikely,1,1,1.0
smote is unlikely to,1,1,1.0
is unlikely to prove,1,1,1.0
unlikely to prove beneficial,1,1,1.0
to prove beneficial to,1,1,1.0
prove beneficial to the,1,1,1.0
beneficial to the given,1,1,1.0
to the given dataset,1,1,1.0
the given dataset because,1,1,1.0
given dataset because does,1,1,1.0
dataset because does not,1,1,1.0
because does not yield,1,1,1.0
does not yield a,1,1,1.0
not yield a significant,1,1,1.0
yield a significant improvement,1,1,1.0
a significant improvement in,1,1,1.0
significant improvement in minority,1,1,1.0
improvement in minority class,1,1,1.0
in minority class prediction,1,1,1.0
minority class prediction on,1,1,1.0
class prediction on the,1,1,1.0
prediction on the other,1,1,1.0
the other hand distributed,1,1,1.0
other hand distributed smote,1,1,1.0
hand distributed smote exhibits,1,1,1.0
distributed smote exhibits superior,1,1,1.0
exhibits superior performance when,1,1,1.0
superior performance when employed,1,1,1.0
performance when employed in,1,1,1.0
when employed in situations,1,1,1.0
employed in situations involving,1,1,1.0
in situations involving large,2,1,2.0
situations involving large sets,1,1,1.0
involving large sets of,1,1,1.0
sets of data outperforming,1,1,1.0
of data outperforming sklearn,1,1,1.0
data outperforming sklearn python,1,1,1.0
outperforming sklearn python implementation,1,1,1.0
sklearn python implementation hence,1,1,1.0
python implementation hence the,1,1,1.0
implementation hence the results,1,1,1.0
hence the results validate,1,1,1.0
the results validate our,1,1,1.0
results validate our implementation,1,1,1.0
validate our implementation indeed,1,1,1.0
our implementation indeed the,1,1,1.0
implementation indeed the findings,1,1,1.0
indeed the findings demonstrate,1,1,1.0
the findings demonstrate that,1,1,1.0
findings demonstrate that the,1,1,1.0
demonstrate that the use,1,1,1.0
that the use of,1,1,1.0
the use of for,1,1,1.0
use of for lishing,1,1,1.0
of for lishing the,1,1,1.0
for lishing the nearest,1,1,1.0
lishing the nearest neighbor,1,1,1.0
the nearest neighbor and,1,1,1.0
nearest neighbor and also,1,1,1.0
neighbor and also the,1,1,1.0
and also the implementation,1,1,1.0
also the implementation of,1,1,1.0
the implementation of the,1,1,1.0
implementation of the distributing,1,1,1.0
of the distributing data,1,1,1.0
the distributing data aid,1,1,1.0
distributing data aid in,1,1,1.0
data aid in preserving,1,1,1.0
aid in preserving spatial,1,1,1.0
in preserving spatial sample,1,1,1.0
preserving spatial sample arrangements,1,1,1.0
spatial sample arrangements in,1,1,1.0
sample arrangements in turn,1,1,1.0
arrangements in turn artificial,1,1,1.0
in turn artificial samples,1,1,1.0
turn artificial samples are,1,1,1.0
artificial samples are generated,1,1,1.0
samples are generated ensuring,1,1,1.0
are generated ensuring that,1,1,1.0
generated ensuring that the,1,1,1.0
ensuring that the desired,1,1,1.0
that the desired scale,1,1,1.0
the desired scale and,1,1,1.0
desired scale and degree,1,1,1.0
scale and degree of,1,1,1.0
and degree of accuracy,1,1,1.0
degree of accuracy are,1,1,1.0
of accuracy are realized,1,1,1.0
accuracy are realized with,1,1,1.0
are realized with increasing,1,1,1.0
realized with increasing volume,1,1,1.0
with increasing volume of,2,1,2.0
increasing volume of datasets,1,1,1.0
volume of datasets table,1,1,1.0
of datasets table random,1,1,1.0
datasets table random forest,1,1,1.0
table random forest parameters,1,1,1.0
random forest parameters algorithm,1,1,1.0
forest parameters algorithm parameters,1,1,1.0
parameters algorithm parameters distributed,1,1,1.0
algorithm parameters distributed random,1,1,1.0
parameters distributed random forest,1,1,1.0
distributed random forest number,1,1,1.0
random forest number of,1,1,1.0
forest number of tress,1,1,1.0
number of tress sample,1,1,1.0
of tress sample rat,1,1,1.0
tress sample rat maximum,1,1,1.0
sample rat maximum tree,1,1,1.0
rat maximum tree depth,1,1,1.0
maximum tree depth nbins,1,1,1.0
tree depth nbins table,1,1,1.0
depth nbins table performance,1,1,1.0
nbins table performance comparison,1,1,1.0
table performance comparison abalone,1,1,1.0
performance comparison abalone dataset,1,1,1.0
comparison abalone dataset technique,1,1,1.0
abalone dataset technique auc,1,1,1.0
dataset technique auc recall,4,1,4.0
technique auc recall gm,4,1,4.0
auc recall gm confusion,4,1,4.0
recall gm confusion matrix,4,1,4.0
gm confusion matrix no,4,1,4.0
confusion matrix no sampling,4,1,4.0
matrix no sampling python,4,1,4.0
no sampling python version,4,1,4.0
sampling python version of,4,1,4.0
version of smote spark,4,1,4.0
of smote spark distributed,4,1,4.0
smote spark distributed smote,4,1,4.0
spark distributed smote gm,4,1,4.0
distributed smote gm geometric,4,1,4.0
smote gm geometric mean,4,1,4.0
gm geometric mean smote,4,1,4.0
geometric mean smote synthetic,4,1,4.0
mean smote synthetic minority,4,1,4.0
minority oversampling technique table,2,1,2.0
oversampling technique table performance,2,1,2.0
technique table performance comparison,2,1,2.0
table performance comparison y,1,1,1.0
performance comparison y dataset,1,1,1.0
comparison y dataset technique,1,1,1.0
y dataset technique auc,1,1,1.0
minority oversampling technique observations,1,1,1.0
oversampling technique observations given,1,1,1.0
technique observations given the,1,1,1.0
observations given the proposed,1,1,1.0
given the proposed algorithm,1,1,1.0
the proposed algorithm the,1,1,1.0
proposed algorithm the findings,1,1,1.0
algorithm the findings suggest,1,1,1.0
the findings suggest the,1,1,1.0
findings suggest the capability,1,1,1.0
suggest the capability of,1,1,1.0
the capability of realizing,1,1,1.0
capability of realizing quality,1,1,1.0
of realizing quality samples,1,1,1.0
realizing quality samples the,1,1,1.0
quality samples the eventuality,1,1,1.0
samples the eventuality is,1,1,1.0
eventuality is that the,1,1,1.0
is that the minority,1,1,1.0
that the minority ples,1,1,1.0
the minority ples prediction,1,1,1.0
minority ples prediction is,1,1,1.0
ples prediction is likely,1,1,1.0
prediction is likely to,1,1,1.0
likely to be improved,1,1,1.0
to be improved with,1,1,1.0
be improved with increasing,1,1,1.0
improved with increasing volume,1,1,1.0
increasing volume of data,1,1,1.0
volume of data the,1,1,1.0
of data the results,1,1,1.0
data the results indicate,1,1,1.0
the results indicate that,1,1,1.0
results indicate that the,1,1,1.0
indicate that the traditional,1,1,1.0
that the traditional smote,1,1,1.0
the traditional smote tends,1,1,1.0
traditional smote tends to,1,1,1.0
smote tends to be,1,1,1.0
tends to be less,1,1,1.0
to be less efficient,1,1,1.0
be less efficient making,1,1,1.0
less efficient making it,1,1,1.0
efficient making it less,1,1,1.0
making it less applicable,1,1,1.0
it less applicable in,1,1,1.0
less applicable in situations,1,1,1.0
applicable in situations involving,1,1,1.0
situations involving large datasets,1,1,1.0
involving large datasets previous,1,1,1.0
large datasets previous research,1,1,1.0
datasets previous research work,1,1,1.0
previous research work guided,1,1,1.0
research work guided this,1,1,1.0
work guided this investigation,1,1,1.0
guided this investigation paving,1,1,1.0
this investigation paving the,1,1,1.0
investigation paving the way,1,1,1.0
paving the way for,1,1,1.0
the way for the,1,1,1.0
way for the expansion,1,1,1.0
for the expansion of,1,1,1.0
the expansion of smote,1,1,1.0
expansion of smote for,1,1,1.0
of smote for a,1,1,1.0
smote for a distributed,1,1,1.0
for a distributed implementation,1,1,1.0
a distributed implementation from,1,1,1.0
distributed implementation from the,1,1,1.0
implementation from the outcomes,1,1,1.0
from the outcomes especially,1,1,1.0
the outcomes especially when,1,1,1.0
outcomes especially when various,1,1,1.0
especially when various parameters,1,1,1.0
when various parameters are,1,1,1.0
various parameters are benchmarked,1,1,1.0
parameters are benchmarked the,1,1,1.0
are benchmarked the tation,1,1,1.0
benchmarked the tation exhibits,1,1,1.0
the tation exhibits superior,1,1,1.0
tation exhibits superior performance,1,1,1.0
superior performance with our,1,1,1.0
performance with our implementation,1,1,1.0
with our implementation of,1,1,1.0
our implementation of smote,1,1,1.0
of smote we are,1,1,1.0
smote we are able,1,1,1.0
we are able to,1,1,1.0
are able to generate,1,1,1.0
able to generate synthetic,1,1,1.0
generate synthetic data in,1,1,1.0
synthetic data in a,1,1,1.0
data in a distributed,1,1,1.0
in a distributed ronment,1,1,1.0
a distributed ronment when,1,1,1.0
distributed ronment when the,1,1,1.0
ronment when the dataset,1,1,1.0
when the dataset may,1,1,1.0
the dataset may not,1,1,1.0
dataset may not fit,1,1,1.0
may not fit in,1,1,1.0
not fit in a,1,1,1.0
fit in a single,1,1,1.0
in a single machine,1,1,1.0
a single machine an,1,1,1.0
single machine an extension,1,1,1.0
machine an extension to,1,1,1.0
an extension to the,1,1,1.0
extension to the existing,1,1,1.0
to the existing implementation,1,1,1.0
the existing implementation can,1,1,1.0
existing implementation can include,1,1,1.0
implementation can include an,1,1,1.0
can include an lap,1,1,1.0
include an lap when,1,1,1.0
an lap when indexing,1,1,1.0
lap when indexing samples,1,1,1.0
when indexing samples in,1,1,1.0
indexing samples in so,1,1,1.0
samples in so that,1,1,1.0
in so that samples,1,1,1.0
so that samples which,1,1,1.0
that samples which are,1,1,1.0
samples which are at,1,1,1.0
which are at the,1,1,1.0
are at the partition,1,1,1.0
at the partition boundary,1,1,1.0
the partition boundary may,1,1,1.0
partition boundary may be,1,1,1.0
boundary may be included,1,1,1.0
may be included in,1,1,1.0
be included in both,1,1,1.0
included in both the,1,1,1.0
in both the partitions,1,1,1.0
both the partitions this,1,1,1.0
the partitions this may,1,1,1.0
partitions this may increase,1,1,1.0
this may increase the,1,1,1.0
may increase the quality,1,1,1.0
increase the quality of,1,1,1.0
the samples generated however,1,1,1.0
samples generated however it,1,1,1.0
generated however it may,1,1,1.0
however it may be,1,1,1.0
it may be possible,1,1,1.0
may be possible that,1,1,1.0
be possible that smote,1,1,1.0
possible that smote may,1,1,1.0
that smote may not,1,1,1.0
smote may not be,1,1,1.0
may not be the,2,2,1.0
not be the answer,1,1,1.0
be the answer of,1,1,1.0
the answer of all,1,1,1.0
answer of all the,1,1,1.0
of all the data,1,1,1.0
all the data imbalance,1,1,1.0
the data imbalance issues,1,1,1.0
data imbalance issues within,1,1,1.0
imbalance issues within class,1,1,1.0
issues within class imbalance,1,1,1.0
within class imbalance is,2,2,1.0
class imbalance is not,3,2,1.5
imbalance is not handled,1,1,1.0
is not handled properly,1,1,1.0
not handled properly with,1,1,1.0
handled properly with this,1,1,1.0
properly with this implementation,1,1,1.0
this implementation of distributed,1,1,1.0
implementation of distributed smote,1,1,1.0
of distributed smote there,1,1,1.0
distributed smote there is,1,1,1.0
smote there is a,1,1,1.0
there is a need,2,1,2.0
is a need to,1,1,1.0
a need to merge,1,1,1.0
need to merge the,1,1,1.0
to merge the current,1,1,1.0
merge the current solution,1,1,1.0
the current solution with,1,1,1.0
current solution with the,1,1,1.0
solution with the techniques,1,1,1.0
with the techniques to,1,1,1.0
the techniques to tify,1,1,1.0
techniques to tify within,1,1,1.0
to tify within class,1,1,1.0
tify within class imbalance,1,1,1.0
within class imbalance and,2,2,1.0
class imbalance and handle,1,1,1.0
imbalance and handle the,1,1,1.0
and handle the within,1,1,1.0
handle the within class,1,1,1.0
the within class imbalance,2,2,1.0
within class imbalance accordingly,1,1,1.0
class imbalance accordingly also,1,1,1.0
imbalance accordingly also standard,1,1,1.0
accordingly also standard might,1,1,1.0
also standard might not,1,1,1.0
standard might not be,1,1,1.0
might not be optimum,1,1,1.0
not be optimum class,1,1,1.0
be optimum class semination,1,1,1.0
optimum class semination in,1,1,1.0
class semination in order,1,1,1.0
semination in order to,1,1,1.0
in order to find,1,1,1.0
order to find the,1,1,1.0
the solution for data,1,1,1.0
solution for data problems,1,1,1.0
for data problems which,1,1,1.0
data problems which are,1,1,1.0
problems which are imbalanced,1,1,1.0
which are imbalanced so,1,1,1.0
are imbalanced so the,1,1,1.0
imbalanced so the ratio,1,1,1.0
so the ratio may,1,1,1.0
the ratio may be,1,1,1.0
ratio may be varied,1,1,1.0
may be varied to,1,1,1.0
be varied to achieve,1,1,1.0
varied to achieve a,1,1,1.0
to achieve a better,1,1,1.0
achieve a better accuracy,1,1,1.0
a better accuracy in,1,1,1.0
better accuracy in different,1,1,1.0
accuracy in different scenarios,1,1,1.0
in different scenarios in,1,1,1.0
different scenarios in future,1,1,1.0
scenarios in future there,1,1,1.0
in future there is,1,1,1.0
future there is a,1,1,1.0
is a need for,1,1,1.0
a need for the,1,1,1.0
need for the further,1,1,1.0
for the further investigation,1,1,1.0
the further investigation of,1,1,1.0
further investigation of smote,1,1,1.0
investigation of smote in,1,1,1.0
of smote in its,1,1,1.0
smote in its entirety,1,1,1.0
in its entirety summary,1,1,1.0
its entirety summary this,1,1,1.0
entirety summary this study,1,1,1.0
summary this study involved,1,1,1.0
this study involved the,1,1,1.0
study involved the implementation,1,1,1.0
involved the implementation of,1,1,1.0
the implementation of smote,1,1,1.0
implementation of smote via,1,1,1.0
of smote via the,1,1,1.0
smote via the apache,1,1,1.0
via the apache spark,1,1,1.0
the apache spark framework,1,1,1.0
apache spark framework the,1,1,1.0
spark framework the investigation,1,1,1.0
framework the investigation was,1,1,1.0
the investigation was conducted,1,1,1.0
investigation was conducted based,1,1,1.0
was conducted based on,1,1,1.0
conducted based on and,1,1,1.0
based on and algorithms,1,1,1.0
on and algorithms in,1,1,1.0
and algorithms in future,1,1,1.0
algorithms in future the,1,1,1.0
in future the scope,1,1,1.0
future the scope of,1,1,1.0
the scope of such,1,1,1.0
scope of such table,1,1,1.0
of such table performance,1,1,1.0
such table performance comparison,1,1,1.0
table performance comparison ucisat,1,1,1.0
performance comparison ucisat dataset,1,1,1.0
comparison ucisat dataset technique,1,1,1.0
ucisat dataset technique auc,1,1,1.0
table performance comparison ecbdl,1,1,1.0
performance comparison ecbdl dataset,1,1,1.0
comparison ecbdl dataset technique,1,1,1.0
ecbdl dataset technique auc,1,1,1.0
minority oversampling technique hooda,1,1,1.0
oversampling technique hooda and,1,1,1.0
technique hooda and mann,1,1,1.0
computational intelligence systems a,1,1,1.0
intelligence systems a study,1,1,1.0
systems a study could,1,1,1.0
a study could be,1,1,1.0
study could be expanded,1,1,1.0
could be expanded by,1,1,1.0
be expanded by considering,1,1,1.0
expanded by considering alternative,1,1,1.0
by considering alternative based,1,1,1.0
considering alternative based algorithms,1,1,1.0
alternative based algorithms including,1,1,1.0
based algorithms including and,1,1,1.0
algorithms including and borderline,1,1,1.0
including and borderline smote,1,1,1.0
and borderline smote in,1,1,1.0
borderline smote in big,1,1,1.0
smote in big data,1,1,1.0
in big data the,1,1,1.0
big data the dependency,1,1,1.0
data the dependency reflects,1,1,1.0
the dependency reflects the,1,1,1.0
dependency reflects the clustering,1,1,1.0
reflects the clustering algorithm,1,1,1.0
the clustering algorithm in,1,1,1.0
clustering algorithm in use,1,1,1.0
algorithm in use proving,1,1,1.0
in use proving more,1,1,1.0
use proving more effective,1,1,1.0
proving more effective the,1,1,1.0
more effective the clustering,1,1,1.0
effective the clustering algorithm,1,1,1.0
the clustering algorithm is,1,1,1.0
clustering algorithm is seen,1,1,1.0
algorithm is seen to,1,1,1.0
is seen to exhibit,1,1,1.0
seen to exhibit superior,1,1,1.0
to exhibit superior performance,1,1,1.0
exhibit superior performance due,1,1,1.0
superior performance due to,1,1,1.0
performance due to the,1,1,1.0
due to the provision,1,1,1.0
to the provision of,1,1,1.0
the provision of more,1,1,1.0
provision of more accurate,1,1,1.0
of more accurate results,1,1,1.0
more accurate results notably,1,1,1.0
accurate results notably the,1,1,1.0
results notably the smote,1,1,1.0
notably the smote mentation,1,1,1.0
the smote mentation could,1,1,1.0
smote mentation could be,1,1,1.0
mentation could be extended,1,1,1.0
could be extended to,1,1,1.0
be extended to ensure,1,1,1.0
extended to ensure that,1,1,1.0
to ensure that it,1,1,1.0
ensure that it incorporates,1,1,1.0
that it incorporates class,1,1,1.0
it incorporates class juncts,1,1,1.0
incorporates class juncts other,1,1,1.0
class juncts other areas,1,1,1.0
juncts other areas that,1,1,1.0
other areas that are,1,1,1.0
areas that are worth,1,1,1.0
that are worth utilizing,1,1,1.0
are worth utilizing include,1,1,1.0
worth utilizing include hybrid,1,1,1.0
utilizing include hybrid trees,1,1,1.0
include hybrid trees and,1,1,1.0
hybrid trees and spill,1,1,1.0
trees and spill trees,1,1,1.0
and spill trees through,1,1,1.0
spill trees through their,1,1,1.0
trees through their implementation,1,1,1.0
through their implementation it,1,1,1.0
their implementation it is,1,1,1.0
implementation it is predicted,1,1,1.0
it is predicted that,1,1,1.0
is predicted that there,1,1,1.0
predicted that there might,1,1,1.0
that there might be,1,1,1.0
there might be improvements,1,1,1.0
might be improvements in,1,1,1.0
be improvements in the,1,1,1.0
improvements in the performance,1,1,1.0
in the performance of,1,1,1.0
performance of the nearest,1,1,1.0
of the nearest neighbor,1,1,1.0
the nearest neighbor algorithm,1,1,1.0
nearest neighbor algorithm as,1,1,1.0
neighbor algorithm as well,1,1,1.0
algorithm as well as,1,1,1.0
as well as the,2,2,1.0
well as the clustering,1,1,1.0
as the clustering process,1,1,1.0
the clustering process references,1,1,1.0
clustering process references yu,1,1,1.0
process references yu hong,1,1,1.0
references yu hong yang,1,1,1.0
yu hong yang ni,1,1,1.0
hong yang ni y,1,1,1.0
yang ni y dan,1,1,1.0
ni y dan qin,1,1,1.0
y dan qin recognition,1,1,1.0
dan qin recognition of,1,1,1.0
qin recognition of multiple,1,1,1.0
recognition of multiple imbalanced,1,1,1.0
of multiple imbalanced cancer,1,1,1.0
multiple imbalanced cancer types,1,1,1.0
imbalanced cancer types based,1,1,1.0
cancer types based on,1,1,1.0
types based on dna,1,1,1.0
based on dna microarray,1,1,1.0
on dna microarray data,1,1,1.0
dna microarray data using,1,1,1.0
microarray data using ensemble,1,1,1.0
data using ensemble classifiers,1,1,1.0
using ensemble classifiers biomed,1,1,1.0
ensemble classifiers biomed res,1,1,1.0
classifiers biomed res int,1,1,1.0
biomed res int y,1,1,1.0
res int y chen,1,1,1.0
int y chen an,1,1,1.0
y chen an empirical,1,1,1.0
chen an empirical study,1,1,1.0
an empirical study of,1,1,1.0
empirical study of a,1,1,1.0
study of a hybrid,1,1,1.0
of a hybrid rst,1,1,1.0
a hybrid rst classification,1,1,1.0
hybrid rst classification procedure,1,1,1.0
rst classification procedure to,1,1,1.0
classification procedure to elucidate,1,1,1.0
procedure to elucidate therapeutic,1,1,1.0
to elucidate therapeutic effects,1,1,1.0
elucidate therapeutic effects in,1,1,1.0
therapeutic effects in uremia,1,1,1.0
effects in uremia patients,1,1,1.0
in uremia patients med,1,1,1.0
uremia patients med biol,1,1,1.0
patients med biol eng,1,1,1.0
med biol eng comput,1,1,1.0
biol eng comput haixiang,1,1,1.0
eng comput haixiang yijing,1,1,1.0
comput haixiang yijing yanan,1,1,1.0
haixiang yijing yanan xiao,1,1,1.0
yijing yanan xiao jinling,1,1,1.0
yanan xiao jinling knn,1,1,1.0
xiao jinling knn ensemble,1,1,1.0
jinling knn ensemble learning,1,1,1.0
knn ensemble learning algorithm,1,1,1.0
ensemble learning algorithm imbalanced,1,1,1.0
learning algorithm imbalanced data,1,1,1.0
algorithm imbalanced data classification,1,1,1.0
imbalanced data classification appl,1,1,1.0
data classification appl artif,1,1,1.0
classification appl artif intell,1,1,1.0
appl artif intell he,1,1,1.0
artif intell he garcía,1,1,1.0
intell he garcía learning,1,1,1.0
he garcía learning from,1,1,1.0
garcía learning from imbalanced,1,1,1.0
imbalanced data ieee trans,2,2,1.0
data ieee trans knowl,2,2,1.0
ieee trans knowl data,6,2,3.0
trans knowl data eng,5,2,2.5
knowl data eng y,1,1,1.0
data eng y sun,1,1,1.0
eng y sun wong,1,1,1.0
y sun wong mohamed,1,1,1.0
sun wong mohamed classification,1,1,1.0
wong mohamed classification of,1,1,1.0
mohamed classification of imbalanced,1,1,1.0
of imbalanced data a,1,1,1.0
imbalanced data a review,1,1,1.0
data a review int,1,1,1.0
a review int pattern,1,1,1.0
review int pattern recognit,1,1,1.0
int pattern recognit artif,1,1,1.0
pattern recognit artif intell,1,1,1.0
recognit artif intell fernández,1,1,1.0
artif intell fernández chawla,1,1,1.0
intell fernández chawla garcía,1,1,1.0
fernández chawla garcía v,1,1,1.0
chawla garcía v palade,1,1,1.0
garcía v palade herrera,1,1,1.0
v palade herrera an,1,1,1.0
palade herrera an insight,1,1,1.0
herrera an insight into,1,1,1.0
an insight into classification,1,1,1.0
insight into classification with,1,1,1.0
into classification with imbalanced,1,1,1.0
classification with imbalanced data,1,1,1.0
with imbalanced data empirical,1,1,1.0
imbalanced data empirical results,1,1,1.0
data empirical results and,1,1,1.0
empirical results and current,1,1,1.0
results and current trends,1,1,1.0
and current trends on,1,1,1.0
current trends on using,1,1,1.0
trends on using data,1,1,1.0
on using data intrinsic,1,1,1.0
using data intrinsic characteristics,1,1,1.0
data intrinsic characteristics plex,1,1,1.0
intrinsic characteristics plex intell,1,1,1.0
characteristics plex intell syst,1,1,1.0
plex intell syst krawczyk,1,1,1.0
intell syst krawczyk learning,1,1,1.0
syst krawczyk learning from,1,1,1.0
krawczyk learning from imbalanced,1,1,1.0
from imbalanced data open,1,1,1.0
imbalanced data open challenges,1,1,1.0
data open challenges and,1,1,1.0
open challenges and future,1,1,1.0
challenges and future directions,1,1,1.0
and future directions prog,1,1,1.0
future directions prog artif,1,1,1.0
directions prog artif intell,1,1,1.0
prog artif intell batista,1,1,1.0
artif intell batista prati,1,1,1.0
intell batista prati monard,1,1,1.0
study of the ior,1,1,1.0
of the ior of,1,1,1.0
the ior of several,1,1,1.0
ior of several methods,1,1,1.0
data sigkdd explor ramentol,1,1,1.0
sigkdd explor ramentol vluymans,1,1,1.0
explor ramentol vluymans verbiest,1,1,1.0
ramentol vluymans verbiest y,1,1,1.0
vluymans verbiest y caballero,1,1,1.0
verbiest y caballero bello,1,1,1.0
y caballero bello cornelis,1,1,1.0
caballero bello cornelis herrera,1,1,1.0
bello cornelis herrera ifrow,1,1,1.0
cornelis herrera ifrow ann,1,1,1.0
herrera ifrow ann imbalanced,1,1,1.0
ifrow ann imbalanced ordered,1,1,1.0
ann imbalanced ordered weighted,1,1,1.0
imbalanced ordered weighted average,1,1,1.0
ordered weighted average nearest,1,1,1.0
weighted average nearest neighbor,1,1,1.0
average nearest neighbor classification,1,1,1.0
nearest neighbor classification ieee,1,1,1.0
neighbor classification ieee trans,1,1,1.0
classification ieee trans fuzzy,1,1,1.0
ieee trans fuzzy syst,1,1,1.0
trans fuzzy syst p,1,1,1.0
fuzzy syst p domingos,1,1,1.0
syst p domingos metacost,1,1,1.0
p domingos metacost a,1,1,1.0
domingos metacost a general,2,2,1.0
metacost a general method,2,2,1.0
a general method for,2,2,1.0
general method for making,2,2,1.0
method for making classifiers,2,2,1.0
for making classifiers in,2,2,1.0
making classifiers in proceedings,2,2,1.0
classifiers in proceedings of,2,2,1.0
and data mining vol,1,1,1.0
data mining vol pp,1,1,1.0
mining vol pp v,1,1,1.0
vol pp v lópez,1,1,1.0
pp v lópez fernández,1,1,1.0
v lópez fernández herrera,1,1,1.0
lópez fernández herrera analysis,1,1,1.0
fernández herrera analysis of,1,1,1.0
herrera analysis of preprocessing,1,1,1.0
analysis of preprocessing learning,1,1,1.0
of preprocessing learning for,1,1,1.0
preprocessing learning for imbalanced,1,1,1.0
learning for imbalanced fication,1,1,1.0
for imbalanced fication open,1,1,1.0
imbalanced fication open problems,1,1,1.0
fication open problems on,1,1,1.0
open problems on intrinsic,1,1,1.0
problems on intrinsic data,1,1,1.0
on intrinsic data characteristics,1,1,1.0
intrinsic data characteristics expert,1,1,1.0
data characteristics expert syst,1,1,1.0
characteristics expert syst appl,1,1,1.0
expert syst appl laney,1,1,1.0
syst appl laney data,1,1,1.0
appl laney data management,1,1,1.0
laney data management controlling,1,1,1.0
data management controlling data,1,1,1.0
management controlling data volume,1,1,1.0
controlling data volume velocity,1,1,1.0
data volume velocity and,1,1,1.0
volume velocity and variety,1,1,1.0
velocity and variety meta,1,1,1.0
and variety meta group,1,1,1.0
variety meta group kambatla,1,1,1.0
meta group kambatla kollias,1,1,1.0
group kambatla kollias v,1,1,1.0
kambatla kollias v kumar,1,1,1.0
kollias v kumar grama,1,1,1.0
v kumar grama trends,1,1,1.0
kumar grama trends in,1,1,1.0
grama trends in big,1,1,1.0
trends in big data,1,1,1.0
in big data analytics,1,1,1.0
big data analytics parallel,1,1,1.0
data analytics parallel distrib,1,1,1.0
analytics parallel distrib comput,1,1,1.0
parallel distrib comput p,1,1,1.0
distrib comput p zikopoulos,1,1,1.0
comput p zikopoulos eaton,1,1,1.0
p zikopoulos eaton deroos,1,1,1.0
zikopoulos eaton deroos deutsch,1,1,1.0
eaton deroos deutsch lapis,1,1,1.0
deroos deutsch lapis understanding,1,1,1.0
deutsch lapis understanding big,1,1,1.0
lapis understanding big for,1,1,1.0
understanding big for enterprise,1,1,1.0
big for enterprise class,1,1,1.0
for enterprise class hadoop,1,1,1.0
enterprise class hadoop and,1,1,1.0
class hadoop and streaming,1,1,1.0
hadoop and streaming data,1,1,1.0
and streaming data osborne,1,1,1.0
streaming data osborne media,1,1,1.0
data osborne media new,1,1,1.0
osborne media new y,1,1,1.0
media new y ork,1,1,1.0
new y ork wu,1,1,1.0
y ork wu zhu,1,1,1.0
ork wu zhu wu,1,1,1.0
wu zhu wu w,1,1,1.0
zhu wu w ding,1,1,1.0
wu w ding data,1,1,1.0
w ding data mining,1,1,1.0
ding data mining with,1,1,1.0
data mining with bigdata,1,1,1.0
mining with bigdata ieee,1,1,1.0
with bigdata ieee trans,1,1,1.0
bigdata ieee trans knowl,1,1,1.0
knowl data eng chawla,1,1,1.0
data eng chawla bowyer,1,1,1.0
eng chawla bowyer hall,1,1,1.0
chawla bowyer hall w,1,1,1.0
bowyer hall w kegelmeyer,1,1,1.0
hall w kegelmeyer synthetic,1,1,1.0
w kegelmeyer synthetic minority,1,1,1.0
kegelmeyer synthetic minority technique,1,1,1.0
synthetic minority technique artif,1,1,1.0
minority technique artif intell,1,1,1.0
technique artif intell res,1,1,1.0
artif intell res apache,1,1,1.0
intell res apache spark,1,1,1.0
res apache spark https,1,1,1.0
apache spark https japkowicz,1,1,1.0
spark https japkowicz the,1,1,1.0
https japkowicz the class,1,1,1.0
japkowicz the class imbalance,1,1,1.0
class imbalance problem significance,1,1,1.0
imbalance problem significance and,1,1,1.0
problem significance and strategies,1,1,1.0
significance and strategies in,1,1,1.0
and strategies in proceedings,1,1,1.0
strategies in proceedings of,1,1,1.0
international conference on artificial,1,1,1.0
on artificial intelligence pp,1,1,1.0
artificial intelligence pp prati,1,1,1.0
intelligence pp prati batista,2,2,1.0
pp prati batista monard,1,1,1.0
prati batista monard learning,1,1,1.0
batista monard learning with,1,1,1.0
monard learning with class,1,1,1.0
learning with class skews,1,1,1.0
with class skews and,1,1,1.0
class skews and small,1,1,1.0
skews and small disjuncts,1,1,1.0
and small disjuncts adv,1,1,1.0
small disjuncts adv artif,1,1,1.0
disjuncts adv artif intell,1,1,1.0
adv artif intell fernández,1,1,1.0
artif intell fernández sara,1,1,1.0
intell fernández sara del,1,1,1.0
fernández sara del chawla,1,1,1.0
sara del chawla herreral,1,1,1.0
del chawla herreral an,1,1,1.0
chawla herreral an insight,1,1,1.0
herreral an insight into,1,1,1.0
an insight into imbalanced,1,1,1.0
insight into imbalanced big,1,1,1.0
into imbalanced big data,1,1,1.0
imbalanced big data classification,1,1,1.0
big data classification outcomes,1,1,1.0
data classification outcomes and,1,1,1.0
classification outcomes and challenges,1,1,1.0
outcomes and challenges complex,1,1,1.0
and challenges complex intell,1,1,1.0
challenges complex intell syst,1,1,1.0
complex intell syst garcía,1,1,1.0
intell syst garcía herrera,1,1,1.0
syst garcía herrera evolutionary,1,1,1.0
garcía herrera evolutionary undersampling,1,1,1.0
herrera evolutionary undersampling for,1,1,1.0
evolutionary undersampling for cation,1,1,1.0
undersampling for cation with,1,1,1.0
for cation with imbalanced,1,1,1.0
cation with imbalanced datasets,1,1,1.0
with imbalanced datasets proposals,1,1,1.0
imbalanced datasets proposals and,1,1,1.0
datasets proposals and taxonomy,1,1,1.0
proposals and taxonomy evol,1,1,1.0
and taxonomy evol comput,1,1,1.0
taxonomy evol comput p,1,1,1.0
evol comput p haghani,1,1,1.0
comput p haghani michel,1,1,1.0
p haghani michel p,1,1,1.0
haghani michel p aberer,1,1,1.0
michel p aberer lsh,1,1,1.0
p aberer lsh at,1,1,1.0
aberer lsh at large,1,1,1.0
lsh at large distributed,1,1,1.0
at large distributed knn,1,1,1.0
large distributed knn search,1,1,1.0
in high dimensions in,1,1,1.0
high dimensions in international,1,1,1.0
dimensions in international workshop,1,1,1.0
in international workshop on,1,1,1.0
international workshop on the,1,1,1.0
workshop on the web,1,1,1.0
on the web and,1,1,1.0
the web and databases,1,1,1.0
web and databases webdb,1,1,1.0
and databases webdb vancouver,1,1,1.0
databases webdb vancouver kanungo,1,1,1.0
webdb vancouver kanungo mount,1,1,1.0
vancouver kanungo mount netanyahu,1,1,1.0
kanungo mount netanyahu piatko,1,1,1.0
mount netanyahu piatko man,1,1,1.0
netanyahu piatko man wu,1,1,1.0
piatko man wu an,1,1,1.0
man wu an efficient,1,1,1.0
wu an efficient clustering,1,1,1.0
an efficient clustering algorithm,1,1,1.0
efficient clustering algorithm analysis,1,1,1.0
clustering algorithm analysis and,1,1,1.0
algorithm analysis and implementation,1,1,1.0
analysis and implementation ieee,1,1,1.0
and implementation ieee trans,1,1,1.0
implementation ieee trans pattern,1,1,1.0
ieee trans pattern anal,1,1,1.0
trans pattern anal mach,1,1,1.0
pattern anal mach intell,1,1,1.0
anal mach intell kaufman,1,1,1.0
mach intell kaufman p,1,1,1.0
intell kaufman p rousseeuw,1,1,1.0
kaufman p rousseeuw finding,1,1,1.0
p rousseeuw finding groups,1,1,1.0
rousseeuw finding groups in,1,1,1.0
finding groups in data,1,1,1.0
groups in data an,1,1,1.0
in data an duction,1,1,1.0
data an duction to,1,1,1.0
an duction to cluster,1,1,1.0
duction to cluster analysis,1,1,1.0
to cluster analysis john,1,1,1.0
cluster analysis john wiley,1,1,1.0
analysis john wiley sons,1,1,1.0
john wiley sons new,1,1,1.0
wiley sons new y,1,1,1.0
sons new y ork,1,1,1.0
new y ork v,1,1,1.0
y ork v capoyleas,1,1,1.0
ork v capoyleas woeginger,1,1,1.0
v capoyleas woeginger ªgeometric,1,1,1.0
capoyleas woeginger ªgeometric clusterings,1,1,1.0
woeginger ªgeometric clusterings algorithms,1,1,1.0
ªgeometric clusterings algorithms jain,1,1,1.0
clusterings algorithms jain dubes,1,1,1.0
algorithms jain dubes algorithms,1,1,1.0
jain dubes algorithms for,1,1,1.0
dubes algorithms for clustering,1,1,1.0
algorithms for clustering data,1,1,1.0
for clustering data prentice,1,1,1.0
clustering data prentice hall,1,1,1.0
data prentice hall englewood,1,1,1.0
prentice hall englewood cliffs,1,1,1.0
hall englewood cliffs jain,1,1,1.0
englewood cliffs jain murty,1,1,1.0
cliffs jain murty p,1,1,1.0
jain murty p ªdata,1,1,1.0
murty p ªdata clustering,1,1,1.0
p ªdata clustering a,1,1,1.0
ªdata clustering a review,1,1,1.0
clustering a review acm,1,1,1.0
a review acm comput,1,1,1.0
review acm comput surv,1,1,1.0
acm comput surv bahmani,1,1,1.0
comput surv bahmani moseley,1,1,1.0
surv bahmani moseley vattani,1,1,1.0
bahmani moseley vattani kumar,1,1,1.0
moseley vattani kumar scalable,1,1,1.0
vattani kumar scalable proc,1,1,1.0
kumar scalable proc vldb,1,1,1.0
scalable proc vldb endowment,1,1,1.0
proc vldb endowment choi,1,1,1.0
vldb endowment choi lee,1,1,1.0
endowment choi lee distributed,1,1,1.0
choi lee distributed high,1,1,1.0
lee distributed high dimensional,1,1,1.0
distributed high dimensional indexing,1,1,1.0
high dimensional indexing for,1,1,1.0
dimensional indexing for search,1,1,1.0
indexing for search supercomput,1,1,1.0
for search supercomput jiang,1,1,1.0
search supercomput jiang tseng,1,1,1.0
supercomput jiang tseng su,1,1,1.0
jiang tseng su clustering,1,1,1.0
tseng su clustering process,1,1,1.0
su clustering process for,1,1,1.0
clustering process for outliers,1,1,1.0
process for outliers detection,1,1,1.0
for outliers detection pattern,1,1,1.0
outliers detection pattern recognit,1,1,1.0
detection pattern recognit lett,1,1,1.0
pattern recognit lett guttman,1,1,1.0
recognit lett guttman a,1,1,1.0
lett guttman a dynamic,1,1,1.0
guttman a dynamic index,1,1,1.0
a dynamic index structure,1,1,1.0
dynamic index structure for,1,1,1.0
index structure for spatial,1,1,1.0
structure for spatial ing,1,1,1.0
for spatial ing in,1,1,1.0
spatial ing in proceedings,1,1,1.0
ing in proceedings of,1,1,1.0
proceedings of the acm,1,1,1.0
of the acm sigmod,1,1,1.0
the acm sigmod international,1,1,1.0
acm sigmod international conference,1,1,1.0
sigmod international conference on,1,1,1.0
international conference on management,1,1,1.0
conference on management of,1,1,1.0
on management of data,1,1,1.0
management of data sigmod,1,1,1.0
of data sigmod acm,1,1,1.0
data sigmod acm new,1,1,1.0
sigmod acm new y,1,1,1.0
acm new y ork,2,1,2.0
new y ork pp,2,1,2.0
y ork pp bentley,1,1,1.0
ork pp bentley trees,1,1,1.0
pp bentley trees for,1,1,1.0
bentley trees for semidynamic,1,1,1.0
trees for semidynamic point,1,1,1.0
for semidynamic point sets,1,1,1.0
semidynamic point sets in,1,1,1.0
point sets in ings,1,1,1.0
sets in ings of,1,1,1.0
in ings of the,3,3,1.0
ings of the sixth,1,1,1.0
of the sixth annual,1,1,1.0
the sixth annual symposium,1,1,1.0
sixth annual symposium on,1,1,1.0
annual symposium on computational,1,1,1.0
symposium on computational try,1,1,1.0
on computational try scg,1,1,1.0
computational try scg acm,1,1,1.0
try scg acm new,1,1,1.0
scg acm new y,1,1,1.0
y ork pp p,1,1,1.0
ork pp p ciaccia,1,1,1.0
pp p ciaccia patella,2,1,2.0
p ciaccia patella p,1,1,1.0
ciaccia patella p zezula,1,1,1.0
patella p zezula an,1,1,1.0
p zezula an efficient,1,1,1.0
zezula an efficient access,1,1,1.0
an efficient access method,1,1,1.0
efficient access method for,1,1,1.0
access method for similarity,1,1,1.0
method for similarity search,1,1,1.0
for similarity search in,1,1,1.0
similarity search in metric,1,1,1.0
search in metric spaces,1,1,1.0
in metric spaces in,1,1,1.0
metric spaces in vldb,1,1,1.0
spaces in vldb in,1,1,1.0
in vldb in proceedings,1,1,1.0
vldb in proceedings of,1,1,1.0
international conference on very,1,1,1.0
conference on very large,1,1,1.0
on very large data,1,1,1.0
very large data bases,1,1,1.0
large data bases pp,1,1,1.0
data bases pp p,1,1,1.0
bases pp p ciaccia,1,1,1.0
p ciaccia patella rabitti,1,1,1.0
ciaccia patella rabitti p,1,1,1.0
patella rabitti p zezula,1,1,1.0
rabitti p zezula indexing,1,1,1.0
p zezula indexing metric,1,1,1.0
zezula indexing metric spaces,1,1,1.0
indexing metric spaces with,1,1,1.0
metric spaces with in,1,1,1.0
spaces with in proceedings,1,1,1.0
with in proceedings qunito,1,1,1.0
in proceedings qunito convegno,1,1,1.0
proceedings qunito convegno nazionale,1,1,1.0
qunito convegno nazionale sebd,1,1,1.0
convegno nazionale sebd pp,1,1,1.0
nazionale sebd pp moore,1,1,1.0
sebd pp moore the,1,1,1.0
pp moore the anchors,1,1,1.0
moore the anchors hierarchy,1,1,1.0
the anchors hierarchy using,1,1,1.0
anchors hierarchy using the,1,1,1.0
hierarchy using the triangle,1,1,1.0
using the triangle ity,1,1,1.0
the triangle ity to,1,1,1.0
triangle ity to survive,1,1,1.0
ity to survive high,1,1,1.0
to survive high dimensional,1,1,1.0
survive high dimensional data,1,1,1.0
high dimensional data in,1,1,1.0
dimensional data in proceedings,1,1,1.0
data in proceedings of,2,2,1.0
proceedings of the teenth,1,1,1.0
of the teenth conference,1,1,1.0
the teenth conference on,1,1,1.0
teenth conference on uncertainty,1,1,1.0
in artificial intelligence pp,1,1,1.0
artificial intelligence pp p,1,1,1.0
intelligence pp p zezula,1,1,1.0
pp p zezula p,1,1,1.0
p zezula p ciaccia,1,1,1.0
zezula p ciaccia rabitti,1,1,1.0
p ciaccia rabitti a,1,1,1.0
ciaccia rabitti a dynamic,1,1,1.0
rabitti a dynamic index,1,1,1.0
a dynamic index for,1,1,1.0
dynamic index for ilarity,1,1,1.0
index for ilarity queries,1,1,1.0
for ilarity queries in,1,1,1.0
ilarity queries in multimedia,1,1,1.0
queries in multimedia databases,1,1,1.0
in multimedia databases in,1,1,1.0
multimedia databases in technical,1,1,1.0
databases in technical report,1,1,1.0
in technical report hermes,1,1,1.0
technical report hermes esprit,1,1,1.0
report hermes esprit ltr,1,1,1.0
hermes esprit ltr projects,1,1,1.0
esprit ltr projects liu,1,1,1.0
ltr projects liu rosenberg,1,1,1.0
projects liu rosenberg rowley,1,1,1.0
liu rosenberg rowley clustering,1,1,1.0
rosenberg rowley clustering billions,1,1,1.0
rowley clustering billions of,1,1,1.0
clustering billions of images,1,1,1.0
billions of images with,1,1,1.0
of images with large,1,1,1.0
images with large scale,1,1,1.0
with large scale nearest,1,1,1.0
large scale nearest neighbor,1,1,1.0
scale nearest neighbor search,1,1,1.0
nearest neighbor search in,1,1,1.0
neighbor search in ieee,1,1,1.0
search in ieee workshop,1,1,1.0
in ieee workshop on,2,1,2.0
ieee workshop on applications,2,1,2.0
workshop on applications of,2,1,2.0
on applications of computer,2,1,2.0
applications of computer vision,2,1,2.0
of computer vision published,1,1,1.0
computer vision published in,1,1,1.0
vision published in ieee,1,1,1.0
published in ieee workshop,1,1,1.0
of computer vision hooda,1,1,1.0
computer vision hooda and,1,1,1.0
vision hooda and mann,1,1,1.0
computational intelligence systems scikitlearn,1,1,1.0
intelligence systems scikitlearn http,1,1,1.0
systems scikitlearn http uci,1,1,1.0
scikitlearn http uci satellite,1,1,1.0
http uci satellite image,1,1,1.0
uci satellite image dataset,1,1,1.0
satellite image dataset https,1,1,1.0
image dataset https y,1,1,1.0
dataset https y east,1,1,1.0
https y east dataset,1,1,1.0
y east dataset https,1,1,1.0
east dataset https abalone,1,1,1.0
dataset https abalone dataset,1,1,1.0
https abalone dataset https,1,1,1.0
abalone dataset https https,1,1,1.0
dataset https https han,1,1,1.0
https https han w,1,1,1.0
https han w wang,1,1,1.0
han w wang mao,1,1,1.0
w wang mao borderline,1,1,1.0
wang mao borderline smote,1,1,1.0
mao borderline smote a,1,1,1.0
borderline smote a new,1,1,1.0
smote a new over,1,1,1.0
a new over sampling,1,1,1.0
new over sampling method,1,1,1.0
over sampling method in,1,1,1.0
sampling method in imbalanced,2,2,1.0
data sets learning int,1,1,1.0
sets learning int conf,1,1,1.0
learning int conf intell,1,1,1.0
int conf intell comput,1,1,1.0
see discussions stats and,1,1,1.0
discussions stats and author,1,1,1.0
stats and author profiles,1,1,1.0
and author profiles for,1,1,1.0
author profiles for this,1,1,1.0
profiles for this publication,1,1,1.0
for this publication at,1,1,1.0
this publication at https,1,1,1.0
publication at https on,1,1,1.0
at https on the,1,1,1.0
https on the class,1,1,1.0
on the class imbalance,2,1,2.0
class imbalance problem article,1,1,1.0
imbalance problem article october,1,1,1.0
problem article october doi,1,1,1.0
article october doi citations,1,1,1.0
october doi citations reads,1,1,1.0
doi citations reads authors,1,1,1.0
citations reads authors including,1,1,1.0
reads authors including gongping,1,1,1.0
authors including gongping yang,1,1,1.0
including gongping yang shandong,1,1,1.0
gongping yang shandong university,1,1,1.0
yang shandong university publications,1,1,1.0
shandong university publications citations,1,1,1.0
university publications citations see,1,1,1.0
publications citations see profile,1,1,1.0
citations see profile all,1,1,1.0
see profile all content,1,1,1.0
profile all content following,1,1,1.0
all content following this,1,1,1.0
content following this page,1,1,1.0
following this page was,1,1,1.0
this page was uploaded,1,1,1.0
page was uploaded by,1,1,1.0
was uploaded by gongping,1,1,1.0
uploaded by gongping yang,1,1,1.0
by gongping yang on,1,1,1.0
gongping yang on october,1,1,1.0
yang on october the,1,1,1.0
on october the user,1,1,1.0
october the user has,1,1,1.0
the user has requested,1,1,1.0
user has requested enhancement,1,1,1.0
has requested enhancement of,1,1,1.0
requested enhancement of the,1,1,1.0
enhancement of the downloaded,1,1,1.0
of the downloaded the,1,1,1.0
the downloaded the class,1,1,1.0
downloaded the class imbalance,1,1,1.0
class imbalance problem xinjian,1,1,1.0
imbalance problem xinjian guo,1,1,1.0
problem xinjian guo yilong,1,1,1.0
xinjian guo yilong cailing,1,1,1.0
guo yilong cailing dong,1,1,1.0
yilong cailing dong gongping,1,1,1.0
cailing dong gongping yang,1,1,1.0
dong gongping yang guangtong,1,1,1.0
gongping yang guangtong zhou,1,1,1.0
yang guangtong zhou school,1,1,1.0
guangtong zhou school of,1,1,1.0
zhou school of computer,1,1,1.0
science and technology shandong,2,1,2.0
and technology shandong university,2,1,2.0
technology shandong university jinan,1,1,1.0
shandong university jinan china,1,1,1.0
university jinan china xinjianguo,1,1,1.0
jinan china xinjianguo ylyin,1,1,1.0
china xinjianguo ylyin this,1,1,1.0
xinjianguo ylyin this paper,1,1,1.0
ylyin this paper concentrates,1,1,1.0
this paper concentrates on,1,1,1.0
paper concentrates on binary,1,1,1.0
concentrates on binary classification,1,1,1.0
on binary classification problem,1,1,1.0
binary classification problem and,1,1,1.0
classification problem and by,1,1,1.0
problem and by convention,1,1,1.0
and by convention the,1,1,1.0
by convention the class,1,1,1.0
convention the class label,1,1,1.0
the class label of,4,2,2.0
class label of the,3,1,3.0
label of the minority,2,1,2.0
minority class is positive,2,1,2.0
class is positive and,2,1,2.0
is positive and that,1,1,1.0
positive and that of,1,1,1.0
and that of the,1,1,1.0
that of the majority,1,1,1.0
majority class is negative,2,1,2.0
class is negative corresponding,1,1,1.0
is negative corresponding author,1,1,1.0
negative corresponding author yilong,1,1,1.0
corresponding author yilong yin,1,1,1.0
author yilong yin is,1,1,1.0
yilong yin is with,1,1,1.0
yin is with school,1,1,1.0
is with school of,1,1,1.0
with school of computer,1,1,1.0
technology shandong university ylyin,1,1,1.0
shandong university ylyin abstract,1,1,1.0
university ylyin abstract the,1,1,1.0
ylyin abstract the class,1,1,1.0
abstract the class imbalance,1,1,1.0
class imbalance problem has,3,1,3.0
imbalance problem has been,3,1,3.0
recognized in many practical,1,1,1.0
in many practical domains,1,1,1.0
many practical domains and,1,1,1.0
practical domains and a,1,1,1.0
domains and a hot,1,1,1.0
and a hot topic,1,1,1.0
a hot topic of,1,1,1.0
hot topic of machine,1,1,1.0
topic of machine learning,1,1,1.0
of machine learning in,1,1,1.0
machine learning in recent,3,1,3.0
learning in recent years,3,1,3.0
in recent years in,1,1,1.0
recent years in su,1,1,1.0
years in su ch,1,1,1.0
in su ch a,1,1,1.0
su ch a problem,1,1,1.0
ch a problem almost,1,1,1.0
a problem almost all,1,1,1.0
problem almost all the,1,1,1.0
almost all the examples,2,1,2.0
all the examples are,2,1,2.0
the examples are labeled,2,1,2.0
examples are labeled as,4,1,4.0
are labeled as one,2,1,2.0
labeled as one class,2,1,2.0
as one class while,2,1,2.0
one class while far,2,1,2.0
class while far fewer,2,1,2.0
while far fewer examples,2,1,2.0
far fewer examples are,2,1,2.0
fewer examples are labeled,2,1,2.0
are labeled as the,2,1,2.0
labeled as the other,2,1,2.0
as the other class,2,1,2.0
the other class usually,2,1,2.0
other class usually the,2,1,2.0
class usually the more,2,1,2.0
usually the more important,2,1,2.0
the more important class,2,1,2.0
more important class in,1,1,1.0
important class in this,1,1,1.0
class in this case,1,1,1.0
in this case standard,2,1,2.0
this case standard machine,1,1,1.0
case standard machine learning,1,1,1.0
standard machine learning algorithms,3,1,3.0
machine learning algorithms tend,1,1,1.0
learning algorithms tend to,1,1,1.0
algorithms tend to be,1,1,1.0
tend to be overwhelmed,2,1,2.0
to be overwhelmed by,2,1,2.0
be overwhelmed by the,2,1,2.0
overwhelmed by the majority,2,1,2.0
by the majority class,3,1,3.0
majority class and ignore,2,1,2.0
class and ignore the,2,1,2.0
and ignore the minority,2,1,2.0
ignore the minority class,2,1,2.0
the minority class since,1,1,1.0
minority class since traditional,1,1,1.0
class since traditional classifiers,1,1,1.0
since traditional classifiers seeking,1,1,1.0
traditional classifiers seeking an,1,1,1.0
classifiers seeking an accurate,1,1,1.0
seeking an accurate performance,1,1,1.0
an accurate performance over,1,1,1.0
accurate performance over a,1,1,1.0
performance over a full,1,1,1.0
over a full range,1,1,1.0
a full range of,1,1,1.0
full range of instances,1,1,1.0
range of instances this,1,1,1.0
of instances this paper,1,1,1.0
instances this paper reviewed,1,1,1.0
this paper reviewed academic,1,1,1.0
paper reviewed academic activities,1,1,1.0
reviewed academic activities special,1,1,1.0
academic activities special for,1,1,1.0
activities special for the,1,1,1.0
special for the class,2,1,2.0
class imbalance problem firstly,1,1,1.0
imbalance problem firstly then,1,1,1.0
problem firstly then investigated,1,1,1.0
firstly then investigated various,1,1,1.0
then investigated various remedies,1,1,1.0
investigated various remedies in,1,1,1.0
various remedies in four,1,1,1.0
remedies in four different,1,1,1.0
in four different levels,1,1,1.0
four different levels acco,1,1,1.0
different levels acco rding,1,1,1.0
levels acco rding to,1,1,1.0
acco rding to learning,1,1,1.0
rding to learning phases,1,1,1.0
to learning phases following,1,1,1.0
learning phases following surveying,1,1,1.0
phases following surveying evaluation,1,1,1.0
following surveying evaluation metrics,1,1,1.0
surveying evaluation metrics and,1,1,1.0
evaluation metrics and some,1,1,1.0
metrics and some other,1,1,1.0
and some other related,2,1,2.0
some other related factors,1,1,1.0
other related factors this,1,1,1.0
related factors this paper,1,1,1.0
factors this paper showed,1,1,1.0
this paper showed some,1,1,1.0
paper showed some future,1,1,1.0
showed some future directions,2,1,2.0
some future directions at,1,1,1.0
future directions at last,1,1,1.0
directions at last introduction,1,1,1.0
at last introduction many,1,1,1.0
last introduction many traditional,1,1,1.0
introduction many traditional algorithms,1,1,1.0
many traditional algorithms to,1,1,1.0
traditional algorithms to machine,1,1,1.0
algorithms to machine learning,1,1,1.0
to machine learning and,1,1,1.0
machine learning and data,1,1,1.0
learning and data mining,1,1,1.0
and data mining problems,1,1,1.0
data mining problems assume,1,1,1.0
mining problems assume that,1,1,1.0
problems assume that the,1,1,1.0
assume that the target,1,1,1.0
that the target classes,1,1,1.0
the target classes share,1,1,1.0
target classes share similar,1,1,1.0
classes share similar prior,1,1,1.0
share similar prior probabilities,1,1,1.0
similar prior probabilities however,1,1,1.0
prior probabilities however in,1,1,1.0
probabilities however in many,1,1,1.0
however in many real,1,1,1.0
in many real world,1,1,1.0
many real world applications,1,1,1.0
real world applications such,1,1,1.0
world applications such as,1,1,1.0
applications such as detection,1,1,1.0
such as detection network,1,1,1.0
as detection network intrusion,1,1,1.0
detection network intrusion detection,1,1,1.0
network intrusion detection fraud,1,1,1.0
intrusion detection fraud detection,2,1,2.0
detection fraud detection this,1,1,1.0
fraud detection this assumption,1,1,1.0
detection this assumption is,1,1,1.0
this assumption is grossly,1,1,1.0
assumption is grossly violated,1,1,1.0
is grossly violated in,1,1,1.0
grossly violated in such,1,1,1.0
violated in such problems,1,1,1.0
in such problems almost,1,1,1.0
such problems almost all,1,1,1.0
problems almost all the,1,1,1.0
more important class this,1,1,1.0
important class this situation,1,1,1.0
class this situation is,1,1,1.0
this situation is known,1,1,1.0
situation is known as,1,1,1.0
is known as the,2,2,1.0
known as the problem,1,1,1.0
as the problem of,1,1,1.0
the problem of class,3,1,3.0
problem of class imbalance,3,1,3.0
imbalance in this case,1,1,1.0
this case standard classifiers,1,1,1.0
case standard classifiers tend,1,1,1.0
standard classifiers tend to,1,1,1.0
classifiers tend to be,1,1,1.0
the minority class its,2,1,2.0
minority class its importance,1,1,1.0
class its importance grew,1,1,1.0
its importance grew as,1,1,1.0
importance grew as more,1,1,1.0
grew as more and,1,1,1.0
as more and more,1,1,1.0
more and more researchers,1,1,1.0
and more researchers realized,1,1,1.0
more researchers realized that,1,1,1.0
researchers realized that this,1,1,1.0
realized that this imbalance,1,1,1.0
that this imbalance causes,1,1,1.0
this imbalance causes suboptimal,1,1,1.0
imbalance causes suboptimal classification,1,1,1.0
causes suboptimal classification performance,1,1,1.0
suboptimal classification performance and,1,1,1.0
classification performance and that,1,1,1.0
performance and that most,1,1,1.0
and that most algorithms,1,1,1.0
that most algorithms behave,1,1,1.0
most algorithms behave badly,1,1,1.0
algorithms behave badly when,1,1,1.0
behave badly when the,1,1,1.0
badly when the data,1,1,1.0
when the data sets,3,1,3.0
the data sets are,3,1,3.0
data sets are highly,2,1,2.0
sets are highly imbalanced,2,1,2.0
are highly imbalanced the,1,1,1.0
highly imbalanced the class,1,1,1.0
imbalanced the class imbalance,1,1,1.0
problem has been a,1,1,1.0
has been a hot,1,1,1.0
been a hot topic,1,1,1.0
hot topic in machine,1,1,1.0
topic in machine learning,1,1,1.0
in machine learning in,2,1,2.0
in recent years class,1,1,1.0
recent years class imbalance,1,1,1.0
years class imbalance problem,1,1,1.0
has been recognized to,1,1,1.0
been recognized to be,1,1,1.0
recognized to be existing,1,1,1.0
to be existing in,1,1,1.0
be existing in lots,1,1,1.0
existing in lots of,1,1,1.0
in lots of application,1,1,1.0
lots of application domains,1,1,1.0
of application domains such,1,1,1.0
domains such as spotting,1,1,1.0
such as spotting unreliable,1,1,1.0
as spotting unreliable telecommunication,1,1,1.0
spotting unreliable telecommunication customers,1,1,1.0
unreliable telecommunication customers detection,1,1,1.0
telecommunication customers detection of,1,1,1.0
customers detection of oil,1,1,1.0
satellite radar images learning,1,1,1.0
radar images learning word,1,1,1.0
images learning word pronunciations,1,1,1.0
learning word pronunciations text,1,1,1.0
word pronunciations text classification,1,1,1.0
pronunciations text classification risk,1,1,1.0
text classification risk management,1,1,1.0
classification risk management information,1,1,1.0
risk management information retrieval,1,1,1.0
management information retrieval and,1,1,1.0
information retrieval and filtering,1,1,1.0
retrieval and filtering tasks,1,1,1.0
and filtering tasks medical,1,1,1.0
filtering tasks medical diagnosis,1,1,1.0
tasks medical diagnosis rare,1,1,1.0
medical diagnosis rare disease,1,1,1.0
diagnosis rare disease and,1,1,1.0
rare disease and rare,1,1,1.0
disease and rare genes,1,1,1.0
and rare genes mutations,1,1,1.0
rare genes mutations network,1,1,1.0
genes mutations network monitoring,1,1,1.0
mutations network monitoring and,1,1,1.0
network monitoring and intrusion,1,1,1.0
monitoring and intrusion detection,1,1,1.0
and intrusion detection fraud,1,1,1.0
detection fraud detection shuttle,1,1,1.0
fraud detection shuttle system,1,1,1.0
detection shuttle system failure,1,1,1.0
shuttle system failure earthquakes,1,1,1.0
system failure earthquakes and,1,1,1.0
failure earthquakes and nuclear,1,1,1.0
earthquakes and nuclear explosions,1,1,1.0
and nuclear explosions and,1,1,1.0
nuclear explosions and helicopter,1,1,1.0
explosions and helicopter fault,1,1,1.0
and helicopter fault monitoring,1,1,1.0
helicopter fault monitoring from,1,1,1.0
fault monitoring from the,1,1,1.0
monitoring from the view,1,1,1.0
from the view of,1,1,1.0
the view of applications,1,1,1.0
view of applications the,1,1,1.0
of applications the nature,1,1,1.0
applications the nature of,1,1,1.0
nature of the imbalance,1,1,1.0
of the imbalance falls,1,1,1.0
the imbalance falls in,1,1,1.0
imbalance falls in two,1,1,1.0
falls in two cases,1,1,1.0
in two cases the,1,1,1.0
two cases the data,1,1,1.0
cases the data are,1,1,1.0
the data are naturally,1,1,1.0
data are naturally imbalanced,1,1,1.0
are naturally imbalanced credit,1,1,1.0
naturally imbalanced credit card,1,1,1.0
imbalanced credit card frauds,1,1,1.0
credit card frauds and,1,1,1.0
card frauds and rare,1,1,1.0
frauds and rare disease,1,1,1.0
and rare disease or,1,1,1.0
rare disease or the,1,1,1.0
disease or the data,1,1,1.0
or the data are,1,1,1.0
the data are not,1,1,1.0
data are not naturally,1,1,1.0
are not naturally imbalanced,1,1,1.0
not naturally imbalanced but,1,1,1.0
naturally imbalanced but it,1,1,1.0
imbalanced but it is,1,1,1.0
but it is too,1,1,1.0
it is too expensive,1,1,1.0
is too expensive to,1,1,1.0
too expensive to obtain,1,1,1.0
expensive to obtain data,1,1,1.0
to obtain data of,1,1,1.0
obtain data of the,1,1,1.0
data of the minority,1,1,1.0
the minority class shuttle,1,1,1.0
minority class shuttle failure,1,1,1.0
class shuttle failure for,1,1,1.0
shuttle failure for learning,1,1,1.0
failure for learning there,1,1,1.0
for learning there have,1,1,1.0
learning there have been,1,1,1.0
there have been lots,1,1,1.0
have been lots of,1,1,1.0
been lots of researches,1,1,1.0
lots of researches on,1,1,1.0
of researches on class,1,1,1.0
researches on class imbalance,1,1,1.0
on class imbalance problem,1,1,1.0
class imbalance problem paper,1,1,1.0
imbalance problem paper reviewed,1,1,1.0
problem paper reviewed various,1,1,1.0
paper reviewed various techniques,1,1,1.0
reviewed various techniques for,1,1,1.0
various techniques for handling,1,1,1.0
techniques for handling imbalance,1,1,1.0
for handling imbalance dataset,1,1,1.0
handling imbalance dataset problems,1,1,1.0
imbalance dataset problems paper,1,1,1.0
dataset problems paper traced,1,1,1.0
problems paper traced some,1,1,1.0
paper traced some of,1,1,1.0
traced some of the,1,1,1.0
some of the recent,1,1,1.0
of the recent progress,1,1,1.0
the recent progress in,1,1,1.0
recent progress in the,1,1,1.0
progress in the field,1,1,1.0
in the field of,1,1,1.0
the field of learning,2,1,2.0
field of learning from,2,1,2.0
of learning from imbalanced,2,1,2.0
imbalanced data sets in,2,2,1.0
data sets in which,1,1,1.0
sets in which sofia,1,1,1.0
in which sofia visa,1,1,1.0
which sofia visa et,1,1,1.0
sofia visa et al,1,1,1.0
visa et al argued,1,1,1.0
et al argued that,1,1,1.0
al argued that the,1,1,1.0
argued that the poor,1,1,1.0
that the poor performance,1,1,1.0
the poor performance of,1,1,1.0
poor performance of the,1,1,1.0
performance of the classifiers,1,1,1.0
of the classifiers produced,1,1,1.0
the classifiers produced by,1,1,1.0
classifiers produced by the,1,1,1.0
produced by the standard,1,1,1.0
by the standard machine,1,1,1.0
the standard machine learning,1,1,1.0
machine learning algorithms on,1,1,1.0
learning algorithms on imbalanced,1,1,1.0
algorithms on imbalanced data,1,1,1.0
imbalanced data sets is,2,1,2.0
data sets is mainly,1,1,1.0
sets is mainly due,1,1,1.0
is mainly due to,1,1,1.0
mainly due to the,1,1,1.0
due to the following,1,1,1.0
to the following three,1,1,1.0
the following three factors,1,1,1.0
following three factors accuracy,1,1,1.0
three factors accuracy class,1,1,1.0
factors accuracy class distribution,1,1,1.0
accuracy class distribution and,1,1,1.0
class distribution and error,1,1,1.0
distribution and error costs,1,1,1.0
and error costs since,1,1,1.0
error costs since they,1,1,1.0
costs since they are,1,1,1.0
since they are rarely,1,1,1.0
they are rarely well,1,1,1.0
are rarely well satisfied,1,1,1.0
rarely well satisfied in,1,1,1.0
well satisfied in real,1,1,1.0
satisfied in real world,1,1,1.0
in real world applications,1,1,1.0
real world applications paper,1,1,1.0
world applications paper discussed,1,1,1.0
applications paper discussed several,1,1,1.0
paper discussed several issues,1,1,1.0
discussed several issues related,1,1,1.0
several issues related to,1,1,1.0
issues related to learning,1,1,1.0
related to learning with,1,1,1.0
to learning with skewed,1,1,1.0
learning with skewed class,3,1,3.0
with skewed class distributions,2,1,2.0
skewed class distributions such,1,1,1.0
class distributions such as,1,1,1.0
distributions such as the,1,1,1.0
such as the relationship,1,1,1.0
as the relationship between,1,1,1.0
the relationship between learning,1,1,1.0
relationship between learning and,1,1,1.0
between learning and class,1,1,1.0
learning and class distributions,1,1,1.0
and class distributions and,1,1,1.0
class distributions and the,1,1,1.0
distributions and the limitations,1,1,1.0
and the limitations of,1,1,1.0
the limitations of accuracy,1,1,1.0
limitations of accuracy and,1,1,1.0
of accuracy and error,1,1,1.0
accuracy and error rate,1,1,1.0
and error rate to,1,1,1.0
error rate to measure,1,1,1.0
rate to measure the,1,1,1.0
to measure the performance,1,1,1.0
measure the performance of,1,1,1.0
performance of classifiers weiss,1,1,1.0
of classifiers weiss presented,1,1,1.0
classifiers weiss presented an,1,1,1.0
weiss presented an overview,1,1,1.0
presented an overview of,1,1,1.0
an overview of the,1,1,1.0
overview of the field,1,1,1.0
of the field of,1,1,1.0
from imbalanced data he,1,1,1.0
imbalanced data he pays,1,1,1.0
data he pays particular,1,1,1.0
he pays particular attention,1,1,1.0
pays particular attention to,1,1,1.0
particular attention to differences,1,1,1.0
attention to differences and,1,1,1.0
to differences and similarities,1,1,1.0
differences and similarities between,1,1,1.0
and similarities between the,1,1,1.0
similarities between the problems,1,1,1.0
between the problems of,1,1,1.0
the problems of rare,1,1,1.0
problems of rare classes,1,1,1.0
of rare classes and,1,1,1.0
rare classes and rare,1,1,1.0
classes and rare cases,1,1,1.0
and rare cases he,1,1,1.0
rare cases he then,1,1,1.0
cases he then discussed,1,1,1.0
he then discussed some,1,1,1.0
then discussed some of,1,1,1.0
discussed some of the,1,1,1.0
some of the common,1,1,1.0
of the common issues,1,1,1.0
the common issues and,1,1,1.0
common issues and their,1,1,1.0
issues and their range,1,1,1.0
and their range of,1,1,1.0
their range of solutions,1,1,1.0
range of solutions in,1,1,1.0
of solutions in mining,1,1,1.0
solutions in mining imbalanced,1,1,1.0
in mining imbalanced data,2,1,2.0
mining imbalanced data sets,1,1,1.0
data sets the reminder,1,1,1.0
sets the reminder of,1,1,1.0
the reminder of the,1,1,1.0
reminder of the paper,1,1,1.0
of the paper is,1,1,1.0
the paper is organized,2,2,1.0
paper is organized as,2,2,1.0
is organized as follows,2,2,1.0
organized as follows section,2,2,1.0
as follows section reviewed,1,1,1.0
follows section reviewed academic,1,1,1.0
section reviewed academic activities,1,1,1.0
reviewed academic activities including,1,1,1.0
academic activities including two,1,1,1.0
activities including two fourth,1,1,1.0
including two fourth international,1,1,1.0
two fourth international conference,1,1,1.0
international conference on natural,3,1,3.0
conference on natural computation,3,1,3.0
on natural computation ieee,3,1,3.0
natural computation ieee doi,3,1,3.0
computation ieee doi fourth,2,1,2.0
ieee doi fourth international,2,1,2.0
doi fourth international conference,2,1,2.0
computation ieee doi workshops,1,1,1.0
ieee doi workshops and,1,1,1.0
doi workshops and one,1,1,1.0
workshops and one special,1,1,1.0
and one special issue,2,1,2.0
one special issue on,2,1,2.0
special issue on the,1,1,1.0
issue on the problem,1,1,1.0
on the problem of,1,1,1.0
of class imbalance sections,1,1,1.0
class imbalance sections surveyed,1,1,1.0
imbalance sections surveyed the,1,1,1.0
sections surveyed the remedies,1,1,1.0
surveyed the remedies to,1,1,1.0
the remedies to the,1,1,1.0
remedies to the class,1,1,1.0
class imbalance problem from,1,1,1.0
imbalance problem from four,1,1,1.0
problem from four different,1,1,1.0
from four different levels,1,1,1.0
four different levels popular,1,1,1.0
different levels popular evaluation,1,1,1.0
levels popular evaluation metrics,1,1,1.0
popular evaluation metrics for,1,1,1.0
evaluation metrics for imbalanced,1,1,1.0
metrics for imbalanced data,1,1,1.0
imbalanced data sets were,1,1,1.0
data sets were summarized,1,1,1.0
sets were summarized in,1,1,1.0
were summarized in section,1,1,1.0
summarized in section section,1,1,1.0
in section section briefly,1,1,1.0
section section briefly analyzed,1,1,1.0
section briefly analyzed some,1,1,1.0
briefly analyzed some other,1,1,1.0
analyzed some other factors,1,1,1.0
some other factors related,1,1,1.0
other factors related to,1,1,1.0
factors related to the,1,1,1.0
related to the class,1,1,1.0
imbalance problem and section,1,1,1.0
problem and section concluded,1,1,1.0
and section concluded the,1,1,1.0
section concluded the paper,1,1,1.0
concluded the paper and,1,1,1.0
the paper and showed,1,1,1.0
paper and showed some,1,1,1.0
and showed some future,1,1,1.0
some future directions academic,1,1,1.0
future directions academic activiti,1,1,1.0
directions academic activiti es,1,1,1.0
academic activiti es on,1,1,1.0
activiti es on the,1,1,1.0
es on the class,1,1,1.0
class imbalance problem as,1,1,1.0
imbalance problem as described,1,1,1.0
problem as described above,1,1,1.0
as described above recognizing,1,1,1.0
described above recognizing class,1,1,1.0
above recognizing class imbalance,1,1,1.0
recognizing class imbalance problem,1,1,1.0
class imbalance problem exists,1,1,1.0
imbalance problem exists in,1,1,1.0
problem exists in extensive,1,1,1.0
exists in extensive application,1,1,1.0
in extensive application domains,1,1,1.0
extensive application domains gave,1,1,1.0
application domains gave rise,1,1,1.0
domains gave rise to,1,1,1.0
gave rise to two,1,1,1.0
rise to two workshops,1,1,1.0
to two workshops held,1,1,1.0
two workshops held at,1,1,1.0
workshops held at the,1,1,1.0
held at the top,1,1,1.0
at the top conferences,1,1,1.0
the top conferences in,1,1,1.0
top conferences in ai,1,1,1.0
conferences in ai and,1,1,1.0
in ai and one,1,1,1.0
ai and one special,1,1,1.0
special issue on dealing,1,1,1.0
issue on dealing with,1,1,1.0
on dealing with the,1,1,1.0
dealing with the class,2,1,2.0
with the class imbalance,2,1,2.0
class imbalance problem the,3,2,1.5
imbalance problem the first,1,1,1.0
problem the first workshop,1,1,1.0
the first workshop dedicated,1,1,1.0
first workshop dedicated to,1,1,1.0
workshop dedicated to the,1,1,1.0
dedicated to the class,1,1,1.0
class imbalance problem was,2,1,2.0
imbalance problem was held,1,1,1.0
problem was held in,1,1,1.0
was held in conjunction,1,1,1.0
held in conjunction with,1,1,1.0
in conjunction with the,1,1,1.0
conjunction with the american,1,1,1.0
with the american association,1,1,1.0
the american association for,1,1,1.0
american association for artificial,1,1,1.0
association for artificial intelligence,1,1,1.0
for artificial intelligence conference,1,1,1.0
artificial intelligence conference aaai,1,1,1.0
intelligence conference aaai its,1,1,1.0
conference aaai its main,1,1,1.0
aaai its main contribution,1,1,1.0
its main contribution includes,1,1,1.0
main contribution includes observation,1,1,1.0
contribution includes observation of,1,1,1.0
includes observation of many,1,1,1.0
observation of many application,1,1,1.0
of many application domains,1,1,1.0
many application domains dealing,1,1,1.0
application domains dealing with,1,1,1.0
domains dealing with imbalanced,1,1,1.0
dealing with imbalanced data,1,1,1.0
with imbalanced data sets,3,2,1.5
imbalanced data sets and,2,1,2.0
data sets and several,1,1,1.0
sets and several important,1,1,1.0
and several important issues,1,1,1.0
several important issues such,1,1,1.0
important issues such as,1,1,1.0
issues such as how,1,1,1.0
such as how to,1,1,1.0
as how to evaluate,1,1,1.0
how to evaluate learning,1,1,1.0
to evaluate learning algorithms,1,1,1.0
evaluate learning algorithms what,1,1,1.0
learning algorithms what evaluation,1,1,1.0
algorithms what evaluation measures,1,1,1.0
what evaluation measures should,1,1,1.0
evaluation measures should be,1,1,1.0
measures should be used,1,1,1.0
should be used one,1,1,1.0
be used one class,1,1,1.0
used one class learning,1,1,1.0
one class learning versus,1,1,1.0
class learning versus discriminating,1,1,1.0
learning versus discriminating methods,1,1,1.0
versus discriminating methods discussions,1,1,1.0
discriminating methods discussions over,1,1,1.0
methods discussions over various,1,1,1.0
discussions over various methods,1,1,1.0
over various methods discussion,1,1,1.0
various methods discussion of,1,1,1.0
methods discussion of the,1,1,1.0
discussion of the relation,1,1,1.0
of the relation between,1,1,1.0
the relation between class,2,1,2.0
relation between class imbalance,2,1,2.0
between class imbalance problem,1,1,1.0
imbalance problem and learning,1,1,1.0
problem and learning the,1,1,1.0
and learning the goal,1,1,1.0
learning the goal of,1,1,1.0
the goal of creating,1,1,1.0
goal of creating classifiers,1,1,1.0
of creating classifiers that,1,1,1.0
creating classifiers that performs,1,1,1.0
classifiers that performs well,1,1,1.0
that performs well across,1,1,1.0
performs well across a,1,1,1.0
well across a range,1,1,1.0
across a range of,1,1,1.0
a range of costs,1,1,1.0
range of costs and,1,1,1.0
of costs and so,1,1,1.0
costs and so on,1,1,1.0
and so on the,1,1,1.0
so on the second,1,1,1.0
on the second workshop,1,1,1.0
the second workshop special,1,1,1.0
second workshop special for,1,1,1.0
workshop special for the,1,1,1.0
class imbalance problem is,3,1,3.0
imbalance problem is part,1,1,1.0
problem is part of,1,1,1.0
is part of the,1,1,1.0
part of the international,1,1,1.0
international conference on machine,6,2,3.0
conference on machine learning,8,2,4.0
on machine learning icml,1,1,1.0
machine learning icml in,1,1,1.0
learning icml in which,1,1,1.0
icml in which most,1,1,1.0
in which most research,1,1,1.0
which most research on,1,1,1.0
most research on the,1,1,1.0
research on the problem,1,1,1.0
on the problem was,1,1,1.0
the problem was guided,1,1,1.0
problem was guided by,1,1,1.0
was guided by the,1,1,1.0
guided by the first,1,1,1.0
by the first workshop,1,1,1.0
the first workshop for,1,1,1.0
first workshop for example,1,1,1.0
workshop for example roc,1,1,1.0
for example roc or,1,1,1.0
example roc or cost,1,1,1.0
roc or cost curves,1,1,1.0
or cost curves were,1,1,1.0
cost curves were used,1,1,1.0
curves were used as,1,1,1.0
were used as evaluation,1,1,1.0
used as evaluation metrics,1,1,1.0
as evaluation metrics rather,1,1,1.0
evaluation metrics rather than,1,1,1.0
metrics rather than accuracy,1,1,1.0
rather than accuracy the,1,1,1.0
than accuracy the workshop,1,1,1.0
accuracy the workshop was,1,1,1.0
the workshop was followed,1,1,1.0
workshop was followed by,1,1,1.0
was followed by an,1,1,1.0
followed by an interesting,1,1,1.0
by an interesting and,1,1,1.0
an interesting and vivid,1,1,1.0
interesting and vivid panel,1,1,1.0
and vivid panel discussion,1,1,1.0
vivid panel discussion two,1,1,1.0
panel discussion two major,1,1,1.0
discussion two major directions,1,1,1.0
two major directions presented,1,1,1.0
major directions presented in,1,1,1.0
directions presented in the,1,1,1.0
presented in the research,1,1,1.0
in the research papers,1,1,1.0
the research papers of,1,1,1.0
research papers of the,1,1,1.0
papers of the workshop,1,1,1.0
of the workshop many,1,1,1.0
the workshop many papers,1,1,1.0
workshop many papers still,1,1,1.0
many papers still reported,1,1,1.0
papers still reported various,1,1,1.0
still reported various tuning,1,1,1.0
reported various tuning methods,1,1,1.0
various tuning methods applied,1,1,1.0
tuning methods applied to,1,1,1.0
methods applied to decision,1,1,1.0
applied to decision trees,1,1,1.0
to decision trees in,1,1,1.0
decision trees in order,1,1,1.0
trees in order to,1,1,1.0
in order to perform,1,1,1.0
order to perform better,1,1,1.0
to perform better on,1,1,1.0
perform better on imbalanced,1,1,1.0
better on imbalanced data,1,1,1.0
imbalanced data sets even,1,1,1.0
data sets even though,1,1,1.0
sets even though presentations,1,1,1.0
even though presentations in,1,1,1.0
though presentations in the,1,1,1.0
presentations in the previous,1,1,1.0
in the previous workshop,1,1,1.0
the previous workshop showed,1,1,1.0
previous workshop showed their,1,1,1.0
workshop showed their shortcomings,1,1,1.0
showed their shortcomings and,1,1,1.0
their shortcomings and it,1,1,1.0
shortcomings and it was,1,1,1.0
and it was commonly,1,1,1.0
it was commonly agreed,1,1,1.0
was commonly agreed that,1,1,1.0
commonly agreed that new,1,1,1.0
agreed that new classifiers,1,1,1.0
that new classifiers are,1,1,1.0
new classifiers are needed,1,1,1.0
classifiers are needed for,1,1,1.0
are needed for imbalanced,1,1,1.0
needed for imbalanced data,1,1,1.0
imbalanced data sets besides,1,1,1.0
data sets besides under,1,1,1.0
sets besides under various,1,1,1.0
besides under various aspects,1,1,1.0
under various aspects was,1,1,1.0
various aspects was present,1,1,1.0
aspects was present in,1,1,1.0
was present in half,1,1,1.0
present in half of,1,1,1.0
in half of the,1,1,1.0
half of the papers,1,1,1.0
of the papers and,1,1,1.0
the papers and was,1,1,1.0
papers and was the,1,1,1.0
and was the most,1,1,1.0
was the most debated,1,1,1.0
the most debated issue,1,1,1.0
most debated issue even,1,1,1.0
debated issue even though,1,1,1.0
issue even though shows,1,1,1.0
even though shows that,1,1,1.0
though shows that sampling,1,1,1.0
shows that sampling has,1,1,1.0
that sampling has the,1,1,1.0
sampling has the same,1,1,1.0
has the same result,1,1,1.0
the same result as,1,1,1.0
same result as moving,1,1,1.0
result as moving the,1,1,1.0
as moving the decision,1,1,1.0
moving the decision threshold,1,1,1.0
the decision threshold or,1,1,1.0
decision threshold or adjusting,1,1,1.0
threshold or adjusting the,1,1,1.0
or adjusting the cost,1,1,1.0
adjusting the cost matrix,1,1,1.0
the cost matrix a,1,1,1.0
cost matrix a result,1,1,1.0
matrix a result known,1,1,1.0
a result known since,1,1,1.0
result known since in,1,1,1.0
known since in addition,1,1,1.0
since in addition japkowicz,1,1,1.0
in addition japkowicz questioned,1,1,1.0
addition japkowicz questioned the,1,1,1.0
japkowicz questioned the fact,1,1,1.0
questioned the fact that,1,1,1.0
fact that the within,1,1,1.0
that the within class,1,1,1.0
class imbalance is responsible,1,1,1.0
imbalance is responsible for,1,1,1.0
is responsible for the,1,1,1.0
responsible for the problem,1,1,1.0
for the problem the,2,1,2.0
the problem the idea,1,1,1.0
problem the idea is,1,1,1.0
the idea is that,1,1,1.0
idea is that within,1,1,1.0
is that within class,1,1,1.0
that within class imbalance,1,1,1.0
within class imbalance leads,1,1,1.0
class imbalance leads to,1,1,1.0
imbalance leads to a,1,1,1.0
leads to a severe,1,1,1.0
to a severe lack,1,1,1.0
a severe lack of,1,1,1.0
severe lack of representation,1,1,1.0
lack of representation of,1,1,1.0
of representation of some,1,1,1.0
representation of some important,1,1,1.0
of some important aspects,1,1,1.0
some important aspects of,1,1,1.0
important aspects of the,1,1,1.0
aspects of the minority,1,1,1.0
minority class the sixth,1,1,1.0
class the sixth issue,1,1,1.0
the sixth issue of,1,1,1.0
sixth issue of sigkdd,1,1,1.0
issue of sigkdd exploration,1,1,1.0
of sigkdd exploration was,1,1,1.0
sigkdd exploration was dedicated,1,1,1.0
exploration was dedicated entirely,1,1,1.0
was dedicated entirely to,1,1,1.0
dedicated entirely to the,1,1,1.0
entirely to the class,1,1,1.0
class imbalance problem in,3,2,1.5
imbalance problem in which,1,1,1.0
problem in which weiss,1,1,1.0
in which weiss presented,1,1,1.0
which weiss presented a,1,1,1.0
weiss presented a very,1,1,1.0
presented a very good,1,1,1.0
a very good review,1,1,1.0
very good review of,1,1,1.0
good review of the,1,1,1.0
review of the current,1,1,1.0
of the current research,1,1,1.0
the current research on,1,1,1.0
current research on learning,1,1,1.0
research on learning from,1,1,1.0
on learning from imbalanced,7,2,3.5
data sets and the,1,1,1.0
sets and the other,1,1,1.0
and the other papers,1,1,1.0
the other papers in,1,1,1.0
other papers in the,1,1,1.0
papers in the volume,1,1,1.0
in the volume address,1,1,1.0
the volume address mainly,1,1,1.0
volume address mainly issues,1,1,1.0
address mainly issues of,1,1,1.0
mainly issues of sampling,1,1,1.0
issues of sampling feature,1,1,1.0
of sampling feature selection,1,1,1.0
sampling feature selection and,1,1,1.0
feature selection and learning,1,1,1.0
selection and learning for,1,1,1.0
and learning for example,1,1,1.0
learning for example investigated,1,1,1.0
for example investigated a,1,1,1.0
example investigated a boosting,1,1,1.0
investigated a boosting method,1,1,1.0
a boosting method combined,1,1,1.0
boosting method combined with,1,1,1.0
method combined with various,1,1,1.0
combined with various techniques,1,1,1.0
with various techniques of,1,1,1.0
various techniques of the,1,1,1.0
techniques of the hard,1,1,1.0
of the hard to,1,1,1.0
the hard to classify,1,1,1.0
hard to classify examples,1,1,1.0
to classify examples the,1,1,1.0
classify examples the method,1,1,1.0
examples the method improves,1,1,1.0
the method improves the,1,1,1.0
method improves the prediction,1,1,1.0
improves the prediction accuracy,1,1,1.0
the prediction accuracy for,1,1,1.0
prediction accuracy for both,1,1,1.0
accuracy for both the,1,1,1.0
for both the classes,1,1,1.0
both the classes and,1,1,1.0
the classes and does,1,1,1.0
classes and does not,1,1,1.0
and does not sacrifice,1,1,1.0
does not sacrifice one,1,1,1.0
not sacrifice one class,1,1,1.0
sacrifice one class for,1,1,1.0
one class for the,1,1,1.0
class for the other,1,1,1.0
for the other with,1,1,1.0
the other with experiments,1,1,1.0
other with experiments on,1,1,1.0
with experiments on data,1,1,1.0
experiments on data sets,1,1,1.0
on data sets remedies,1,1,1.0
data sets remedies for,1,1,1.0
sets remedies for the,1,1,1.0
remedies for the class,1,1,1.0
imbalance problem the remedies,1,1,1.0
problem the remedies to,1,1,1.0
the remedies to deal,1,1,1.0
remedies to deal with,1,1,1.0
to deal with the,3,2,1.5
deal with the problem,2,1,2.0
with the problem of,3,1,3.0
of class imbalance are,1,1,1.0
class imbalance are of,1,1,1.0
imbalance are of four,1,1,1.0
are of four different,1,1,1.0
of four different levels,1,1,1.0
four different levels according,1,1,1.0
different levels according to,1,1,1.0
levels according to the,1,1,1.0
according to the phases,1,1,1.0
to the phases in,1,1,1.0
the phases in learning,1,1,1.0
phases in learning changing,1,1,1.0
in learning changing class,1,1,1.0
learning changing class distributions,1,1,1.0
changing class distributions mainly,1,1,1.0
class distributions mainly by,1,1,1.0
distributions mainly by techniques,1,1,1.0
mainly by techniques features,1,1,1.0
by techniques features selection,1,1,1.0
techniques features selection in,1,1,1.0
features selection in the,1,1,1.0
selection in the feature,1,1,1.0
in the feature level,1,1,1.0
the feature level classifiers,1,1,1.0
feature level classifiers level,1,1,1.0
level classifiers level by,1,1,1.0
classifiers level by manipulating,1,1,1.0
level by manipulating classifiers,1,1,1.0
by manipulating classifiers internally,1,1,1.0
manipulating classifiers internally and,1,1,1.0
classifiers internally and ensemble,1,1,1.0
internally and ensemble learning,1,1,1.0
and ensemble learning for,1,1,1.0
ensemble learning for final,1,1,1.0
learning for final classification,1,1,1.0
for final classification changing,1,1,1.0
final classification changing class,1,1,1.0
classification changing class distributions,1,1,1.0
changing class distributions since,1,1,1.0
class distributions since examples,1,1,1.0
distributions since examples belong,1,1,1.0
since examples belong to,1,1,1.0
examples belong to the,1,1,1.0
belong to the minority,2,2,1.0
minority class are far,1,1,1.0
class are far fewer,1,1,1.0
are far fewer than,1,1,1.0
far fewer than those,1,1,1.0
fewer than those belong,1,1,1.0
than those belong to,1,1,1.0
those belong to the,1,1,1.0
majority class in situations,1,1,1.0
class in situations of,1,1,1.0
in situations of the,1,1,1.0
situations of the class,1,1,1.0
of the class imbalance,3,2,1.5
class imbalance problem one,1,1,1.0
imbalance problem one direct,1,1,1.0
problem one direct way,1,1,1.0
one direct way to,1,1,1.0
direct way to counter,1,1,1.0
way to counter the,1,1,1.0
to counter the problem,1,1,1.0
counter the problem is,1,1,1.0
problem is to change,1,1,1.0
is to change class,1,1,1.0
to change class distributions,1,1,1.0
change class distributions balanced,1,1,1.0
class distributions balanced distributions,1,1,1.0
distributions balanced distributions can,1,1,1.0
balanced distributions can be,1,1,1.0
distributions can be obtained,1,1,1.0
can be obtained by,1,1,1.0
be obtained by the,1,1,1.0
obtained by the majority,1,1,1.0
the majority class minority,1,1,1.0
majority class minority class,1,1,1.0
class minority class combining,1,1,1.0
minority class combining the,1,1,1.0
class combining the both,1,1,1.0
combining the both and,1,1,1.0
the both and some,1,1,1.0
both and some other,1,1,1.0
and some other advanced,1,1,1.0
some other advanced sampling,1,1,1.0
other advanced sampling ways,1,1,1.0
advanced sampling ways there,1,1,1.0
sampling ways there are,1,1,1.0
ways there are numerous,1,1,1.0
there are numerous researches,1,1,1.0
are numerous researches on,1,1,1.0
numerous researches on changing,1,1,1.0
researches on changing class,1,1,1.0
on changing class distributions,1,1,1.0
changing class distributions weiss,1,1,1.0
class distributions weiss investigated,1,1,1.0
distributions weiss investigated the,1,1,1.0
weiss investigated the effect,1,1,1.0
investigated the effect of,1,1,1.0
the effect of class,3,2,1.5
effect of class distributions,1,1,1.0
of class distributions on,1,1,1.0
class distributions on decision,1,1,1.0
distributions on decision tree,1,1,1.0
on decision tree by,1,1,1.0
decision tree by altering,1,1,1.0
tree by altering class,1,1,1.0
by altering class distributions,1,1,1.0
altering class distributions in,1,1,1.0
class distributions in several,1,1,1.0
distributions in several ratios,1,1,1.0
in several ratios with,1,1,1.0
several ratios with accuracy,1,1,1.0
ratios with accuracy and,1,1,1.0
with accuracy and auc,1,1,1.0
accuracy and auc as,1,1,1.0
and auc as metrics,1,1,1.0
auc as metrics in,1,1,1.0
as metrics in his,1,1,1.0
metrics in his doctoral,1,1,1.0
in his doctoral dissertation,1,1,1.0
his doctoral dissertation all,1,1,1.0
doctoral dissertation all the,1,1,1.0
dissertation all the methods,1,1,1.0
all the methods falls,1,1,1.0
the methods falls into,1,1,1.0
methods falls into three,1,1,1.0
falls into three basic,1,1,1.0
into three basic techniques,1,1,1.0
three basic techniques heuristic,1,1,1.0
basic techniques heuristic and,1,1,1.0
techniques heuristic and heuristic,1,1,1.0
heuristic and heuristic and,1,1,1.0
and heuristic and sampling,1,1,1.0
heuristic and sampling and,1,1,1.0
and sampling and advanced,1,1,1.0
sampling and advanced sampling,1,1,1.0
and advanced sampling compared,1,1,1.0
advanced sampling compared the,1,1,1.0
sampling compared the most,1,1,1.0
compared the most naivest,1,1,1.0
the most naivest sampling,1,1,1.0
most naivest sampling method,1,1,1.0
naivest sampling method is,1,1,1.0
sampling method is random,1,1,1.0
method is random a,1,1,1.0
is random a method,1,1,1.0
random a method trying,1,1,1.0
a method trying to,1,1,1.0
method trying to balance,1,1,1.0
trying to balance class,1,1,1.0
to balance class distributions,2,1,2.0
balance class distributions through,2,1,2.0
class distributions through the,2,1,2.0
distributions through the random,2,1,2.0
through the random elimination,1,1,1.0
the random elimination of,1,1,1.0
random elimination of majority,1,1,1.0
elimination of majority class,1,1,1.0
of majority class examples,1,1,1.0
majority class examples this,1,1,1.0
class examples this leads,1,1,1.0
examples this leads to,1,1,1.0
this leads to discarding,1,1,1.0
leads to discarding useful,1,1,1.0
to discarding useful data,1,1,1.0
discarding useful data that,1,1,1.0
useful data that could,1,1,1.0
data that could be,1,1,1.0
that could be important,1,1,1.0
could be important for,1,1,1.0
be important for classifiers,1,1,1.0
important for classifiers there,1,1,1.0
for classifiers there have,1,1,1.0
classifiers there have been,1,1,1.0
there have been several,1,1,1.0
have been several heuristic,1,1,1.0
been several heuristic methods,1,1,1.0
several heuristic methods proposed,1,1,1.0
heuristic methods proposed or,1,1,1.0
methods proposed or introduced,1,1,1.0
proposed or introduced from,2,1,2.0
or introduced from data,1,1,1.0
introduced from data cleaning,1,1,1.0
from data cleaning in,1,1,1.0
data cleaning in recent,1,1,1.0
cleaning in recent years,1,1,1.0
in recent years they,1,1,1.0
recent years they are,1,1,1.0
years they are based,1,1,1.0
are based on either,1,1,1.0
based on either of,1,1,1.0
on either of two,1,1,1.0
either of two different,1,1,1.0
of two different noise,1,1,1.0
two different noise model,1,1,1.0
different noise model hypotheses,1,1,1.0
noise model hypotheses one,1,1,1.0
model hypotheses one thinks,1,1,1.0
hypotheses one thinks examples,1,1,1.0
one thinks examples that,1,1,1.0
thinks examples that are,1,1,1.0
examples that are near,1,1,1.0
that are near to,1,1,1.0
are near to the,1,1,1.0
near to the classification,1,1,1.0
to the classification boundary,1,1,1.0
the classification boundary of,1,1,1.0
classification boundary of the,1,1,1.0
boundary of the two,1,1,1.0
of the two classes,1,1,1.0
the two classes are,1,1,1.0
two classes are noise,1,1,1.0
classes are noise while,1,1,1.0
are noise while the,1,1,1.0
noise while the other,1,1,1.0
while the other considers,1,1,1.0
the other considers examples,1,1,1.0
other considers examples with,1,1,1.0
considers examples with more,1,1,1.0
examples with more neighbors,1,1,1.0
with more neighbors of,1,1,1.0
more neighbors of different,1,1,1.0
neighbors of different labels,1,1,1.0
of different labels are,1,1,1.0
different labels are noise,1,1,1.0
labels are noise condensed,1,1,1.0
are noise condensed nearest,1,1,1.0
noise condensed nearest neighbor,1,1,1.0
condensed nearest neighbor rule,2,1,2.0
nearest neighbor rule cnn,1,1,1.0
neighbor rule cnn bases,1,1,1.0
rule cnn bases on,1,1,1.0
cnn bases on the,1,1,1.0
bases on the notion,1,1,1.0
on the notion of,1,1,1.0
the notion of a,1,1,1.0
notion of a consistent,1,1,1.0
of a consistent subset,2,1,2.0
a consistent subset of,1,1,1.0
consistent subset of a,1,1,1.0
subset of a sample,1,1,1.0
of a sample set,1,1,1.0
a sample set which,1,1,1.0
sample set which is,1,1,1.0
set which is a,1,1,1.0
which is a subset,1,1,1.0
is a subset who,1,1,1.0
a subset who can,1,1,1.0
subset who can correctly,1,1,1.0
who can correctly classifies,1,1,1.0
can correctly classifies all,1,1,1.0
correctly classifies all of,1,1,1.0
classifies all of the,1,1,1.0
all of the remaining,1,1,1.0
of the remaining examples,1,1,1.0
the remaining examples in,1,1,1.0
remaining examples in the,1,1,1.0
examples in the training,5,2,2.5
in the training set,4,2,2.0
the training set when,1,1,1.0
training set when used,1,1,1.0
set when used as,1,1,1.0
when used as a,1,1,1.0
used as a stored,1,1,1.0
as a stored reference,1,1,1.0
a stored reference set,1,1,1.0
stored reference set for,1,1,1.0
reference set for the,1,1,1.0
set for the nn,1,1,1.0
for the nn rule,1,1,1.0
the nn rule if,1,1,1.0
nn rule if the,1,1,1.0
rule if the bayesian,1,1,1.0
if the bayesian risk,2,1,2.0
the bayesian risk is,2,1,2.0
bayesian risk is small,1,1,1.0
risk is small if,1,1,1.0
is small if the,1,1,1.0
small if the underlying,1,1,1.0
if the underlying densities,1,1,1.0
the underlying densities of,1,1,1.0
underlying densities of the,1,1,1.0
densities of the various,1,1,1.0
of the various classes,1,1,1.0
the various classes have,1,1,1.0
various classes have small,1,1,1.0
classes have small overlapping,1,1,1.0
have small overlapping then,1,1,1.0
small overlapping then the,1,1,1.0
overlapping then the algorithm,1,1,1.0
then the algorithm will,1,1,1.0
the algorithm will tend,1,1,1.0
algorithm will tend to,1,1,1.0
will tend to pick,1,1,1.0
tend to pick out,1,1,1.0
to pick out examples,1,1,1.0
pick out examples near,1,1,1.0
out examples near the,1,1,1.0
examples near the perhaps,1,1,1.0
near the perhaps fuzzy,1,1,1.0
the perhaps fuzzy boundary,1,1,1.0
perhaps fuzzy boundary between,1,1,1.0
fuzzy boundary between the,1,1,1.0
boundary between the classes,1,1,1.0
between the classes typically,1,1,1.0
the classes typically points,1,1,1.0
classes typically points deeply,1,1,1.0
typically points deeply imbedded,1,1,1.0
points deeply imbedded within,1,1,1.0
deeply imbedded within a,1,1,1.0
imbedded within a class,1,1,1.0
within a class will,1,1,1.0
a class will not,1,1,1.0
class will not be,1,1,1.0
will not be transferred,1,1,1.0
not be transferred to,1,1,1.0
be transferred to store,1,1,1.0
transferred to store since,1,1,1.0
to store since they,1,1,1.0
store since they will,1,1,1.0
since they will be,1,1,1.0
they will be correctly,1,1,1.0
will be correctly classified,1,1,1.0
be correctly classified if,1,1,1.0
correctly classified if the,1,1,1.0
classified if the bayesian,1,1,1.0
bayesian risk is high,1,1,1.0
risk is high then,1,1,1.0
is high then store,1,1,1.0
high then store will,1,1,1.0
then store will contain,1,1,1.0
store will contain essentially,1,1,1.0
will contain essentially all,1,1,1.0
contain essentially all the,1,1,1.0
essentially all the examples,1,1,1.0
all the examples in,1,1,1.0
the examples in the,1,1,1.0
examples in the original,1,1,1.0
in the original training,1,1,1.0
the original training set,1,1,1.0
original training set and,1,1,1.0
training set and no,1,1,1.0
set and no important,1,1,1.0
and no important reduction,1,1,1.0
no important reduction in,1,1,1.0
important reduction in training,1,1,1.0
reduction in training size,1,1,1.0
in training size will,1,1,1.0
training size will have,1,1,1.0
size will have been,1,1,1.0
will have been achieved,1,1,1.0
have been achieved so,1,1,1.0
been achieved so cnn,1,1,1.0
achieved so cnn is,1,1,1.0
so cnn is effective,1,1,1.0
cnn is effective only,1,1,1.0
is effective only binary,1,1,1.0
effective only binary classes,1,1,1.0
only binary classes are,1,1,1.0
binary classes are of,1,1,1.0
classes are of small,1,1,1.0
are of small overlapping,1,1,1.0
of small overlapping oss,1,1,1.0
small overlapping oss randomly,1,1,1.0
overlapping oss randomly draws,1,1,1.0
oss randomly draws one,1,1,1.0
randomly draws one majority,1,1,1.0
draws one majority class,1,1,1.0
one majority class example,1,1,1.0
majority class example and,1,1,1.0
class example and all,1,1,1.0
example and all examples,1,1,1.0
and all examples from,1,1,1.0
all examples from the,1,1,1.0
examples from the minority,2,1,2.0
minority class and then,1,1,1.0
class and then puts,1,1,1.0
and then puts these,1,1,1.0
then puts these examples,1,1,1.0
puts these examples in,1,1,1.0
these examples in afterwards,1,1,1.0
examples in afterwards use,1,1,1.0
in afterwards use a,1,1,1.0
afterwards use a over,1,1,1.0
use a over the,1,1,1.0
a over the examples,1,1,1.0
over the examples in,1,1,1.0
the examples in e,1,1,1.0
examples in e to,1,1,1.0
in e to classify,1,1,1.0
e to classify the,1,1,1.0
to classify the examples,1,1,1.0
classify the examples in,1,1,1.0
the examples in every,1,1,1.0
examples in every misclassified,1,1,1.0
in every misclassified example,1,1,1.0
every misclassified example from,1,1,1.0
misclassified example from e,1,1,1.0
example from e is,1,1,1.0
from e is moved,1,1,1.0
e is moved to,1,1,1.0
is moved to e,1,1,1.0
moved to e the,1,1,1.0
to e the idea,1,1,1.0
e the idea behind,1,1,1.0
the idea behind this,1,1,1.0
idea behind this implementation,1,1,1.0
behind this implementation of,1,1,1.0
this implementation of a,1,1,1.0
implementation of a consistent,1,1,1.0
a consistent subset is,1,1,1.0
consistent subset is to,1,1,1.0
subset is to eliminate,1,1,1.0
is to eliminate the,1,1,1.0
to eliminate the examples,1,1,1.0
eliminate the examples from,1,1,1.0
the examples from the,2,1,2.0
class that are distant,1,1,1.0
that are distant from,1,1,1.0
are distant from the,1,1,1.0
distant from the decision,1,1,1.0
from the decision border,1,1,1.0
the decision border since,1,1,1.0
decision border since these,1,1,1.0
border since these examples,1,1,1.0
since these examples might,1,1,1.0
these examples might be,1,1,1.0
examples might be considered,1,1,1.0
might be considered less,1,1,1.0
be considered less relevant,1,1,1.0
considered less relevant for,1,1,1.0
less relevant for learning,1,1,1.0
relevant for learning wilson,1,1,1.0
for learning wilson s,1,1,1.0
learning wilson s edited,1,1,1.0
wilson s edited nearest,1,1,1.0
s edited nearest neighbor,1,1,1.0
edited nearest neighbor rule,1,1,1.0
nearest neighbor rule enn,1,1,1.0
neighbor rule enn removes,1,1,1.0
rule enn removes any,1,1,1.0
enn removes any example,1,1,1.0
removes any example whose,1,1,1.0
any example whose class,1,1,1.0
example whose class label,1,1,1.0
whose class label differs,1,1,1.0
class label differs from,1,1,1.0
label differs from the,1,1,1.0
differs from the class,1,1,1.0
from the class of,1,1,1.0
the class of at,1,1,1.0
class of at least,1,1,1.0
of at least two,1,1,1.0
at least two of,1,1,1.0
least two of its,1,1,1.0
two of its three,1,1,1.0
of its three nearest,1,1,1.0
its three nearest neighbors,4,1,4.0
three nearest neighbors different,1,1,1.0
nearest neighbors different from,1,1,1.0
neighbors different from enn,1,1,1.0
different from enn neighborhood,1,1,1.0
from enn neighborhood cleaning,1,1,1.0
enn neighborhood cleaning rule,1,1,1.0
neighborhood cleaning rule ncl,1,1,1.0
cleaning rule ncl deals,1,1,1.0
rule ncl deals with,1,1,1.0
ncl deals with majority,1,1,1.0
deals with majority and,1,1,1.0
with majority and minority,1,1,1.0
majority and minority samples,1,1,1.0
and minority samples separately,1,1,1.0
minority samples separately when,1,1,1.0
samples separately when cleaning,1,1,1.0
separately when cleaning the,1,1,1.0
when cleaning the data,1,1,1.0
cleaning the data sets,1,1,1.0
the data sets ncl,1,1,1.0
data sets ncl uses,1,1,1.0
sets ncl uses enn,1,1,1.0
ncl uses enn to,1,1,1.0
uses enn to remove,1,1,1.0
enn to remove majority,1,1,1.0
to remove majority examples,1,1,1.0
remove majority examples for,1,1,1.0
majority examples for each,1,1,1.0
examples for each example,1,1,1.0
for each example e,1,1,1.0
each example e i,1,1,1.0
example e i in,1,1,1.0
e i in the,1,1,1.0
i in the training,1,1,1.0
the training set its,1,1,1.0
training set its three,1,1,1.0
set its three nearest,1,1,1.0
three nearest neighbors are,1,1,1.0
nearest neighbors are found,1,1,1.0
neighbors are found if,1,1,1.0
are found if ei,1,1,1.0
found if ei belongs,1,1,1.0
if ei belongs to,1,1,1.0
ei belongs to the,1,1,1.0
class and the classification,1,1,1.0
and the classification given,1,1,1.0
the classification given by,1,1,1.0
classification given by its,1,1,1.0
given by its three,1,1,1.0
by its three nearest,1,1,1.0
three nearest neighbors contradicts,1,1,1.0
nearest neighbors contradicts the,1,1,1.0
neighbors contradicts the original,1,1,1.0
contradicts the original class,1,1,1.0
the original class of,1,1,1.0
original class of e,1,1,1.0
class of e i,1,1,1.0
of e i then,1,1,1.0
e i then ei,1,1,1.0
i then ei is,1,1,1.0
then ei is removed,1,1,1.0
ei is removed if,1,1,1.0
is removed if e,1,1,1.0
removed if e i,1,1,1.0
if e i belongs,1,1,1.0
e i belongs to,1,1,1.0
i belongs to the,1,1,1.0
minority class and its,1,1,1.0
class and its three,1,1,1.0
and its three nearest,1,1,1.0
three nearest neighbors misclassify,1,1,1.0
nearest neighbors misclassify e,1,1,1.0
neighbors misclassify e i,1,1,1.0
misclassify e i then,1,1,1.0
e i then the,1,1,1.0
i then the nearest,1,1,1.0
then the nearest neighbors,1,1,1.0
the nearest neighbors that,1,1,1.0
nearest neighbors that belong,1,1,1.0
majority class are removed,1,1,1.0
class are removed compared,1,1,1.0
are removed compared with,1,1,1.0
removed compared with above,1,1,1.0
compared with above four,1,1,1.0
with above four methods,1,1,1.0
above four methods tomek,1,1,1.0
four methods tomek links,1,1,1.0
methods tomek links consider,1,1,1.0
tomek links consider samples,1,1,1.0
links consider samples near,1,1,1.0
consider samples near the,1,1,1.0
samples near the borderline,1,1,1.0
near the borderline should,1,1,1.0
the borderline should be,1,1,1.0
borderline should be paid,1,1,1.0
should be paid more,1,1,1.0
be paid more attention,1,1,1.0
paid more attention given,1,1,1.0
more attention given two,1,1,1.0
attention given two examples,1,1,1.0
given two examples e,1,1,1.0
two examples e i,1,1,1.0
examples e i and,1,1,1.0
e i and e,1,1,1.0
i and e j,1,1,1.0
and e j belonging,1,1,1.0
e j belonging to,1,1,1.0
j belonging to different,1,1,1.0
belonging to different classes,1,1,1.0
to different classes and,1,1,1.0
different classes and d,1,1,1.0
classes and d e,1,1,1.0
and d e i,1,1,1.0
d e i e,1,1,1.0
e i e j,1,1,1.0
i e j is,1,1,1.0
e j is the,1,1,1.0
j is the distance,1,1,1.0
is the distance between,1,1,1.0
the distance between ei,1,1,1.0
distance between ei and,1,1,1.0
between ei and ej,1,1,1.0
ei and ej a,1,1,1.0
and ej a ei,1,1,1.0
ej a ei ej,1,1,1.0
a ei ej pair,1,1,1.0
ei ej pair is,1,1,1.0
ej pair is called,1,1,1.0
pair is called a,1,1,1.0
is called a tomek,1,1,1.0
called a tomek link,1,1,1.0
a tomek link if,1,1,1.0
tomek link if there,1,1,1.0
link if there is,1,1,1.0
if there is not,1,1,1.0
there is not an,1,1,1.0
is not an example,1,1,1.0
not an example such,1,1,1.0
an example such that,1,1,1.0
example such that d,1,1,1.0
such that d ei,1,1,1.0
that d ei d,1,1,1.0
d ei d ei,1,1,1.0
ei d ei ej,1,1,1.0
d ei ej or,1,1,1.0
ei ej or d,1,1,1.0
ej or d ej,1,1,1.0
or d ej d,1,1,1.0
d ej d ei,1,1,1.0
ej d ei ej,1,1,1.0
d ei ej if,1,1,1.0
ei ej if two,1,1,1.0
ej if two examples,1,1,1.0
if two examples form,1,1,1.0
two examples form a,1,1,1.0
examples form a tomek,1,1,1.0
form a tomek link,1,1,1.0
a tomek link then,1,1,1.0
tomek link then either,1,1,1.0
link then either one,1,1,1.0
then either one of,1,1,1.0
either one of these,1,1,1.0
one of these examples,1,1,1.0
of these examples is,1,1,1.0
these examples is noise,1,1,1.0
examples is noise or,1,1,1.0
is noise or both,1,1,1.0
noise or both examples,1,1,1.0
or both examples form,1,1,1.0
both examples form borderline,1,1,1.0
examples form borderline so,1,1,1.0
form borderline so tomek,1,1,1.0
borderline so tomek link,1,1,1.0
so tomek link can,1,1,1.0
tomek link can be,1,1,1.0
link can be viewed,1,1,1.0
can be viewed as,2,1,2.0
be viewed as an,1,1,1.0
viewed as an method,1,1,1.0
as an method when,1,1,1.0
an method when examples,1,1,1.0
method when examples of,1,1,1.0
when examples of both,1,1,1.0
examples of both classes,1,1,1.0
of both classes are,1,1,1.0
both classes are removed,1,1,1.0
classes are removed it,1,1,1.0
are removed it should,1,1,1.0
removed it should be,1,1,1.0
be noted that tomek,1,1,1.0
noted that tomek link,1,1,1.0
that tomek link enn,1,1,1.0
tomek link enn and,1,1,1.0
link enn and ncl,1,1,1.0
enn and ncl are,1,1,1.0
and ncl are highly,1,1,1.0
ncl are highly since,1,1,1.0
are highly since for,1,1,1.0
highly since for any,1,1,1.0
since for any example,1,1,1.0
for any example in,1,1,1.0
any example in the,1,1,1.0
example in the data,1,1,1.0
in the data sets,1,1,1.0
the data sets nearest,1,1,1.0
data sets nearest neighbors,1,1,1.0
sets nearest neighbors of,1,1,1.0
nearest neighbors of the,3,2,1.5
neighbors of the sample,1,1,1.0
of the sample must,1,1,1.0
the sample must be,1,1,1.0
sample must be found,1,1,1.0
must be found so,1,1,1.0
be found so it,1,1,1.0
found so it is,1,1,1.0
so it is impossible,1,1,1.0
it is impossible for,1,1,1.0
is impossible for large,1,1,1.0
impossible for large datasets,1,1,1.0
for large datasets random,1,1,1.0
large datasets random is,1,1,1.0
datasets random is a,1,1,1.0
random is a method,1,1,1.0
is a method that,1,1,1.0
a method that aims,1,1,1.0
method that aims to,1,1,1.0
that aims to balance,1,1,1.0
aims to balance class,1,1,1.0
through the random replication,1,1,1.0
the random replication of,1,1,1.0
random replication of minority,2,2,1.0
replication of minority class,1,1,1.0
of minority class examples,2,2,1.0
minority class examples random,1,1,1.0
class examples random has,1,1,1.0
examples random has two,1,1,1.0
random has two shortcomings,1,1,1.0
has two shortcomings first,1,1,1.0
two shortcomings first it,1,1,1.0
shortcomings first it will,1,1,1.0
first it will increase,1,1,1.0
it will increase the,1,1,1.0
will increase the likelihood,1,1,1.0
increase the likelihood of,1,1,1.0
the likelihood of occurring,1,1,1.0
likelihood of occurring since,1,1,1.0
of occurring since it,1,1,1.0
occurring since it makes,1,1,1.0
since it makes exact,1,1,1.0
it makes exact copies,1,1,1.0
makes exact copies of,1,1,1.0
exact copies of the,1,1,1.0
copies of the minority,1,1,1.0
the minority class examples,1,1,1.0
minority class examples second,1,1,1.0
class examples second sampling,1,1,1.0
examples second sampling makes,1,1,1.0
second sampling makes learning,1,1,1.0
sampling makes learning process,1,1,1.0
makes learning process more,1,1,1.0
learning process more consuming,1,1,1.0
process more consuming if,1,1,1.0
more consuming if the,1,1,1.0
consuming if the original,1,1,1.0
if the original data,1,1,1.0
the original data set,1,1,1.0
original data set is,1,1,1.0
data set is already,1,1,1.0
set is already fairly,1,1,1.0
is already fairly large,1,1,1.0
already fairly large but,1,1,1.0
fairly large but imbalanced,1,1,1.0
large but imbalanced there,1,1,1.0
but imbalanced there are,1,1,1.0
imbalanced there are several,1,1,1.0
there are several heuristic,1,1,1.0
are several heuristic methods,1,1,1.0
several heuristic methods mainly,1,1,1.0
heuristic methods mainly based,1,1,1.0
methods mainly based on,1,1,1.0
mainly based on smote,1,1,1.0
based on smote smote,1,1,1.0
on smote smote generates,1,1,1.0
smote smote generates synthetic,1,1,1.0
smote generates synthetic minority,1,1,1.0
generates synthetic minority examples,1,1,1.0
synthetic minority examples to,1,1,1.0
minority examples to the,1,1,1.0
examples to the minority,1,1,1.0
minority class its main,1,1,1.0
class its main idea,1,1,1.0
its main idea is,1,1,1.0
main idea is to,1,1,1.0
idea is to form,1,1,1.0
is to form new,1,1,1.0
to form new minority,1,1,1.0
form new minority class,1,1,1.0
new minority class examples,1,1,1.0
minority class examples by,1,1,1.0
class examples by interpolating,1,1,1.0
examples by interpolating between,1,1,1.0
by interpolating between several,1,1,1.0
interpolating between several minority,1,1,1.0
several minority class examples,1,1,1.0
minority class examples that,2,2,1.0
class examples that lie,1,1,1.0
examples that lie together,1,1,1.0
that lie together by,1,1,1.0
lie together by interpolating,1,1,1.0
together by interpolating instead,1,1,1.0
interpolating instead of replication,1,1,1.0
instead of replication smote,1,1,1.0
of replication smote avoids,1,1,1.0
replication smote avoids the,1,1,1.0
the problem and causes,1,1,1.0
problem and causes the,1,1,1.0
and causes the decision,1,1,1.0
causes the decision boundaries,1,1,1.0
the decision boundaries for,1,1,1.0
decision boundaries for the,1,1,1.0
boundaries for the minority,1,1,1.0
minority class to spread,1,1,1.0
class to spread further,1,1,1.0
to spread further into,1,1,1.0
spread further into the,1,1,1.0
further into the majority,1,1,1.0
into the majority class,1,1,1.0
the majority class space,1,1,1.0
majority class space recognizing,1,1,1.0
class space recognizing examples,1,1,1.0
space recognizing examples near,1,1,1.0
recognizing examples near the,1,1,1.0
examples near the borderline,1,1,1.0
near the borderline of,1,1,1.0
the borderline of the,1,1,1.0
borderline of the classes,1,1,1.0
of the classes are,1,1,1.0
the classes are more,1,1,1.0
classes are more important,1,1,1.0
are more important and,1,1,1.0
more important and more,1,1,1.0
important and more easily,1,1,1.0
and more easily misclassified,1,1,1.0
more easily misclassified than,1,1,1.0
easily misclassified than those,1,1,1.0
misclassified than those far,1,1,1.0
than those far from,1,1,1.0
those far from the,1,1,1.0
far from the borderline,1,1,1.0
from the borderline was,1,1,1.0
the borderline was proposed,1,1,1.0
borderline was proposed it,1,1,1.0
was proposed it only,1,1,1.0
proposed it only sample,1,1,1.0
it only sample the,1,1,1.0
only sample the borderline,1,1,1.0
sample the borderline examples,1,1,1.0
the borderline examples of,1,1,1.0
borderline examples of the,1,1,1.0
examples of the minority,3,2,1.5
the minority class while,1,1,1.0
minority class while smote,1,1,1.0
class while smote and,1,1,1.0
while smote and random,1,1,1.0
smote and random augment,1,1,1.0
and random augment the,1,1,1.0
random augment the minority,1,1,1.0
augment the minority class,1,1,1.0
the minority class through,2,2,1.0
minority class through all,1,1,1.0
class through all the,1,1,1.0
through all the examples,1,1,1.0
all the examples from,1,1,1.0
minority class or a,1,1,1.0
class or a random,1,1,1.0
or a random subset,1,1,1.0
a random subset of,1,1,1.0
random subset of the,1,1,1.0
subset of the minority,1,1,1.0
minority class for the,1,1,1.0
class for the minority,1,1,1.0
the minority class experiments,1,1,1.0
minority class experiments show,1,1,1.0
class experiments show that,1,1,1.0
experiments show that their,1,1,1.0
show that their approaches,1,1,1.0
that their approaches achieve,1,1,1.0
their approaches achieve better,1,1,1.0
approaches achieve better tp,1,1,1.0
achieve better tp rate,1,1,1.0
better tp rate and,1,1,1.0
tp rate and than,1,1,1.0
rate and than smote,1,1,1.0
and than smote and,1,1,1.0
than smote and random,1,1,1.0
smote and random methods,1,1,1.0
and random methods advanced,1,1,1.0
random methods advanced sampling,1,1,1.0
methods advanced sampling different,1,1,1.0
advanced sampling different from,1,1,1.0
sampling different from various,1,1,1.0
different from various and,1,1,1.0
from various and methods,1,1,1.0
various and methods above,1,1,1.0
and methods above the,1,1,1.0
methods above the following,1,1,1.0
above the following advanced,1,1,1.0
the following advanced sampling,1,1,1.0
following advanced sampling methods,1,1,1.0
advanced sampling methods do,1,1,1.0
sampling methods do based,1,1,1.0
methods do based on,1,1,1.0
do based on the,1,1,1.0
based on the results,3,2,1.5
on the results of,1,1,1.0
the results of preliminary,1,1,1.0
results of preliminary classifications,1,1,1.0
of preliminary classifications boosting,1,1,1.0
preliminary classifications boosting is,1,1,1.0
classifications boosting is an,1,1,1.0
boosting is an iterative,1,1,1.0
is an iterative algorithm,1,1,1.0
an iterative algorithm that,1,1,1.0
iterative algorithm that place,1,1,1.0
algorithm that place different,1,1,1.0
that place different weights,1,1,1.0
place different weights on,1,1,1.0
different weights on the,1,1,1.0
weights on the training,1,1,1.0
on the training distributions,1,1,1.0
the training distributions each,1,1,1.0
training distributions each iteration,1,1,1.0
distributions each iteration after,1,1,1.0
each iteration after each,1,1,1.0
iteration after each iteration,1,1,1.0
after each iteration boosting,1,1,1.0
each iteration boosting increases,1,1,1.0
iteration boosting increases the,1,1,1.0
boosting increases the weights,1,1,1.0
increases the weights associated,1,1,1.0
the weights associated with,3,1,3.0
weights associated with the,2,1,2.0
associated with the incorrectly,1,1,1.0
with the incorrectly classified,1,1,1.0
the incorrectly classified examples,2,1,2.0
incorrectly classified examples and,1,1,1.0
classified examples and decreases,1,1,1.0
examples and decreases the,1,1,1.0
and decreases the weights,1,1,1.0
decreases the weights associated,1,1,1.0
associated with the correctly,1,1,1.0
with the correctly classified,1,1,1.0
the correctly classified examples,1,1,1.0
correctly classified examples separately,1,1,1.0
classified examples separately this,1,1,1.0
examples separately this forces,1,1,1.0
separately this forces the,1,1,1.0
this forces the learner,1,1,1.0
forces the learner to,1,1,1.0
the learner to focus,1,1,1.0
learner to focus more,1,1,1.0
to focus more on,2,2,1.0
focus more on the,1,1,1.0
more on the incorrectly,1,1,1.0
on the incorrectly classified,1,1,1.0
incorrectly classified examples in,1,1,1.0
classified examples in the,1,1,1.0
examples in the next,1,1,1.0
in the next iteration,2,2,1.0
the next iteration note,1,1,1.0
next iteration note that,1,1,1.0
iteration note that boosting,1,1,1.0
note that boosting effectively,1,1,1.0
that boosting effectively alters,1,1,1.0
boosting effectively alters the,1,1,1.0
effectively alters the distributions,1,1,1.0
alters the distributions of,1,1,1.0
the distributions of the,1,1,1.0
distributions of the training,1,1,1.0
of the training data,4,2,2.0
the training data can,1,1,1.0
training data can be,1,1,1.0
data can be considered,1,1,1.0
can be considered to,2,2,1.0
be considered to be,2,2,1.0
considered to be a,1,1,1.0
to be a type,1,1,1.0
be a type of,1,1,1.0
a type of advanced,1,1,1.0
type of advanced sampling,1,1,1.0
of advanced sampling technique,1,1,1.0
advanced sampling technique weiss,1,1,1.0
sampling technique weiss proposed,1,1,1.0
technique weiss proposed a,1,1,1.0
weiss proposed a heuristic,1,1,1.0
proposed a heuristic algorithm,1,1,1.0
a heuristic algorithm for,1,1,1.0
heuristic algorithm for selecting,1,1,1.0
algorithm for selecting training,1,1,1.0
for selecting training data,1,1,1.0
selecting training data that,1,1,1.0
training data that approximates,1,1,1.0
data that approximates optimum,1,1,1.0
that approximates optimum the,1,1,1.0
approximates optimum the sensitive,1,1,1.0
optimum the sensitive sampling,1,1,1.0
the sensitive sampling strategy,1,1,1.0
sensitive sampling strategy makes,1,1,1.0
sampling strategy makes two,1,1,1.0
strategy makes two additional,1,1,1.0
makes two additional assumptions,1,1,1.0
two additional assumptions first,1,1,1.0
additional assumptions first it,1,1,1.0
assumptions first it assumes,1,1,1.0
first it assumes that,1,1,1.0
it assumes that the,1,1,1.0
assumes that the number,1,1,1.0
the number of potentially,1,1,1.0
number of potentially available,1,1,1.0
of potentially available training,1,1,1.0
potentially available training examples,1,1,1.0
available training examples from,1,1,1.0
training examples from each,1,1,1.0
examples from each class,1,1,1.0
from each class is,1,1,1.0
each class is sufficiently,1,1,1.0
class is sufficiently large,1,1,1.0
is sufficiently large so,1,1,1.0
sufficiently large so that,1,1,1.0
large so that a,1,1,1.0
so that a training,1,1,1.0
that a training set,1,1,1.0
a training set with,1,1,1.0
training set with n,1,1,1.0
set with n examples,1,1,1.0
with n examples can,1,1,1.0
n examples can be,1,1,1.0
examples can be formed,1,1,1.0
can be formed with,1,1,1.0
be formed with any,1,1,1.0
formed with any desired,1,1,1.0
with any desired marginal,1,1,1.0
any desired marginal class,1,1,1.0
desired marginal class distributions,1,1,1.0
marginal class distributions the,1,1,1.0
class distributions the second,1,1,1.0
distributions the second assumption,1,1,1.0
the second assumption is,1,1,1.0
second assumption is that,1,1,1.0
assumption is that the,1,1,1.0
is that the cost,1,1,1.0
that the cost of,1,1,1.0
the cost of executing,1,1,1.0
cost of executing the,1,1,1.0
of executing the learning,1,1,1.0
executing the learning algorithm,1,1,1.0
the learning algorithm is,2,1,2.0
learning algorithm is negligible,1,1,1.0
algorithm is negligible compared,1,1,1.0
is negligible compared to,1,1,1.0
negligible compared to the,1,1,1.0
compared to the cost,1,1,1.0
to the cost of,1,1,1.0
the cost of procuring,1,1,1.0
cost of procuring examples,1,1,1.0
of procuring examples this,1,1,1.0
procuring examples this assumption,1,1,1.0
examples this assumption permits,1,1,1.0
this assumption permits the,1,1,1.0
assumption permits the learning,1,1,1.0
permits the learning algorithm,1,1,1.0
the learning algorithm to,2,2,1.0
learning algorithm to be,2,2,1.0
algorithm to be run,1,1,1.0
to be run multiple,1,1,1.0
be run multiple times,1,1,1.0
run multiple times in,1,1,1.0
multiple times in order,1,1,1.0
times in order to,1,1,1.0
order to provide guidance,1,1,1.0
to provide guidance about,1,1,1.0
provide guidance about which,1,1,1.0
guidance about which examples,1,1,1.0
about which examples to,1,1,1.0
which examples to select,1,1,1.0
examples to select he,1,1,1.0
to select he argued,1,1,1.0
select he argued that,1,1,1.0
he argued that though,1,1,1.0
argued that though the,1,1,1.0
that though the heuristically,1,1,1.0
though the heuristically determined,1,1,1.0
the heuristically determined class,1,1,1.0
heuristically determined class distributions,1,1,1.0
determined class distributions associated,1,1,1.0
class distributions associated with,1,1,1.0
distributions associated with the,1,1,1.0
associated with the final,1,1,1.0
with the final training,1,1,1.0
the final training set,1,1,1.0
final training set is,1,1,1.0
training set is not,1,1,1.0
set is not guaranteed,1,1,1.0
is not guaranteed to,1,1,1.0
not guaranteed to yield,1,1,1.0
guaranteed to yield the,1,1,1.0
to yield the classifier,1,1,1.0
yield the classifier the,1,1,1.0
the classifier the classifier,1,1,1.0
classifier the classifier induced,1,1,1.0
the classifier induced using,1,1,1.0
classifier induced using this,1,1,1.0
induced using this class,1,1,1.0
using this class distributions,1,1,1.0
this class distributions performs,1,1,1.0
class distributions performs well,1,1,1.0
distributions performs well in,1,1,1.0
performs well in practice,1,1,1.0
well in practice han,1,1,1.0
in practice han et,1,1,1.0
practice han et al,1,1,1.0
han et al proposed,1,1,1.0
et al proposed an,1,1,1.0
al proposed an algorithm,1,1,1.0
proposed an algorithm based,1,1,1.0
an algorithm based on,1,1,1.0
algorithm based on preliminary,2,1,2.0
based on preliminary classification,2,1,2.0
on preliminary classification ospc,1,1,1.0
preliminary classification ospc firstly,1,1,1.0
classification ospc firstly preliminary,1,1,1.0
ospc firstly preliminary classification,1,1,1.0
firstly preliminary classification was,1,1,1.0
preliminary classification was made,1,1,1.0
classification was made on,1,1,1.0
was made on the,1,1,1.0
made on the test,1,1,1.0
on the test data,1,1,1.0
the test data in,1,1,1.0
test data in order,1,1,1.0
data in order to,1,1,1.0
in order to save,1,1,1.0
order to save the,1,1,1.0
to save the useful,1,1,1.0
save the useful information,1,1,1.0
the useful information of,1,1,1.0
useful information of the,1,1,1.0
information of the majority,1,1,1.0
majority class as much,1,1,1.0
class as much as,1,1,1.0
as much as possible,1,1,1.0
much as possible then,1,1,1.0
as possible then the,1,1,1.0
possible then the test,1,1,1.0
then the test data,1,1,1.0
test data that were,1,1,1.0
data that were predicted,1,1,1.0
that were predicted to,1,1,1.0
were predicted to belong,1,1,1.0
predicted to belong to,1,1,1.0
to belong to minority,1,1,1.0
belong to minority class,1,1,1.0
to minority class were,1,1,1.0
minority class were reclassified,1,1,1.0
class were reclassified to,1,1,1.0
were reclassified to improve,1,1,1.0
reclassified to improve the,1,1,1.0
to improve the classification,1,1,1.0
improve the classification performance,1,1,1.0
the classification performance of,2,1,2.0
classification performance of the,2,1,2.0
performance of the minority,2,1,2.0
the minority class ospc,1,1,1.0
minority class ospc was,1,1,1.0
class ospc was argued,1,1,1.0
ospc was argued to,1,1,1.0
was argued to perform,1,1,1.0
argued to perform better,1,1,1.0
to perform better than,1,1,1.0
perform better than methods,1,1,1.0
better than methods and,1,1,1.0
than methods and smote,1,1,1.0
methods and smote in,1,1,1.0
and smote in terms,1,1,1.0
smote in terms of,1,1,1.0
in terms of the,2,2,1.0
terms of the classification,1,1,1.0
of the classification performance,1,1,1.0
minority class and majority,2,2,1.0
class and majority class,3,2,1.5
and majority class it,1,1,1.0
majority class it should,1,1,1.0
class it should be,1,1,1.0
noted that all the,1,1,1.0
that all the methods,1,1,1.0
all the methods of,1,1,1.0
the methods of changing,1,1,1.0
methods of changing class,1,1,1.0
of changing class distributions,1,1,1.0
changing class distributions above,1,1,1.0
class distributions above are,1,1,1.0
distributions above are trying,1,1,1.0
above are trying to,1,1,1.0
are trying to deal,1,1,1.0
trying to deal with,1,1,1.0
the problem of imbalance,1,1,1.0
problem of imbalance a,1,1,1.0
of imbalance a proposed,1,1,1.0
imbalance a proposed to,1,1,1.0
a proposed to improve,1,1,1.0
proposed to improve accuracy,1,1,1.0
to improve accuracy of,1,1,1.0
improve accuracy of minority,1,1,1.0
accuracy of minority class,1,1,1.0
of minority class by,1,1,1.0
minority class by dealing,1,1,1.0
class by dealing with,1,1,1.0
by dealing with the,1,1,1.0
dealing with the problems,2,1,2.0
with the problems of,2,1,2.0
the problems of between,1,1,1.0
problems of between and,1,1,1.0
of between and within,1,1,1.0
between and within class,1,1,1.0
and within class imbalance,1,1,1.0
within class imbalance simultaneously,1,1,1.0
class imbalance simultaneously when,1,1,1.0
imbalance simultaneously when the,1,1,1.0
simultaneously when the data,1,1,1.0
data sets are severely,1,1,1.0
sets are severely skewed,1,1,1.0
are severely skewed sampling,1,1,1.0
severely skewed sampling and,1,1,1.0
skewed sampling and methods,1,1,1.0
sampling and methods are,1,1,1.0
and methods are often,1,1,1.0
methods are often combined,1,1,1.0
are often combined to,1,1,1.0
often combined to improve,1,1,1.0
combined to improve generalization,1,1,1.0
to improve generalization of,1,1,1.0
improve generalization of the,1,1,1.0
generalization of the learner,1,1,1.0
of the learner batista,1,1,1.0
the learner batista et,1,1,1.0
learner batista et al,1,1,1.0
batista et al presented,1,1,1.0
et al presented a,1,1,1.0
al presented a comparison,1,1,1.0
presented a comparison and,1,1,1.0
a comparison and combination,1,1,1.0
comparison and combination of,1,1,1.0
and combination of various,1,1,1.0
combination of various sampling,1,1,1.0
of various sampling strategies,1,1,1.0
various sampling strategies they,1,1,1.0
sampling strategies they noted,1,1,1.0
strategies they noted that,1,1,1.0
they noted that combining,1,1,1.0
noted that combining focused,1,1,1.0
that combining focused and,1,1,1.0
combining focused and such,1,1,1.0
focused and such as,1,1,1.0
and such as smote,1,1,1.0
such as smote combining,1,1,1.0
as smote combining with,1,1,1.0
smote combining with tomek,1,1,1.0
combining with tomek link,1,1,1.0
with tomek link or,1,1,1.0
tomek link or smote,1,1,1.0
link or smote combining,1,1,1.0
or smote combining with,1,1,1.0
smote combining with enn,1,1,1.0
combining with enn is,1,1,1.0
with enn is applicable,1,1,1.0
enn is applicable when,1,1,1.0
is applicable when the,1,1,1.0
applicable when the data,1,1,1.0
are highly imbalanced or,1,1,1.0
highly imbalanced or there,1,1,1.0
imbalanced or there are,1,1,1.0
or there are very,1,1,1.0
there are very few,1,1,1.0
are very few examples,1,1,1.0
very few examples of,1,1,1.0
few examples of the,1,1,1.0
the minority class feature,1,1,1.0
minority class feature selection,1,1,1.0
class feature selection the,1,1,1.0
feature selection the majority,1,1,1.0
selection the majority of,1,1,1.0
the majority of work,1,1,1.0
majority of work in,1,1,1.0
of work in feature,1,1,1.0
work in feature selection,1,1,1.0
in feature selection for,1,1,1.0
feature selection for imbalanced,1,1,1.0
selection for imbalanced data,1,1,1.0
imbalanced data sets has,1,1,1.0
data sets has focused,1,1,1.0
sets has focused on,1,1,1.0
has focused on text,1,1,1.0
focused on text classification,1,1,1.0
on text classification or,1,1,1.0
text classification or web,2,1,2.0
classification or web categorization,2,1,2.0
or web categorization domain,1,1,1.0
web categorization domain a,1,1,1.0
categorization domain a couple,1,1,1.0
domain a couple of,1,1,1.0
a couple of papers,1,1,1.0
couple of papers in,1,1,1.0
of papers in this,1,1,1.0
papers in this issue,1,1,1.0
in this issue look,1,1,1.0
this issue look at,1,1,1.0
issue look at feature,1,1,1.0
look at feature selection,1,1,1.0
at feature selection in,1,1,1.0
feature selection in situations,1,1,1.0
selection in situations of,1,1,1.0
in situations of imbalanced,1,1,1.0
situations of imbalanced data,1,1,1.0
of imbalanced data sets,1,1,1.0
imbalanced data sets albeit,1,1,1.0
data sets albeit in,1,1,1.0
sets albeit in text,1,1,1.0
albeit in text classification,1,1,1.0
in text classification or,1,1,1.0
or web categorization zheng,1,1,1.0
web categorization zheng et,1,1,1.0
categorization zheng et al,1,1,1.0
zheng et al suggested,1,1,1.0
al suggested that existing,1,1,1.0
suggested that existing measures,1,1,1.0
that existing measures used,1,1,1.0
existing measures used for,1,1,1.0
measures used for feature,1,1,1.0
used for feature selection,1,1,1.0
for feature selection are,1,1,1.0
feature selection are not,1,1,1.0
selection are not very,1,1,1.0
are not very appropriate,1,1,1.0
not very appropriate for,1,1,1.0
very appropriate for imbalanced,1,1,1.0
appropriate for imbalanced data,1,1,1.0
imbalanced data sets they,2,1,2.0
data sets they proposed,1,1,1.0
sets they proposed a,1,1,1.0
they proposed a feature,1,1,1.0
proposed a feature selection,1,1,1.0
a feature selection framework,1,1,1.0
feature selection framework which,1,1,1.0
selection framework which selects,1,1,1.0
framework which selects features,1,1,1.0
which selects features for,1,1,1.0
selects features for positive,1,1,1.0
features for positive and,1,1,1.0
for positive and negative,1,1,1.0
positive and negative classes,1,1,1.0
and negative classes separately,1,1,1.0
negative classes separately and,1,1,1.0
classes separately and then,1,1,1.0
separately and then explicitly,1,1,1.0
and then explicitly combines,1,1,1.0
then explicitly combines them,1,1,1.0
explicitly combines them the,1,1,1.0
combines them the authors,1,1,1.0
them the authors showed,1,1,1.0
the authors showed simple,1,1,1.0
authors showed simple ways,1,1,1.0
showed simple ways of,1,1,1.0
simple ways of converting,1,1,1.0
ways of converting existing,1,1,1.0
of converting existing measures,1,1,1.0
converting existing measures so,1,1,1.0
existing measures so that,1,1,1.0
measures so that they,1,1,1.0
so that they separately,1,1,1.0
that they separately consider,1,1,1.0
they separately consider features,1,1,1.0
separately consider features for,1,1,1.0
consider features for negative,1,1,1.0
features for negative and,1,1,1.0
and positive classes castillo,1,1,1.0
positive classes castillo and,1,1,1.0
classes castillo and serrano,1,1,1.0
castillo and serrano did,1,1,1.0
and serrano did not,1,1,1.0
serrano did not particularly,1,1,1.0
did not particularly focus,1,1,1.0
not particularly focus on,2,1,2.0
particularly focus on feature,2,1,2.0
focus on feature selection,2,1,2.0
on feature selection but,2,1,2.0
feature selection but made,1,1,1.0
selection but made it,1,1,1.0
but made it a,1,1,1.0
made it a part,1,1,1.0
it a part of,2,1,2.0
a part of their,2,1,2.0
part of their complete,2,1,2.0
of their complete framework,2,1,2.0
their complete framework putten,1,1,1.0
complete framework putten and,1,1,1.0
framework putten and someren,1,1,1.0
putten and someren analyzed,1,1,1.0
and someren analyzed the,1,1,1.0
someren analyzed the coil,1,1,1.0
analyzed the coil data,1,1,1.0
the coil data sets,1,1,1.0
coil data sets using,1,1,1.0
data sets using the,1,1,1.0
sets using the decomposition,1,1,1.0
using the decomposition and,1,1,1.0
the decomposition and they,1,1,1.0
decomposition and they reported,1,1,1.0
and they reported that,1,1,1.0
they reported that the,1,1,1.0
reported that the key,1,1,1.0
that the key issue,1,1,1.0
the key issue for,1,1,1.0
key issue for this,1,1,1.0
issue for this particular,1,1,1.0
for this particular data,1,1,1.0
this particular data set,1,1,1.0
particular data set was,1,1,1.0
data set was avoiding,1,1,1.0
set was avoiding they,1,1,1.0
was avoiding they concluded,1,1,1.0
avoiding they concluded that,1,1,1.0
they concluded that feature,1,1,1.0
concluded that feature selection,1,1,1.0
that feature selection in,1,1,1.0
feature selection in such,1,1,1.0
selection in such domains,1,1,1.0
in such domains is,1,1,1.0
such domains is even,1,1,1.0
domains is even more,1,1,1.0
is even more important,1,1,1.0
even more important than,1,1,1.0
more important than the,2,2,1.0
important than the choice,1,1,1.0
than the choice of,1,1,1.0
the choice of the,1,1,1.0
choice of the learning,1,1,1.0
of the learning method,1,1,1.0
the learning method classifiers,1,1,1.0
learning method classifiers level,1,1,1.0
method classifiers level manipulating,1,1,1.0
classifiers level manipulating classifiers,1,1,1.0
level manipulating classifiers internally,1,1,1.0
manipulating classifiers internally drummond,1,1,1.0
classifiers internally drummond and,1,1,1.0
internally drummond and holte,1,1,1.0
drummond and holte reported,1,1,1.0
and holte reported that,1,1,1.0
holte reported that when,1,1,1.0
reported that when using,1,1,1.0
that when using s,1,1,1.0
when using s default,1,1,1.0
using s default settings,1,1,1.0
s default settings is,1,1,1.0
default settings is surprisingly,1,1,1.0
settings is surprisingly ineffective,1,1,1.0
is surprisingly ineffective often,1,1,1.0
surprisingly ineffective often producing,1,1,1.0
ineffective often producing little,1,1,1.0
often producing little or,1,1,1.0
producing little or no,1,1,1.0
little or no change,1,1,1.0
or no change in,1,1,1.0
no change in performance,1,1,1.0
change in performance in,1,1,1.0
in performance in response,1,1,1.0
performance in response to,1,1,1.0
in response to modifications,1,1,1.0
response to modifications of,1,1,1.0
to modifications of misclassification,1,1,1.0
modifications of misclassification costs,1,1,1.0
of misclassification costs and,1,1,1.0
misclassification costs and class,1,1,1.0
costs and class distributions,1,1,1.0
and class distributions moreover,1,1,1.0
class distributions moreover they,1,1,1.0
distributions moreover they noted,1,1,1.0
moreover they noted that,1,1,1.0
they noted that prunes,1,1,1.0
noted that prunes less,1,1,1.0
that prunes less and,1,1,1.0
prunes less and therefore,1,1,1.0
less and therefore generalizes,1,1,1.0
and therefore generalizes less,1,1,1.0
therefore generalizes less than,1,1,1.0
generalizes less than and,1,1,1.0
less than and that,1,1,1.0
than and that a,1,1,1.0
and that a modification,1,1,1.0
that a modification of,1,1,1.0
a modification of the,1,1,1.0
modification of the s,1,1,1.0
of the s parameter,1,1,1.0
the s parameter settings,1,1,1.0
s parameter settings to,1,1,1.0
parameter settings to increase,1,1,1.0
settings to increase the,1,1,1.0
to increase the influence,1,1,1.0
increase the influence of,1,1,1.0
the influence of pruning,1,1,1.0
influence of pruning and,1,1,1.0
of pruning and other,1,1,1.0
pruning and other avoidance,1,1,1.0
and other avoidance factors,1,1,1.0
other avoidance factors can,1,1,1.0
avoidance factors can reestablish,1,1,1.0
factors can reestablish the,1,1,1.0
can reestablish the performance,1,1,1.0
reestablish the performance of,1,1,1.0
the performance of some,1,1,1.0
performance of some classifiers,1,1,1.0
of some classifiers such,1,1,1.0
some classifiers such as,1,1,1.0
classifiers such as the,1,1,1.0
such as the naive,1,1,1.0
as the naive bayes,1,1,1.0
the naive bayes classifier,1,1,1.0
naive bayes classifier or,1,1,1.0
bayes classifier or some,1,1,1.0
classifier or some neural,1,1,1.0
or some neural networks,1,1,1.0
some neural networks yield,1,1,1.0
neural networks yield a,1,1,1.0
networks yield a score,1,1,1.0
yield a score that,1,1,1.0
a score that represents,1,1,1.0
score that represents the,1,1,1.0
that represents the degree,1,1,1.0
represents the degree to,1,1,1.0
the degree to which,1,1,1.0
degree to which an,1,1,1.0
to which an example,1,1,1.0
which an example is,1,1,1.0
example is a member,1,1,1.0
is a member of,1,1,1.0
a member of a,1,1,1.0
member of a class,1,1,1.0
of a class such,1,1,1.0
a class such ranking,1,1,1.0
class such ranking can,1,1,1.0
such ranking can be,1,1,1.0
ranking can be used,1,1,1.0
can be used to,3,2,1.5
be used to produce,1,1,1.0
used to produce several,1,1,1.0
to produce several classifiers,1,1,1.0
produce several classifiers by,1,1,1.0
several classifiers by varying,1,1,1.0
classifiers by varying the,1,1,1.0
by varying the threshold,1,1,1.0
varying the threshold of,1,1,1.0
the threshold of an,1,1,1.0
threshold of an example,1,1,1.0
of an example pertaining,1,1,1.0
an example pertaining to,1,1,1.0
example pertaining to a,1,1,1.0
pertaining to a class,1,1,1.0
to a class for,1,1,1.0
a class for internally,1,1,1.0
class for internally biasing,1,1,1.0
for internally biasing the,1,1,1.0
internally biasing the discrimination,1,1,1.0
biasing the discrimination procedure,1,1,1.0
the discrimination procedure a,1,1,1.0
discrimination procedure a weighted,1,1,1.0
procedure a weighted distance,1,1,1.0
a weighted distance function,1,1,1.0
weighted distance function was,1,1,1.0
distance function was proposed,1,1,1.0
function was proposed in,1,1,1.0
was proposed in to,1,1,1.0
proposed in to be,1,1,1.0
in to be used,1,1,1.0
to be used in,2,2,1.0
be used in the,2,2,1.0
used in the classification,1,1,1.0
in the classification phase,1,1,1.0
the classification phase of,1,1,1.0
classification phase of knn,1,1,1.0
phase of knn the,1,1,1.0
of knn the basic,1,1,1.0
knn the basic idea,1,1,1.0
the basic idea behind,2,2,1.0
basic idea behind this,2,2,1.0
idea behind this weighted,1,1,1.0
behind this weighted distance,1,1,1.0
this weighted distance is,1,1,1.0
weighted distance is to,1,1,1.0
distance is to compensate,1,1,1.0
is to compensate for,1,1,1.0
to compensate for the,2,1,2.0
compensate for the imbalance,1,1,1.0
for the imbalance in,1,1,1.0
the imbalance in the,1,1,1.0
imbalance in the training,1,1,1.0
in the training sample,1,1,1.0
the training sample without,1,1,1.0
training sample without actually,1,1,1.0
sample without actually altering,1,1,1.0
without actually altering the,1,1,1.0
actually altering the class,1,1,1.0
altering the class distributions,1,1,1.0
the class distributions thus,1,1,1.0
class distributions thus weights,1,1,1.0
distributions thus weights are,1,1,1.0
thus weights are assigned,1,1,1.0
weights are assigned unlike,1,1,1.0
are assigned unlike in,1,1,1.0
assigned unlike in the,1,1,1.0
unlike in the usual,1,1,1.0
in the usual weighted,1,1,1.0
the usual weighted rule,1,1,1.0
usual weighted rule to,1,1,1.0
weighted rule to the,1,1,1.0
rule to the respective,1,1,1.0
the respective classes and,1,1,1.0
respective classes and not,1,1,1.0
classes and not to,1,1,1.0
and not to the,1,1,1.0
not to the individual,1,1,1.0
to the individual prototypes,1,1,1.0
the individual prototypes in,1,1,1.0
individual prototypes in this,1,1,1.0
prototypes in this way,1,1,1.0
in this way since,1,1,1.0
this way since the,1,1,1.0
way since the weighting,1,1,1.0
since the weighting factor,1,1,1.0
the weighting factor is,1,1,1.0
weighting factor is greater,1,1,1.0
factor is greater for,1,1,1.0
is greater for the,1,1,1.0
greater for the majority,1,1,1.0
for the majority class,2,2,1.0
the majority class than,2,1,2.0
majority class than for,1,1,1.0
class than for the,1,1,1.0
than for the minority,1,1,1.0
for the minority one,1,1,1.0
the minority one the,1,1,1.0
minority one the distance,1,1,1.0
one the distance to,1,1,1.0
the distance to positive,1,1,1.0
distance to positive minority,1,1,1.0
to positive minority class,1,1,1.0
positive minority class prototypes,1,1,1.0
minority class prototypes becomes,1,1,1.0
class prototypes becomes much,1,1,1.0
prototypes becomes much lower,1,1,1.0
becomes much lower than,1,1,1.0
much lower than the,1,1,1.0
lower than the distance,1,1,1.0
than the distance to,1,1,1.0
the distance to prototypes,1,1,1.0
distance to prototypes of,1,1,1.0
to prototypes of the,1,1,1.0
prototypes of the majority,1,1,1.0
the majority class this,1,1,1.0
majority class this produces,1,1,1.0
class this produces a,1,1,1.0
this produces a tendency,1,1,1.0
produces a tendency for,1,1,1.0
a tendency for the,1,1,1.0
tendency for the new,1,1,1.0
for the new patterns,1,1,1.0
the new patterns to,1,1,1.0
new patterns to find,1,1,1.0
patterns to find their,1,1,1.0
to find their nearest,1,1,1.0
find their nearest neighbor,1,1,1.0
their nearest neighbor among,1,1,1.0
nearest neighbor among the,1,1,1.0
neighbor among the prototypes,1,1,1.0
among the prototypes of,1,1,1.0
the prototypes of the,1,1,1.0
prototypes of the minority,1,1,1.0
the minority class approach,1,1,1.0
minority class approach to,1,1,1.0
class approach to dealing,1,1,1.0
approach to dealing with,1,1,1.0
to dealing with imbalanced,1,1,1.0
dealing with imbalanced datasets,1,1,1.0
with imbalanced datasets using,1,1,1.0
imbalanced datasets using svm,1,1,1.0
datasets using svm biases,1,1,1.0
using svm biases the,1,1,1.0
svm biases the algorithm,1,1,1.0
biases the algorithm so,1,1,1.0
the algorithm so that,1,1,1.0
algorithm so that the,1,1,1.0
so that the academic,1,1,1.0
that the academic is,1,1,1.0
the academic is further,1,1,1.0
academic is further away,1,1,1.0
is further away from,1,1,1.0
further away from the,1,1,1.0
away from the positive,1,1,1.0
from the positive class,1,1,1.0
the positive class this,2,1,2.0
positive class this is,1,1,1.0
class this is done,1,1,1.0
this is done in,2,2,1.0
is done in order,1,1,1.0
done in order to,1,1,1.0
in order to compensate,2,2,1.0
order to compensate for,2,2,1.0
compensate for the skew,1,1,1.0
for the skew associated,1,1,1.0
the skew associated with,1,1,1.0
skew associated with imbalanced,1,1,1.0
associated with imbalanced datasets,1,1,1.0
with imbalanced datasets which,1,1,1.0
imbalanced datasets which pushes,1,1,1.0
datasets which pushes the,1,1,1.0
which pushes the closer,1,1,1.0
pushes the closer to,1,1,1.0
the closer to the,1,1,1.0
closer to the positive,1,1,1.0
to the positive class,1,1,1.0
positive class this biasing,1,1,1.0
class this biasing can,1,1,1.0
this biasing can be,1,1,1.0
biasing can be accomplished,1,1,1.0
can be accomplished in,1,1,1.0
be accomplished in various,1,1,1.0
accomplished in various ways,1,1,1.0
in various ways in,1,1,1.0
various ways in an,1,1,1.0
ways in an algorithm,1,1,1.0
in an algorithm is,1,1,1.0
an algorithm is proposed,1,1,1.0
algorithm is proposed by,1,1,1.0
is proposed by changing,1,1,1.0
proposed by changing the,1,1,1.0
by changing the kernel,1,1,1.0
changing the kernel function,1,1,1.0
the kernel function to,1,1,1.0
kernel function to develop,1,1,1.0
function to develop this,1,1,1.0
to develop this bias,1,1,1.0
develop this bias veropoulos,1,1,1.0
this bias veropoulos et,1,1,1.0
bias veropoulos et al,1,1,1.0
veropoulos et al s,1,1,1.0
et al s uggested,1,1,1.0
al s uggested using,1,1,1.0
s uggested using different,1,1,1.0
uggested using different penalty,1,1,1.0
using different penalty constants,1,1,1.0
different penalty constants for,1,1,1.0
penalty constants for different,1,1,1.0
constants for different classes,1,1,1.0
for different classes of,1,1,1.0
different classes of data,1,1,1.0
classes of data making,1,1,1.0
of data making errors,1,1,1.0
data making errors on,1,1,1.0
making errors on positive,1,1,1.0
errors on positive examples,1,1,1.0
on positive examples costlier,1,1,1.0
positive examples costlier than,1,1,1.0
examples costlier than errors,1,1,1.0
costlier than errors on,1,1,1.0
than errors on negative,1,1,1.0
errors on negative examples,1,1,1.0
on negative examples kaizhu,1,1,1.0
negative examples kaizhu huang,1,1,1.0
examples kaizhu huang et,1,1,1.0
kaizhu huang et al,1,1,1.0
huang et al presented,1,1,1.0
et al presented biased,1,1,1.0
al presented biased minimax,1,1,1.0
presented biased minimax probability,1,1,1.0
biased minimax probability machine,2,1,2.0
minimax probability machine bmpm,1,1,1.0
probability machine bmpm to,1,1,1.0
machine bmpm to resolve,1,1,1.0
bmpm to resolve the,1,1,1.0
to resolve the imbalance,1,1,1.0
resolve the imbalance problem,1,1,1.0
the imbalance problem given,1,1,1.0
imbalance problem given the,1,1,1.0
problem given the reliable,1,1,1.0
given the reliable mean,1,1,1.0
the reliable mean and,1,1,1.0
reliable mean and covariance,1,1,1.0
mean and covariance matrices,1,1,1.0
and covariance matrices of,1,1,1.0
covariance matrices of the,1,1,1.0
matrices of the majority,1,1,1.0
and minority classes bmpm,1,1,1.0
minority classes bmpm can,1,1,1.0
classes bmpm can derive,1,1,1.0
bmpm can derive the,1,1,1.0
can derive the decision,1,1,1.0
derive the decision by,1,1,1.0
the decision by adjusting,1,1,1.0
decision by adjusting the,1,1,1.0
by adjusting the lower,1,1,1.0
adjusting the lower bound,1,1,1.0
the lower bound of,1,1,1.0
lower bound of the,1,1,1.0
bound of the real,1,1,1.0
of the real accuracy,1,1,1.0
the real accuracy of,1,1,1.0
real accuracy of the,1,1,1.0
accuracy of the testing,1,1,1.0
of the testing set,1,1,1.0
the testing set learning,1,1,1.0
testing set learning besides,1,1,1.0
set learning besides changing,1,1,1.0
learning besides changing the,1,1,1.0
besides changing the class,1,1,1.0
changing the class distributions,1,1,1.0
the class distributions incorporating,1,1,1.0
class distributions incorporating costs,1,1,1.0
distributions incorporating costs in,1,1,1.0
incorporating costs in making,1,1,1.0
costs in making is,1,1,1.0
in making is another,1,1,1.0
making is another way,1,1,1.0
is another way to,1,1,1.0
another way to improve,1,1,1.0
way to improve classifier,1,1,1.0
to improve classifier s,1,1,1.0
improve classifier s performance,1,1,1.0
classifier s performance when,1,1,1.0
s performance when learning,1,1,1.0
performance when learning from,1,1,1.0
when learning from imbalanced,1,1,1.0
learning from imbalanced datasets,1,1,1.0
from imbalanced datasets cost,1,1,1.0
imbalanced datasets cost model,1,1,1.0
datasets cost model takes,1,1,1.0
cost model takes the,1,1,1.0
model takes the form,1,1,1.0
takes the form of,1,1,1.0
the form of a,1,1,1.0
form of a cost,1,1,1.0
of a cost matrix,1,1,1.0
a cost matrix as,1,1,1.0
cost matrix as shown,1,1,1.0
matrix as shown in,1,1,1.0
as shown in where,2,1,2.0
shown in where the,1,1,1.0
in where the cost,1,1,1.0
where the cost of,1,1,1.0
the cost of classifying,1,1,1.0
cost of classifying a,1,1,1.0
of classifying a sample,1,1,1.0
classifying a sample from,1,1,1.0
a sample from a,1,1,1.0
sample from a true,1,1,1.0
from a true class,1,1,1.0
a true class j,1,1,1.0
true class j to,1,1,1.0
class j to class,1,1,1.0
j to class i,1,1,1.0
to class i corresponds,1,1,1.0
class i corresponds to,1,1,1.0
i corresponds to the,1,1,1.0
corresponds to the matrix,1,1,1.0
to the matrix entry,1,1,1.0
the matrix entry λij,1,1,1.0
matrix entry λij this,1,1,1.0
entry λij this matrix,1,1,1.0
λij this matrix is,1,1,1.0
this matrix is usually,1,1,1.0
matrix is usually expressed,1,1,1.0
is usually expressed in,1,1,1.0
usually expressed in terms,1,1,1.0
expressed in terms of,1,1,1.0
in terms of average,1,1,1.0
terms of average misclassification,1,1,1.0
of average misclassification costs,1,1,1.0
average misclassification costs for,1,1,1.0
misclassification costs for the,1,1,1.0
costs for the problem,1,1,1.0
the problem the diagonal,1,1,1.0
problem the diagonal elements,1,1,1.0
the diagonal elements are,1,1,1.0
diagonal elements are usually,1,1,1.0
elements are usually set,1,1,1.0
are usually set to,1,1,1.0
usually set to zero,1,1,1.0
set to zero meaning,1,1,1.0
to zero meaning correct,1,1,1.0
zero meaning correct classification,1,1,1.0
meaning correct classification has,1,1,1.0
correct classification has no,1,1,1.0
classification has no cost,1,1,1.0
has no cost the,1,1,1.0
no cost the goal,1,1,1.0
cost the goal in,1,1,1.0
the goal in sensitive,1,1,1.0
goal in sensitive classification,1,1,1.0
in sensitive classification is,1,1,1.0
sensitive classification is to,1,1,1.0
classification is to minimize,1,1,1.0
is to minimize the,1,1,1.0
to minimize the cost,1,1,1.0
minimize the cost of,1,1,1.0
the cost of misclassification,1,1,1.0
cost of misclassification which,1,1,1.0
of misclassification which can,1,1,1.0
misclassification which can be,1,1,1.0
which can be realized,1,1,1.0
can be realized by,1,1,1.0
be realized by choosing,1,1,1.0
realized by choosing the,1,1,1.0
by choosing the class,1,1,1.0
choosing the class with,1,1,1.0
the class with the,1,1,1.0
class with the minimum,1,1,1.0
with the minimum conditional,1,1,1.0
the minimum conditional risk,1,1,1.0
minimum conditional risk prediction,1,1,1.0
conditional risk prediction class,1,1,1.0
risk prediction class i,1,1,1.0
prediction class i class,1,1,1.0
class i class j,1,1,1.0
i class j true,1,1,1.0
class j true class,1,1,1.0
j true class i,1,1,1.0
true class i ijλ,1,1,1.0
class i ijλ class,1,1,1.0
i ijλ class j,1,1,1.0
ijλ class j jiλ,1,1,1.0
class j jiλ fig,1,1,1.0
j jiλ fig cost,1,1,1.0
jiλ fig cost matrix,1,1,1.0
fig cost matrix metacost,1,1,1.0
cost matrix metacost is,1,1,1.0
matrix metacost is another,1,1,1.0
metacost is another method,1,1,1.0
is another method to,1,1,1.0
another method to make,1,1,1.0
method to make a,1,1,1.0
to make a classifier,1,1,1.0
make a classifier the,1,1,1.0
a classifier the procedure,1,1,1.0
classifier the procedure begins,1,1,1.0
the procedure begins to,1,1,1.0
procedure begins to learn,1,1,1.0
begins to learn an,2,1,2.0
to learn an internal,2,1,2.0
learn an internal model,2,1,2.0
an internal model by,1,1,1.0
internal model by applying,1,1,1.0
model by applying a,1,1,1.0
by applying a sensitive,1,1,1.0
applying a sensitive procedure,1,1,1.0
a sensitive procedure which,1,1,1.0
sensitive procedure which employs,1,1,1.0
procedure which employs a,1,1,1.0
which employs a base,1,1,1.0
employs a base learning,1,1,1.0
a base learning algorithm,1,1,1.0
base learning algorithm then,1,1,1.0
learning algorithm then metacost,1,1,1.0
algorithm then metacost procedure,1,1,1.0
then metacost procedure estimates,1,1,1.0
metacost procedure estimates class,1,1,1.0
procedure estimates class probabilities,1,1,1.0
estimates class probabilities using,2,1,2.0
class probabilities using bagging,2,1,2.0
probabilities using bagging and,2,1,2.0
using bagging and then,2,1,2.0
bagging and then the,1,1,1.0
and then the training,1,1,1.0
then the training examples,1,1,1.0
the training examples with,2,1,2.0
training examples with their,2,1,2.0
examples with their minimum,2,1,2.0
with their minimum expected,2,1,2.0
their minimum expected cost,2,1,2.0
minimum expected cost classes,2,1,2.0
expected cost classes and,2,1,2.0
cost classes and finally,2,1,2.0
classes and finally relearns,2,1,2.0
and finally relearns a,2,1,2.0
finally relearns a model,2,1,2.0
relearns a model using,2,1,2.0
a model using the,2,1,2.0
model using the modified,2,1,2.0
using the modified training,2,1,2.0
the modified training set,2,1,2.0
modified training set adaboost,1,1,1.0
training set adaboost s,1,1,1.0
set adaboost s rule,1,1,1.0
adaboost s rule has,1,1,1.0
s rule has been,1,1,1.0
rule has been made,1,1,1.0
has been made sensitive,1,1,1.0
been made sensitive so,1,1,1.0
made sensitive so that,1,1,1.0
sensitive so that examples,1,1,1.0
so that examples belonging,1,1,1.0
that examples belonging to,1,1,1.0
examples belonging to rare,1,1,1.0
belonging to rare class,1,1,1.0
to rare class that,1,1,1.0
rare class that are,1,1,1.0
class that are misclassified,1,1,1.0
that are misclassified are,1,1,1.0
are misclassified are assigned,1,1,1.0
misclassified are assigned higher,1,1,1.0
are assigned higher weights,1,1,1.0
assigned higher weights than,1,1,1.0
higher weights than those,1,1,1.0
weights than those belonging,1,1,1.0
than those belonging to,1,1,1.0
those belonging to common,1,1,1.0
belonging to common class,1,1,1.0
to common class the,1,1,1.0
common class the resulting,1,1,1.0
class the resulting system,1,1,1.0
the resulting system adacost,2,1,2.0
resulting system adacost has,2,1,2.0
system adacost has been,2,1,2.0
adacost has been empirically,2,1,2.0
has been empirically shown,2,1,2.0
been empirically shown to,2,1,2.0
empirically shown to produce,2,1,2.0
shown to produce lower,2,1,2.0
to produce lower cumulative,2,1,2.0
produce lower cumulative misclassification,2,1,2.0
lower cumulative misclassification costs,2,1,2.0
cumulative misclassification costs than,2,1,2.0
misclassification costs than adaboost,2,1,2.0
costs than adaboost learning,1,1,1.0
than adaboost learning learning,1,1,1.0
adaboost learning learning is,1,1,1.0
learning learning is a,1,1,1.0
learning is a approach,1,1,1.0
is a approach which,1,1,1.0
a approach which provides,1,1,1.0
approach which provides an,1,1,1.0
which provides an alternative,1,1,1.0
provides an alternative to,1,1,1.0
an alternative to discrimination,1,1,1.0
alternative to discrimination where,1,1,1.0
to discrimination where the,1,1,1.0
discrimination where the model,1,1,1.0
where the model can,1,1,1.0
the model can be,1,1,1.0
model can be created,1,1,1.0
can be created based,1,1,1.0
be created based on,1,1,1.0
created based on the,1,1,1.0
based on the examples,1,1,1.0
on the examples of,1,1,1.0
the examples of the,2,2,1.0
examples of the target,1,1,1.0
of the target class,1,1,1.0
the target class alone,1,1,1.0
target class alone here,1,1,1.0
class alone here classification,1,1,1.0
alone here classification is,1,1,1.0
here classification is accomplished,1,1,1.0
classification is accomplished by,1,1,1.0
is accomplished by imposing,1,1,1.0
accomplished by imposing a,1,1,1.0
by imposing a threshold,1,1,1.0
imposing a threshold on,1,1,1.0
a threshold on the,1,1,1.0
threshold on the similarity,1,1,1.0
on the similarity value,1,1,1.0
the similarity value between,1,1,1.0
similarity value between a,1,1,1.0
value between a query,1,1,1.0
between a query object,1,1,1.0
a query object and,1,1,1.0
query object and the,1,1,1.0
object and the target,1,1,1.0
and the target class,1,1,1.0
the target class mainly,1,1,1.0
target class mainly two,1,1,1.0
class mainly two classes,1,1,1.0
mainly two classes of,1,1,1.0
two classes of learners,1,1,1.0
classes of learners were,1,1,1.0
of learners were previously,1,1,1.0
learners were previously studied,1,1,1.0
were previously studied in,1,1,1.0
previously studied in the,1,1,1.0
studied in the context,1,1,1.0
the context of the,1,1,1.0
context of the approach,1,1,1.0
of the approach svms,1,1,1.0
the approach svms and,1,1,1.0
approach svms and and,1,1,1.0
svms and and they,1,1,1.0
and and they were,1,1,1.0
and they were found,1,1,1.0
they were found to,1,1,1.0
found to be competitive,1,1,1.0
to be competitive besides,1,1,1.0
be competitive besides systems,1,1,1.0
competitive besides systems that,1,1,1.0
besides systems that learn,1,1,1.0
systems that learn only,1,1,1.0
that learn only the,1,1,1.0
learn only the minority,1,1,1.0
only the minority class,1,1,1.0
the minority class may,2,2,1.0
minority class may still,1,1,1.0
class may still train,1,1,1.0
may still train using,1,1,1.0
still train using examples,1,1,1.0
train using examples belonging,1,1,1.0
using examples belonging to,1,1,1.0
examples belonging to all,1,1,1.0
belonging to all classes,1,1,1.0
to all classes brute,1,1,1.0
all classes brute shrink,1,1,1.0
classes brute shrink and,1,1,1.0
brute shrink and ripper,1,1,1.0
shrink and ripper are,1,1,1.0
and ripper are three,1,1,1.0
ripper are three such,1,1,1.0
are three such data,1,1,1.0
three such data mining,1,1,1.0
such data mining systems,1,1,1.0
data mining systems brute,1,1,1.0
mining systems brute has,1,1,1.0
systems brute has been,1,1,1.0
brute has been used,1,1,1.0
has been used to,1,1,1.0
been used to look,1,1,1.0
used to look for,1,1,1.0
to look for flaws,1,1,1.0
look for flaws in,1,1,1.0
for flaws in the,1,1,1.0
flaws in the boeing,1,1,1.0
in the boeing manufacturing,1,1,1.0
the boeing manufacturing process,1,1,1.0
boeing manufacturing process shrink,1,1,1.0
manufacturing process shrink uses,1,1,1.0
process shrink uses a,1,1,1.0
shrink uses a similar,1,1,1.0
uses a similar approach,1,1,1.0
a similar approach to,1,1,1.0
similar approach to detect,1,1,1.0
approach to detect rare,1,1,1.0
to detect rare oil,1,1,1.0
detect rare oil spills,1,1,1.0
rare oil spills from,1,1,1.0
oil spills from satellite,1,1,1.0
spills from satellite radar,1,1,1.0
from satellite radar images,1,1,1.0
satellite radar images based,1,1,1.0
radar images based on,1,1,1.0
images based on the,1,1,1.0
based on the assumption,1,1,1.0
on the assumption that,1,1,1.0
the assumption that there,1,1,1.0
assumption that there will,1,1,1.0
that there will be,1,1,1.0
there will be many,1,1,1.0
will be many more,1,1,1.0
be many more negative,1,1,1.0
many more negative examples,1,1,1.0
more negative examples than,1,1,1.0
negative examples than positive,1,1,1.0
examples than positive examples,1,1,1.0
than positive examples shrink,1,1,1.0
positive examples shrink labels,1,1,1.0
examples shrink labels mixed,1,1,1.0
shrink labels mixed regions,1,1,1.0
labels mixed regions regions,1,1,1.0
mixed regions regions with,1,1,1.0
regions regions with positive,1,1,1.0
regions with positive and,1,1,1.0
with positive and negative,1,1,1.0
positive and negative examples,3,1,3.0
and negative examples with,1,1,1.0
negative examples with the,1,1,1.0
examples with the positive,1,1,1.0
with the positive class,1,1,1.0
the positive class ripper,1,1,1.0
positive class ripper is,1,1,1.0
class ripper is a,1,1,1.0
ripper is a rule,1,1,1.0
is a rule induction,1,1,1.0
a rule induction system,1,1,1.0
rule induction system that,1,1,1.0
induction system that utilizes,1,1,1.0
system that utilizes a,1,1,1.0
that utilizes a approach,1,1,1.0
utilizes a approach to,1,1,1.0
a approach to iteratively,1,1,1.0
approach to iteratively build,1,1,1.0
to iteratively build rules,1,1,1.0
iteratively build rules to,1,1,1.0
build rules to cover,1,1,1.0
rules to cover previously,1,1,1.0
to cover previously uncovered,1,1,1.0
cover previously uncovered training,1,1,1.0
previously uncovered training examples,1,1,1.0
uncovered training examples each,1,1,1.0
training examples each rule,1,1,1.0
examples each rule is,1,1,1.0
each rule is grown,1,1,1.0
rule is grown by,1,1,1.0
is grown by adding,1,1,1.0
grown by adding conditions,1,1,1.0
by adding conditions until,1,1,1.0
adding conditions until no,1,1,1.0
conditions until no negative,1,1,1.0
until no negative examples,1,1,1.0
no negative examples are,1,1,1.0
negative examples are covered,1,1,1.0
examples are covered it,1,1,1.0
are covered it normally,1,1,1.0
covered it normally generates,1,1,1.0
it normally generates rules,1,1,1.0
normally generates rules for,1,1,1.0
generates rules for each,1,1,1.0
rules for each class,1,1,1.0
for each class from,1,1,1.0
each class from the,1,1,1.0
class from the most,1,1,1.0
from the most rare,1,1,1.0
the most rare class,1,1,1.0
most rare class to,1,1,1.0
rare class to the,1,1,1.0
class to the most,1,1,1.0
to the most common,1,1,1.0
the most common class,1,1,1.0
most common class so,1,1,1.0
common class so in,1,1,1.0
class so in this,1,1,1.0
so in this view,1,1,1.0
in this view ripper,1,1,1.0
this view ripper can,1,1,1.0
view ripper can be,1,1,1.0
ripper can be view,1,1,1.0
can be view as,1,1,1.0
be view as a,1,1,1.0
view as a learner,1,1,1.0
as a learner an,1,1,1.0
a learner an interesting,1,1,1.0
learner an interesting aspect,1,1,1.0
an interesting aspect of,1,1,1.0
interesting aspect of based,1,1,1.0
aspect of based learning,1,1,1.0
of based learning is,1,1,1.0
based learning is that,1,1,1.0
learning is that under,1,1,1.0
is that under certain,1,1,1.0
that under certain conditions,1,1,1.0
under certain conditions such,1,1,1.0
certain conditions such as,1,1,1.0
conditions such as of,1,1,1.0
such as of the,1,1,1.0
as of the domain,1,1,1.0
of the domain space,1,1,1.0
the domain space one,1,1,1.0
domain space one class,1,1,1.0
space one class approaches,1,1,1.0
one class approaches to,1,1,1.0
class approaches to solving,1,1,1.0
approaches to solving the,1,1,1.0
to solving the classification,1,1,1.0
solving the classification problem,1,1,1.0
the classification problem may,1,1,1.0
classification problem may in,1,1,1.0
problem may in fact,1,1,1.0
may in fact be,1,1,1.0
in fact be superior,1,1,1.0
fact be superior to,1,1,1.0
be superior to discriminative,1,1,1.0
superior to discriminative approaches,1,1,1.0
to discriminative approaches such,1,1,1.0
discriminative approaches such as,1,1,1.0
approaches such as decision,1,1,1.0
such as decision trees,1,1,1.0
as decision trees or,1,1,1.0
decision trees or neural,1,1,1.0
trees or neural networks,1,1,1.0
or neural networks raskutti,1,1,1.0
neural networks raskutti and,1,1,1.0
networks raskutti and kowalczyk,1,1,1.0
raskutti and kowalczyk demonstrated,1,1,1.0
and kowalczyk demonstrated the,1,1,1.0
kowalczyk demonstrated the optimality,1,1,1.0
demonstrated the optimality of,1,1,1.0
the optimality of svms,1,1,1.0
optimality of svms over,1,1,1.0
of svms over ones,1,1,1.0
svms over ones in,1,1,1.0
over ones in certain,1,1,1.0
ones in certain important,1,1,1.0
in certain important domains,1,1,1.0
certain important domains including,1,1,1.0
important domains including genomic,1,1,1.0
domains including genomic data,1,1,1.0
including genomic data in,1,1,1.0
genomic data in particular,1,1,1.0
data in particular they,1,1,1.0
in particular they showed,1,1,1.0
particular they showed that,1,1,1.0
they showed that class,1,1,1.0
showed that class learning,1,1,1.0
that class learning is,1,1,1.0
class learning is particularly,1,1,1.0
learning is particularly useful,1,1,1.0
is particularly useful when,1,1,1.0
particularly useful when used,1,1,1.0
useful when used on,1,1,1.0
when used on extremely,1,1,1.0
used on extremely unbalanced,1,1,1.0
on extremely unbalanced data,1,1,1.0
extremely unbalanced data sets,1,1,1.0
unbalanced data sets composed,1,1,1.0
data sets composed of,1,1,1.0
sets composed of a,1,1,1.0
composed of a high,1,1,1.0
of a high dimensional,1,1,1.0
a high dimensional noisy,1,1,1.0
high dimensional noisy feature,1,1,1.0
dimensional noisy feature space,1,1,1.0
noisy feature space they,1,1,1.0
feature space they argued,1,1,1.0
space they argued that,1,1,1.0
they argued that the,1,1,1.0
argued that the approach,1,1,1.0
that the approach is,1,1,1.0
the approach is related,1,1,1.0
approach is related to,1,1,1.0
is related to aggressive,1,1,1.0
related to aggressive feature,1,1,1.0
to aggressive feature selection,1,1,1.0
aggressive feature selection methods,1,1,1.0
feature selection methods but,1,1,1.0
selection methods but is,1,1,1.0
methods but is more,1,1,1.0
but is more practical,1,1,1.0
is more practical since,1,1,1.0
more practical since feature,1,1,1.0
practical since feature selection,1,1,1.0
since feature selection can,1,1,1.0
feature selection can often,1,1,1.0
selection can often be,1,1,1.0
can often be too,1,1,1.0
often be too expensive,1,1,1.0
be too expensive to,1,1,1.0
too expensive to apply,1,1,1.0
expensive to apply ensemble,1,1,1.0
to apply ensemble learning,1,1,1.0
apply ensemble learning methods,1,1,1.0
ensemble learning methods ensemble,1,1,1.0
learning methods ensemble learning,1,1,1.0
methods ensemble learning has,1,1,1.0
ensemble learning has established,1,1,1.0
learning has established its,1,1,1.0
has established its superiority,1,1,1.0
established its superiority in,1,1,1.0
its superiority in machine,1,1,1.0
superiority in machine learning,1,1,1.0
in recent years of,1,1,1.0
recent years of which,1,1,1.0
years of which boosting,1,1,1.0
of which boosting bagging,1,1,1.0
which boosting bagging are,1,1,1.0
boosting bagging are the,1,1,1.0
bagging are the most,1,1,1.0
are the most successful,1,1,1.0
the most successful approaches,1,1,1.0
most successful approaches ensemble,1,1,1.0
successful approaches ensemble learning,1,1,1.0
approaches ensemble learning methods,1,1,1.0
ensemble learning methods have,1,1,1.0
learning methods have been,1,1,1.0
methods have been extensively,1,1,1.0
have been extensively used,1,1,1.0
been extensively used to,1,1,1.0
extensively used to handle,1,1,1.0
used to handle class,1,1,1.0
to handle class imbalance,1,1,1.0
handle class imbalance problems,1,1,1.0
class imbalance problems these,1,1,1.0
imbalance problems these methods,1,1,1.0
problems these methods combine,1,1,1.0
these methods combine the,1,1,1.0
methods combine the results,1,1,1.0
combine the results of,1,1,1.0
the results of many,1,1,1.0
results of many classifiers,1,1,1.0
of many classifiers their,1,1,1.0
many classifiers their successes,1,1,1.0
classifiers their successes attribute,1,1,1.0
their successes attribute to,1,1,1.0
successes attribute to the,1,1,1.0
attribute to the fact,1,1,1.0
to the fact that,1,1,1.0
the fact that their,1,1,1.0
fact that their base,1,1,1.0
that their base learners,1,1,1.0
their base learners usually,1,1,1.0
base learners usually are,1,1,1.0
learners usually are of,1,1,1.0
usually are of diversity,1,1,1.0
are of diversity in,1,1,1.0
of diversity in principle,1,1,1.0
diversity in principle or,1,1,1.0
in principle or induced,1,1,1.0
principle or induced with,1,1,1.0
or induced with various,1,1,1.0
induced with various class,1,1,1.0
with various class distributions,1,1,1.0
various class distributions adaboost,1,1,1.0
class distributions adaboost introduced,1,1,1.0
distributions adaboost introduced by,1,1,1.0
adaboost introduced by freund,1,1,1.0
introduced by freund and,1,1,1.0
by freund and schapire,1,1,1.0
freund and schapire solved,1,1,1.0
and schapire solved many,1,1,1.0
schapire solved many of,1,1,1.0
solved many of the,1,1,1.0
many of the practical,1,1,1.0
of the practical difficulties,1,1,1.0
the practical difficulties of,1,1,1.0
practical difficulties of the,1,1,1.0
difficulties of the earlier,1,1,1.0
of the earlier boosting,1,1,1.0
the earlier boosting algorithms,1,1,1.0
earlier boosting algorithms initially,1,1,1.0
boosting algorithms initially all,1,1,1.0
algorithms initially all weights,1,1,1.0
initially all weights are,1,1,1.0
all weights are set,1,1,1.0
weights are set equally,1,1,1.0
are set equally but,1,1,1.0
set equally but on,1,1,1.0
equally but on each,1,1,1.0
but on each round,1,1,1.0
on each round the,1,1,1.0
each round the weights,1,1,1.0
round the weights of,1,1,1.0
the weights of incorrectly,1,1,1.0
weights of incorrectly classified,1,1,1.0
of incorrectly classified examples,1,1,1.0
incorrectly classified examples are,1,1,1.0
classified examples are increased,1,1,1.0
examples are increased so,1,1,1.0
are increased so that,1,1,1.0
increased so that the,1,1,1.0
so that the weak,1,1,1.0
that the weak learner,1,1,1.0
the weak learner is,1,1,1.0
weak learner is forced,1,1,1.0
learner is forced to,1,1,1.0
is forced to focus,1,1,1.0
forced to focus on,1,1,1.0
to focus on the,2,2,1.0
focus on the hard,1,1,1.0
on the hard examples,1,1,1.0
the hard examples in,1,1,1.0
hard examples in the,1,1,1.0
the training set as,2,2,1.0
training set as stated,1,1,1.0
set as stated in,1,1,1.0
as stated in learning,1,1,1.0
stated in learning by,1,1,1.0
in learning by making,1,1,1.0
learning by making adaboost,1,1,1.0
by making adaboost s,1,1,1.0
making adaboost s rule,1,1,1.0
adaboost s rule the,1,1,1.0
s rule the resulting,1,1,1.0
rule the resulting system,1,1,1.0
costs than adaboost thus,1,1,1.0
than adaboost thus it,1,1,1.0
adaboost thus it can,1,1,1.0
thus it can be,1,1,1.0
it can be used,2,1,2.0
be used to address,1,1,1.0
used to address class,1,1,1.0
to address class imbalance,1,1,1.0
address class imbalance problem,1,1,1.0
class imbalance problem scales,1,1,1.0
imbalance problem scales examples,1,1,1.0
problem scales examples in,1,1,1.0
scales examples in proportion,2,1,2.0
examples in proportion to,2,1,2.0
in proportion to how,2,1,2.0
proportion to how well,2,1,2.0
to how well they,2,1,2.0
how well they are,2,1,2.0
well they are distinguished,2,1,2.0
they are distinguished from,2,1,2.0
are distinguished from examples,2,1,2.0
distinguished from examples and,1,1,1.0
from examples and scales,1,1,1.0
examples and scales examples,1,1,1.0
and scales examples in,1,1,1.0
distinguished from examples another,1,1,1.0
from examples another algorithm,1,1,1.0
examples another algorithm that,1,1,1.0
another algorithm that uses,1,1,1.0
algorithm that uses boosting,1,1,1.0
that uses boosting to,1,1,1.0
uses boosting to address,1,1,1.0
boosting to address the,1,1,1.0
to address the class,1,1,1.0
address the class imbalance,1,1,1.0
imbalance problem is smoteboost,1,1,1.0
problem is smoteboost this,1,1,1.0
is smoteboost this algorithm,1,1,1.0
smoteboost this algorithm recognizes,1,1,1.0
this algorithm recognizes that,1,1,1.0
algorithm recognizes that boosting,1,1,1.0
recognizes that boosting may,1,1,1.0
that boosting may suffer,1,1,1.0
boosting may suffer from,1,1,1.0
may suffer from the,1,1,1.0
suffer from the same,1,1,1.0
from the same problems,1,1,1.0
the same problems as,1,1,1.0
same problems as overfitting,1,1,1.0
problems as overfitting instead,1,1,1.0
as overfitting instead of,1,1,1.0
overfitting instead of changing,1,1,1.0
instead of changing the,1,1,1.0
of changing the distributions,1,1,1.0
changing the distributions of,1,1,1.0
the distributions of training,1,1,1.0
distributions of training data,1,1,1.0
of training data by,1,1,1.0
training data by updating,1,1,1.0
data by updating the,1,1,1.0
by updating the weights,1,1,1.0
updating the weights associated,1,1,1.0
weights associated with each,1,1,1.0
associated with each example,1,1,1.0
with each example smoteboost,1,1,1.0
each example smoteboost alters,1,1,1.0
example smoteboost alters the,1,1,1.0
smoteboost alters the distributions,1,1,1.0
alters the distributions by,1,1,1.0
the distributions by adding,1,1,1.0
distributions by adding new,1,1,1.0
by adding new examples,1,1,1.0
adding new examples of,1,1,1.0
new examples of minority,1,1,1.0
examples of minority class,1,1,1.0
of minority class using,1,1,1.0
minority class using the,1,1,1.0
class using the smote,1,1,1.0
the smote algorithm experiment,1,1,1.0
smote algorithm experiment results,1,1,1.0
algorithm experiment results indicated,1,1,1.0
experiment results indicated that,1,1,1.0
results indicated that the,1,1,1.0
indicated that the experts,1,1,1.0
that the experts approach,1,1,1.0
the experts approach performs,1,1,1.0
experts approach performs well,1,1,1.0
approach performs well generally,1,1,1.0
performs well generally outperforming,1,1,1.0
well generally outperforming adaboost,1,1,1.0
generally outperforming adaboost with,1,1,1.0
outperforming adaboost with respect,1,1,1.0
adaboost with respect to,1,1,1.0
with respect to precision,1,1,1.0
respect to precision and,1,1,1.0
to precision and recall,1,1,1.0
precision and recall on,1,1,1.0
and recall on text,1,1,1.0
recall on text classification,1,1,1.0
on text classification problems,1,1,1.0
text classification problems and,1,1,1.0
classification problems and doing,1,1,1.0
problems and doing especially,1,1,1.0
and doing especially well,1,1,1.0
doing especially well at,1,1,1.0
especially well at covering,1,1,1.0
well at covering the,1,1,1.0
at covering the minority,1,1,1.0
covering the minority examples,1,1,1.0
the minority examples more,1,1,1.0
minority examples more detailed,1,1,1.0
examples more detailed experiments,1,1,1.0
more detailed experiments are,1,1,1.0
detailed experiments are presented,1,1,1.0
experiments are presented in,1,1,1.0
are presented in metacost,1,1,1.0
presented in metacost is,1,1,1.0
in metacost is another,1,1,1.0
metacost is another ensemble,1,1,1.0
is another ensemble method,1,1,1.0
another ensemble method it,1,1,1.0
ensemble method it begins,1,1,1.0
method it begins to,1,1,1.0
it begins to learn,1,1,1.0
an internal model then,1,1,1.0
internal model then estimates,1,1,1.0
model then estimates class,1,1,1.0
then estimates class probabilities,1,1,1.0
bagging and then labels,1,1,1.0
and then labels the,1,1,1.0
then labels the training,1,1,1.0
labels the training examples,1,1,1.0
modified training set chan,1,1,1.0
training set chan and,1,1,1.0
set chan and stolfo,1,1,1.0
chan and stolfo run,1,1,1.0
and stolfo run a,1,1,1.0
stolfo run a set,1,1,1.0
run a set of,1,1,1.0
a set of preliminary,1,1,1.0
set of preliminary experiments,1,1,1.0
of preliminary experiments to,1,1,1.0
preliminary experiments to identify,1,1,1.0
experiments to identify a,1,1,1.0
to identify a good,1,1,1.0
identify a good class,1,1,1.0
a good class distributions,2,1,2.0
good class distributions and,1,1,1.0
class distributions and then,1,1,1.0
distributions and then do,1,1,1.0
and then do resampling,1,1,1.0
then do resampling to,1,1,1.0
do resampling to generate,1,1,1.0
resampling to generate multiple,1,1,1.0
to generate multiple training,1,1,1.0
generate multiple training sets,1,1,1.0
multiple training sets with,1,1,1.0
training sets with the,1,1,1.0
sets with the desired,1,1,1.0
with the desired class,1,1,1.0
the desired class distributions,1,1,1.0
desired class distributions each,1,1,1.0
class distributions each training,1,1,1.0
distributions each training set,1,1,1.0
each training set typically,1,1,1.0
training set typically includes,1,1,1.0
set typically includes all,1,1,1.0
typically includes all examples,1,1,1.0
includes all examples and,1,1,1.0
all examples and a,1,1,1.0
examples and a subset,1,1,1.0
and a subset of,1,1,1.0
a subset of the,1,1,1.0
subset of the examples,1,1,1.0
of the examples however,1,1,1.0
the examples however each,1,1,1.0
examples however each example,1,1,1.0
however each example is,1,1,1.0
each example is guaranteed,1,1,1.0
example is guaranteed to,1,1,1.0
is guaranteed to occur,1,1,1.0
guaranteed to occur in,1,1,1.0
to occur in at,1,1,1.0
occur in at least,1,1,1.0
in at least one,1,1,1.0
at least one training,1,1,1.0
least one training set,1,1,1.0
one training set so,1,1,1.0
training set so no,1,1,1.0
set so no data,1,1,1.0
so no data is,1,1,1.0
no data is wasted,1,1,1.0
data is wasted the,1,1,1.0
is wasted the learning,1,1,1.0
wasted the learning algorithm,1,1,1.0
learning algorithm is applied,1,1,1.0
algorithm is applied to,1,1,1.0
is applied to each,1,1,1.0
applied to each training,1,1,1.0
to each training set,1,1,1.0
each training set and,1,1,1.0
training set and is,1,1,1.0
set and is used,1,1,1.0
and is used to,1,1,1.0
is used to form,1,1,1.0
used to form a,1,1,1.0
to form a composite,1,1,1.0
form a composite learner,1,1,1.0
a composite learner from,1,1,1.0
composite learner from the,1,1,1.0
learner from the resulting,1,1,1.0
from the resulting classifiers,1,1,1.0
the resulting classifiers since,1,1,1.0
resulting classifiers since it,1,1,1.0
classifiers since it is,1,1,1.0
since it is a,1,1,1.0
it is a wrapper,1,1,1.0
is a wrapper method,1,1,1.0
a wrapper method it,1,1,1.0
wrapper method it can,1,1,1.0
method it can be,2,2,1.0
can be used with,1,1,1.0
be used with any,1,1,1.0
used with any learning,1,1,1.0
with any learning method,1,1,1.0
any learning method internally,1,1,1.0
learning method internally the,1,1,1.0
method internally the same,1,1,1.0
internally the same basic,1,1,1.0
the same basic approach,1,1,1.0
same basic approach for,1,1,1.0
basic approach for partitioning,1,1,1.0
approach for partitioning the,1,1,1.0
for partitioning the data,1,1,1.0
partitioning the data and,1,1,1.0
the data and learning,1,1,1.0
data and learning multiple,1,1,1.0
and learning multiple classifiers,1,1,1.0
learning multiple classifiers has,1,1,1.0
multiple classifiers has been,1,1,1.0
classifiers has been used,1,1,1.0
has been used with,1,1,1.0
been used with support,1,1,1.0
used with support vector,1,1,1.0
with support vector machines,1,1,1.0
support vector machines the,1,1,1.0
vector machines the resulting,1,1,1.0
machines the resulting svm,1,1,1.0
the resulting svm ensembles,1,1,1.0
resulting svm ensembles was,1,1,1.0
svm ensembles was shown,1,1,1.0
ensembles was shown to,1,1,1.0
was shown to outperform,1,1,1.0
shown to outperform both,1,1,1.0
to outperform both and,1,1,1.0
outperform both and while,1,1,1.0
both and while these,1,1,1.0
and while these ensemble,1,1,1.0
while these ensemble approaches,1,1,1.0
these ensemble approaches are,1,1,1.0
ensemble approaches are effective,1,1,1.0
approaches are effective for,1,1,1.0
are effective for dealing,1,1,1.0
effective for dealing with,1,1,1.0
for dealing with the,2,1,2.0
class imbalance problem they,1,1,1.0
imbalance problem they assume,1,1,1.0
problem they assume that,1,1,1.0
they assume that a,1,1,1.0
assume that a good,1,1,1.0
that a good class,1,1,1.0
good class distributions is,1,1,1.0
class distributions is known,1,1,1.0
distributions is known this,1,1,1.0
is known this can,1,1,1.0
known this can be,1,1,1.0
this can be estimated,1,1,1.0
can be estimated using,1,1,1.0
be estimated using some,1,1,1.0
estimated using some preliminary,1,1,1.0
using some preliminary runs,1,1,1.0
some preliminary runs but,1,1,1.0
preliminary runs but it,1,1,1.0
runs but it is,1,1,1.0
but it is time,1,1,1.0
it is time consuming,1,1,1.0
is time consuming from,1,1,1.0
time consuming from the,1,1,1.0
consuming from the style,1,1,1.0
from the style constructing,1,1,1.0
the style constructing the,1,1,1.0
style constructing the training,1,1,1.0
constructing the training data,1,1,1.0
the training data sets,1,1,1.0
training data sets they,1,1,1.0
data sets they can,1,1,1.0
sets they can be,1,1,1.0
they can be viewed,1,1,1.0
be viewed as a,1,1,1.0
viewed as a variant,1,1,1.0
as a variant of,1,1,1.0
a variant of bagging,1,1,1.0
variant of bagging phua,1,1,1.0
of bagging phua et,1,1,1.0
bagging phua et al,1,1,1.0
phua et al combined,1,1,1.0
et al combined bagging,1,1,1.0
al combined bagging and,1,1,1.0
combined bagging and stacking,1,1,1.0
bagging and stacking to,1,1,1.0
and stacking to identify,1,1,1.0
stacking to identify the,1,1,1.0
to identify the best,1,1,1.0
identify the best mix,1,1,1.0
the best mix of,1,1,1.0
best mix of classifiers,1,1,1.0
mix of classifiers in,1,1,1.0
of classifiers in their,1,1,1.0
classifiers in their insurance,1,1,1.0
in their insurance fraud,1,1,1.0
their insurance fraud detection,1,1,1.0
insurance fraud detection domain,1,1,1.0
fraud detection domain they,1,1,1.0
detection domain they noted,1,1,1.0
domain they noted that,1,1,1.0
they noted that bagging,1,1,1.0
noted that bagging achieves,1,1,1.0
that bagging achieves the,1,1,1.0
bagging achieves the best,1,1,1.0
achieves the best besides,1,1,1.0
the best besides ensemble,1,1,1.0
best besides ensemble learning,1,1,1.0
besides ensemble learning algorithms,1,1,1.0
ensemble learning algorithms of,1,1,1.0
learning algorithms of boosting,1,1,1.0
algorithms of boosting and,1,1,1.0
of boosting and bagging,1,1,1.0
boosting and bagging style,1,1,1.0
and bagging style kotsiantis,1,1,1.0
bagging style kotsiantis and,1,1,1.0
style kotsiantis and pintelas,1,1,1.0
kotsiantis and pintelas used,1,1,1.0
and pintelas used three,1,1,1.0
pintelas used three agents,1,1,1.0
used three agents the,1,1,1.0
three agents the first,1,1,1.0
agents the first learns,1,1,1.0
the first learns using,1,1,1.0
first learns using naive,1,1,1.0
learns using naive bayes,1,1,1.0
using naive bayes the,1,1,1.0
naive bayes the second,1,1,1.0
bayes the second using,1,1,1.0
the second using and,1,1,1.0
second using and the,1,1,1.0
using and the third,1,1,1.0
and the third using,1,1,1.0
the third using on,1,1,1.0
third using on a,1,1,1.0
using on a filtered,1,1,1.0
on a filtered version,1,1,1.0
a filtered version of,1,1,1.0
filtered version of training,1,1,1.0
version of training data,1,1,1.0
of training data and,1,1,1.0
training data and combined,1,1,1.0
data and combined their,1,1,1.0
and combined their predictions,1,1,1.0
combined their predictions according,1,1,1.0
their predictions according to,1,1,1.0
predictions according to a,1,1,1.0
according to a voting,1,1,1.0
to a voting scheme,1,1,1.0
a voting scheme this,1,1,1.0
voting scheme this technique,1,1,1.0
scheme this technique attempts,1,1,1.0
this technique attempts to,1,1,1.0
technique attempts to achieve,1,1,1.0
attempts to achieve diversity,1,1,1.0
to achieve diversity in,1,1,1.0
achieve diversity in the,1,1,1.0
diversity in the errors,1,1,1.0
in the errors of,1,1,1.0
the errors of the,1,1,1.0
errors of the academic,1,1,1.0
of the academic models,1,1,1.0
the academic models by,1,1,1.0
academic models by using,1,1,1.0
models by using different,1,1,1.0
by using different learning,1,1,1.0
using different learning algorithms,1,1,1.0
different learning algorithms the,1,1,1.0
learning algorithms the intuition,1,1,1.0
algorithms the intuition is,1,1,1.0
the intuition is that,1,1,1.0
intuition is that the,1,1,1.0
is that the models,1,1,1.0
that the models generated,1,1,1.0
the models generated using,1,1,1.0
models generated using different,1,1,1.0
generated using different learning,1,1,1.0
using different learning biases,1,1,1.0
different learning biases are,1,1,1.0
learning biases are more,1,1,1.0
biases are more likely,1,1,1.0
are more likely to,4,3,1.3333333333333333
more likely to make,1,1,1.0
likely to make errors,1,1,1.0
to make errors in,1,1,1.0
make errors in different,1,1,1.0
errors in different ways,1,1,1.0
in different ways they,1,1,1.0
different ways they also,1,1,1.0
ways they also used,1,1,1.0
they also used feature,1,1,1.0
also used feature selection,1,1,1.0
used feature selection of,1,1,1.0
feature selection of the,1,1,1.0
selection of the training,1,1,1.0
the training data because,1,1,1.0
training data because in,1,1,1.0
data because in small,1,1,1.0
because in small data,1,1,1.0
in small data sets,1,1,1.0
small data sets the,1,1,1.0
data sets the amount,1,1,1.0
sets the amount of,1,1,1.0
the amount of class,1,1,1.0
amount of class imbalance,1,1,1.0
of class imbalance affects,1,1,1.0
class imbalance affects more,1,1,1.0
imbalance affects more the,1,1,1.0
affects more the induction,1,1,1.0
more the induction and,1,1,1.0
the induction and thus,1,1,1.0
induction and thus feature,1,1,1.0
and thus feature selection,1,1,1.0
thus feature selection makes,1,1,1.0
feature selection makes the,1,1,1.0
selection makes the problem,1,1,1.0
makes the problem less,1,1,1.0
the problem less difficult,1,1,1.0
problem less difficult motivated,1,1,1.0
less difficult motivated zheng,1,1,1.0
difficult motivated zheng and,1,1,1.0
motivated zheng and srihari,1,1,1.0
zheng and srihari s,1,1,1.0
and srihari s work,1,1,1.0
srihari s work castillo,1,1,1.0
s work castillo and,1,1,1.0
work castillo and serrano,1,1,1.0
castillo and serrano do,1,1,1.0
and serrano do not,1,1,1.0
serrano do not particularly,1,1,1.0
do not particularly focus,1,1,1.0
feature selection but make,1,1,1.0
selection but make it,1,1,1.0
but make it a,1,1,1.0
make it a part,1,1,1.0
their complete framework they,1,1,1.0
complete framework they use,1,1,1.0
framework they use a,1,1,1.0
they use a classifier,1,1,1.0
use a classifier system,1,1,1.0
a classifier system to,1,1,1.0
classifier system to construct,1,1,1.0
system to construct multiple,1,1,1.0
to construct multiple learners,1,1,1.0
construct multiple learners each,1,1,1.0
multiple learners each doing,1,1,1.0
learners each doing its,1,1,1.0
each doing its own,1,1,1.0
doing its own feature,1,1,1.0
its own feature selection,1,1,1.0
own feature selection based,1,1,1.0
feature selection based on,1,1,1.0
selection based on genetic,1,1,1.0
on genetic algorithm their,1,1,1.0
genetic algorithm their proposed,1,1,1.0
algorithm their proposed system,1,1,1.0
their proposed system also,1,1,1.0
proposed system also combines,1,1,1.0
system also combines the,1,1,1.0
also combines the predictions,1,1,1.0
combines the predictions of,1,1,1.0
the predictions of each,1,1,1.0
predictions of each learner,1,1,1.0
of each learner using,1,1,1.0
each learner using genetic,1,1,1.0
learner using genetic algorithms,1,1,1.0
using genetic algorithms evaluation,1,1,1.0
genetic algorithms evaluation metrics,1,1,1.0
algorithms evaluation metrics accuracy,1,1,1.0
evaluation metrics accuracy is,1,1,1.0
metrics accuracy is the,1,1,1.0
accuracy is the most,1,1,1.0
is the most common,1,1,1.0
the most common evaluation,1,1,1.0
most common evaluation metric,1,1,1.0
common evaluation metric for,1,1,1.0
evaluation metric for most,1,1,1.0
metric for most traditional,1,1,1.0
for most traditional application,1,1,1.0
most traditional application but,1,1,1.0
traditional application but accuracy,1,1,1.0
application but accuracy is,1,1,1.0
but accuracy is not,1,1,1.0
accuracy is not suitable,1,1,1.0
is not suitable to,1,1,1.0
not suitable to evaluate,1,1,1.0
suitable to evaluate imbalanced,1,1,1.0
to evaluate imbalanced data,1,1,1.0
evaluate imbalanced data sets,1,1,1.0
imbalanced data sets since,1,1,1.0
data sets since many,1,1,1.0
sets since many practitioners,1,1,1.0
since many practitioners have,1,1,1.0
many practitioners have observed,1,1,1.0
practitioners have observed that,1,1,1.0
have observed that for,1,1,1.0
observed that for extremely,1,1,1.0
that for extremely skewed,1,1,1.0
for extremely skewed class,1,1,1.0
extremely skewed class distributions,1,1,1.0
skewed class distributions the,1,1,1.0
class distributions the recall,1,1,1.0
distributions the recall of,1,1,1.0
the recall of the,1,1,1.0
recall of the minority,2,1,2.0
minority class is often,1,1,1.0
class is often which,1,1,1.0
is often which means,1,1,1.0
often which means that,1,1,1.0
which means that there,1,1,1.0
means that there are,1,1,1.0
that there are no,2,2,1.0
there are no classification,1,1,1.0
are no classification rules,1,1,1.0
no classification rules generated,1,1,1.0
classification rules generated for,1,1,1.0
rules generated for the,1,1,1.0
generated for the minority,1,1,1.0
minority class using terminology,1,1,1.0
class using terminology from,1,1,1.0
using terminology from information,1,1,1.0
terminology from information retrieval,1,1,1.0
from information retrieval the,1,1,1.0
information retrieval the minority,1,1,1.0
retrieval the minority class,1,1,1.0
the minority class has,1,1,1.0
minority class has much,1,1,1.0
class has much lower,1,1,1.0
has much lower precision,1,1,1.0
much lower precision and,1,1,1.0
lower precision and recall,1,1,1.0
precision and recall than,1,1,1.0
and recall than the,1,1,1.0
recall than the majority,1,1,1.0
than the majority class,1,1,1.0
the majority class places,1,1,1.0
majority class places more,1,1,1.0
class places more weight,1,1,1.0
places more weight on,1,1,1.0
more weight on the,1,1,1.0
weight on the majority,1,1,1.0
on the majority class,4,2,2.0
majority class than on,1,1,1.0
class than on minority,1,1,1.0
than on minority class,1,1,1.0
on minority class which,1,1,1.0
minority class which makes,1,1,1.0
class which makes it,1,1,1.0
which makes it difficult,1,1,1.0
makes it difficult for,1,1,1.0
it difficult for a,1,1,1.0
difficult for a classifier,1,1,1.0
for a classifier to,1,1,1.0
a classifier to perform,1,1,1.0
classifier to perform well,1,1,1.0
to perform well on,1,1,1.0
perform well on the,1,1,1.0
well on the minority,1,1,1.0
on the minority class,2,2,1.0
minority class for this,1,1,1.0
class for this reason,1,1,1.0
for this reason additional,1,1,1.0
this reason additional metrics,1,1,1.0
reason additional metrics are,1,1,1.0
additional metrics are coming,1,1,1.0
metrics are coming into,1,1,1.0
are coming into widespread,1,1,1.0
coming into widespread use,1,1,1.0
into widespread use in,1,1,1.0
widespread use in recent,1,1,1.0
use in recent years,1,1,1.0
in recent years several,1,1,1.0
recent years several new,1,1,1.0
years several new metrics,1,1,1.0
several new metrics have,1,1,1.0
new metrics have been,1,1,1.0
metrics have been proposed,1,1,1.0
have been proposed or,1,1,1.0
been proposed or introduced,1,1,1.0
or introduced from other,1,1,1.0
introduced from other domains,1,1,1.0
from other domains for,1,1,1.0
other domains for imbalanced,1,1,1.0
domains for imbalanced data,1,1,1.0
data sets they are,1,1,1.0
sets they are precision,1,1,1.0
they are precision and,1,1,1.0
are precision and recall,2,1,2.0
precision and recall from,1,1,1.0
and recall from information,1,1,1.0
recall from information retrieval,1,1,1.0
from information retrieval domain,1,1,1.0
information retrieval domain roc,1,1,1.0
retrieval domain roc and,1,1,1.0
domain roc and auc,1,1,1.0
roc and auc area,1,1,1.0
and auc area under,1,1,1.0
auc area under the,1,1,1.0
area under the roc,4,2,2.0
the roc curve from,1,1,1.0
roc curve from medical,1,1,1.0
curve from medical domain,1,1,1.0
from medical domain value,1,1,1.0
medical domain value maximum,1,1,1.0
domain value maximum geometry,1,1,1.0
value maximum geometry mean,1,1,1.0
maximum geometry mean mgm,1,1,1.0
geometry mean mgm of,1,1,1.0
mean mgm of the,1,1,1.0
mgm of the accuracy,2,1,2.0
of the accuracy on,2,1,2.0
the accuracy on the,2,1,2.0
accuracy on the majority,2,1,2.0
the minority class maximum,1,1,1.0
minority class maximum sum,1,1,1.0
class maximum sum ms,1,1,1.0
maximum sum ms of,1,1,1.0
sum ms of the,1,1,1.0
ms of the accuracy,1,1,1.0
of the accuracy all,1,1,1.0
the accuracy all the,1,1,1.0
accuracy all the metrics,1,1,1.0
all the metrics can,1,1,1.0
the metrics can be,1,1,1.0
metrics can be classified,1,1,1.0
can be classified into,1,1,1.0
be classified into two,1,1,1.0
classified into two categories,1,1,1.0
into two categories metrics,1,1,1.0
two categories metrics based,1,1,1.0
categories metrics based on,1,1,1.0
metrics based on confusion,2,1,2.0
based on confusion matrix,2,1,2.0
on confusion matrix directly,1,1,1.0
confusion matrix directly and,1,1,1.0
matrix directly and that,1,1,1.0
directly and that based,1,1,1.0
and that based on,1,1,1.0
that based on accuracy,1,1,1.0
based on accuracy of,1,1,1.0
on accuracy of binary,1,1,1.0
accuracy of binary classes,1,1,1.0
of binary classes or,1,1,1.0
binary classes or precision,1,1,1.0
classes or precision and,1,1,1.0
or precision and recall,1,1,1.0
precision and recall directly,1,1,1.0
and recall directly accuracy,1,1,1.0
recall directly accuracy precision,1,1,1.0
directly accuracy precision and,1,1,1.0
accuracy precision and recall,1,1,1.0
precision and recall fp,1,1,1.0
and recall fp rate,1,1,1.0
recall fp rate tp,1,1,1.0
fp rate tp rate,2,1,2.0
rate tp rate roc,1,1,1.0
tp rate roc and,1,1,1.0
rate roc and auc,1,1,1.0
roc and auc fall,1,1,1.0
and auc fall into,1,1,1.0
auc fall into the,1,1,1.0
fall into the first,1,1,1.0
into the first while,1,1,1.0
the first while and,1,1,1.0
first while and other,1,1,1.0
while and other more,1,1,1.0
and other more complex,1,1,1.0
other more complex metrics,1,1,1.0
more complex metrics such,1,1,1.0
complex metrics such as,1,1,1.0
metrics such as mgm,1,1,1.0
such as mgm of,1,1,1.0
as mgm of the,1,1,1.0
the minority class ms,1,1,1.0
minority class ms fall,1,1,1.0
class ms fall into,1,1,1.0
ms fall into the,1,1,1.0
fall into the other,1,1,1.0
into the other table,1,1,1.0
the other table shows,1,1,1.0
other table shows the,1,1,1.0
table shows the confusion,1,1,1.0
shows the confusion matrix,1,1,1.0
the confusion matrix and,1,1,1.0
confusion matrix and a,1,1,1.0
matrix and a good,1,1,1.0
and a good understanding,1,1,1.0
a good understanding to,1,1,1.0
good understanding to confusion,1,1,1.0
understanding to confusion matrix,1,1,1.0
to confusion matrix will,1,1,1.0
confusion matrix will be,1,1,1.0
matrix will be helpful,1,1,1.0
will be helpful table,1,1,1.0
be helpful table confusion,1,1,1.0
helpful table confusion matrix,1,1,1.0
table confusion matrix prediction,1,1,1.0
confusion matrix prediction positive,1,1,1.0
matrix prediction positive negative,1,1,1.0
prediction positive negative real,1,1,1.0
positive negative real positive,1,1,1.0
negative real positive tp,1,1,1.0
real positive tp true,1,1,1.0
positive tp true positive,1,1,1.0
fn false negative negative,1,1,1.0
false negative negative fp,1,1,1.0
negative negative fp false,1,1,1.0
negative fp false positive,1,1,1.0
tn true negative as,1,1,1.0
true negative as promised,1,1,1.0
negative as promised at,1,1,1.0
as promised at the,1,1,1.0
promised at the beginning,1,1,1.0
at the beginning of,2,2,1.0
the beginning of the,1,1,1.0
beginning of the paper,1,1,1.0
of the paper the,1,1,1.0
the paper the class,1,1,1.0
paper the class label,1,1,1.0
is positive and the,1,1,1.0
positive and the class,1,1,1.0
and the class label,2,2,1.0
label of the majority,1,1,1.0
class is negative fig,1,1,1.0
is negative fig presents,1,1,1.0
negative fig presents the,1,1,1.0
fig presents the most,1,1,1.0
presents the most well,1,1,1.0
the most well known,1,1,1.0
most well known evaluation,1,1,1.0
well known evaluation metrics,1,1,1.0
known evaluation metrics as,1,1,1.0
evaluation metrics as shown,1,1,1.0
metrics as shown in,1,1,1.0
as shown in table,1,1,1.0
shown in table tp,1,1,1.0
in table tp and,1,1,1.0
table tp and tn,1,1,1.0
tp and tn denote,1,1,1.0
and tn denote the,1,1,1.0
tn denote the number,1,1,1.0
denote the number of,2,1,2.0
the number of positive,2,1,2.0
number of positive and,1,1,1.0
of positive and negative,1,1,1.0
and negative examples that,1,1,1.0
negative examples that are,1,1,1.0
examples that are classified,1,1,1.0
that are classified correctly,1,1,1.0
are classified correctly while,1,1,1.0
classified correctly while fn,1,1,1.0
correctly while fn and,1,1,1.0
while fn and fp,1,1,1.0
fn and fp denote,1,1,1.0
and fp denote the,1,1,1.0
fp denote the number,1,1,1.0
the number of misclassified,1,1,1.0
number of misclassified positive,1,1,1.0
of misclassified positive and,1,1,1.0
misclassified positive and negative,1,1,1.0
and negative examples respectively,1,1,1.0
negative examples respectively by,1,1,1.0
examples respectively by definition,1,1,1.0
respectively by definition accuracy,1,1,1.0
by definition accuracy precision,1,1,1.0
definition accuracy precision fp,1,1,1.0
accuracy precision fp rate,1,1,1.0
precision fp rate tp,1,1,1.0
rate tp rate and,1,1,1.0
tp rate and value,1,1,1.0
rate and value can,1,1,1.0
and value can be,1,1,1.0
value can be represented,1,1,1.0
can be represented by,1,1,1.0
be represented by equations,1,1,1.0
represented by equations from,1,1,1.0
by equations from to,1,1,1.0
equations from to as,1,1,1.0
from to as shown,1,1,1.0
to as shown in,1,1,1.0
shown in where precision,1,1,1.0
in where precision and,1,1,1.0
where precision and recall,1,1,1.0
precision and recall are,1,1,1.0
and recall are precision,1,1,1.0
recall are precision and,1,1,1.0
precision and recall of,1,1,1.0
and recall of the,1,1,1.0
the minority class fp,1,1,1.0
minority class fp rate,1,1,1.0
class fp rate denotes,1,1,1.0
fp rate denotes the,1,1,1.0
rate denotes the percentage,1,1,1.0
denotes the percentage of,1,1,1.0
the percentage of the,2,1,2.0
percentage of the misclassified,1,1,1.0
of the misclassified negative,1,1,1.0
the misclassified negative examples,1,1,1.0
misclassified negative examples and,1,1,1.0
negative examples and tp,1,1,1.0
examples and tp rate,1,1,1.0
and tp rate is,1,1,1.0
tp rate is the,1,1,1.0
rate is the percentage,1,1,1.0
is the percentage of,1,1,1.0
percentage of the correctly,1,1,1.0
of the correctly classified,1,1,1.0
the correctly classified positive,1,1,1.0
correctly classified positive examples,1,1,1.0
classified positive examples the,1,1,1.0
positive examples the point,1,1,1.0
examples the point is,1,1,1.0
the point is the,1,1,1.0
point is the ideal,1,1,1.0
is the ideal point,1,1,1.0
the ideal point of,1,1,1.0
ideal point of the,1,1,1.0
point of the learners,1,1,1.0
of the learners that,1,1,1.0
the learners that is,1,1,1.0
learners that is there,1,1,1.0
that is there is,1,1,1.0
is there is no,1,1,1.0
there is no positive,1,1,1.0
is no positive examples,1,1,1.0
no positive examples were,1,1,1.0
positive examples were misclassified,1,1,1.0
examples were misclassified to,1,1,1.0
were misclassified to negative,1,1,1.0
misclassified to negative class,1,1,1.0
to negative class and,1,1,1.0
negative class and vice,1,1,1.0
class and vice versa,1,1,1.0
and vice versa or,1,1,1.0
vice versa or is,1,1,1.0
versa or is a,1,1,1.0
or is a popular,1,1,1.0
is a popular evaluation,1,1,1.0
a popular evaluation metric,1,1,1.0
popular evaluation metric for,1,1,1.0
evaluation metric for imbalance,1,1,1.0
metric for imbalance problem,1,1,1.0
for imbalance problem it,1,1,1.0
imbalance problem it is,1,1,1.0
problem it is a,1,1,1.0
it is a kind,1,1,1.0
is a kind of,1,1,1.0
a kind of combination,1,1,1.0
kind of combination of,1,1,1.0
of combination of recall,1,1,1.0
combination of recall and,1,1,1.0
of recall and precision,1,1,1.0
recall and precision which,1,1,1.0
and precision which are,1,1,1.0
precision which are effective,1,1,1.0
which are effective metrics,1,1,1.0
are effective metrics for,1,1,1.0
effective metrics for information,1,1,1.0
metrics for information retrieval,1,1,1.0
for information retrieval community,1,1,1.0
information retrieval community where,1,1,1.0
retrieval community where the,1,1,1.0
community where the imbalance,1,1,1.0
where the imbalance problem,1,1,1.0
the imbalance problem exists,1,1,1.0
imbalance problem exists is,1,1,1.0
problem exists is high,1,1,1.0
exists is high when,1,1,1.0
is high when both,1,1,1.0
high when both recall,1,1,1.0
when both recall and,1,1,1.0
both recall and precision,1,1,1.0
recall and precision are,1,1,1.0
and precision are high,1,1,1.0
precision are high and,1,1,1.0
are high and can,1,1,1.0
high and can be,1,1,1.0
and can be adjusted,1,1,1.0
can be adjusted through,1,1,1.0
be adjusted through changing,1,1,1.0
adjusted through changing the,1,1,1.0
through changing the value,1,1,1.0
changing the value of,1,1,1.0
the value of β,1,1,1.0
value of β where,1,1,1.0
of β where β,1,1,1.0
β where β corresponds,1,1,1.0
where β corresponds to,1,1,1.0
β corresponds to relative,1,1,1.0
corresponds to relative importance,1,1,1.0
to relative importance of,1,1,1.0
relative importance of precision,1,1,1.0
importance of precision recall,1,1,1.0
of precision recall for,1,1,1.0
precision recall for example,1,1,1.0
recall for example counts,1,1,1.0
for example counts both,1,1,1.0
example counts both equally,1,1,1.0
counts both equally while,1,1,1.0
both equally while counts,1,1,1.0
equally while counts recall,1,1,1.0
while counts recall twice,1,1,1.0
counts recall twice as,1,1,1.0
recall twice as much,1,1,1.0
twice as much perhaps,1,1,1.0
as much perhaps the,1,1,1.0
much perhaps the most,1,1,1.0
perhaps the most common,2,2,1.0
the most common metric,1,1,1.0
most common metric to,1,1,1.0
common metric to assess,1,1,1.0
metric to assess overall,1,1,1.0
to assess overall classification,1,1,1.0
assess overall classification performance,1,1,1.0
overall classification performance is,1,1,1.0
classification performance is roc,1,1,1.0
performance is roc analysis,1,1,1.0
is roc analysis and,1,1,1.0
roc analysis and the,1,1,1.0
analysis and the associated,1,1,1.0
and the associated use,1,1,1.0
the associated use of,1,1,1.0
associated use of the,1,1,1.0
use of the area,2,1,2.0
of the area under,2,1,2.0
the roc curve auc,1,1,1.0
roc curve auc in,1,1,1.0
curve auc in detail,1,1,1.0
auc in detail roc,1,1,1.0
in detail roc curve,1,1,1.0
detail roc curve is,1,1,1.0
graph in which tp,1,1,1.0
in which tp rate,1,1,1.0
which tp rate is,1,1,1.0
on the and fp,1,1,1.0
the and fp rate,1,1,1.0
and fp rate is,1,1,1.0
plotted on the roc,1,1,1.0
on the roc curves,1,1,1.0
the roc curves like,1,1,1.0
roc curves like curves,1,1,1.0
curves like curves can,1,1,1.0
like curves can also,1,1,1.0
curves can also be,1,1,1.0
can also be used,1,1,1.0
also be used to,1,1,1.0
be used to assess,2,2,1.0
used to assess different,1,1,1.0
to assess different roc,1,1,1.0
assess different roc curve,1,1,1.0
different roc curve depicts,1,1,1.0
roc curve depicts relative,1,1,1.0
curve depicts relative between,1,1,1.0
depicts relative between benefits,1,1,1.0
relative between benefits tp,1,1,1.0
between benefits tp rate,1,1,1.0
benefits tp rate and,1,1,1.0
tp rate and costs,1,1,1.0
rate and costs fp,1,1,1.0
and costs fp rate,1,1,1.0
costs fp rate that,1,1,1.0
fp rate that is,1,1,1.0
rate that is the,1,1,1.0
that is the number,1,1,1.0
is the number of,5,3,1.6666666666666667
number of positive examples,1,1,1.0
of positive examples correctly,1,1,1.0
positive examples correctly classified,1,1,1.0
examples correctly classified can,1,1,1.0
correctly classified can be,1,1,1.0
classified can be increased,1,1,1.0
can be increased at,1,1,1.0
be increased at the,1,1,1.0
increased at the expense,1,1,1.0
at the expense of,1,1,1.0
the expense of introducing,1,1,1.0
expense of introducing additional,1,1,1.0
of introducing additional false,1,1,1.0
introducing additional false positives,1,1,1.0
additional false positives a,1,1,1.0
false positives a major,1,1,1.0
positives a major disadvantage,1,1,1.0
a major disadvantage of,1,1,1.0
major disadvantage of roc,1,1,1.0
disadvantage of roc analysis,1,1,1.0
of roc analysis is,1,1,1.0
roc analysis is that,1,1,1.0
analysis is that it,1,1,1.0
is that it does,1,1,1.0
that it does not,1,1,1.0
it does not deliver,1,1,1.0
does not deliver a,1,1,1.0
not deliver a single,1,1,1.0
deliver a single easy,1,1,1.0
a single easy to,1,1,1.0
single easy to use,1,1,1.0
easy to use performance,1,1,1.0
to use performance measure,1,1,1.0
use performance measure like,1,1,1.0
performance measure like accuracy,1,1,1.0
measure like accuracy directly,1,1,1.0
like accuracy directly auc,1,1,1.0
accuracy directly auc does,1,1,1.0
directly auc does not,1,1,1.0
auc does not place,1,1,1.0
does not place more,1,1,1.0
not place more emphasis,1,1,1.0
place more emphasis on,1,1,1.0
more emphasis on one,1,1,1.0
emphasis on one class,1,1,1.0
on one class over,1,1,1.0
one class over the,1,1,1.0
class over the other,1,1,1.0
over the other so,1,1,1.0
the other so it,1,1,1.0
other so it is,1,1,1.0
it is not biased,1,1,1.0
is not biased against,1,1,1.0
not biased against the,1,1,1.0
biased against the minority,1,1,1.0
against the minority class,1,1,1.0
the minority class tnfpfntptntpaccuracy,1,1,1.0
minority class tnfpfntptntpaccuracy pr,1,1,1.0
class tnfpfntptntpaccuracy pr fptptpecison,1,1,1.0
tnfpfntptntpaccuracy pr fptptpecison re,1,1,1.0
pr fptptpecison re fntptpcall,1,1,1.0
fptptpecison re fntptpcall tnfpfpratefp,1,1,1.0
re fntptpcall tnfpfpratefp fntptpratetp,1,1,1.0
fntptpcall tnfpfpratefp fntptpratetp ecisioncall,1,1,1.0
tnfpfpratefp fntptpratetp ecisioncall ecisioncallf,1,1,1.0
fntptpratetp ecisioncall ecisioncallf value,1,1,1.0
ecisioncall ecisioncallf value prre,1,1,1.0
ecisioncallf value prre pr,1,1,1.0
value prre pr re,1,1,1.0
prre pr re β,1,1,1.0
pr re β β,1,1,1.0
re β β accuracyaccuracymgm,1,1,1.0
β β accuracyaccuracymgm accuracyaccuracyms,1,1,1.0
β accuracyaccuracymgm accuracyaccuracyms evaluation,1,1,1.0
accuracyaccuracymgm accuracyaccuracyms evaluation metrics,1,1,1.0
accuracyaccuracyms evaluation metrics based,1,1,1.0
evaluation metrics based on,1,1,1.0
on confusion matrix besides,1,1,1.0
confusion matrix besides minimum,1,1,1.0
matrix besides minimum cost,1,1,1.0
besides minimum cost criterion,1,1,1.0
minimum cost criterion is,1,1,1.0
cost criterion is also,1,1,1.0
criterion is also used,1,1,1.0
is also used to,1,1,1.0
also used to evaluate,1,1,1.0
used to evaluate the,1,1,1.0
to evaluate the performance,1,1,1.0
evaluate the performance of,1,1,1.0
performance of classifiers in,1,1,1.0
of classifiers in learning,1,1,1.0
classifiers in learning from,1,1,1.0
in learning from imbalanced,3,2,1.5
imbalanced data sets when,1,1,1.0
data sets when performing,1,1,1.0
sets when performing learning,1,1,1.0
when performing learning when,1,1,1.0
performing learning when applying,1,1,1.0
learning when applying machine,1,1,1.0
when applying machine learning,1,1,1.0
applying machine learning algorithms,1,1,1.0
machine learning algorithms to,1,1,1.0
learning algorithms to real,1,1,1.0
algorithms to real world,1,1,1.0
to real world applications,1,1,1.0
real world applications rarely,1,1,1.0
world applications rarely would,1,1,1.0
applications rarely would one,1,1,1.0
rarely would one or,1,1,1.0
would one or more,1,1,1.0
one or more of,2,2,1.0
or more of these,1,1,1.0
more of these assumptions,1,1,1.0
of these assumptions hold,1,1,1.0
these assumptions hold but,1,1,1.0
assumptions hold but to,1,1,1.0
hold but to select,1,1,1.0
but to select a,1,1,1.0
to select a classifier,2,1,2.0
select a classifier certain,1,1,1.0
a classifier certain conditions,1,1,1.0
classifier certain conditions must,1,1,1.0
certain conditions must exist,1,1,1.0
conditions must exist and,1,1,1.0
must exist and we,1,1,1.0
exist and we may,1,1,1.0
and we may need,1,1,1.0
we may need more,1,1,1.0
may need more information,1,1,1.0
need more information if,1,1,1.0
more information if one,1,1,1.0
information if one roc,1,1,1.0
if one roc curve,1,1,1.0
one roc curve dominates,1,1,1.0
roc curve dominates all,1,1,1.0
curve dominates all others,1,1,1.0
dominates all others then,1,1,1.0
all others then the,1,1,1.0
others then the best,1,1,1.0
then the best method,1,1,1.0
the best method is,2,2,1.0
best method is the,1,1,1.0
method is the one,1,1,1.0
is the one that,1,1,1.0
the one that produced,1,1,1.0
one that produced the,1,1,1.0
that produced the dominant,1,1,1.0
produced the dominant curve,1,1,1.0
the dominant curve which,1,1,1.0
dominant curve which is,1,1,1.0
curve which is also,1,1,1.0
which is also the,1,1,1.0
is also the curve,1,1,1.0
also the curve with,1,1,1.0
the curve with the,1,1,1.0
curve with the largest,1,1,1.0
with the largest area,1,1,1.0
the largest area with,1,1,1.0
largest area with maximum,1,1,1.0
area with maximum auc,1,1,1.0
with maximum auc to,1,1,1.0
maximum auc to select,1,1,1.0
auc to select a,1,1,1.0
select a classifier from,1,1,1.0
a classifier from the,1,1,1.0
classifier from the dominant,1,1,1.0
from the dominant curve,1,1,1.0
the dominant curve we,1,1,1.0
dominant curve we need,1,1,1.0
curve we need additional,1,1,1.0
we need additional information,1,1,1.0
need additional information such,1,1,1.0
additional information such as,1,1,1.0
information such as a,1,1,1.0
such as a target,1,1,1.0
as a target fp,1,1,1.0
a target fp rate,1,1,1.0
target fp rate on,1,1,1.0
fp rate on the,1,1,1.0
rate on the other,1,1,1.0
the other hand if,1,1,1.0
other hand if multiple,1,1,1.0
hand if multiple curves,1,1,1.0
if multiple curves dominate,1,1,1.0
multiple curves dominate in,1,1,1.0
curves dominate in different,1,1,1.0
dominate in different parts,1,1,1.0
in different parts of,1,1,1.0
different parts of the,1,1,1.0
parts of the roc,1,1,1.0
of the roc space,1,1,1.0
the roc space then,1,1,1.0
roc space then we,1,1,1.0
space then we can,1,1,1.0
then we can use,1,1,1.0
we can use the,1,1,1.0
can use the roc,1,1,1.0
use the roc convex,1,1,1.0
the roc convex hull,1,1,1.0
roc convex hull method,1,1,1.0
convex hull method to,1,1,1.0
hull method to select,1,1,1.0
method to select the,1,1,1.0
to select the optimal,1,1,1.0
select the optimal classifier,1,1,1.0
the optimal classifier relations,1,1,1.0
optimal classifier relations to,1,1,1.0
classifier relations to other,1,1,1.0
relations to other problems,1,1,1.0
to other problems it,1,1,1.0
other problems it has,1,1,1.0
problems it has also,1,1,1.0
it has also been,1,1,1.0
has also been observed,1,1,1.0
also been observed that,1,1,1.0
been observed that in,1,1,1.0
observed that in some,1,1,1.0
that in some domains,1,1,1.0
in some domains for,1,1,1.0
some domains for example,1,1,1.0
domains for example the,1,1,1.0
for example the sick,1,1,1.0
example the sick data,1,1,1.0
the sick data set,1,1,1.0
sick data set standard,1,1,1.0
data set standard machine,1,1,1.0
set standard machine learning,1,1,1.0
machine learning algorithms are,1,1,1.0
learning algorithms are capable,1,1,1.0
algorithms are capable of,1,1,1.0
are capable of inducing,1,1,1.0
capable of inducing good,1,1,1.0
of inducing good classifiers,1,1,1.0
inducing good classifiers even,1,1,1.0
good classifiers even using,1,1,1.0
classifiers even using highly,1,1,1.0
even using highly imbalanced,1,1,1.0
using highly imbalanced training,1,1,1.0
highly imbalanced training sets,1,1,1.0
imbalanced training sets this,1,1,1.0
training sets this shows,1,1,1.0
sets this shows that,1,1,1.0
this shows that class,1,1,1.0
shows that class imbalance,1,1,1.0
that class imbalance is,1,1,1.0
imbalance is not the,2,1,2.0
is not the only,3,2,1.5
not the only problem,2,1,2.0
the only problem responsible,1,1,1.0
only problem responsible for,1,1,1.0
problem responsible for the,1,1,1.0
responsible for the decrease,1,1,1.0
for the decrease in,1,1,1.0
the decrease in performance,1,1,1.0
decrease in performance of,1,1,1.0
in performance of learning,1,1,1.0
performance of learning algorithms,2,2,1.0
of learning algorithms class,1,1,1.0
learning algorithms class imbalance,1,1,1.0
algorithms class imbalance is,1,1,1.0
the only problem to,1,1,1.0
only problem to contend,1,1,1.0
problem to contend with,1,1,1.0
to contend with besides,1,1,1.0
contend with besides the,1,1,1.0
with besides the distributions,1,1,1.0
besides the distributions within,1,1,1.0
the distributions within each,1,1,1.0
distributions within each class,1,1,1.0
within each class of,1,1,1.0
each class of the,1,1,1.0
class of the data,1,1,1.0
of the data within,1,1,1.0
the data within class,1,1,1.0
data within class imbalance,1,1,1.0
within class imbalance are,1,1,1.0
class imbalance are also,1,1,1.0
imbalance are also relevant,1,1,1.0
are also relevant it,1,1,1.0
also relevant it was,1,1,1.0
relevant it was found,1,1,1.0
it was found that,1,1,1.0
was found that in,1,1,1.0
found that in certain,1,1,1.0
that in certain cases,1,1,1.0
in certain cases addressing,1,1,1.0
certain cases addressing the,1,1,1.0
cases addressing the small,1,1,1.0
addressing the small disjuncts,1,1,1.0
the small disjuncts problem,1,1,1.0
small disjuncts problem with,1,1,1.0
disjuncts problem with regardless,1,1,1.0
problem with regardless of,1,1,1.0
with regardless of the,1,1,1.0
regardless of the class,1,1,1.0
imbalance problem was sufficient,1,1,1.0
problem was sufficient to,1,1,1.0
was sufficient to increase,1,1,1.0
sufficient to increase performance,1,1,1.0
to increase performance experiments,1,1,1.0
increase performance experiments by,1,1,1.0
performance experiments by jo,1,1,1.0
experiments by jo and,1,1,1.0
by jo and japkowicz,1,1,1.0
jo and japkowicz suggested,1,1,1.0
and japkowicz suggested that,1,1,1.0
japkowicz suggested that the,1,1,1.0
suggested that the problem,2,1,2.0
that the problem is,2,1,2.0
the problem is not,2,1,2.0
problem is not directly,2,1,2.0
is not directly caused,2,1,2.0
not directly caused by,2,1,2.0
directly caused by class,2,1,2.0
caused by class imbalances,1,1,1.0
by class imbalances but,1,1,1.0
class imbalances but rather,1,1,1.0
imbalances but rather that,1,1,1.0
but rather that class,2,1,2.0
rather that class imbalances,1,1,1.0
that class imbalances may,1,1,1.0
class imbalances may yield,1,1,1.0
imbalances may yield small,1,1,1.0
may yield small disjuncts,2,1,2.0
yield small disjuncts which,2,1,2.0
small disjuncts which in,2,1,2.0
disjuncts which in turn,2,1,2.0
which in turn will,2,1,2.0
in turn will cause,2,1,2.0
turn will cause degradation,2,1,2.0
will cause degradation a,1,1,1.0
cause degradation a approach,1,1,1.0
degradation a approach was,1,1,1.0
a approach was proposed,1,1,1.0
approach was proposed whose,1,1,1.0
was proposed whose idea,1,1,1.0
proposed whose idea is,1,1,1.0
whose idea is to,1,1,1.0
idea is to consider,1,1,1.0
is to consider not,1,1,1.0
to consider not only,1,1,1.0
consider not only the,1,1,1.0
not only the imbalance,1,1,1.0
only the imbalance but,1,1,1.0
the imbalance but also,1,1,1.0
imbalance but also the,1,1,1.0
but also the class,1,1,1.0
also the class imbalance,1,1,1.0
class imbalance and to,1,1,1.0
imbalance and to the,1,1,1.0
and to the dataset,1,1,1.0
to the dataset by,1,1,1.0
the dataset by rectifying,1,1,1.0
dataset by rectifying these,1,1,1.0
by rectifying these two,1,1,1.0
rectifying these two types,1,1,1.0
these two types of,1,1,1.0
two types of imbalances,1,1,1.0
types of imbalances simultaneously,1,1,1.0
of imbalances simultaneously the,1,1,1.0
imbalances simultaneously the experiments,1,1,1.0
simultaneously the experiments results,1,1,1.0
the experiments results of,1,1,1.0
experiments results of prati,1,1,1.0
results of prati et,1,1,1.0
of prati et al,1,1,1.0
prati et al using,1,1,1.0
et al using a,1,1,1.0
al using a inductive,1,1,1.0
using a inductive scheme,1,1,1.0
a inductive scheme suggested,1,1,1.0
inductive scheme suggested that,1,1,1.0
scheme suggested that the,1,1,1.0
problem is not solely,1,1,1.0
is not solely caused,1,1,1.0
not solely caused by,1,1,1.0
solely caused by class,1,1,1.0
caused by class imbalance,2,1,2.0
by class imbalance but,2,1,2.0
class imbalance but is,1,1,1.0
imbalance but is also,1,1,1.0
but is also related,1,1,1.0
is also related to,1,1,1.0
also related to the,1,1,1.0
related to the degree,1,1,1.0
to the degree of,1,1,1.0
the degree of data,1,1,1.0
degree of data overlapping,1,1,1.0
of data overlapping among,1,1,1.0
data overlapping among the,1,1,1.0
overlapping among the classes,1,1,1.0
among the classes it,1,1,1.0
the classes it was,1,1,1.0
classes it was also,1,1,1.0
it was also found,1,1,1.0
was also found that,1,1,1.0
also found that data,1,1,1.0
found that data duplication,1,1,1.0
that data duplication is,1,1,1.0
data duplication is generally,1,1,1.0
duplication is generally harmful,1,1,1.0
is generally harmful although,1,1,1.0
generally harmful although for,1,1,1.0
harmful although for classifiers,1,1,1.0
although for classifiers such,1,1,1.0
for classifiers such as,1,1,1.0
classifiers such as naive,1,1,1.0
such as naive bayes,1,1,1.0
as naive bayes and,1,1,1.0
naive bayes and perceptrons,1,1,1.0
bayes and perceptrons with,1,1,1.0
and perceptrons with margins,1,1,1.0
perceptrons with margins high,1,1,1.0
with margins high degrees,1,1,1.0
margins high degrees of,1,1,1.0
high degrees of duplication,1,1,1.0
degrees of duplication are,1,1,1.0
of duplication are necessary,1,1,1.0
duplication are necessary to,1,1,1.0
are necessary to harm,1,1,1.0
necessary to harm classification,1,1,1.0
to harm classification it,1,1,1.0
harm classification it was,1,1,1.0
classification it was argued,1,1,1.0
it was argued that,2,1,2.0
was argued that the,2,1,2.0
argued that the reason,1,1,1.0
that the reason why,1,1,1.0
the reason why class,1,1,1.0
reason why class imbalances,1,1,1.0
why class imbalances and,1,1,1.0
class imbalances and overlapping,1,1,1.0
imbalances and overlapping classes,1,1,1.0
and overlapping classes are,1,1,1.0
overlapping classes are related,1,1,1.0
classes are related is,1,1,1.0
are related is that,1,1,1.0
related is that misclassification,1,1,1.0
is that misclassification often,1,1,1.0
that misclassification often occurs,1,1,1.0
misclassification often occurs near,1,1,1.0
often occurs near class,1,1,1.0
occurs near class boundaries,1,1,1.0
near class boundaries where,1,1,1.0
class boundaries where overlap,1,1,1.0
boundaries where overlap usually,1,1,1.0
where overlap usually occurs,1,1,1.0
overlap usually occurs as,1,1,1.0
usually occurs as well,1,1,1.0
occurs as well weiss,1,1,1.0
as well weiss investigated,1,1,1.0
well weiss investigated the,1,1,1.0
weiss investigated the relation,1,1,1.0
investigated the relation between,1,1,1.0
between class imbalance and,1,1,1.0
class imbalance and training,1,1,1.0
imbalance and training set,1,1,1.0
and training set size,1,1,1.0
training set size experiments,1,1,1.0
set size experiments showed,1,1,1.0
size experiments showed that,1,1,1.0
experiments showed that while,1,1,1.0
showed that while the,1,1,1.0
that while the position,1,1,1.0
while the position of,1,1,1.0
the position of the,1,1,1.0
position of the best,1,1,1.0
of the best class,1,1,1.0
the best class distributions,1,1,1.0
best class distributions varies,1,1,1.0
class distributions varies somewhat,1,1,1.0
distributions varies somewhat with,1,1,1.0
varies somewhat with size,1,1,1.0
somewhat with size in,1,1,1.0
with size in many,1,1,1.0
size in many cases,1,1,1.0
in many cases especially,1,1,1.0
many cases especially with,1,1,1.0
cases especially with error,1,1,1.0
especially with error rate,1,1,1.0
with error rate the,1,1,1.0
error rate the variation,1,1,1.0
rate the variation is,1,1,1.0
the variation is small,1,1,1.0
variation is small which,1,1,1.0
is small which gives,1,1,1.0
small which gives support,1,1,1.0
which gives support to,1,1,1.0
gives support to the,1,1,1.0
support to the notion,1,1,1.0
to the notion that,1,1,1.0
the notion that there,1,1,1.0
notion that there is,1,1,1.0
that there is a,1,1,1.0
there is a best,1,1,1.0
is a best marginal,1,1,1.0
a best marginal class,1,1,1.0
best marginal class distribution,1,1,1.0
marginal class distribution for,1,1,1.0
class distribution for a,1,1,1.0
distribution for a learning,1,1,1.0
for a learning task,1,1,1.0
a learning task the,1,1,1.0
learning task the results,1,1,1.0
task the results also,1,1,1.0
the results also indicated,1,1,1.0
results also indicated that,1,1,1.0
also indicated that for,1,1,1.0
indicated that for any,1,1,1.0
that for any fixed,1,1,1.0
for any fixed class,1,1,1.0
any fixed class distribution,1,1,1.0
fixed class distribution increasing,1,1,1.0
class distribution increasing the,1,1,1.0
distribution increasing the size,1,1,1.0
increasing the size of,1,1,1.0
the size of the,3,3,1.0
size of the training,2,2,1.0
of the training set,2,2,1.0
the training set always,1,1,1.0
training set always leads,1,1,1.0
set always leads to,1,1,1.0
always leads to improved,1,1,1.0
leads to improved classifier,1,1,1.0
to improved classifier performance,1,1,1.0
improved classifier performance conclusion,1,1,1.0
classifier performance conclusion learning,1,1,1.0
performance conclusion learning from,1,1,1.0
conclusion learning from imbalanced,1,1,1.0
data sets is an,1,1,1.0
sets is an important,1,1,1.0
is an important issue,1,1,1.0
an important issue in,1,1,1.0
important issue in machine,1,1,1.0
issue in machine learning,1,1,1.0
in machine learning a,1,1,1.0
machine learning a direct,1,1,1.0
learning a direct method,1,1,1.0
a direct method to,1,1,1.0
direct method to solve,1,1,1.0
method to solve the,1,1,1.0
to solve the imbalance,1,1,1.0
solve the imbalance problem,1,1,1.0
the imbalance problem is,1,1,1.0
imbalance problem is artificially,1,1,1.0
problem is artificially balancing,1,1,1.0
is artificially balancing the,1,1,1.0
artificially balancing the class,1,1,1.0
balancing the class distributions,1,1,1.0
the class distributions and,1,1,1.0
class distributions and its,1,1,1.0
distributions and its effectiveness,1,1,1.0
and its effectiveness has,1,1,1.0
its effectiveness has been,1,1,1.0
effectiveness has been empirically,1,1,1.0
has been empirically analyzed,1,1,1.0
been empirically analyzed in,1,1,1.0
empirically analyzed in however,1,1,1.0
analyzed in however there,1,1,1.0
in however there is,1,1,1.0
however there is some,1,1,1.0
there is some evidence,1,1,1.0
is some evidence that,1,1,1.0
some evidence that the,1,1,1.0
evidence that the class,1,1,1.0
that the class distributions,1,1,1.0
the class distributions artificially,1,1,1.0
class distributions artificially does,1,1,1.0
distributions artificially does not,1,1,1.0
artificially does not have,1,1,1.0
does not have much,1,1,1.0
not have much effect,1,1,1.0
have much effect on,1,1,1.0
much effect on the,1,1,1.0
effect on the performance,1,1,1.0
on the performance of,2,2,1.0
performance of the induced,1,1,1.0
of the induced classifier,1,1,1.0
the induced classifier since,1,1,1.0
induced classifier since some,1,1,1.0
classifier since some learning,1,1,1.0
since some learning systems,1,1,1.0
some learning systems are,1,1,1.0
learning systems are not,1,1,1.0
systems are not sensitive,1,1,1.0
are not sensitive to,1,1,1.0
not sensitive to differences,1,1,1.0
sensitive to differences in,1,1,1.0
to differences in class,1,1,1.0
differences in class distributions,1,1,1.0
in class distributions it,1,1,1.0
class distributions it seems,1,1,1.0
distributions it seems that,1,1,1.0
it seems that we,1,1,1.0
seems that we still,1,1,1.0
that we still need,1,1,1.0
we still need a,1,1,1.0
still need a clearer,1,1,1.0
need a clearer and,1,1,1.0
a clearer and deeper,1,1,1.0
clearer and deeper understanding,1,1,1.0
and deeper understanding of,1,1,1.0
deeper understanding of how,1,1,1.0
understanding of how class,1,1,1.0
of how class distribution,1,1,1.0
how class distribution affects,1,1,1.0
class distribution affects each,1,1,1.0
distribution affects each phase,1,1,1.0
affects each phase of,1,1,1.0
each phase of the,1,1,1.0
phase of the learning,1,1,1.0
of the learning process,1,1,1.0
the learning process for,1,1,1.0
learning process for more,1,1,1.0
process for more learners,1,1,1.0
for more learners except,1,1,1.0
more learners except decision,1,1,1.0
learners except decision trees,1,1,1.0
except decision trees a,1,1,1.0
decision trees a deeper,1,1,1.0
trees a deeper understanding,1,1,1.0
a deeper understanding of,1,1,1.0
deeper understanding of the,1,1,1.0
understanding of the basics,1,1,1.0
of the basics will,1,1,1.0
the basics will help,1,1,1.0
basics will help us,1,1,1.0
will help us to,1,1,1.0
help us to design,1,1,1.0
us to design better,1,1,1.0
to design better methods,1,1,1.0
design better methods for,1,1,1.0
better methods for dealing,1,1,1.0
methods for dealing with,1,1,1.0
dealing with the problem,1,1,1.0
the problem of learning,1,1,1.0
problem of learning with,1,1,1.0
of learning with skewed,1,1,1.0
skewed class distributions as,1,1,1.0
class distributions as is,1,1,1.0
distributions as is stated,1,1,1.0
as is stated in,1,1,1.0
is stated in section,1,1,1.0
stated in section some,1,1,1.0
in section some data,1,1,1.0
section some data sets,1,1,1.0
some data sets are,1,1,1.0
data sets are immune,1,1,1.0
sets are immune to,1,1,1.0
are immune to class,1,1,1.0
immune to class imbalance,1,1,1.0
to class imbalance problem,1,1,1.0
class imbalance problem it,1,1,1.0
imbalance problem it was,1,1,1.0
problem it was argued,1,1,1.0
argued that the class,1,1,1.0
that the class imbalance,1,1,1.0
imbalance problem is not,1,1,1.0
class imbalance but rather,1,1,1.0
imbalance but rather that,1,1,1.0
rather that class imbalance,1,1,1.0
that class imbalance may,1,1,1.0
class imbalance may yield,1,1,1.0
imbalance may yield small,1,1,1.0
will cause degradation though,1,1,1.0
cause degradation though maximum,1,1,1.0
degradation though maximum specification,1,1,1.0
though maximum specification bias,1,1,1.0
maximum specification bias in,1,1,1.0
specification bias in induction,1,1,1.0
bias in induction processes,1,1,1.0
in induction processes and,1,1,1.0
induction processes and dealing,1,1,1.0
processes and dealing with,1,1,1.0
and dealing with the,1,1,1.0
the problems of within,1,1,1.0
problems of within class,1,1,1.0
of within class imbalance,1,1,1.0
class imbalance and between,1,1,1.0
imbalance and between class,1,1,1.0
and between class imbalance,1,1,1.0
between class imbalance have,1,1,1.0
class imbalance have present,1,1,1.0
imbalance have present their,1,1,1.0
have present their effectiveness,1,1,1.0
present their effectiveness according,1,1,1.0
their effectiveness according to,1,1,1.0
effectiveness according to minority,1,1,1.0
according to minority class,1,1,1.0
to minority class more,1,1,1.0
minority class more effective,1,1,1.0
class more effective methods,1,1,1.0
more effective methods are,1,1,1.0
effective methods are needed,1,1,1.0
methods are needed current,1,1,1.0
are needed current researches,1,1,1.0
needed current researches on,1,1,1.0
current researches on small,1,1,1.0
researches on small disjuncts,1,1,1.0
on small disjuncts are,1,1,1.0
small disjuncts are ad,1,1,1.0
disjuncts are ad hoc,1,1,1.0
are ad hoc so,1,1,1.0
ad hoc so standard,1,1,1.0
hoc so standard metrics,1,1,1.0
so standard metrics for,1,1,1.0
standard metrics for the,1,1,1.0
metrics for the degree,1,1,1.0
for the degree of,1,1,1.0
the degree of small,1,1,1.0
degree of small disjuncts,1,1,1.0
of small disjuncts are,1,1,1.0
small disjuncts are deadly,1,1,1.0
disjuncts are deadly in,1,1,1.0
are deadly in need,1,1,1.0
deadly in need since,1,1,1.0
in need since machine,1,1,1.0
need since machine learning,1,1,1.0
since machine learning is,1,1,1.0
machine learning is an,1,1,1.0
learning is an science,1,1,1.0
is an science and,1,1,1.0
an science and the,1,1,1.0
science and the class,1,1,1.0
and the class imbalance,1,1,1.0
imbalance problem and some,1,1,1.0
problem and some other,1,1,1.0
some other related ones,1,1,1.0
other related ones are,1,1,1.0
related ones are of,1,1,1.0
ones are of nature,1,1,1.0
are of nature realizing,1,1,1.0
of nature realizing to,1,1,1.0
nature realizing to explore,1,1,1.0
realizing to explore idiographic,1,1,1.0
to explore idiographic solutions,1,1,1.0
explore idiographic solutions for,1,1,1.0
idiographic solutions for specific,1,1,1.0
solutions for specific applications,1,1,1.0
for specific applications is,1,1,1.0
specific applications is very,1,1,1.0
applications is very important,1,1,1.0
is very important and,1,1,1.0
very important and valuable,1,1,1.0
important and valuable for,1,1,1.0
and valuable for practitioners,1,1,1.0
valuable for practitioners and,1,1,1.0
for practitioners and a,1,1,1.0
practitioners and a better,1,1,1.0
and a better data,1,1,1.0
a better data understanding,1,1,1.0
better data understanding and,1,1,1.0
data understanding and more,1,1,1.0
understanding and more knowledge,1,1,1.0
and more knowledge on,1,1,1.0
more knowledge on the,1,1,1.0
knowledge on the domain,1,1,1.0
on the domain will,1,1,1.0
the domain will be,1,1,1.0
domain will be helpful,1,1,1.0
will be helpful in,1,1,1.0
be helpful in the,1,1,1.0
helpful in the process,1,1,1.0
in the process references,1,1,1.0
the process references kotsiantis,1,1,1.0
process references kotsiantis kanellopoulos,1,1,1.0
references kotsiantis kanellopoulos and,1,1,1.0
kotsiantis kanellopoulos and pintelas,1,1,1.0
kanellopoulos and pintelas handling,1,1,1.0
and pintelas handling imbalanced,1,1,1.0
pintelas handling imbalanced datasets,1,1,1.0
handling imbalanced datasets a,1,1,1.0
imbalanced datasets a review,1,1,1.0
datasets a review gests,1,1,1.0
a review gests international,1,1,1.0
review gests international transactions,1,1,1.0
gests international transactions on,1,1,1.0
international transactions on computer,1,1,1.0
transactions on computer science,1,1,1.0
science and engineering pp,1,1,1.0
and engineering pp visa,1,1,1.0
engineering pp visa and,1,1,1.0
pp visa and ralescu,1,1,1.0
visa and ralescu issues,1,1,1.0
and ralescu issues in,1,1,1.0
ralescu issues in mining,1,1,1.0
issues in mining imbalanced,1,1,1.0
mining imbalanced data review,1,1,1.0
imbalanced data review paper,1,1,1.0
data review paper in,1,1,1.0
review paper in proceedings,1,1,1.0
paper in proceedings of,1,1,1.0
proceedings of the sixteen,1,1,1.0
of the sixteen midwest,1,1,1.0
the sixteen midwest artificial,1,1,1.0
sixteen midwest artificial intelligence,1,1,1.0
midwest artificial intelligence and,1,1,1.0
artificial intelligence and cognitive,1,1,1.0
intelligence and cognitive science,1,1,1.0
and cognitive science conference,1,1,1.0
cognitive science conference dayton,1,1,1.0
science conference dayton pp,1,1,1.0
conference dayton pp monard,1,1,1.0
dayton pp monard and,1,1,1.0
pp monard and batista,1,1,1.0
monard and batista learning,1,1,1.0
and batista learning with,1,1,1.0
batista learning with skewed,1,1,1.0
with skewed class distribution,1,1,1.0
skewed class distribution in,1,1,1.0
class distribution in advances,1,1,1.0
distribution in advances in,1,1,1.0
in advances in logic,1,1,1.0
advances in logic artificial,1,1,1.0
in logic artificial intelligence,1,1,1.0
logic artificial intelligence and,1,1,1.0
artificial intelligence and robotics,1,1,1.0
intelligence and robotics sao,1,1,1.0
and robotics sao paulo,1,1,1.0
robotics sao paulo sp,1,1,1.0
sao paulo sp ios,1,1,1.0
paulo sp ios press,1,1,1.0
sp ios press pp,1,1,1.0
ios press pp weiss,1,1,1.0
press pp weiss mining,1,1,1.0
pp weiss mining with,2,2,1.0
weiss mining with rarity,2,2,1.0
mining with rarity a,2,2,1.0
with rarity a unifying,2,2,1.0
rarity a unifying framework,2,2,1.0
a unifying framework sigkdd,1,1,1.0
unifying framework sigkdd explorations,1,1,1.0
framework sigkdd explorations pp,1,1,1.0
sigkdd explorations pp maloof,1,1,1.0
explorations pp maloof learning,1,1,1.0
pp maloof learning when,2,2,1.0
maloof learning when data,2,2,1.0
learning when data sets,2,2,1.0
when data sets are,2,2,1.0
data sets are imbalanced,2,2,1.0
sets are imbalanced and,2,2,1.0
are imbalanced and when,2,2,1.0
imbalanced and when costs,2,2,1.0
and when costs are,2,2,1.0
when costs are unequal,2,2,1.0
costs are unequal and,2,2,1.0
are unequal and unknown,2,2,1.0
unequal and unknown in,2,2,1.0
and unknown in proceedings,1,1,1.0
unknown in proceedings of,1,1,1.0
proceedings of the icml,2,1,2.0
of the icml workshop,2,1,2.0
the icml workshop on,2,1,2.0
icml workshop on learning,4,1,4.0
workshop on learning from,5,1,5.0
imbalanced data sets ii,9,2,4.5
data sets ii pp,4,2,2.0
sets ii pp stone,1,1,1.0
ii pp stone and,1,1,1.0
pp stone and olshen,1,1,1.0
stone and olshen classification,1,1,1.0
and olshen classification and,1,1,1.0
olshen classification and regression,1,1,1.0
classification and regression trees,1,1,1.0
and regression trees chapman,1,1,1.0
regression trees chapman and,1,1,1.0
trees chapman and press,1,1,1.0
chapman and press japkowicz,1,1,1.0
and press japkowicz class,1,1,1.0
press japkowicz class imbalances,1,1,1.0
japkowicz class imbalances are,2,2,1.0
class imbalances are we,2,2,1.0
imbalances are we focusing,2,2,1.0
are we focusing on,2,2,1.0
we focusing on the,2,2,1.0
focusing on the right,2,2,1.0
on the right issue,2,2,1.0
the right issue proceedings,1,1,1.0
right issue proceedings of,1,1,1.0
issue proceedings of the,1,1,1.0
proceedings of the workshop,1,1,1.0
of the workshop learning,1,1,1.0
the workshop learning with,1,1,1.0
workshop learning with imbalanced,1,1,1.0
learning with imbalanced data,2,2,1.0
sets ii pp laurikkala,1,1,1.0
ii pp laurikkala improving,1,1,1.0
pp laurikkala improving identification,1,1,1.0
laurikkala improving identification of,1,1,1.0
improving identification of difficult,1,1,1.0
identification of difficult small,1,1,1.0
of difficult small classes,1,1,1.0
difficult small classes by,1,1,1.0
small classes by balancing,1,1,1.0
classes by balancing class,1,1,1.0
by balancing class distribution,1,1,1.0
balancing class distribution technical,1,1,1.0
class distribution technical report,1,1,1.0
distribution technical report university,1,1,1.0
technical report university of,1,1,1.0
report university of tampere,1,1,1.0
university of tampere guo,1,1,1.0
of tampere guo and,1,1,1.0
tampere guo and herna,1,1,1.0
guo and herna learning,1,1,1.0
and herna learning from,1,1,1.0
herna learning from imbalanced,1,1,1.0
imbalanced data sets with,2,2,1.0
data sets with boosting,2,2,1.0
sets with boosting and,2,2,1.0
with boosting and data,2,2,1.0
boosting and data generation,2,2,1.0
and data generation the,2,2,1.0
data generation the data,1,1,1.0
generation the data boosting,1,1,1.0
the data boosting approach,1,1,1.0
data boosting approach sigkdd,1,1,1.0
boosting approach sigkdd explorations,1,1,1.0
approach sigkdd explorations pp,1,1,1.0
sigkdd explorations pp weiss,1,1,1.0
explorations pp weiss the,1,1,1.0
pp weiss the effect,1,1,1.0
weiss the effect of,1,1,1.0
the effect of small,1,1,1.0
effect of small disjuncts,1,1,1.0
of small disjuncts and,1,1,1.0
small disjuncts and class,1,1,1.0
disjuncts and class distribution,1,1,1.0
and class distribution on,1,1,1.0
class distribution on decision,1,1,1.0
distribution on decision tree,1,1,1.0
on decision tree learning,1,1,1.0
decision tree learning dissertation,1,1,1.0
tree learning dissertation department,1,1,1.0
learning dissertation department of,1,1,1.0
dissertation department of computer,1,1,1.0
of computer science rutgers,1,1,1.0
computer science rutgers university,1,1,1.0
science rutgers university new,1,1,1.0
rutgers university new brunswick,1,1,1.0
university new brunswick new,1,1,1.0
new brunswick new jersey,1,1,1.0
brunswick new jersey may,1,1,1.0
new jersey may japkowicz,1,1,1.0
jersey may japkowicz learning,1,1,1.0
may japkowicz learning from,1,1,1.0
japkowicz learning from imbalanced,2,2,1.0
imbalanced data sets a,2,2,1.0
data sets a comparison,2,2,1.0
sets a comparison of,2,2,1.0
a comparison of various,2,2,1.0
comparison of various strategies,2,2,1.0
of various strategies aaai,1,1,1.0
various strategies aaai workshop,1,1,1.0
strategies aaai workshop on,1,1,1.0
aaai workshop on learning,1,1,1.0
imbalanced data sets menlo,1,1,1.0
data sets menlo park,1,1,1.0
sets menlo park ca,1,1,1.0
menlo park ca aaai,2,2,1.0
park ca aaai press,2,2,1.0
ca aaai press kotsiantis,1,1,1.0
aaai press kotsiantis and,1,1,1.0
press kotsiantis and pintelas,1,1,1.0
kotsiantis and pintelas mixture,1,1,1.0
and pintelas mixture of,1,1,1.0
pintelas mixture of expert,1,1,1.0
mixture of expert agents,1,1,1.0
of expert agents for,1,1,1.0
expert agents for handling,1,1,1.0
agents for handling imbalanced,1,1,1.0
for handling imbalanced data,2,2,1.0
handling imbalanced data sets,1,1,1.0
imbalanced data sets annals,1,1,1.0
data sets annals of,1,1,1.0
sets annals of mathematics,1,1,1.0
annals of mathematics computing,1,1,1.0
of mathematics computing teleinformatics,1,1,1.0
mathematics computing teleinformatics vol,1,1,1.0
computing teleinformatics vol pp,1,1,1.0
teleinformatics vol pp hart,1,1,1.0
vol pp hart the,1,1,1.0
pp hart the condensed,1,1,1.0
hart the condensed nearest,1,1,1.0
the condensed nearest neighbor,1,1,1.0
nearest neighbor rule ieee,1,1,1.0
neighbor rule ieee transactions,1,1,1.0
rule ieee transactions on,1,1,1.0
ieee transactions on information,2,2,1.0
transactions on information theory,2,2,1.0
on information theory pp,1,1,1.0
information theory pp kubat,1,1,1.0
theory pp kubat and,1,1,1.0
pp kubat and matwin,2,2,1.0
kubat and matwin addressing,2,2,1.0
and matwin addressing the,2,2,1.0
matwin addressing the curse,2,2,1.0
addressing the curse of,2,2,1.0
the curse of imbalanced,3,3,1.0
curse of imbalanced training,3,3,1.0
of imbalanced training sets,3,3,1.0
imbalanced training sets one,1,1,1.0
training sets one sided,1,1,1.0
sets one sided selection,1,1,1.0
one sided selection in,1,1,1.0
sided selection in proceedings,1,1,1.0
selection in proceedings of,2,2,1.0
proceedings of the fourteenth,2,1,2.0
of the fourteenth international,1,1,1.0
the fourteenth international conference,1,1,1.0
fourteenth international conference on,1,1,1.0
on machine learning nashville,2,2,1.0
machine learning nashville tennesse,1,1,1.0
learning nashville tennesse morgan,1,1,1.0
nashville tennesse morgan kaufmann,1,1,1.0
tennesse morgan kaufmann pp,1,1,1.0
morgan kaufmann pp chawla,1,1,1.0
kaufmann pp chawla hall,1,1,1.0
pp chawla hall bowyer,1,1,1.0
chawla hall bowyer and,1,1,1.0
hall bowyer and kegelmeyer,1,1,1.0
bowyer and kegelmeyer smote,1,1,1.0
and kegelmeyer smote synthetic,2,2,1.0
minority oversampling technique journal,1,1,1.0
oversampling technique journal of,1,1,1.0
artificial intelligence research pp,2,1,2.0
intelligence research pp han,2,1,2.0
research pp han wang,2,1,2.0
pp han wang and,2,2,1.0
han wang and mao,2,2,1.0
wang and mao smote,1,1,1.0
and mao smote a,1,1,1.0
mao smote a new,1,1,1.0
smote a new method,1,1,1.0
sets learning in proceedings,1,1,1.0
learning in proceedings of,3,2,1.5
on intelligent computing part,1,1,1.0
intelligent computing part i,1,1,1.0
computing part i lncs,1,1,1.0
part i lncs pp,1,1,1.0
i lncs pp weiss,1,1,1.0
lncs pp weiss and,1,1,1.0
pp weiss and provost,1,1,1.0
weiss and provost learning,2,2,1.0
and provost learning when,2,2,1.0
provost learning when training,2,2,1.0
learning when training data,2,2,1.0
when training data are,2,2,1.0
training data are costly,2,2,1.0
data are costly the,2,2,1.0
are costly the effect,2,2,1.0
costly the effect of,2,2,1.0
effect of class distribution,2,2,1.0
of class distribution on,2,2,1.0
class distribution on tree,2,2,1.0
distribution on tree induction,2,2,1.0
on tree induction journal,1,1,1.0
tree induction journal of,1,1,1.0
induction journal of artificial,1,1,1.0
pp han wang wen,1,1,1.0
han wang wen and,1,1,1.0
wang wen and wang,1,1,1.0
wen and wang sampling,1,1,1.0
and wang sampling algorithm,1,1,1.0
wang sampling algorithm based,1,1,1.0
sampling algorithm based on,1,1,1.0
on preliminary classification in,1,1,1.0
preliminary classification in imbalanced,1,1,1.0
classification in imbalanced data,1,1,1.0
data sets learning journal,1,1,1.0
sets learning journal of,1,1,1.0
learning journal of computer,1,1,1.0
journal of computer allocations,1,1,1.0
of computer allocations in,1,1,1.0
computer allocations in chinese,1,1,1.0
allocations in chinese taeho,1,1,1.0
in chinese taeho jo,1,1,1.0
chinese taeho jo and,1,1,1.0
taeho jo and japkowicz,2,1,2.0
jo and japkowicz class,2,2,1.0
and japkowicz class imbalances,2,2,1.0
japkowicz class imbalances versus,1,1,1.0
class imbalances versus small,1,1,1.0
imbalances versus small disjuncts,1,1,1.0
versus small disjuncts sigkdd,1,1,1.0
small disjuncts sigkdd explorations,1,1,1.0
disjuncts sigkdd explorations volume,1,1,1.0
sigkdd explorations volume issue,1,1,1.0
explorations volume issue pp,1,1,1.0
volume issue pp batista,1,1,1.0
issue pp batista prati,1,1,1.0
pp batista prati and,1,1,1.0
batista prati and monard,1,1,1.0
prati and monard a,1,1,1.0
and monard a study,1,1,1.0
training data sigkdd explorations,1,1,1.0
data sigkdd explorations pp,3,1,3.0
sigkdd explorations pp forman,1,1,1.0
explorations pp forman an,1,1,1.0
pp forman an extensive,1,1,1.0
forman an extensive empirical,1,1,1.0
an extensive empirical study,1,1,1.0
extensive empirical study of,1,1,1.0
empirical study of feature,1,1,1.0
study of feature selection,1,1,1.0
of feature selection metrics,1,1,1.0
feature selection metrics for,1,1,1.0
selection metrics for text,1,1,1.0
metrics for text classification,1,1,1.0
for text classification journal,1,1,1.0
text classification journal of,1,1,1.0
classification journal of machine,2,1,2.0
journal of machine learning,6,2,3.0
of machine learning research,6,2,3.0
machine learning research pp,2,1,2.0
learning research pp mladenic,1,1,1.0
research pp mladenic and,1,1,1.0
pp mladenic and grobelnik,1,1,1.0
mladenic and grobelnik feature,1,1,1.0
and grobelnik feature selection,1,1,1.0
grobelnik feature selection for,1,1,1.0
feature selection for unbalanced,1,1,1.0
selection for unbalanced class,1,1,1.0
for unbalanced class distribution,1,1,1.0
unbalanced class distribution and,1,1,1.0
class distribution and naive,1,1,1.0
distribution and naive bayes,1,1,1.0
and naive bayes in,1,1,1.0
naive bayes in proceedings,1,1,1.0
bayes in proceedings of,1,1,1.0
proceedings of the sixteenth,2,1,2.0
of the sixteenth international,2,1,2.0
the sixteenth international conference,2,1,2.0
sixteenth international conference on,2,1,2.0
on machine learning pp,3,1,3.0
machine learning pp zheng,1,1,1.0
learning pp zheng wu,1,1,1.0
pp zheng wu and,1,1,1.0
zheng wu and srihari,1,1,1.0
wu and srihari feature,1,1,1.0
and srihari feature selection,1,1,1.0
on imbalanced data sigkdd,1,1,1.0
imbalanced data sigkdd explorations,1,1,1.0
sigkdd explorations pp castillo,1,1,1.0
explorations pp castillo and,1,1,1.0
pp castillo and serrano,1,1,1.0
castillo and serrano a,1,1,1.0
and serrano a multistrategy,1,1,1.0
serrano a multistrategy approach,1,1,1.0
a multistrategy approach for,1,1,1.0
multistrategy approach for digital,1,1,1.0
approach for digital text,1,1,1.0
for digital text categorization,1,1,1.0
digital text categorization from,1,1,1.0
text categorization from imbalanced,1,1,1.0
categorization from imbalanced documents,1,1,1.0
from imbalanced documents sigkdd,1,1,1.0
imbalanced documents sigkdd explorations,1,1,1.0
documents sigkdd explorations pp,1,1,1.0
sigkdd explorations pp van,1,1,1.0
explorations pp van der,1,1,1.0
pp van der putten,1,1,1.0
van der putten and,1,1,1.0
der putten and van,1,1,1.0
putten and van someren,1,1,1.0
and van someren a,1,1,1.0
van someren a variance,1,1,1.0
someren a variance analysis,1,1,1.0
a variance analysis of,1,1,1.0
variance analysis of a,1,1,1.0
analysis of a real,1,1,1.0
of a real world,1,1,1.0
a real world learning,1,1,1.0
real world learning problem,1,1,1.0
world learning problem the,1,1,1.0
learning problem the coil,1,1,1.0
problem the coil challenge,1,1,1.0
the coil challenge machine,1,1,1.0
coil challenge machine learning,1,1,1.0
challenge machine learning pp,1,1,1.0
machine learning pp wilson,1,1,1.0
learning pp wilson asymptotic,1,1,1.0
pp wilson asymptotic properties,1,1,1.0
wilson asymptotic properties of,1,1,1.0
asymptotic properties of nearest,1,1,1.0
properties of nearest neighbor,1,1,1.0
of nearest neighbor rules,1,1,1.0
nearest neighbor rules using,1,1,1.0
neighbor rules using edited,1,1,1.0
rules using edited data,1,1,1.0
using edited data ieee,1,1,1.0
edited data ieee trans,1,1,1.0
data ieee trans on,1,1,1.0
ieee trans on systems,1,1,1.0
trans on systems man,1,1,1.0
man and cybernetics vol,1,1,1.0
and cybernetics vol pp,1,1,1.0
cybernetics vol pp drummond,1,1,1.0
vol pp drummond and,1,1,1.0
pp drummond and holte,1,1,1.0
drummond and holte class,1,1,1.0
and holte class imbalance,1,1,1.0
holte class imbalance and,1,1,1.0
class imbalance and cost,1,1,1.0
imbalance and cost sensitivity,1,1,1.0
and cost sensitivity why,1,1,1.0
cost sensitivity why beats,1,1,1.0
sensitivity why beats in,1,1,1.0
why beats in icml,1,1,1.0
beats in icml workshop,1,1,1.0
in icml workshop on,2,1,2.0
data sets ii washington,4,2,2.0
sets ii washington dc,2,1,2.0
ii washington dc barandela,1,1,1.0
washington dc barandela sánchez,1,1,1.0
dc barandela sánchez garcía,1,1,1.0
barandela sánchez garcía and,1,1,1.0
sánchez garcía and rangel,1,1,1.0
garcía and rangel strategies,1,1,1.0
and rangel strategies for,2,2,1.0
rangel strategies for learning,2,2,1.0
strategies for learning in,2,2,1.0
for learning in class,2,2,1.0
learning in class imbalance,2,2,1.0
in class imbalance problems,2,2,1.0
class imbalance problems pattern,1,1,1.0
imbalance problems pattern recognition,1,1,1.0
problems pattern recognition pp,1,1,1.0
pattern recognition pp wu,1,1,1.0
recognition pp wu and,1,1,1.0
pp wu and chang,1,1,1.0
wu and chang alignment,1,1,1.0
and chang alignment for,1,1,1.0
chang alignment for imbalanced,1,1,1.0
alignment for imbalanced dataset,1,1,1.0
for imbalanced dataset learning,1,1,1.0
imbalanced dataset learning in,1,1,1.0
dataset learning in icml,1,1,1.0
learning in icml workshop,1,1,1.0
ii washington dc veropoulos,1,1,1.0
washington dc veropoulos campbell,1,1,1.0
dc veropoulos campbell and,1,1,1.0
veropoulos campbell and cristianini,1,1,1.0
campbell and cristianini controlling,1,1,1.0
and cristianini controlling the,1,1,1.0
cristianini controlling the sensitivity,1,1,1.0
controlling the sensitivity of,1,1,1.0
the sensitivity of support,1,1,1.0
sensitivity of support vector,1,1,1.0
of support vector machines,2,2,1.0
support vector machines in,1,1,1.0
vector machines in proceedings,1,1,1.0
machines in proceedings of,2,2,1.0
joint conference on ai,1,1,1.0
conference on ai pp,1,1,1.0
on ai pp huang,1,1,1.0
ai pp huang yang,1,1,1.0
pp huang yang king,1,1,1.0
huang yang king and,1,1,1.0
yang king and lyu,1,1,1.0
king and lyu learning,1,1,1.0
and lyu learning classifiers,1,1,1.0
lyu learning classifiers from,1,1,1.0
learning classifiers from imbalanced,1,1,1.0
classifiers from imbalanced data,1,1,1.0
from imbalanced data based,1,1,1.0
imbalanced data based on,1,1,1.0
data based on biased,1,1,1.0
based on biased minimax,1,1,1.0
on biased minimax probability,1,1,1.0
minimax probability machine in,1,1,1.0
probability machine in proceedings,1,1,1.0
machine in proceedings of,1,1,1.0
proceedings of the ieee,1,1,1.0
of the ieee computer,1,1,1.0
the ieee computer society,1,1,1.0
ieee computer society conference,1,1,1.0
computer society conference on,1,1,1.0
society conference on computer,1,1,1.0
conference on computer vision,1,1,1.0
on computer vision and,1,1,1.0
computer vision and pattern,1,1,1.0
vision and pattern recognition,1,1,1.0
and pattern recognition domingos,1,1,1.0
pattern recognition domingos metacost,1,1,1.0
recognition domingos metacost a,1,1,1.0
proceedings of the fifth,2,2,1.0
of the fifth international,1,1,1.0
the fifth international conference,1,1,1.0
fifth international conference on,1,1,1.0
and data mining acm,1,1,1.0
data mining acm press,1,1,1.0
mining acm press pp,1,1,1.0
acm press pp w,1,1,1.0
press pp w f,1,1,1.0
pp w f a,1,1,1.0
w f a n,1,1,1.0
f a n s,1,1,1.0
a n s j,1,1,1.0
n s j s,1,1,1.0
s j s t,1,1,1.0
j s t o,1,1,1.0
s t o l,1,1,1.0
t o l f,1,1,1.0
o l f o,1,1,1.0
l f o j,1,1,1.0
f o j z,1,1,1.0
o j z h,1,1,1.0
j z h a,1,1,1.0
z h a n,1,1,1.0
h a n g,1,1,1.0
a n g a,1,1,1.0
n g a n,1,1,1.0
g a n d,1,1,1.0
a n d p,1,1,1.0
n d p k,1,1,1.0
d p k c,1,1,1.0
p k c h,1,1,1.0
k c h a,1,1,1.0
c h a n,1,1,1.0
h a n adacost,1,1,1.0
a n adacost misclassification,1,1,1.0
n adacost misclassification boosting,1,1,1.0
adacost misclassification boosting in,1,1,1.0
misclassification boosting in proceedings,1,1,1.0
boosting in proceedings of,2,1,2.0
machine learning pp japkowicz,2,1,2.0
learning pp japkowicz supervised,1,1,1.0
pp japkowicz supervised versus,1,1,1.0
japkowicz supervised versus unsupervised,1,1,1.0
supervised versus unsupervised binary,1,1,1.0
versus unsupervised binary learning,1,1,1.0
unsupervised binary learning by,1,1,1.0
binary learning by feed,1,1,1.0
learning by feed forward,1,1,1.0
by feed forward neural,1,1,1.0
feed forward neural networks,1,1,1.0
forward neural networks machine,1,1,1.0
neural networks machine learning,1,1,1.0
networks machine learning pp,1,1,1.0
machine learning pp scholkopf,1,1,1.0
learning pp scholkopf platt,1,1,1.0
pp scholkopf platt smola,1,1,1.0
scholkopf platt smola and,1,1,1.0
platt smola and williamson,1,1,1.0
smola and williamson estimating,1,1,1.0
and williamson estimating the,1,1,1.0
williamson estimating the support,1,1,1.0
estimating the support of,1,1,1.0
the support of a,1,1,1.0
support of a dimensional,1,1,1.0
of a dimensional distribution,1,1,1.0
a dimensional distribution neural,1,1,1.0
dimensional distribution neural computation,1,1,1.0
distribution neural computation pp,1,1,1.0
neural computation pp tax,1,1,1.0
computation pp tax classification,1,1,1.0
pp tax classification dissertation,1,1,1.0
tax classification dissertation delft,1,1,1.0
classification dissertation delft university,1,1,1.0
dissertation delft university of,1,1,1.0
delft university of technology,1,1,1.0
university of technology manevitz,1,1,1.0
of technology manevitz and,1,1,1.0
technology manevitz and yousef,1,1,1.0
manevitz and yousef svms,1,1,1.0
and yousef svms for,1,1,1.0
yousef svms for document,1,1,1.0
svms for document classification,1,1,1.0
for document classification journal,1,1,1.0
document classification journal of,1,1,1.0
learning research pp riddle,1,1,1.0
research pp riddle segal,1,1,1.0
pp riddle segal and,1,1,1.0
riddle segal and etzioni,1,1,1.0
segal and etzioni representation,1,1,1.0
and etzioni representation design,1,1,1.0
etzioni representation design and,1,1,1.0
representation design and induction,1,1,1.0
design and induction in,1,1,1.0
and induction in a,1,1,1.0
induction in a boeing,1,1,1.0
in a boeing manufacturing,1,1,1.0
a boeing manufacturing design,1,1,1.0
boeing manufacturing design applied,1,1,1.0
manufacturing design applied artificial,1,1,1.0
design applied artificial intelligence,1,1,1.0
applied artificial intelligence pp,1,1,1.0
artificial intelligence pp kubat,1,1,1.0
intelligence pp kubat holte,1,1,1.0
pp kubat holte and,1,1,1.0
kubat holte and matwin,2,2,1.0
holte and matwin learning,1,1,1.0
and matwin learning when,1,1,1.0
matwin learning when negative,1,1,1.0
learning when negative examples,1,1,1.0
when negative examples abound,1,1,1.0
negative examples abound in,1,1,1.0
examples abound in proceedings,1,1,1.0
abound in proceedings of,1,1,1.0
proceedings of the ninth,1,1,1.0
of the ninth european,1,1,1.0
the ninth european conference,1,1,1.0
ninth european conference on,1,1,1.0
european conference on machine,2,2,1.0
on machine learning lnai,1,1,1.0
machine learning lnai springer,1,1,1.0
learning lnai springer pp,1,1,1.0
lnai springer pp cohen,1,1,1.0
springer pp cohen fast,1,1,1.0
pp cohen fast effective,1,1,1.0
cohen fast effective rule,1,1,1.0
fast effective rule induction,1,1,1.0
effective rule induction in,1,1,1.0
rule induction in proceedings,1,1,1.0
induction in proceedings of,1,1,1.0
proceedings of the twelfth,1,1,1.0
of the twelfth international,1,1,1.0
the twelfth international conference,1,1,1.0
twelfth international conference on,1,1,1.0
machine learning pp tomek,1,1,1.0
learning pp tomek two,1,1,1.0
pp tomek two modifications,1,1,1.0
tomek two modifications of,1,1,1.0
two modifications of cnn,1,1,1.0
modifications of cnn ieee,1,1,1.0
of cnn ieee transactions,1,1,1.0
cnn ieee transactions on,1,1,1.0
systems man and communications,1,1,1.0
man and communications pp,1,1,1.0
and communications pp raskutti,1,1,1.0
communications pp raskutti and,1,1,1.0
pp raskutti and kowalczyk,1,1,1.0
raskutti and kowalczyk extreme,1,1,1.0
and kowalczyk extreme rebalancing,1,1,1.0
kowalczyk extreme rebalancing svms,1,1,1.0
extreme rebalancing svms a,1,1,1.0
rebalancing svms a case,1,1,1.0
svms a case study,1,1,1.0
a case study sigkdd,1,1,1.0
case study sigkdd explorations,1,1,1.0
study sigkdd explorations pp,1,1,1.0
sigkdd explorations pp freund,1,1,1.0
explorations pp freund and,1,1,1.0
pp freund and schapire,1,1,1.0
freund and schapire a,2,2,1.0
and schapire a generalization,2,2,1.0
and system sciences pp,1,1,1.0
system sciences pp m,1,1,1.0
sciences pp m v,1,1,1.0
pp m v j,1,1,1.0
m v j o,1,1,1.0
v j o s,1,1,1.0
j o s h,1,1,1.0
o s h i,1,1,1.0
s h i v,1,1,1.0
h i v k,1,1,1.0
i v k u,1,1,1.0
v k u m,1,1,1.0
k u m a,1,1,1.0
u m a r,1,1,1.0
m a r a,1,1,1.0
a r a n,1,1,1.0
r a n d,1,1,1.0
a n d r,1,1,1.0
n d r c,1,1,1.0
d r c a,1,1,1.0
r c a g,1,1,1.0
c a g a,1,1,1.0
a g a r,1,1,1.0
g a r w,1,1,1.0
a r w a,1,1,1.0
r w a l,1,1,1.0
w a l evaluating,1,1,1.0
a l evaluating boosting,1,1,1.0
l evaluating boosting algorithms,1,1,1.0
evaluating boosting algorithms to,1,1,1.0
boosting algorithms to classify,1,1,1.0
algorithms to classify rare,1,1,1.0
to classify rare cases,1,1,1.0
classify rare cases comparison,1,1,1.0
rare cases comparison and,1,1,1.0
cases comparison and improvements,1,1,1.0
comparison and improvements in,1,1,1.0
and improvements in proceedings,1,1,1.0
improvements in proceedings of,1,1,1.0
proceedings of the first,1,1,1.0
of the first ieee,1,1,1.0
the first ieee international,1,1,1.0
first ieee international conference,1,1,1.0
ieee international conference on,2,1,2.0
on data mining pp,1,1,1.0
data mining pp chawla,1,1,1.0
mining pp chawla lazarevic,1,1,1.0
pp chawla lazarevic hall,1,1,1.0
chawla lazarevic hall and,2,2,1.0
lazarevic hall and bowyer,2,2,1.0
hall and bowyer smoteboost,1,1,1.0
and bowyer smoteboost improving,1,1,1.0
class in boosting in,2,2,1.0
in boosting in proceedings,1,1,1.0
proceedings of the seventh,1,1,1.0
of the seventh european,1,1,1.0
the seventh european conference,1,1,1.0
seventh european conference on,1,1,1.0
european conference on principles,1,1,1.0
conference on principles and,1,1,1.0
on principles and practice,1,1,1.0
principles and practice of,1,1,1.0
and practice of knowledge,1,1,1.0
practice of knowledge discovery,1,1,1.0
of knowledge discovery in,1,1,1.0
discovery in databases dubrovnik,1,1,1.0
in databases dubrovnik croatia,1,1,1.0
databases dubrovnik croatia pp,1,1,1.0
dubrovnik croatia pp estabrooks,1,1,1.0
croatia pp estabrooks taeho,1,1,1.0
pp estabrooks taeho jo,1,1,1.0
estabrooks taeho jo and,1,1,1.0
jo and japkowicz a,1,1,1.0
and japkowicz a multiple,1,1,1.0
japkowicz a multiple resampling,1,1,1.0
a multiple resampling method,1,1,1.0
multiple resampling method for,1,1,1.0
resampling method for learning,1,1,1.0
method for learning from,1,1,1.0
imbalanced data sets computational,1,1,1.0
data sets computational intelligence,1,1,1.0
sets computational intelligence pp,1,1,1.0
computational intelligence pp chan,1,1,1.0
intelligence pp chan and,1,1,1.0
pp chan and stolfo,1,1,1.0
chan and stolfo toward,1,1,1.0
and stolfo toward scalable,1,1,1.0
stolfo toward scalable learning,1,1,1.0
toward scalable learning with,1,1,1.0
scalable learning with class,1,1,1.0
learning with class and,1,1,1.0
with class and cost,1,1,1.0
class and cost distributions,2,2,1.0
and cost distributions a,1,1,1.0
cost distributions a case,1,1,1.0
distributions a case study,1,1,1.0
a case study in,1,1,1.0
case study in credit,1,1,1.0
study in credit card,1,1,1.0
in credit card fraud,1,1,1.0
credit card fraud detection,1,1,1.0
card fraud detection in,1,1,1.0
fraud detection in proceedings,1,1,1.0
detection in proceedings of,1,1,1.0
and data mining pp,1,1,1.0
data mining pp yan,1,1,1.0
mining pp yan liu,1,1,1.0
pp yan liu jin,1,1,1.0
yan liu jin and,1,1,1.0
liu jin and hauptmann,1,1,1.0
jin and hauptmann on,1,1,1.0
and hauptmann on predicting,1,1,1.0
hauptmann on predicting rare,1,1,1.0
on predicting rare classes,1,1,1.0
predicting rare classes with,1,1,1.0
rare classes with svm,1,1,1.0
classes with svm ensembles,1,1,1.0
with svm ensembles in,2,2,1.0
svm ensembles in scene,1,1,1.0
ensembles in scene classification,1,1,1.0
in scene classification in,1,1,1.0
scene classification in ieee,1,1,1.0
classification in ieee international,1,1,1.0
in ieee international conference,1,1,1.0
international conference on acoustics,1,1,1.0
conference on acoustics speech,1,1,1.0
on acoustics speech and,1,1,1.0
acoustics speech and signal,1,1,1.0
speech and signal processing,1,1,1.0
and signal processing phua,1,1,1.0
signal processing phua and,1,1,1.0
processing phua and alahakoon,1,1,1.0
phua and alahakoon minority,1,1,1.0
and alahakoon minority report,1,1,1.0
alahakoon minority report in,1,1,1.0
minority report in fraud,1,1,1.0
report in fraud detection,1,1,1.0
in fraud detection classification,1,1,1.0
fraud detection classification of,1,1,1.0
detection classification of skewed,1,1,1.0
classification of skewed data,1,1,1.0
of skewed data sigkdd,1,1,1.0
skewed data sigkdd explorations,1,1,1.0
sigkdd explorations pp estabrooks,1,1,1.0
explorations pp estabrooks and,1,1,1.0
pp estabrooks and japkowicz,1,1,1.0
estabrooks and japkowicz a,1,1,1.0
and japkowicz a framework,1,1,1.0
japkowicz a framework for,1,1,1.0
a framework for learning,1,1,1.0
framework for learning from,1,1,1.0
for learning from unbalanced,1,1,1.0
learning from unbalanced data,1,1,1.0
from unbalanced data sets,1,1,1.0
unbalanced data sets in,1,1,1.0
data sets in proceedings,1,1,1.0
sets in proceedings of,1,1,1.0
proceedings of the intelligent,1,1,1.0
of the intelligent data,1,1,1.0
the intelligent data analysis,1,1,1.0
intelligent data analysis conference,1,1,1.0
data analysis conference pp,1,1,1.0
analysis conference pp bradley,1,1,1.0
conference pp bradley the,1,1,1.0
pp bradley the use,1,1,1.0
bradley the use of,1,1,1.0
the roc curve in,1,1,1.0
roc curve in the,1,1,1.0
curve in the evaluation,1,1,1.0
in the evaluation of,1,1,1.0
the evaluation of machine,1,1,1.0
evaluation of machine learning,1,1,1.0
of machine learning algorithms,1,1,1.0
machine learning algorithms pattern,1,1,1.0
learning algorithms pattern recognition,1,1,1.0
algorithms pattern recognition pp,1,1,1.0
pattern recognition pp provost,1,1,1.0
recognition pp provost and,1,1,1.0
pp provost and fawcett,3,2,1.5
provost and fawcett robust,2,2,1.0
and fawcett robust classification,1,1,1.0
fawcett robust classification for,1,1,1.0
robust classification for imprecise,1,1,1.0
classification for imprecise environments,1,1,1.0
for imprecise environments machine,1,1,1.0
imprecise environments machine learning,1,1,1.0
environments machine learning pp,1,1,1.0
learning pp japkowicz in,1,1,1.0
pp japkowicz in the,1,1,1.0
japkowicz in the presence,1,1,1.0
in the presence of,1,1,1.0
the presence of and,1,1,1.0
presence of and imbalances,1,1,1.0
of and imbalances in,1,1,1.0
and imbalances in proceedings,1,1,1.0
imbalances in proceedings of,1,1,1.0
of the fourteenth conference,1,1,1.0
the fourteenth conference of,1,1,1.0
fourteenth conference of the,1,1,1.0
conference of the canadian,1,1,1.0
of the canadian society,1,1,1.0
the canadian society for,1,1,1.0
canadian society for computational,1,1,1.0
society for computational studies,1,1,1.0
for computational studies of,1,1,1.0
computational studies of intelligence,1,1,1.0
studies of intelligence pp,1,1,1.0
of intelligence pp prati,1,1,1.0
pp prati batista and,2,2,1.0
prati batista and monard,2,2,1.0
batista and monard class,2,2,1.0
and monard class imbalances,2,2,1.0
monard class imbalances versus,1,1,1.0
class imbalances versus class,1,1,1.0
imbalances versus class overlapping,1,1,1.0
versus class overlapping an,1,1,1.0
class overlapping an analysis,2,2,1.0
overlapping an analysis of,2,2,1.0
an analysis of a,2,2,1.0
analysis of a learning,2,2,1.0
of a learning system,2,2,1.0
a learning system behavior,2,2,1.0
learning system behavior in,2,2,1.0
system behavior in micai,1,1,1.0
behavior in micai lnai,1,1,1.0
in micai lnai pp,1,1,1.0
micai lnai pp kolez,1,1,1.0
lnai pp kolez chowdhury,1,1,1.0
pp kolez chowdhury and,1,1,1.0
kolez chowdhury and alspector,1,1,1.0
chowdhury and alspector data,1,1,1.0
and alspector data duplication,1,1,1.0
alspector data duplication an,1,1,1.0
data duplication an imbalance,1,1,1.0
duplication an imbalance problem,1,1,1.0
an imbalance problem in,1,1,1.0
imbalance problem in proceedings,1,1,1.0
data sets ii view,1,1,1.0
sets ii view publication,1,1,1.0
ii view publication stats,1,1,1.0
minority class in the,5,1,5.0
class in the feature,4,1,4.0
in the feature space,20,1,20.0
the feature space p,1,1,1.0
feature space p p,1,1,1.0
space p p guti,1,1,1.0
p p guti senior,1,1,1.0
p guti senior member,1,1,1.0
guti senior member ieee,1,1,1.0
senior member ieee p,1,1,1.0
member ieee p ti,1,1,1.0
ieee p ti ˇno,1,1,1.0
p ti ˇno and,1,1,1.0
ti ˇno and c,1,1,1.0
ˇno and c herv,1,1,1.0
and c herv senior,1,1,1.0
c herv senior member,1,1,1.0
herv senior member ieee,1,1,1.0
senior member ieee imbalanced,1,1,1.0
member ieee imbalanced nature,1,1,1.0
ieee imbalanced nature of,1,1,1.0
imbalanced nature of some,1,1,1.0
nature of some data,1,1,1.0
of some data is,1,1,1.0
some data is one,1,1,1.0
data is one of,1,1,1.0
one of the current,1,1,1.0
of the current challenges,1,1,1.0
the current challenges for,1,1,1.0
current challenges for machine,1,1,1.0
challenges for machine learning,1,1,1.0
for machine learning researchers,2,1,2.0
machine learning researchers one,1,1,1.0
learning researchers one common,1,1,1.0
researchers one common approach,1,1,1.0
one common approach the,1,1,1.0
common approach the minority,1,1,1.0
approach the minority class,1,1,1.0
minority class through convex,1,1,1.0
class through convex combination,1,1,1.0
through convex combination of,1,1,1.0
convex combination of its,1,1,1.0
combination of its patterns,1,1,1.0
of its patterns w,1,1,1.0
its patterns w e,1,1,1.0
patterns w e explore,1,1,1.0
w e explore the,2,1,2.0
e explore the general,1,1,1.0
explore the general idea,1,1,1.0
the general idea of,1,1,1.0
general idea of synthetic,1,1,1.0
idea of synthetic in,1,1,1.0
synthetic in the feature,1,1,1.0
the feature space induced,3,1,3.0
feature space induced by,3,1,3.0
space induced by a,3,1,3.0
induced by a kernel,3,1,3.0
by a kernel function,3,1,3.0
a kernel function as,1,1,1.0
kernel function as opposed,1,1,1.0
function as opposed to,1,1,1.0
as opposed to input,1,1,1.0
opposed to input space,1,1,1.0
to input space if,1,1,1.0
input space if the,1,1,1.0
space if the kernel,1,1,1.0
if the kernel function,3,1,3.0
the kernel function matches,2,1,2.0
kernel function matches the,2,1,2.0
function matches the underlying,2,1,2.0
matches the underlying problem,1,1,1.0
the underlying problem the,1,1,1.0
underlying problem the classes,1,1,1.0
problem the classes will,1,1,1.0
the classes will be,1,1,1.0
classes will be linearly,1,1,1.0
will be linearly separable,1,1,1.0
be linearly separable and,1,1,1.0
linearly separable and synthetically,1,1,1.0
separable and synthetically generated,1,1,1.0
and synthetically generated patterns,1,1,1.0
synthetically generated patterns will,1,1,1.0
generated patterns will lie,1,1,1.0
patterns will lie on,1,1,1.0
will lie on the,2,1,2.0
lie on the minority,1,1,1.0
the minority class region,1,1,1.0
minority class region since,1,1,1.0
class region since the,1,1,1.0
region since the feature,1,1,1.0
since the feature space,2,1,2.0
the feature space is,4,1,4.0
feature space is not,2,1,2.0
space is not directly,2,1,2.0
is not directly accessible,2,1,2.0
not directly accessible we,1,1,1.0
directly accessible we use,1,1,1.0
accessible we use the,1,1,1.0
we use the empirical,1,1,1.0
use the empirical feature,1,1,1.0
the empirical feature space,13,1,13.0
empirical feature space a,1,1,1.0
feature space a euclidean,1,1,1.0
space a euclidean space,1,1,1.0
a euclidean space isomorphic,1,1,1.0
euclidean space isomorphic to,1,1,1.0
space isomorphic to the,1,1,1.0
isomorphic to the feature,1,1,1.0
to the feature space,2,1,2.0
the feature space for,1,1,1.0
feature space for purposes,1,1,1.0
space for purposes the,1,1,1.0
for purposes the proposed,1,1,1.0
purposes the proposed method,1,1,1.0
the proposed method is,1,1,1.0
proposed method is framed,1,1,1.0
method is framed in,1,1,1.0
is framed in the,1,1,1.0
framed in the context,1,1,1.0
the context of support,1,1,1.0
context of support vector,1,1,1.0
support vector machines where,1,1,1.0
vector machines where imbalanced,1,1,1.0
machines where imbalanced datasets,1,1,1.0
where imbalanced datasets can,1,1,1.0
imbalanced datasets can pose,1,1,1.0
datasets can pose a,1,1,1.0
can pose a serious,1,1,1.0
pose a serious hindrance,1,1,1.0
a serious hindrance the,1,1,1.0
serious hindrance the idea,1,1,1.0
hindrance the idea is,1,1,1.0
the idea is investigated,1,1,1.0
idea is investigated in,1,1,1.0
is investigated in three,1,1,1.0
investigated in three scenarios,1,1,1.0
in three scenarios sampling,1,1,1.0
three scenarios sampling in,1,1,1.0
scenarios sampling in the,1,1,1.0
sampling in the full,1,1,1.0
in the full and,3,1,3.0
the full and empirical,2,1,2.0
full and empirical feature,2,1,2.0
and empirical feature spaces,1,1,1.0
empirical feature spaces a,1,1,1.0
feature spaces a kernel,1,1,1.0
spaces a kernel learning,1,1,1.0
a kernel learning technique,1,1,1.0
kernel learning technique maximising,1,1,1.0
learning technique maximising the,1,1,1.0
technique maximising the data,1,1,1.0
maximising the data class,1,1,1.0
the data class separation,1,1,1.0
data class separation to,1,1,1.0
class separation to study,1,1,1.0
separation to study the,1,1,1.0
to study the inﬂuence,1,1,1.0
study the inﬂuence of,1,1,1.0
the inﬂuence of the,2,1,2.0
inﬂuence of the feature,1,1,1.0
of the feature space,6,2,3.0
the feature space structure,3,1,3.0
feature space structure implicitly,1,1,1.0
space structure implicitly deﬁned,1,1,1.0
structure implicitly deﬁned by,1,1,1.0
implicitly deﬁned by the,1,1,1.0
deﬁned by the kernel,1,1,1.0
by the kernel function,2,1,2.0
the kernel function a,1,1,1.0
kernel function a uniﬁed,1,1,1.0
function a uniﬁed framework,1,1,1.0
a uniﬁed framework for,2,1,2.0
uniﬁed framework for preferential,1,1,1.0
framework for preferential that,1,1,1.0
for preferential that spans,1,1,1.0
preferential that spans some,1,1,1.0
that spans some of,1,1,1.0
spans some of the,1,1,1.0
some of the previous,1,1,1.0
of the previous approaches,1,1,1.0
the previous approaches in,1,1,1.0
previous approaches in the,1,1,1.0
approaches in the literature,4,1,4.0
in the literature w,1,1,1.0
the literature w e,1,1,1.0
literature w e support,1,1,1.0
w e support our,1,1,1.0
e support our investigation,1,1,1.0
support our investigation with,1,1,1.0
our investigation with extensive,1,1,1.0
investigation with extensive experiments,1,1,1.0
with extensive experiments over,1,1,1.0
extensive experiments over imbalanced,1,1,1.0
experiments over imbalanced datasets,2,1,2.0
over imbalanced datasets index,1,1,1.0
imbalanced datasets index t,1,1,1.0
datasets index t imbalanced,1,1,1.0
index t imbalanced classiﬁcation,1,1,1.0
t imbalanced classiﬁcation kernel,1,1,1.0
imbalanced classiﬁcation kernel methods,1,1,1.0
classiﬁcation kernel methods empirical,1,1,1.0
kernel methods empirical feature,1,1,1.0
methods empirical feature space,1,1,1.0
empirical feature space support,1,1,1.0
feature space support vector,1,1,1.0
space support vector machines,1,1,1.0
support vector machines i,1,1,1.0
vector machines i n,1,1,1.0
machines i n t,1,1,1.0
i n t ro,1,1,1.0
n t ro d,1,1,1.0
t ro d u,1,1,1.0
ro d u c,1,1,1.0
d u c t,1,1,1.0
u c t i,1,1,1.0
c t i o,1,1,1.0
t i o n,1,1,1.0
i o n classiﬁcation,1,1,1.0
o n classiﬁcation methods,1,1,1.0
n classiﬁcation methods often,1,1,1.0
classiﬁcation methods often conveniently,1,1,1.0
methods often conveniently assume,1,1,1.0
often conveniently assume that,1,1,1.0
conveniently assume that the,1,1,1.0
assume that the prior,1,1,1.0
that the prior class,1,1,1.0
the prior class probability,1,1,1.0
prior class probability distribution,1,1,1.0
class probability distribution is,1,1,1.0
probability distribution is of,1,1,1.0
distribution is of high,1,1,1.0
is of high entropy,1,1,1.0
of high entropy however,1,1,1.0
high entropy however this,1,1,1.0
entropy however this is,1,1,1.0
however this is not,2,1,2.0
this is not the,2,1,2.0
is not the case,2,2,1.0
not the case in,1,1,1.0
the case in many,1,1,1.0
case in many applications,1,1,1.0
in many applications from,1,1,1.0
many applications from areas,1,1,1.0
applications from areas such,1,1,1.0
from areas such as,1,1,1.0
areas such as medical,1,1,1.0
as medical diagnosis information,1,1,1.0
medical diagnosis information retrieval,1,1,1.0
diagnosis information retrieval fraud,1,1,1.0
information retrieval fraud detection,1,1,1.0
retrieval fraud detection etc,1,1,1.0
fraud detection etc the,1,1,1.0
detection etc the classiﬁcation,1,1,1.0
etc the classiﬁcation paradigm,1,1,1.0
the classiﬁcation paradigm when,1,1,1.0
classiﬁcation paradigm when one,1,1,1.0
paradigm when one or,1,1,1.0
when one or several,1,1,1.0
one or several classes,1,1,1.0
or several classes have,1,1,1.0
several classes have a,1,1,1.0
classes have a much,1,1,1.0
have a much lower,1,1,1.0
a much lower prior,1,1,1.0
much lower prior probability,1,1,1.0
lower prior probability in,1,1,1.0
prior probability in the,1,1,1.0
probability in the training,1,1,1.0
the training set is,1,1,1.0
training set is known,1,1,1.0
set is known as,1,1,1.0
is known as imbalanced,1,1,1.0
known as imbalanced classiﬁcation,1,1,1.0
as imbalanced classiﬁcation and,1,1,1.0
imbalanced classiﬁcation and it,1,1,1.0
classiﬁcation and it poses,1,1,1.0
and it poses a,1,1,1.0
it poses a difﬁcult,1,1,1.0
poses a difﬁcult challenge,1,1,1.0
a difﬁcult challenge for,1,1,1.0
difﬁcult challenge for machine,1,1,1.0
challenge for machine learning,1,1,1.0
machine learning researchers because,1,1,1.0
learning researchers because of,1,1,1.0
researchers because of that,1,1,1.0
because of that imbalanced,1,1,1.0
of that imbalanced classiﬁcation,1,1,1.0
that imbalanced classiﬁcation is,1,1,1.0
imbalanced classiﬁcation is currently,1,1,1.0
classiﬁcation is currently receiving,1,1,1.0
is currently receiving a,1,1,1.0
currently receiving a lot,1,1,1.0
receiving a lot of,1,1,1.0
a lot of attention,1,1,1.0
lot of attention from,1,1,1.0
of attention from the,1,1,1.0
attention from the pattern,1,1,1.0
from the pattern recognition,1,1,1.0
the pattern recognition and,1,1,1.0
and machine learning communities,1,1,1.0
machine learning communities often,1,1,1.0
learning communities often the,1,1,1.0
communities often the minority,1,1,1.0
often the minority class,1,1,1.0
the minority class happens,1,1,1.0
minority class happens to,1,1,1.0
class happens to be,1,1,1.0
happens to be more,1,1,1.0
to be more important,1,1,1.0
be more important than,1,1,1.0
important than the majority,1,1,1.0
than the majority one,1,1,1.0
the majority one but,1,1,1.0
majority one but it,1,1,1.0
one but it may,1,1,1.0
but it may also,1,1,1.0
it may also be,1,1,1.0
may also be much,1,1,1.0
also be much more,1,1,1.0
be much more difﬁcult,1,1,1.0
much more difﬁcult to,1,1,1.0
more difﬁcult to model,1,1,1.0
difﬁcult to model due,1,1,1.0
to model due to,1,1,1.0
model due to the,1,1,1.0
due to the low,1,1,1.0
to the low number,1,1,1.0
the low number of,1,1,1.0
low number of available,1,1,1.0
number of available samples,1,1,1.0
of available samples since,1,1,1.0
available samples since most,1,1,1.0
samples since most traditional,1,1,1.0
since most traditional learning,1,1,1.0
most traditional learning systems,1,1,1.0
traditional learning systems have,1,1,1.0
learning systems have been,1,1,1.0
systems have been designed,1,1,1.0
have been designed to,1,1,1.0
been designed to work,1,1,1.0
designed to work on,1,1,1.0
to work on balanced,1,1,1.0
work on balanced data,1,1,1.0
on balanced data they,1,1,1.0
balanced data they will,1,1,1.0
data they will usually,1,1,1.0
they will usually be,1,1,1.0
will usually be focused,1,1,1.0
usually be focused on,1,1,1.0
be focused on improving,1,1,1.0
focused on improving overall,1,1,1.0
on improving overall performance,2,1,2.0
improving overall performance and,1,1,1.0
overall performance and be,1,1,1.0
performance and be biased,1,1,1.0
and be biased towards,1,1,1.0
be biased towards the,1,1,1.0
biased towards the majority,1,1,1.0
towards the majority class,1,1,1.0
the majority class consequently,1,1,1.0
majority class consequently harming,1,1,1.0
class consequently harming the,1,1,1.0
consequently harming the minority,1,1,1.0
harming the minority one,1,1,1.0
the minority one although,1,1,1.0
minority one although from,1,1,1.0
one although from a,1,1,1.0
although from a formal,1,1,1.0
from a formal deﬁnition,1,1,1.0
a formal deﬁnition an,1,1,1.0
formal deﬁnition an imbalanced,1,1,1.0
deﬁnition an imbalanced the,1,1,1.0
an imbalanced the work,1,1,1.0
imbalanced the work of,1,1,1.0
the work of p,2,1,2.0
work of p p,1,1,1.0
of p p guti,1,1,1.0
p p guti and,1,1,1.0
p guti and herv,2,1,2.0
guti and herv has,1,1,1.0
and herv has been,1,1,1.0
herv has been subsidized,1,1,1.0
has been subsidized by,1,1,1.0
been subsidized by the,1,1,1.0
subsidized by the project,1,1,1.0
by the project of,1,1,1.0
the project of the,2,1,2.0
project of the spanish,1,1,1.0
of the spanish ministerial,1,1,1.0
the spanish ministerial commission,1,1,1.0
spanish ministerial commission of,1,1,1.0
ministerial commission of science,1,1,1.0
commission of science and,1,1,1.0
of science and t,1,1,1.0
science and t echnology,1,1,1.0
and t echnology micyt,1,1,1.0
t echnology micyt feder,1,1,1.0
echnology micyt feder funds,1,1,1.0
micyt feder funds and,1,1,1.0
feder funds and the,1,1,1.0
funds and the project,1,1,1.0
and the project of,1,1,1.0
project of the junta,1,1,1.0
of the junta de,1,1,1.0
the junta de andaluc,1,1,1.0
junta de andaluc spain,1,1,1.0
de andaluc spain the,1,1,1.0
andaluc spain the work,1,1,1.0
spain the work of,1,1,1.0
work of p tino,1,1,1.0
of p tino has,1,1,1.0
p tino has been,1,1,1.0
tino has been supported,1,1,1.0
has been supported by,1,1,1.0
been supported by epsrc,1,1,1.0
supported by epsrc grant,1,1,1.0
by epsrc grant p,1,1,1.0
epsrc grant p ortiz,1,1,1.0
grant p ortiz p,1,1,1.0
p ortiz p guti,1,1,1.0
ortiz p guti and,1,1,1.0
guti and herv are,1,1,1.0
and herv are with,1,1,1.0
herv are with the,1,1,1.0
are with the department,2,2,1.0
with the department of,3,2,1.5
the department of computer,1,1,1.0
computer science and numerical,1,1,1.0
science and numerical analysis,1,1,1.0
and numerical analysis of,1,1,1.0
numerical analysis of the,1,1,1.0
analysis of the university,1,1,1.0
of the university of,2,1,2.0
the university of c,1,1,1.0
university of c spain,1,1,1.0
of c spain email,1,1,1.0
c spain email pagutierrez,1,1,1.0
spain email pagutierrez chervas,1,1,1.0
email pagutierrez chervas p,1,1,1.0
pagutierrez chervas p ti,1,1,1.0
chervas p ti ˇno,1,1,1.0
p ti ˇno is,1,1,1.0
ti ˇno is with,1,1,1.0
ˇno is with the,1,1,1.0
is with the school,1,1,1.0
with the school of,1,1,1.0
the school of computer,1,1,1.0
of computer science of,1,1,1.0
computer science of the,1,1,1.0
science of the university,1,1,1.0
the university of birmingham,1,1,1.0
university of birmingham birmingham,1,1,1.0
of birmingham birmingham united,1,1,1.0
birmingham birmingham united kingdom,1,1,1.0
birmingham united kingdom email,1,1,1.0
united kingdom email dataset,1,1,1.0
kingdom email dataset is,1,1,1.0
email dataset is any,1,1,1.0
dataset is any set,1,1,1.0
is any set of,1,1,1.0
any set of labelled,1,1,1.0
set of labelled data,1,1,1.0
of labelled data exhibiting,1,1,1.0
labelled data exhibiting an,1,1,1.0
data exhibiting an unequal,1,1,1.0
exhibiting an unequal distribution,1,1,1.0
an unequal distribution between,2,2,1.0
unequal distribution between classes,1,1,1.0
distribution between classes it,1,1,1.0
between classes it has,1,1,1.0
classes it has been,1,1,1.0
been shown that this,1,1,1.0
shown that this is,1,1,1.0
that this is not,2,1,2.0
not the only factor,1,1,1.0
the only factor involved,1,1,1.0
only factor involved hindering,1,1,1.0
factor involved hindering the,1,1,1.0
involved hindering the learning,1,1,1.0
hindering the learning in,1,1,1.0
the learning in this,1,1,1.0
learning in this context,1,1,1.0
in this context the,1,1,1.0
this context the complexity,1,1,1.0
context the complexity of,1,1,1.0
the complexity of the,1,1,1.0
complexity of the data,1,1,1.0
of the data existence,1,1,1.0
the data existence of,1,1,1.0
data existence of noisy,1,1,1.0
existence of noisy and,1,1,1.0
of noisy and samples,1,1,1.0
noisy and samples or,1,1,1.0
and samples or class,1,1,1.0
samples or class overlapping,1,1,1.0
or class overlapping or,1,1,1.0
class overlapping or the,1,1,1.0
overlapping or the size,1,1,1.0
or the size of,1,1,1.0
the training set data,1,1,1.0
training set data or,1,1,1.0
set data or small,1,1,1.0
data or small sample,1,1,1.0
or small sample size,1,1,1.0
small sample size can,1,1,1.0
sample size can also,1,1,1.0
size can also be,1,1,1.0
can also be part,1,1,1.0
also be part of,1,1,1.0
be part of the,1,1,1.0
part of the nature,1,1,1.0
of the nature of,1,1,1.0
nature of the class,1,1,1.0
imbalance problem the approaches,1,1,1.0
problem the approaches developed,1,1,1.0
the approaches developed over,1,1,1.0
approaches developed over the,1,1,1.0
developed over the years,1,1,1.0
over the years for,1,1,1.0
the years for tackling,1,1,1.0
years for tackling the,1,1,1.0
for tackling the class,1,1,1.0
tackling the class imbalance,1,1,1.0
class imbalance problem can,1,1,1.0
imbalance problem can be,1,1,1.0
problem can be categorised,1,1,1.0
can be categorised in,1,1,1.0
be categorised in two,1,1,1.0
categorised in two groups,1,1,1.0
in two groups data,1,1,1.0
two groups data approach,1,1,1.0
groups data approach based,1,1,1.0
data approach based on,1,1,1.0
approach based on sampling,1,1,1.0
based on sampling methods,1,1,1.0
on sampling methods including,1,1,1.0
sampling methods including minority,1,1,1.0
methods including minority groups,1,1,1.0
including minority groups groups,1,1,1.0
minority groups groups of,1,1,1.0
groups groups of interesting,1,1,1.0
groups of interesting rare,1,1,1.0
of interesting rare examples,1,1,1.0
interesting rare examples or,1,1,1.0
rare examples or majority,1,1,1.0
examples or majority groups,1,1,1.0
or majority groups groups,1,1,1.0
majority groups groups with,1,1,1.0
groups groups with large,1,1,1.0
groups with large example,1,1,1.0
with large example sizes,1,1,1.0
large example sizes the,1,1,1.0
example sizes the combination,1,1,1.0
sizes the combination of,1,1,1.0
the combination of both,1,1,1.0
combination of both being,1,1,1.0
of both being also,1,1,1.0
both being also very,1,1,1.0
being also very popular,1,1,1.0
also very popular algorithm,1,1,1.0
very popular algorithm approach,1,1,1.0
popular algorithm approach forces,1,1,1.0
algorithm approach forces the,1,1,1.0
approach forces the classiﬁer,1,1,1.0
forces the classiﬁer to,1,1,1.0
the classiﬁer to pay,1,1,1.0
classiﬁer to pay more,1,1,1.0
to pay more attention,1,1,1.0
pay more attention to,1,1,1.0
attention to the minority,1,1,1.0
minority class by learning,1,1,1.0
class by learning the,1,1,1.0
by learning the analysis,1,1,1.0
learning the analysis made,1,1,1.0
the analysis made in,1,1,1.0
analysis made in this,1,1,1.0
made in this paper,2,1,2.0
in this paper is,1,1,1.0
this paper is contextualised,1,1,1.0
paper is contextualised on,1,1,1.0
is contextualised on data,1,1,1.0
contextualised on data approaches,1,1,1.0
on data approaches thus,1,1,1.0
data approaches thus a,1,1,1.0
approaches thus a brief,1,1,1.0
thus a brief discussion,1,1,1.0
a brief discussion on,1,1,1.0
brief discussion on these,1,1,1.0
discussion on these techniques,1,1,1.0
on these techniques is,1,1,1.0
these techniques is now,1,1,1.0
techniques is now given,1,1,1.0
is now given for,1,1,1.0
now given for a,1,1,1.0
given for a detailed,1,1,1.0
for a detailed review,1,1,1.0
a detailed review of,1,1,1.0
detailed review of see,1,1,1.0
review of see roughly,1,1,1.0
of see roughly speaking,1,1,1.0
see roughly speaking it,1,1,1.0
roughly speaking it can,1,1,1.0
speaking it can be,1,1,1.0
it can be said,3,1,3.0
can be said that,3,1,3.0
be said that and,1,1,1.0
said that and sampling,1,1,1.0
that and sampling are,1,1,1.0
and sampling are opposite,1,1,1.0
sampling are opposite and,1,1,1.0
are opposite and equivalent,1,1,1.0
opposite and equivalent since,1,1,1.0
and equivalent since they,1,1,1.0
equivalent since they are,1,1,1.0
since they are aimed,1,1,1.0
they are aimed at,1,1,1.0
are aimed at the,1,1,1.0
aimed at the same,1,1,1.0
at the same purpose,1,1,1.0
the same purpose balance,1,1,1.0
same purpose balance the,1,1,1.0
purpose balance the class,1,1,1.0
balance the class distribution,1,1,1.0
the class distribution but,1,1,1.0
class distribution but using,1,1,1.0
distribution but using different,1,1,1.0
but using different approaches,1,1,1.0
using different approaches formally,1,1,1.0
different approaches formally concerns,1,1,1.0
approaches formally concerns to,1,1,1.0
formally concerns to the,1,1,1.0
concerns to the process,1,1,1.0
to the process of,2,1,2.0
the process of sampling,1,1,1.0
process of sampling a,1,1,1.0
of sampling a distribution,1,1,1.0
sampling a distribution with,1,1,1.0
a distribution with a,1,1,1.0
distribution with a signiﬁcantly,1,1,1.0
with a signiﬁcantly higher,1,1,1.0
a signiﬁcantly higher frequency,1,1,1.0
signiﬁcantly higher frequency than,1,1,1.0
higher frequency than the,1,1,1.0
frequency than the given,1,1,1.0
than the given one,1,1,1.0
the given one and,1,1,1.0
given one and to,1,1,1.0
one and to the,1,1,1.0
and to the process,1,1,1.0
the process of reducing,1,1,1.0
process of reducing the,1,1,1.0
of reducing the frequency,1,1,1.0
reducing the frequency of,1,1,1.0
the frequency of the,1,1,1.0
frequency of the majority,1,1,1.0
majority class in both,1,1,1.0
class in both cases,1,1,1.0
in both cases the,1,1,1.0
both cases the methodologies,1,1,1.0
cases the methodologies impose,1,1,1.0
the methodologies impose a,1,1,1.0
methodologies impose a balance,1,1,1.0
impose a balance in,1,1,1.0
a balance in the,1,1,1.0
balance in the class,1,1,1.0
in the class distribution,1,1,1.0
the class distribution in,1,1,1.0
class distribution in order,1,1,1.0
distribution in order to,1,1,1.0
in order to avoid,1,1,1.0
order to avoid aliasing,1,1,1.0
to avoid aliasing and,1,1,1.0
avoid aliasing and focus,1,1,1.0
aliasing and focus on,1,1,1.0
and focus on the,1,1,1.0
focus on the classiﬁcation,1,1,1.0
on the classiﬁcation of,1,1,1.0
the classiﬁcation of minority,1,1,1.0
classiﬁcation of minority classes,1,1,1.0
of minority classes although,1,1,1.0
minority classes although both,1,1,1.0
classes although both sampling,1,1,1.0
although both sampling and,1,1,1.0
both sampling and approaches,1,1,1.0
sampling and approaches have,1,1,1.0
and approaches have been,1,1,1.0
approaches have been shown,1,1,1.0
have been shown to,1,1,1.0
been shown to improve,1,1,1.0
shown to improve classiﬁer,1,1,1.0
to improve classiﬁer performance,1,1,1.0
improve classiﬁer performance over,1,1,1.0
classiﬁer performance over imbalanced,1,1,1.0
performance over imbalanced datasets,1,1,1.0
over imbalanced datasets different,1,1,1.0
imbalanced datasets different studies,1,1,1.0
datasets different studies suggest,1,1,1.0
different studies suggest that,1,1,1.0
studies suggest that is,1,1,1.0
suggest that is more,1,1,1.0
that is more useful,1,1,1.0
is more useful than,1,1,1.0
more useful than specially,1,1,1.0
useful than specially for,1,1,1.0
than specially for highly,1,1,1.0
specially for highly imbalanced,1,1,1.0
for highly imbalanced and,1,1,1.0
highly imbalanced and complex,1,1,1.0
imbalanced and complex datasets,1,1,1.0
and complex datasets recall,1,1,1.0
complex datasets recall that,1,1,1.0
datasets recall that could,1,1,1.0
recall that could entail,1,1,1.0
that could entail a,1,1,1.0
could entail a loss,1,1,1.0
entail a loss of,1,1,1.0
a loss of potentially,1,1,1.0
loss of potentially meaningful,1,1,1.0
of potentially meaningful information,1,1,1.0
potentially meaningful information of,1,1,1.0
meaningful information of the,1,1,1.0
information of the dataset,1,1,1.0
of the dataset concerning,1,1,1.0
the dataset concerning the,1,1,1.0
dataset concerning the ﬁrst,1,1,1.0
concerning the ﬁrst idea,1,1,1.0
the ﬁrst idea is,2,1,2.0
ﬁrst idea is to,2,1,2.0
idea is to perform,1,1,1.0
is to perform a,1,1,1.0
to perform a random,1,1,1.0
perform a random replication,1,1,1.0
a random replication of,1,1,1.0
replication of minority data,1,1,1.0
of minority data but,1,1,1.0
minority data but this,1,1,1.0
data but this often,1,1,1.0
but this often leads,1,1,1.0
this often leads to,1,1,1.0
often leads to another,1,1,1.0
leads to another common,1,1,1.0
to another common approach,1,1,1.0
another common approach is,1,1,1.0
common approach is to,1,1,1.0
approach is to generate,1,1,1.0
is to generate new,1,1,1.0
to generate new synthetic,2,1,2.0
generate new synthetic patterns,1,1,1.0
new synthetic patterns according,1,1,1.0
synthetic patterns according to,1,1,1.0
patterns according to the,2,1,2.0
according to the minority,1,1,1.0
the minority class distribution,1,1,1.0
minority class distribution one,1,1,1.0
class distribution one of,1,1,1.0
distribution one of the,1,1,1.0
of the most methods,1,1,1.0
the most methods to,1,1,1.0
most methods to do,1,1,1.0
methods to do so,1,1,1.0
to do so is,1,1,1.0
do so is the,1,1,1.0
so is the synthetic,1,1,1.0
is the synthetic minority,1,1,1.0
minority technique smote based,1,1,1.0
technique smote based on,1,1,1.0
smote based on erating,1,1,1.0
based on erating new,1,1,1.0
on erating new instances,1,1,1.0
erating new instances by,1,1,1.0
new instances by convex,1,1,1.0
instances by convex combination,1,1,1.0
by convex combination of,7,1,7.0
convex combination of one,1,1,1.0
combination of one point,1,1,1.0
of one point and,1,1,1.0
one point and one,1,1,1.0
point and one of,1,1,1.0
and one of its,2,1,2.0
one of its neighbours,2,1,2.0
of its neighbours both,2,1,2.0
its neighbours both belonging,2,1,2.0
neighbours both belonging to,3,1,3.0
both belonging to the,3,1,3.0
belonging to the minority,5,1,5.0
the minority class however,1,1,1.0
minority class however the,1,1,1.0
class however the classes,1,1,1.0
however the classes in,1,1,1.0
the classes in general,1,1,1.0
classes in general can,1,1,1.0
in general can not,1,1,1.0
general can not be,1,1,1.0
can not be assumed,1,1,1.0
not be assumed to,1,1,1.0
be assumed to be,1,1,1.0
assumed to be convex,1,1,1.0
to be convex and,1,1,1.0
be convex and hence,1,1,1.0
convex and hence smote,1,1,1.0
and hence smote does,1,1,1.0
hence smote does not,1,1,1.0
smote does not avoid,1,1,1.0
does not avoid synthetic,1,1,1.0
not avoid synthetic patterns,1,1,1.0
avoid synthetic patterns to,1,1,1.0
synthetic patterns to fall,1,1,1.0
patterns to fall inside,1,1,1.0
to fall inside majority,1,1,1.0
fall inside majority regions,1,1,1.0
inside majority regions therefore,1,1,1.0
majority regions therefore more,1,1,1.0
regions therefore more careful,1,1,1.0
therefore more careful techniques,1,1,1.0
more careful techniques have,1,1,1.0
careful techniques have been,1,1,1.0
techniques have been developed,1,1,1.0
been developed to prevent,1,1,1.0
developed to prevent this,1,1,1.0
to prevent this issue,1,1,1.0
prevent this issue prevent,1,1,1.0
this issue prevent but,1,1,1.0
issue prevent but solve,1,1,1.0
prevent but solve adaptive,1,1,1.0
but solve adaptive synthetic,1,1,1.0
solve adaptive synthetic and,1,1,1.0
adaptive synthetic and sampling,1,1,1.0
synthetic and sampling methods,1,1,1.0
and sampling methods are,1,1,1.0
sampling methods are examples,1,1,1.0
methods are examples of,1,1,1.0
are examples of more,1,1,1.0
examples of more powerful,1,1,1.0
of more powerful techniques,1,1,1.0
more powerful techniques based,1,1,1.0
powerful techniques based on,1,1,1.0
techniques based on extracting,1,1,1.0
based on extracting knowledge,1,1,1.0
on extracting knowledge from,1,1,1.0
extracting knowledge from the,1,1,1.0
knowledge from the data,1,1,1.0
from the data to,1,1,1.0
the data to analyse,1,1,1.0
data to analyse which,1,1,1.0
to analyse which patterns,2,1,2.0
analyse which patterns and,1,1,1.0
which patterns and regions,1,1,1.0
patterns and regions of,1,1,1.0
and regions of the,1,1,1.0
regions of the space,2,1,2.0
of the space are,1,1,1.0
the space are more,1,1,1.0
space are more suitable,1,1,1.0
are more suitable for,2,1,2.0
more suitable for sampling,1,1,1.0
suitable for sampling this,1,1,1.0
for sampling this will,1,1,1.0
sampling this will be,1,1,1.0
this will be referred,1,1,1.0
will be referred in,1,1,1.0
be referred in the,1,1,1.0
referred in the paper,1,1,1.0
in the paper to,1,1,1.0
the paper to as,1,1,1.0
paper to as preferential,1,1,1.0
to as preferential at,1,1,1.0
as preferential at the,1,1,1.0
preferential at the same,1,1,1.0
at the same time,2,1,2.0
the same time kernel,1,1,1.0
same time kernel methods,1,1,1.0
time kernel methods have,1,1,1.0
kernel methods have been,1,1,1.0
methods have been spreading,1,1,1.0
have been spreading rapidly,1,1,1.0
been spreading rapidly and,1,1,1.0
spreading rapidly and gaining,1,1,1.0
rapidly and gaining acceptance,1,1,1.0
and gaining acceptance in,1,1,1.0
gaining acceptance in machine,1,1,1.0
acceptance in machine learning,1,1,1.0
in machine learning due,1,1,1.0
machine learning due to,1,1,1.0
learning due to their,1,1,1.0
due to their good,1,1,1.0
to their good generalisation,1,1,1.0
their good generalisation ability,1,1,1.0
good generalisation ability and,2,1,2.0
generalisation ability and determinism,1,1,1.0
ability and determinism being,1,1,1.0
and determinism being one,1,1,1.0
determinism being one of,1,1,1.0
being one of the,1,1,1.0
most widely used the,1,1,1.0
widely used the support,1,1,1.0
used the support v,1,1,1.0
the support v ector,1,1,1.0
support v ector machine,1,1,1.0
v ector machine svm,1,1,1.0
ector machine svm however,1,1,1.0
machine svm however for,1,1,1.0
svm however for svm,1,1,1.0
however for svm imbalanced,1,1,1.0
for svm imbalanced data,1,1,1.0
svm imbalanced data pose,1,1,1.0
imbalanced data pose a,1,1,1.0
data pose a serious,1,1,1.0
pose a serious challenge,1,1,1.0
a serious challenge due,1,1,1.0
serious challenge due to,1,1,1.0
challenge due to the,1,1,1.0
due to the formulation,1,1,1.0
to the formulation of,1,1,1.0
the formulation of the,1,1,1.0
formulation of the maximisation,1,1,1.0
of the maximisation which,1,1,1.0
the maximisation which focus,1,1,1.0
maximisation which focus on,1,1,1.0
which focus on improving,1,1,1.0
focus on improving overall,1,1,1.0
improving overall performance thus,1,1,1.0
overall performance thus the,1,1,1.0
performance thus the combination,1,1,1.0
thus the combination of,1,1,1.0
the combination of kernel,1,1,1.0
combination of kernel methods,1,1,1.0
of kernel methods with,1,1,1.0
kernel methods with techniques,1,1,1.0
methods with techniques for,1,1,1.0
with techniques for tackling,1,1,1.0
techniques for tackling class,1,1,1.0
for tackling class imbalance,1,1,1.0
tackling class imbalance is,1,1,1.0
class imbalance is widely,1,1,1.0
imbalance is widely spread,1,1,1.0
is widely spread it,1,1,1.0
widely spread it is,1,1,1.0
spread it is clear,1,1,1.0
it is clear that,5,2,2.5
is clear that by,1,1,1.0
clear that by linear,1,1,1.0
that by linear interpolation,1,1,1.0
by linear interpolation is,1,1,1.0
linear interpolation is not,1,1,1.0
interpolation is not as,1,1,1.0
is not as suitable,1,1,1.0
not as suitable when,1,1,1.0
as suitable when dealing,1,1,1.0
suitable when dealing with,1,1,1.0
when dealing with nonlinear,1,1,1.0
dealing with nonlinear classiﬁers,1,1,1.0
with nonlinear classiﬁers as,1,1,1.0
nonlinear classiﬁers as it,1,1,1.0
classiﬁers as it could,1,1,1.0
as it could be,1,1,1.0
it could be than,1,1,1.0
could be than when,1,1,1.0
be than when applying,1,1,1.0
than when applying linear,1,1,1.0
when applying linear classiﬁers,1,1,1.0
applying linear classiﬁers however,1,1,1.0
linear classiﬁers however linearly,1,1,1.0
classiﬁers however linearly separable,1,1,1.0
however linearly separable datasets,1,1,1.0
linearly separable datasets are,1,1,1.0
separable datasets are not,1,1,1.0
datasets are not common,1,1,1.0
are not common in,1,1,1.0
not common in applications,1,1,1.0
common in applications thus,1,1,1.0
in applications thus making,1,1,1.0
applications thus making advisable,1,1,1.0
thus making advisable the,1,1,1.0
making advisable the application,1,1,1.0
advisable the application of,1,1,1.0
the application of classiﬁers,1,1,1.0
application of classiﬁers able,1,1,1.0
of classiﬁers able to,1,1,1.0
classiﬁers able to capture,1,1,1.0
able to capture this,1,1,1.0
to capture this nonlinearity,1,1,1.0
capture this nonlinearity besides,1,1,1.0
this nonlinearity besides the,1,1,1.0
nonlinearity besides the development,1,1,1.0
besides the development of,1,1,1.0
the development of a,1,1,1.0
development of a able,1,1,1.0
of a able nonlinear,1,1,1.0
a able nonlinear strategy,1,1,1.0
able nonlinear strategy could,1,1,1.0
nonlinear strategy could be,1,1,1.0
strategy could be tricky,1,1,1.0
could be tricky thus,1,1,1.0
be tricky thus in,1,1,1.0
tricky thus in contrast,1,1,1.0
thus in contrast to,1,1,1.0
in contrast to previous,1,1,1.0
contrast to previous approaches,1,1,1.0
to previous approaches we,1,1,1.0
previous approaches we propose,1,1,1.0
approaches we propose to,1,1,1.0
we propose to generate,1,1,1.0
propose to generate new,1,1,1.0
generate new synthetic data,1,1,1.0
new synthetic data by,1,1,1.0
synthetic data by convex,1,1,1.0
data by convex combination,1,1,1.0
convex combination of points,1,1,1.0
combination of points in,1,1,1.0
of points in a,1,1,1.0
points in a space,1,1,1.0
in a space where,1,1,1.0
a space where the,1,1,1.0
space where the classes,1,1,1.0
where the classes are,1,1,1.0
the classes are ideally,1,1,1.0
classes are ideally linearly,1,1,1.0
are ideally linearly separated,1,1,1.0
ideally linearly separated making,1,1,1.0
linearly separated making generation,1,1,1.0
separated making generation of,1,1,1.0
making generation of new,1,1,1.0
generation of new synthetic,1,1,1.0
of new synthetic points,1,1,1.0
new synthetic points by,1,1,1.0
synthetic points by convex,1,1,1.0
points by convex combination,1,1,1.0
convex combination of the,1,1,1.0
combination of the original,2,2,1.0
of the original points,1,1,1.0
the original points belonging,1,1,1.0
original points belonging to,1,1,1.0
points belonging to the,1,1,1.0
belonging to the same,1,1,1.0
to the same class,1,1,1.0
the same class safe,1,1,1.0
same class safe this,1,1,1.0
class safe this is,1,1,1.0
safe this is done,1,1,1.0
this is done using,1,1,1.0
is done using the,1,1,1.0
done using the feature,1,1,1.0
using the feature space,1,1,1.0
a kernel function for,1,1,1.0
kernel function for the,1,1,1.0
function for the patterns,1,1,1.0
for the patterns rather,1,1,1.0
the patterns rather than,1,1,1.0
patterns rather than using,1,1,1.0
rather than using the,1,1,1.0
than using the input,1,1,1.0
using the input space,1,1,1.0
the input space however,2,1,2.0
input space however this,2,1,2.0
space however this is,1,1,1.0
this is not so,1,1,1.0
is not so straightforward,1,1,1.0
not so straightforward because,1,1,1.0
so straightforward because when,1,1,1.0
straightforward because when dealing,1,1,1.0
because when dealing with,1,1,1.0
when dealing with kernel,2,1,2.0
dealing with kernel methods,2,1,2.0
with kernel methods the,1,1,1.0
kernel methods the only,1,1,1.0
methods the only information,1,1,1.0
the only information available,1,1,1.0
only information available is,1,1,1.0
information available is the,1,1,1.0
available is the dot,1,1,1.0
is the dot products,1,1,1.0
the dot products of,2,1,2.0
dot products of the,2,1,2.0
products of the images,1,1,1.0
of the images of,1,1,1.0
the images of the,2,1,2.0
images of the patterns,1,1,1.0
of the patterns t,1,1,1.0
the patterns t o,1,1,1.0
patterns t o cope,1,1,1.0
t o cope with,2,1,2.0
o cope with this,2,1,2.0
cope with this issue,2,1,2.0
with this issue this,1,1,1.0
this issue this paper,1,1,1.0
issue this paper makes,1,1,1.0
this paper makes use,1,1,1.0
paper makes use of,1,1,1.0
makes use of the,1,1,1.0
use of the notion,1,1,1.0
of the notion of,1,1,1.0
the notion of the,1,1,1.0
notion of the empirical,1,1,1.0
of the empirical feature,3,1,3.0
empirical feature space efs,3,1,3.0
feature space efs which,1,1,1.0
space efs which is,1,1,1.0
efs which is euclidean,1,1,1.0
which is euclidean and,1,1,1.0
is euclidean and preserves,1,1,1.0
euclidean and preserves the,1,1,1.0
and preserves the geometrical,1,1,1.0
preserves the geometrical structure,1,1,1.0
the geometrical structure of,1,1,1.0
geometrical structure of the,1,1,1.0
structure of the original,2,1,2.0
of the original feature,2,1,2.0
the original feature space,2,1,2.0
original feature space given,1,1,1.0
feature space given that,1,1,1.0
space given that distances,1,1,1.0
given that distances and,1,1,1.0
that distances and angles,1,1,1.0
distances and angles in,1,1,1.0
and angles in the,1,1,1.0
angles in the feature,1,1,1.0
the feature space are,4,1,4.0
feature space are uniquely,2,1,2.0
space are uniquely determined,2,1,2.0
are uniquely determined by,2,1,2.0
uniquely determined by dot,1,1,1.0
determined by dot products,1,1,1.0
by dot products and,1,1,1.0
dot products and that,1,1,1.0
products and that the,1,1,1.0
and that the dot,1,1,1.0
that the dot products,2,1,2.0
products of the corresponding,1,1,1.0
of the corresponding images,1,1,1.0
the corresponding images are,1,1,1.0
corresponding images are the,1,1,1.0
images are the original,1,1,1.0
are the original kernel,1,1,1.0
the original kernel values,1,1,1.0
original kernel values the,1,1,1.0
kernel values the main,1,1,1.0
values the main motivation,1,1,1.0
the main motivation for,1,1,1.0
main motivation for performing,1,1,1.0
motivation for performing in,1,1,1.0
for performing in the,1,1,1.0
performing in the efs,1,1,1.0
in the efs instead,1,1,1.0
the efs instead of,1,1,1.0
efs instead of in,1,1,1.0
instead of in the,1,1,1.0
of in the input,1,1,1.0
in the input space,9,1,9.0
the input space is,2,1,2.0
input space is the,1,1,1.0
space is the hypothesis,1,1,1.0
is the hypothesis that,1,1,1.0
the hypothesis that the,2,1,2.0
hypothesis that the feature,1,1,1.0
that the feature space,1,1,1.0
the feature space provide,1,1,1.0
feature space provide a,1,1,1.0
space provide a more,1,1,1.0
provide a more suitable,1,1,1.0
a more suitable space,4,1,4.0
more suitable space for,4,1,4.0
suitable space for via,1,1,1.0
space for via convex,1,1,1.0
for via convex combination,1,1,1.0
via convex combination because,1,1,1.0
convex combination because the,1,1,1.0
combination because the class,1,1,1.0
because the class separation,1,1,1.0
the class separation will,1,1,1.0
class separation will be,1,1,1.0
separation will be simpler,1,1,1.0
will be simpler and,1,1,1.0
be simpler and larger,1,1,1.0
simpler and larger ideally,1,1,1.0
and larger ideally due,1,1,1.0
larger ideally due to,1,1,1.0
ideally due to the,1,1,1.0
due to the kernel,1,1,1.0
to the kernel trick,2,1,2.0
the kernel trick linearly,1,1,1.0
kernel trick linearly separable,1,1,1.0
trick linearly separable at,1,1,1.0
linearly separable at the,1,1,1.0
separable at the same,1,1,1.0
the same time this,1,1,1.0
same time this technique,1,1,1.0
time this technique can,1,1,1.0
this technique can be,1,1,1.0
technique can be seen,1,1,1.0
be seen as a,1,1,1.0
seen as a general,1,1,1.0
as a general nonlinear,1,1,1.0
a general nonlinear in,1,1,1.0
general nonlinear in the,1,1,1.0
nonlinear in the input,1,1,1.0
the input space due,1,1,1.0
input space due to,1,1,1.0
space due to the,1,1,1.0
due to the application,2,1,2.0
to the application of,2,1,2.0
the application of the,2,1,2.0
application of the nonlinear,1,1,1.0
of the nonlinear map,1,1,1.0
the nonlinear map φ,1,1,1.0
nonlinear map φ related,1,1,1.0
map φ related to,1,1,1.0
φ related to the,1,1,1.0
related to the kernel,1,1,1.0
the kernel trick and,1,1,1.0
kernel trick and could,1,1,1.0
trick and could be,1,1,1.0
and could be used,1,1,1.0
could be used in,1,1,1.0
be used in combination,1,1,1.0
used in combination with,1,1,1.0
in combination with any,1,1,1.0
combination with any classiﬁer,1,1,1.0
with any classiﬁer t,1,1,1.0
any classiﬁer t o,1,1,1.0
classiﬁer t o the,1,1,1.0
t o the best,1,1,1.0
o the best of,1,1,1.0
the best of our,1,1,1.0
best of our knowledge,1,1,1.0
of our knowledge performing,1,1,1.0
our knowledge performing in,1,1,1.0
knowledge performing in the,1,1,1.0
performing in the feature,1,1,1.0
the feature space has,1,1,1.0
feature space has only,1,1,1.0
space has only been,1,1,1.0
has only been researched,1,1,1.0
only been researched in,1,1,1.0
been researched in recall,1,1,1.0
researched in recall that,1,1,1.0
in recall that in,1,1,1.0
recall that in our,1,1,1.0
that in our case,1,1,1.0
in our case it,1,1,1.0
our case it is,1,1,1.0
case it is performed,1,1,1.0
it is performed in,1,1,1.0
is performed in the,1,1,1.0
performed in the efs,1,1,1.0
in the efs in,2,1,2.0
the efs in this,2,1,2.0
efs in this previous,1,1,1.0
in this previous work,1,1,1.0
this previous work the,1,1,1.0
previous work the synthetic,1,1,1.0
work the synthetic instances,1,1,1.0
the synthetic instances were,2,1,2.0
synthetic instances were generated,1,1,1.0
instances were generated by,1,1,1.0
were generated by using,1,1,1.0
generated by using the,1,1,1.0
by using the geometric,1,1,1.0
using the geometric interpretation,1,1,1.0
the geometric interpretation of,1,1,1.0
geometric interpretation of the,1,1,1.0
interpretation of the dot,1,1,1.0
of the dot products,1,1,1.0
the dot products in,1,1,1.0
dot products in the,1,1,1.0
products in the kernel,1,1,1.0
in the kernel matrix,1,1,1.0
the kernel matrix and,1,1,1.0
kernel matrix and the,1,1,1.0
matrix and the of,1,1,1.0
and the of the,1,1,1.0
the of the synthetic,2,1,2.0
of the synthetic instances,2,2,1.0
synthetic instances were approximated,1,1,1.0
instances were approximated based,1,1,1.0
were approximated based on,1,1,1.0
approximated based on a,1,1,1.0
based on a distance,1,1,1.0
on a distance relation,1,1,1.0
a distance relation between,1,1,1.0
distance relation between the,1,1,1.0
relation between the feature,1,1,1.0
between the feature space,1,1,1.0
the feature space and,2,1,2.0
feature space and the,1,1,1.0
space and the input,1,1,1.0
and the input one,1,1,1.0
the input one since,1,1,1.0
input one since inverse,1,1,1.0
one since inverse mapping,1,1,1.0
since inverse mapping φ,1,1,1.0
inverse mapping φ from,1,1,1.0
mapping φ from the,1,1,1.0
φ from the feature,1,1,1.0
from the feature space,1,1,1.0
the feature space to,2,1,2.0
feature space to the,1,1,1.0
space to the input,1,1,1.0
to the input space,2,1,2.0
input space is not,1,1,1.0
space is not available,1,1,1.0
is not available our,1,1,1.0
not available our proposal,1,1,1.0
available our proposal is,1,1,1.0
our proposal is free,1,1,1.0
proposal is free of,1,1,1.0
is free of the,1,1,1.0
free of the assumptions,1,1,1.0
of the assumptions of,1,1,1.0
the assumptions of this,1,1,1.0
assumptions of this inverse,1,1,1.0
of this inverse mapping,1,1,1.0
this inverse mapping approximation,1,1,1.0
inverse mapping approximation the,1,1,1.0
mapping approximation the study,1,1,1.0
approximation the study made,1,1,1.0
the study made in,1,1,1.0
study made in this,1,1,1.0
in this paper intends,1,1,1.0
this paper intends to,1,1,1.0
paper intends to provide,1,1,1.0
intends to provide an,1,1,1.0
to provide an extensive,1,1,1.0
provide an extensive analysis,1,1,1.0
an extensive analysis of,1,1,1.0
extensive analysis of in,1,1,1.0
analysis of in the,1,1,1.0
of in the efs,2,1,2.0
in the efs and,1,1,1.0
the efs and can,1,1,1.0
efs and can be,1,1,1.0
and can be subdivided,1,1,1.0
can be subdivided in,1,1,1.0
be subdivided in three,1,1,1.0
subdivided in three sections,1,1,1.0
in three sections the,1,1,1.0
three sections the ﬁrst,1,1,1.0
sections the ﬁrst one,1,1,1.0
the ﬁrst one deals,1,1,1.0
ﬁrst one deals with,1,1,1.0
one deals with the,1,1,1.0
deals with the issue,1,1,1.0
with the issue of,1,1,1.0
the issue of extending,1,1,1.0
issue of extending the,1,1,1.0
of extending the smote,1,1,1.0
extending the smote algorithm,1,1,1.0
the smote algorithm to,1,1,1.0
smote algorithm to be,1,1,1.0
algorithm to be used,1,1,1.0
used in the full,1,1,1.0
the full and efs,1,1,1.0
full and efs the,1,1,1.0
and efs the objective,1,1,1.0
efs the objective is,1,1,1.0
the objective is to,2,1,2.0
objective is to test,1,1,1.0
is to test whether,2,1,2.0
to test whether the,3,1,3.0
test whether the efs,1,1,1.0
whether the efs provides,1,1,1.0
the efs provides a,2,1,2.0
efs provides a more,2,1,2.0
provides a more suitable,3,1,3.0
a more suitable framework,1,1,1.0
more suitable framework for,1,1,1.0
suitable framework for by,1,1,1.0
framework for by convex,1,1,1.0
for by convex combination,2,1,2.0
convex combination of patterns,5,1,5.0
combination of patterns and,1,1,1.0
of patterns and to,1,1,1.0
patterns and to deal,1,1,1.0
and to deal with,1,1,1.0
deal with the dimensionality,1,1,1.0
with the dimensionality of,1,1,1.0
the dimensionality of the,6,1,6.0
dimensionality of the efs,6,1,6.0
of the efs the,2,1,2.0
the efs the second,1,1,1.0
efs the second part,1,1,1.0
the second part deals,1,1,1.0
second part deals with,1,1,1.0
part deals with the,1,1,1.0
deals with the kernel,1,1,1.0
with the kernel function,1,1,1.0
the kernel function choice,1,1,1.0
kernel function choice since,1,1,1.0
function choice since our,1,1,1.0
choice since our methodology,1,1,1.0
since our methodology depends,1,1,1.0
our methodology depends on,1,1,1.0
methodology depends on how,1,1,1.0
depends on how the,1,1,1.0
on how the kernel,1,1,1.0
how the kernel matches,1,1,1.0
the kernel matches the,1,1,1.0
kernel matches the underlying,1,1,1.0
matches the underlying classiﬁcation,1,1,1.0
the underlying classiﬁcation problem,1,1,1.0
underlying classiﬁcation problem and,1,1,1.0
classiﬁcation problem and we,1,1,1.0
problem and we develop,1,1,1.0
and we develop a,1,1,1.0
we develop a strategy,1,1,1.0
develop a strategy for,1,1,1.0
a strategy for optimising,1,1,1.0
strategy for optimising the,1,1,1.0
for optimising the feature,1,1,1.0
optimising the feature space,1,1,1.0
the feature space based,1,1,1.0
feature space based on,1,1,1.0
space based on analytical,1,1,1.0
based on analytical knowledge,1,1,1.0
on analytical knowledge using,1,1,1.0
analytical knowledge using the,1,1,1.0
knowledge using the notion,1,1,1.0
using the notion of,1,1,1.0
the notion of alignment,1,1,1.0
notion of alignment ideally,1,1,1.0
of alignment ideally a,1,1,1.0
alignment ideally a better,1,1,1.0
ideally a better ﬁtted,1,1,1.0
a better ﬁtted kernel,1,1,1.0
better ﬁtted kernel will,1,1,1.0
ﬁtted kernel will increase,1,1,1.0
kernel will increase the,1,1,1.0
will increase the class,1,1,1.0
increase the class separability,1,1,1.0
the class separability providing,1,1,1.0
class separability providing a,1,1,1.0
separability providing a safer,1,1,1.0
providing a safer environment,1,1,1.0
a safer environment for,1,1,1.0
safer environment for the,1,1,1.0
environment for the generation,1,1,1.0
for the generation of,2,1,2.0
the generation of synthetic,1,1,1.0
generation of synthetic patterns,1,1,1.0
of synthetic patterns the,1,1,1.0
synthetic patterns the last,1,1,1.0
patterns the last part,1,1,1.0
the last part of,1,1,1.0
last part of this,1,1,1.0
part of this paper,1,1,1.0
of this paper proposes,1,1,1.0
this paper proposes a,1,1,1.0
paper proposes a uniﬁed,1,1,1.0
proposes a uniﬁed adaptive,1,1,1.0
a uniﬁed adaptive framework,1,1,1.0
uniﬁed adaptive framework for,1,1,1.0
adaptive framework for preferential,1,1,1.0
framework for preferential generalising,1,1,1.0
for preferential generalising several,1,1,1.0
preferential generalising several approaches,1,1,1.0
generalising several approaches in,1,1,1.0
several approaches in the,1,1,1.0
in the literature the,1,1,1.0
the literature the optimal,1,1,1.0
literature the optimal svm,1,1,1.0
the optimal svm hyperplane,2,1,2.0
optimal svm hyperplane and,1,1,1.0
svm hyperplane and kernel,1,1,1.0
hyperplane and kernel learning,1,1,1.0
and kernel learning techniques,1,1,1.0
kernel learning techniques are,1,1,1.0
learning techniques are used,1,1,1.0
techniques are used for,1,1,1.0
are used for optimising,1,1,1.0
used for optimising the,1,1,1.0
for optimising the synthetically,1,1,1.0
optimising the synthetically generated,1,1,1.0
the synthetically generated patterns,1,1,1.0
synthetically generated patterns the,1,1,1.0
generated patterns the objective,1,1,1.0
patterns the objective is,1,1,1.0
objective is to check,1,1,1.0
is to check if,1,1,1.0
to check if some,1,1,1.0
check if some regions,1,1,1.0
if some regions of,1,1,1.0
some regions of the,2,1,2.0
of the space can,1,1,1.0
the space can be,1,1,1.0
space can be more,1,1,1.0
can be more useful,1,1,1.0
be more useful for,1,1,1.0
more useful for than,1,1,1.0
useful for than others,1,1,1.0
for than others t,1,1,1.0
than others t o,1,1,1.0
others t o test,1,1,1.0
t o test the,1,1,1.0
o test the different,1,1,1.0
test the different hypotheses,1,1,1.0
the different hypotheses exposed,1,1,1.0
different hypotheses exposed in,1,1,1.0
hypotheses exposed in this,1,1,1.0
exposed in this paper,1,1,1.0
this paper we perform,1,1,1.0
paper we perform a,1,1,1.0
we perform a thorough,1,1,1.0
perform a thorough set,1,1,1.0
a thorough set of,2,1,2.0
thorough set of experiments,2,1,2.0
set of experiments with,1,1,1.0
of experiments with binary,1,1,1.0
experiments with binary imbalanced,1,1,1.0
with binary imbalanced datasets,1,1,1.0
binary imbalanced datasets the,1,1,1.0
imbalanced datasets the paper,1,1,1.0
datasets the paper is,1,1,1.0
as follows section ii,1,1,1.0
follows section ii introduces,1,1,1.0
section ii introduces some,1,1,1.0
ii introduces some useful,1,1,1.0
introduces some useful notions,1,1,1.0
some useful notions section,1,1,1.0
useful notions section iii,1,1,1.0
notions section iii exposes,1,1,1.0
section iii exposes how,1,1,1.0
iii exposes how to,1,1,1.0
exposes how to perform,1,1,1.0
how to perform sampling,1,1,1.0
to perform sampling in,1,1,1.0
perform sampling in the,1,1,1.0
sampling in the efs,3,1,3.0
in the efs section,1,1,1.0
the efs section iv,1,1,1.0
efs section iv develops,1,1,1.0
section iv develops a,1,1,1.0
iv develops a new,1,1,1.0
develops a new methodology,1,1,1.0
a new methodology for,1,1,1.0
new methodology for kernel,1,1,1.0
methodology for kernel learning,1,1,1.0
for kernel learning section,1,1,1.0
kernel learning section v,1,1,1.0
learning section v proposes,1,1,1.0
section v proposes a,1,1,1.0
v proposes a general,1,1,1.0
proposes a general preferential,1,1,1.0
a general preferential framework,1,1,1.0
general preferential framework section,1,1,1.0
preferential framework section vi,1,1,1.0
framework section vi exposes,1,1,1.0
section vi exposes the,1,1,1.0
vi exposes the tal,1,1,1.0
exposes the tal study,1,1,1.0
the tal study and,1,1,1.0
tal study and analyses,1,1,1.0
study and analyses the,1,1,1.0
and analyses the results,1,1,1.0
analyses the results obtained,1,1,1.0
the results obtained and,1,1,1.0
results obtained and ﬁnally,1,1,1.0
obtained and ﬁnally section,1,1,1.0
and ﬁnally section vii,1,1,1.0
ﬁnally section vii outlines,1,1,1.0
section vii outlines some,1,1,1.0
vii outlines some conclusions,1,1,1.0
outlines some conclusions and,1,1,1.0
some conclusions and future,1,1,1.0
conclusions and future work,1,1,1.0
and future work ii,1,1,1.0
future work ii b,1,1,1.0
work ii b ac,1,1,1.0
ii b ac k,1,1,1.0
b ac k g,1,1,1.0
ac k g ro,1,1,1.0
k g ro u,1,1,1.0
g ro u n,1,1,1.0
ro u n d,1,1,1.0
u n d this,1,1,1.0
n d this section,1,1,1.0
d this section is,1,1,1.0
this section is intended,1,1,1.0
section is intended to,1,1,1.0
is intended to introduce,1,1,1.0
intended to introduce the,1,1,1.0
to introduce the notation,1,1,1.0
introduce the notation used,1,1,1.0
the notation used throughout,1,1,1.0
notation used throughout all,1,1,1.0
used throughout all the,1,1,1.0
throughout all the paper,1,1,1.0
all the paper and,1,1,1.0
the paper and to,1,1,1.0
paper and to provide,1,1,1.0
and to provide some,1,1,1.0
to provide some previous,1,1,1.0
provide some previous notions,1,1,1.0
some previous notions about,1,1,1.0
previous notions about svm,1,1,1.0
notions about svm classiﬁers,1,1,1.0
about svm classiﬁers and,1,1,1.0
svm classiﬁers and the,1,1,1.0
classiﬁers and the empirical,1,1,1.0
and the empirical feature,1,1,1.0
empirical feature space consider,1,1,1.0
feature space consider a,1,1,1.0
space consider a sample,1,1,1.0
consider a sample d,1,1,1.0
a sample d xi,1,1,1.0
sample d xi y,1,1,1.0
d xi y i,1,1,1.0
xi y i m,1,1,1.0
y i m x,1,1,1.0
i m x y,1,1,1.0
m x y generated,1,1,1.0
x y generated from,1,1,1.0
y generated from a,1,1,1.0
generated from a unknown,1,1,1.0
from a unknown joint,1,1,1.0
a unknown joint distribution,1,1,1.0
unknown joint distribution p,1,1,1.0
joint distribution p x,1,1,1.0
distribution p x y,1,1,1.0
p x y where,1,1,1.0
x y where x,1,1,1.0
y where x rd,1,1,1.0
where x rd y,1,1,1.0
x rd y the,1,1,1.0
rd y the goal,1,1,1.0
y the goal in,1,1,1.0
the goal in binary,1,1,1.0
goal in binary classiﬁcation,1,1,1.0
in binary classiﬁcation is,1,1,1.0
binary classiﬁcation is to,1,1,1.0
classiﬁcation is to assign,1,1,1.0
is to assign an,1,1,1.0
to assign an input,1,1,1.0
assign an input vector,1,1,1.0
an input vector x,1,1,1.0
input vector x to,1,1,1.0
vector x to one,1,1,1.0
x to one of,1,1,1.0
to one of classes,1,1,1.0
one of classes denote,1,1,1.0
of classes denote by,1,1,1.0
classes denote by xtr,1,1,1.0
denote by xtr and,1,1,1.0
by xtr and xts,1,1,1.0
xtr and xts the,1,1,1.0
and xts the sets,1,1,1.0
xts the sets of,1,1,1.0
the sets of training,1,1,1.0
sets of training and,1,1,1.0
of training and testing,1,1,1.0
training and testing inputs,1,1,1.0
and testing inputs respectively,1,1,1.0
testing inputs respectively furthermore,1,1,1.0
inputs respectively furthermore we,1,1,1.0
respectively furthermore we will,1,1,1.0
furthermore we will mark,1,1,1.0
we will mark by,1,1,1.0
will mark by subscript,1,1,1.0
mark by subscript and,1,1,1.0
by subscript and to,1,1,1.0
subscript and to the,1,1,1.0
and to the sets,1,1,1.0
to the sets containing,1,1,1.0
the sets containing inputs,1,1,1.0
sets containing inputs from,1,1,1.0
containing inputs from the,1,1,1.0
inputs from the positive,1,1,1.0
from the positive and,1,1,1.0
the positive and negative,2,1,2.0
positive and negative class,1,1,1.0
and negative class respectively,1,1,1.0
negative class respectively for,1,1,1.0
class respectively for a,1,1,1.0
respectively for a set,1,1,1.0
for a set x,1,1,1.0
a set x we,1,1,1.0
set x we denote,1,1,1.0
x we denote by,1,1,1.0
we denote by x,1,1,1.0
denote by x the,1,1,1.0
by x the design,1,1,1.0
x the design matrix,1,1,1.0
the design matrix storing,3,1,3.0
design matrix storing points,2,1,2.0
matrix storing points of,2,1,2.0
storing points of x,1,1,1.0
points of x as,1,1,1.0
of x as rows,1,1,1.0
x as rows reproducing,1,1,1.0
as rows reproducing kernels,1,1,1.0
rows reproducing kernels often,1,1,1.0
reproducing kernels often referred,1,1,1.0
kernels often referred as,1,1,1.0
often referred as mercer,1,1,1.0
referred as mercer kernels,1,1,1.0
as mercer kernels are,1,1,1.0
mercer kernels are functions,1,1,1.0
kernels are functions k,1,1,1.0
are functions k x,1,1,1.0
functions k x x,1,1,1.0
k x x r,1,1,1.0
x x r which,1,1,1.0
x r which for,1,1,1.0
r which for all,1,1,1.0
which for all pattern,1,1,1.0
for all pattern sets,1,1,1.0
all pattern sets xm,1,1,1.0
pattern sets xm give,1,1,1.0
sets xm give rise,1,1,1.0
xm give rise to,1,1,1.0
give rise to semideﬁnite,1,1,1.0
rise to semideﬁnite positive,1,1,1.0
to semideﬁnite positive matrices,1,1,1.0
semideﬁnite positive matrices where,1,1,1.0
positive matrices where kij,1,1,1.0
matrices where kij k,1,1,1.0
where kij k xi,1,1,1.0
kij k xi xj,1,1,1.0
k xi xj kernel,1,1,1.0
xi xj kernel functions,1,1,1.0
xj kernel functions allow,1,1,1.0
kernel functions allow us,1,1,1.0
functions allow us to,1,1,1.0
allow us to derive,1,1,1.0
us to derive nonlinear,1,1,1.0
to derive nonlinear classiﬁers,1,1,1.0
derive nonlinear classiﬁers by,1,1,1.0
nonlinear classiﬁers by reducing,1,1,1.0
classiﬁers by reducing them,1,1,1.0
by reducing them to,1,1,1.0
reducing them to linear,1,1,1.0
them to linear ones,1,1,1.0
to linear ones but,1,1,1.0
linear ones but in,1,1,1.0
ones but in some,1,1,1.0
but in some hilbert,1,1,1.0
in some hilbert space,1,1,1.0
some hilbert space h,1,1,1.0
hilbert space h nonlinearly,1,1,1.0
space h nonlinearly related,1,1,1.0
h nonlinearly related to,1,1,1.0
nonlinearly related to the,1,1,1.0
related to the input,1,1,1.0
the input space and,2,1,2.0
input space and furnished,1,1,1.0
space and furnished with,1,1,1.0
and furnished with a,1,1,1.0
furnished with a dot,1,1,1.0
with a dot product,1,1,1.0
a dot product k,1,1,1.0
dot product k xi,1,1,1.0
product k xi xj,1,1,1.0
k xi xj xi,1,1,1.0
xi xj xi φ,1,1,1.0
xj xi φ xj,1,1,1.0
xi φ xj the,1,1,1.0
φ xj the use,1,1,1.0
xj the use of,1,1,1.0
use of this kernel,1,1,1.0
of this kernel function,1,1,1.0
this kernel function instead,1,1,1.0
kernel function instead of,1,1,1.0
function instead of the,1,1,1.0
instead of the dot,1,1,1.0
of the dot product,1,1,1.0
the dot product in,1,1,1.0
dot product in rm,1,1,1.0
product in rm corresponds,1,1,1.0
in rm corresponds to,1,1,1.0
rm corresponds to using,1,1,1.0
corresponds to using a,1,1,1.0
to using a usually,1,1,1.0
using a usually nonlinear,1,1,1.0
a usually nonlinear mapping,1,1,1.0
usually nonlinear mapping of,1,1,1.0
nonlinear mapping of patterns,1,1,1.0
mapping of patterns from,1,1,1.0
of patterns from x,1,1,1.0
patterns from x to,1,1,1.0
from x to a,1,1,1.0
x to a or,1,1,1.0
to a or dimensional,1,1,1.0
a or dimensional hilbert,1,1,1.0
or dimensional hilbert space,1,1,1.0
dimensional hilbert space h,1,1,1.0
hilbert space h such,1,1,1.0
space h such that,1,1,1.0
h such that φ,1,1,1.0
such that φ x,1,1,1.0
that φ x h,1,1,1.0
φ x h where,1,1,1.0
x h where the,1,1,1.0
h where the separation,1,1,1.0
where the separation would,1,1,1.0
the separation would ideally,1,1,1.0
separation would ideally be,1,1,1.0
would ideally be easier,1,1,1.0
ideally be easier and,1,1,1.0
be easier and take,1,1,1.0
easier and take the,1,1,1.0
and take the dot,1,1,1.0
take the dot product,1,1,1.0
the dot product there,1,1,1.0
dot product there kernel,1,1,1.0
product there kernel machines,1,1,1.0
there kernel machines trained,1,1,1.0
kernel machines trained on,1,1,1.0
machines trained on d,1,1,1.0
trained on d do,1,1,1.0
on d do not,1,1,1.0
d do not operate,1,1,1.0
do not operate on,1,1,1.0
not operate on the,1,1,1.0
operate on the whole,1,1,1.0
on the whole of,1,1,1.0
the whole of h,1,1,1.0
whole of h but,1,1,1.0
of h but on,1,1,1.0
h but on its,1,1,1.0
but on its subset,1,1,1.0
on its subset f,1,1,1.0
its subset f span,1,1,1.0
subset f span φ,1,1,1.0
f span φ φ,1,1,1.0
span φ φ xm,1,1,1.0
φ φ xm which,1,1,1.0
φ xm which we,1,1,1.0
xm which we will,1,1,1.0
which we will refer,1,1,1.0
we will refer to,1,1,1.0
will refer to as,1,1,1.0
refer to as the,1,1,1.0
to as the feature,1,1,1.0
as the feature space,1,1,1.0
the feature space such,1,1,1.0
feature space such that,1,1,1.0
space such that f,1,1,1.0
such that f h,1,1,1.0
that f h note,1,1,1.0
f h note that,1,1,1.0
h note that f,1,1,1.0
note that f is,1,1,1.0
that f is at,1,1,1.0
f is at most,1,1,1.0
is at most an,1,1,1.0
at most an linear,1,1,1.0
most an linear support,1,1,1.0
an linear support v,1,1,1.0
linear support v ector,1,1,1.0
support v ector machines,3,1,3.0
v ector machines svm,2,1,2.0
ector machines svm is,1,1,1.0
machines svm is perhaps,1,1,1.0
svm is perhaps the,1,1,1.0
is perhaps the most,1,1,1.0
the most common kernel,1,1,1.0
most common kernel method,1,1,1.0
common kernel method for,1,1,1.0
kernel method for statistical,1,1,1.0
method for statistical pattern,1,1,1.0
for statistical pattern recognition,1,1,1.0
statistical pattern recognition due,1,1,1.0
pattern recognition due to,1,1,1.0
recognition due to its,1,1,1.0
due to its good,1,1,1.0
to its good generalisation,1,1,1.0
its good generalisation ability,1,1,1.0
generalisation ability and freedom,1,1,1.0
ability and freedom from,1,1,1.0
and freedom from local,1,1,1.0
freedom from local minima,1,1,1.0
from local minima the,1,1,1.0
local minima the basic,1,1,1.0
minima the basic idea,1,1,1.0
idea behind this technique,1,1,1.0
behind this technique is,1,1,1.0
this technique is the,1,1,1.0
technique is the separation,1,1,1.0
is the separation of,1,1,1.0
the separation of two,1,1,1.0
separation of two different,1,1,1.0
of two different classes,1,1,1.0
two different classes through,1,1,1.0
different classes through a,1,1,1.0
classes through a hyperplane,1,1,1.0
through a hyperplane which,1,1,1.0
a hyperplane which is,1,1,1.0
hyperplane which is speciﬁed,1,1,1.0
which is speciﬁed by,1,1,1.0
is speciﬁed by a,1,1,1.0
speciﬁed by a normal,1,1,1.0
by a normal vector,1,1,1.0
a normal vector w,1,1,1.0
normal vector w and,1,1,1.0
vector w and a,1,1,1.0
w and a bias,2,1,2.0
and a bias the,1,1,1.0
a bias the optimal,1,1,1.0
bias the optimal separating,1,1,1.0
the optimal separating hyperplane,1,1,1.0
optimal separating hyperplane is,1,1,1.0
separating hyperplane is the,1,1,1.0
hyperplane is the one,1,1,1.0
is the one which,1,1,1.0
the one which maximises,1,1,1.0
one which maximises the,1,1,1.0
which maximises the distance,2,1,2.0
maximises the distance between,2,1,2.0
the distance between the,1,1,1.0
distance between the hyperplane,1,1,1.0
between the hyperplane and,1,1,1.0
the hyperplane and the,1,1,1.0
hyperplane and the nearest,1,1,1.0
and the nearest points,1,1,1.0
the nearest points in,1,1,1.0
nearest points in both,1,1,1.0
points in both classes,1,1,1.0
in both classes called,1,1,1.0
both classes called margin,1,1,1.0
classes called margin beyond,1,1,1.0
called margin beyond the,1,1,1.0
margin beyond the application,1,1,1.0
beyond the application of,1,1,1.0
the application of kernel,1,1,1.0
application of kernel techniques,1,1,1.0
of kernel techniques to,1,1,1.0
kernel techniques to allow,1,1,1.0
techniques to allow decision,1,1,1.0
to allow decision discriminants,1,1,1.0
allow decision discriminants the,1,1,1.0
decision discriminants the kernel,1,1,1.0
discriminants the kernel trick,1,1,1.0
the kernel trick another,1,1,1.0
kernel trick another generalisation,1,1,1.0
trick another generalisation was,1,1,1.0
another generalisation was made,1,1,1.0
generalisation was made to,1,1,1.0
was made to replace,1,1,1.0
made to replace hard,1,1,1.0
to replace hard margins,1,1,1.0
replace hard margins with,1,1,1.0
hard margins with soft,1,1,1.0
margins with soft margins,1,1,1.0
with soft margins using,1,1,1.0
soft margins using the,1,1,1.0
margins using the ξi,1,1,1.0
using the ξi in,1,1,1.0
the ξi in order,1,1,1.0
ξi in order to,1,1,1.0
in order to deal,1,1,1.0
order to deal with,1,1,1.0
to deal with overlapping,1,1,1.0
deal with overlapping classes,1,1,1.0
with overlapping classes therefore,1,1,1.0
overlapping classes therefore this,1,1,1.0
classes therefore this algorithm,1,1,1.0
therefore this algorithm seeks,1,1,1.0
this algorithm seeks for,1,1,1.0
algorithm seeks for a,1,1,1.0
seeks for a classiﬁer,1,1,1.0
for a classiﬁer f,1,1,1.0
a classiﬁer f rd,1,1,1.0
classiﬁer f rd r,1,1,1.0
f rd r of,1,1,1.0
rd r of the,1,1,1.0
r of the form,1,1,1.0
of the form f,1,1,1.0
the form f x,1,1,1.0
form f x w,1,1,1.0
f x w φ,1,1,1.0
x w φ x,1,1,1.0
w φ x b,1,1,1.0
φ x b φ,1,1,1.0
x b φ being,1,1,1.0
b φ being the,1,1,1.0
φ being the mapping,1,1,1.0
being the mapping function,1,1,1.0
the mapping function induced,1,1,1.0
mapping function induced by,1,1,1.0
function induced by the,1,1,1.0
induced by the kernel,1,1,1.0
by the kernel that,1,1,1.0
the kernel that minimises,2,1,2.0
kernel that minimises the,2,1,2.0
that minimises the objective,1,1,1.0
minimises the objective function,1,1,1.0
the objective function c,1,1,1.0
objective function c ξi,1,1,1.0
function c ξi for,1,1,1.0
c ξi for some,1,1,1.0
ξi for some parameter,1,1,1.0
for some parameter c,1,1,1.0
some parameter c subject,1,1,1.0
parameter c subject to,1,1,1.0
c subject to the,1,1,1.0
subject to the constraints,2,1,2.0
to the constraints yi,2,1,2.0
the constraints yi w,2,1,2.0
constraints yi w φ,2,1,2.0
yi w φ xi,2,1,2.0
w φ xi b,2,1,2.0
φ xi b ξi,2,1,2.0
xi b ξi ξ,2,1,2.0
b ξi ξ i,2,1,2.0
ξi ξ i m,2,1,2.0
ξ i m it,1,1,1.0
i m it is,1,1,1.0
m it is clear,1,1,1.0
is clear that using,1,1,1.0
clear that using svms,1,1,1.0
that using svms the,1,1,1.0
using svms the maximization,1,1,1.0
svms the maximization paradigm,1,1,1.0
the maximization paradigm poses,1,1,1.0
maximization paradigm poses a,1,1,1.0
paradigm poses a serious,2,1,2.0
poses a serious hindrance,1,1,1.0
a serious hindrance for,1,1,1.0
serious hindrance for imbalanced,1,1,1.0
hindrance for imbalanced datasets,1,1,1.0
for imbalanced datasets the,1,1,1.0
imbalanced datasets the main,1,1,1.0
datasets the main reason,1,1,1.0
the main reason for,1,1,1.0
main reason for this,1,1,1.0
reason for this is,1,1,1.0
for this is that,1,1,1.0
this is that svm,1,1,1.0
is that svm optimisation,1,1,1.0
that svm optimisation is,1,1,1.0
svm optimisation is focused,1,1,1.0
optimisation is focused on,1,1,1.0
is focused on overall,1,1,1.0
focused on overall error,1,1,1.0
on overall error therefore,1,1,1.0
overall error therefore they,1,1,1.0
error therefore they are,1,1,1.0
therefore they are inherently,1,1,1.0
they are inherently biased,1,1,1.0
are inherently biased toward,1,1,1.0
inherently biased toward the,1,1,1.0
biased toward the majority,1,1,1.0
toward the majority class,1,1,1.0
class in the worst,1,1,1.0
in the worst case,1,1,1.0
the worst case for,1,1,1.0
worst case for a,1,1,1.0
case for a noisy,1,1,1.0
for a noisy and,1,1,1.0
a noisy and highly,1,1,1.0
noisy and highly imbalanced,1,1,1.0
and highly imbalanced dataset,1,1,1.0
highly imbalanced dataset the,1,1,1.0
imbalanced dataset the svm,1,1,1.0
dataset the svm paradigm,1,1,1.0
the svm paradigm is,1,1,1.0
svm paradigm is very,1,1,1.0
paradigm is very likely,1,1,1.0
is very likely to,1,1,1.0
very likely to obtain,1,1,1.0
likely to obtain a,1,1,1.0
to obtain a trivial,1,1,1.0
obtain a trivial classiﬁer,2,1,2.0
a trivial classiﬁer the,1,1,1.0
trivial classiﬁer the one,1,1,1.0
classiﬁer the one that,1,1,1.0
the one that classiﬁes,1,1,1.0
one that classiﬁes all,1,1,1.0
that classiﬁes all the,1,1,1.0
classiﬁes all the patterns,1,1,1.0
all the patterns in,1,1,1.0
the patterns in the,1,1,1.0
patterns in the majority,1,1,1.0
majority class a solution,1,1,1.0
class a solution that,1,1,1.0
a solution that as,1,1,1.0
solution that as said,1,1,1.0
that as said if,1,1,1.0
as said if the,1,1,1.0
said if the imbalance,1,1,1.0
if the imbalance is,1,1,1.0
the imbalance is severe,1,1,1.0
imbalance is severe could,1,1,1.0
is severe could provide,1,1,1.0
severe could provide the,1,1,1.0
could provide the minimal,1,1,1.0
provide the minimal error,1,1,1.0
the minimal error t,1,1,1.0
minimal error t o,1,1,1.0
error t o cope,1,1,1.0
with this issue several,1,1,1.0
this issue several studies,1,1,1.0
issue several studies in,1,1,1.0
several studies in the,1,1,1.0
studies in the machine,1,1,1.0
in the machine learning,2,1,2.0
the machine learning literature,2,1,2.0
machine learning literature have,1,1,1.0
learning literature have explored,1,1,1.0
literature have explored different,1,1,1.0
have explored different solutions,1,1,1.0
explored different solutions to,1,1,1.0
different solutions to the,1,1,1.0
solutions to the imbalanced,1,1,1.0
to the imbalanced classiﬁcation,1,1,1.0
the imbalanced classiﬁcation problem,1,1,1.0
imbalanced classiﬁcation problem considering,1,1,1.0
classiﬁcation problem considering the,1,1,1.0
problem considering the svm,1,1,1.0
considering the svm paradigm,1,1,1.0
the svm paradigm most,1,1,1.0
svm paradigm most of,1,1,1.0
paradigm most of them,1,1,1.0
most of them are,1,1,1.0
of them are based,1,1,1.0
them are based on,1,1,1.0
are based on classiﬁcation,1,1,1.0
based on classiﬁcation ensembles,1,1,1.0
on classiﬁcation ensembles and,1,1,1.0
classiﬁcation ensembles and kernel,1,1,1.0
ensembles and kernel optimisation,1,1,1.0
and kernel optimisation techniques,1,1,1.0
kernel optimisation techniques among,1,1,1.0
optimisation techniques among others,1,1,1.0
techniques among others however,1,1,1.0
among others however some,1,1,1.0
others however some studies,1,1,1.0
however some studies suggest,1,1,1.0
some studies suggest that,1,1,1.0
studies suggest that sampling,1,1,1.0
suggest that sampling is,1,1,1.0
that sampling is not,1,1,1.0
sampling is not as,1,1,1.0
is not as effective,1,1,1.0
not as effective as,1,1,1.0
as effective as in,1,1,1.0
effective as in this,1,1,1.0
as in this case,1,1,1.0
in this case because,1,1,1.0
this case because of,1,1,1.0
case because of the,1,1,1.0
because of the potential,1,1,1.0
of the potential loss,1,1,1.0
the potential loss of,1,1,1.0
potential loss of information,1,1,1.0
loss of information on,1,1,1.0
of information on the,1,1,1.0
information on the class,1,1,1.0
on the class boundaries,1,1,1.0
the class boundaries which,1,1,1.0
class boundaries which is,1,1,1.0
boundaries which is crucial,1,1,1.0
which is crucial for,1,1,1.0
is crucial for the,1,1,1.0
crucial for the svm,1,1,1.0
for the svm solution,1,1,1.0
the svm solution synthetic,1,1,1.0
svm solution synthetic minority,1,1,1.0
solution synthetic minority technique,1,1,1.0
minority technique smote as,1,1,1.0
technique smote as stated,1,1,1.0
smote as stated one,1,1,1.0
as stated one of,1,1,1.0
stated one of the,1,1,1.0
most widely used techniques,1,1,1.0
widely used techniques for,1,1,1.0
used techniques for sampling,1,1,1.0
techniques for sampling is,1,1,1.0
for sampling is the,1,1,1.0
sampling is the smote,1,1,1.0
is the smote algorithm,1,1,1.0
the smote algorithm the,2,2,1.0
smote algorithm the process,1,1,1.0
algorithm the process is,1,1,1.0
the process is very,1,1,1.0
process is very simple,1,1,1.0
is very simple the,1,1,1.0
very simple the method,1,1,1.0
simple the method consists,1,1,1.0
the method consists on,1,1,1.0
method consists on generating,1,1,1.0
consists on generating new,1,1,1.0
on generating new instances,1,1,1.0
generating new instances on,1,1,1.0
new instances on the,1,1,1.0
instances on the line,1,1,1.0
on the line that,1,1,1.0
the line that connects,1,1,1.0
line that connects one,1,1,1.0
that connects one randomly,1,1,1.0
connects one randomly chosen,1,1,1.0
one randomly chosen point,1,1,1.0
randomly chosen point with,1,1,1.0
chosen point with one,1,1,1.0
point with one of,1,1,1.0
with one of its,1,1,1.0
one of its nearest,1,1,1.0
of its nearest neighbours,1,1,1.0
its nearest neighbours both,1,1,1.0
nearest neighbours both belonging,1,1,1.0
the minority class therefore,1,1,1.0
minority class therefore this,1,1,1.0
class therefore this methodology,1,1,1.0
therefore this methodology relies,1,1,1.0
this methodology relies on,1,1,1.0
methodology relies on a,1,1,1.0
relies on a convex,1,1,1.0
on a convex combination,1,1,1.0
a convex combination of,2,1,2.0
convex combination of two,1,1,1.0
combination of two patterns,1,1,1.0
of two patterns note,1,1,1.0
two patterns note that,1,1,1.0
patterns note that with,1,1,1.0
note that with this,1,1,1.0
that with this approach,1,1,1.0
with this approach new,1,1,1.0
this approach new patterns,1,1,1.0
approach new patterns could,1,1,1.0
new patterns could lie,1,1,1.0
patterns could lie inside,1,1,1.0
could lie inside the,1,1,1.0
lie inside the majority,1,1,1.0
inside the majority class,1,1,1.0
the majority class region,4,1,4.0
majority class region although,1,1,1.0
class region although choosing,1,1,1.0
region although choosing a,1,1,1.0
although choosing a correct,1,1,1.0
choosing a correct value,1,1,1.0
a correct value for,1,1,1.0
correct value for the,1,1,1.0
value for the k,1,1,1.0
for the k parameter,1,1,1.0
the k parameter of,1,1,1.0
k parameter of the,1,1,1.0
parameter of the neighbours,1,1,1.0
of the neighbours method,2,1,2.0
the neighbours method could,1,1,1.0
neighbours method could avoid,1,1,1.0
method could avoid this,1,1,1.0
could avoid this to,1,1,1.0
avoid this to happen,1,1,1.0
this to happen in,1,1,1.0
to happen in some,1,1,1.0
happen in some cases,1,1,1.0
in some cases empirical,1,1,1.0
some cases empirical feature,1,1,1.0
cases empirical feature space,1,1,1.0
feature space efs w,1,1,1.0
space efs w e,1,1,1.0
efs w e can,1,1,1.0
w e can endow,1,1,1.0
e can endow an,1,1,1.0
can endow an r,1,1,1.0
endow an r m,1,1,1.0
an r m space,1,1,1.0
r m space f,1,1,1.0
m space f with,1,1,1.0
space f with an,1,1,1.0
f with an orthonormal,1,1,1.0
with an orthonormal basis,1,1,1.0
an orthonormal basis ug,1,1,1.0
orthonormal basis ug g,1,1,1.0
basis ug g b,1,1,1.0
ug g b b,1,1,1.0
g b b r,1,1,1.0
b b r satisfying,1,1,1.0
b r satisfying orthogonality,1,1,1.0
r satisfying orthogonality normalisation,1,1,1.0
satisfying orthogonality normalisation and,1,1,1.0
orthogonality normalisation and completeness,1,1,1.0
normalisation and completeness consider,1,1,1.0
and completeness consider the,1,1,1.0
completeness consider the set,1,1,1.0
consider the set e,1,1,1.0
the set e ϕ,1,1,1.0
set e ϕ v,1,1,1.0
e ϕ v f,1,1,1.0
ϕ v f where,1,1,1.0
v f where ϕ,1,1,1.0
f where ϕ v,1,1,1.0
where ϕ v the,1,1,1.0
ϕ v the map,1,1,1.0
v the map ϕ,1,1,1.0
the map ϕ is,1,1,1.0
map ϕ is an,1,1,1.0
ϕ is an isometric,1,1,1.0
is an isometric isomorphism,1,1,1.0
an isometric isomorphism of,1,1,1.0
isometric isomorphism of f,1,1,1.0
isomorphism of f and,1,1,1.0
of f and e,1,1,1.0
f and e a,1,1,1.0
and e a bijective,1,1,1.0
e a bijective linear,1,1,1.0
a bijective linear mapping,2,1,2.0
bijective linear mapping such,1,1,1.0
linear mapping such that,1,1,1.0
mapping such that the,1,1,1.0
such that the dot,1,1,1.0
the dot products are,1,1,1.0
dot products are preserved,1,1,1.0
products are preserved v,1,1,1.0
are preserved v ϕ,1,1,1.0
preserved v ϕ when,1,1,1.0
v ϕ when f,1,1,1.0
ϕ when f is,1,1,1.0
when f is the,1,1,1.0
f is the feature,1,1,1.0
is the feature space,1,1,1.0
the feature space the,1,1,1.0
feature space the set,1,1,1.0
space the set e,1,1,1.0
the set e is,1,1,1.0
set e is referred,1,1,1.0
e is referred to,1,1,1.0
is referred to as,2,1,2.0
referred to as empirical,1,1,1.0
to as empirical feature,1,1,1.0
as empirical feature space,1,1,1.0
feature space efs consider,1,1,1.0
space efs consider a,1,1,1.0
efs consider a set,1,1,1.0
consider a set of,1,1,1.0
a set of training,1,1,1.0
set of training points,1,1,1.0
of training points xi,1,1,1.0
training points xi m,1,1,1.0
points xi m x,1,1,1.0
xi m x then,1,1,1.0
m x then when,1,1,1.0
x then when working,1,1,1.0
then when working with,1,1,1.0
when working with kernel,1,1,1.0
working with kernel methods,1,1,1.0
with kernel methods we,1,1,1.0
kernel methods we use,1,1,1.0
methods we use a,1,1,1.0
we use a kernel,1,1,1.0
use a kernel function,1,1,1.0
a kernel function k,1,1,1.0
kernel function k to,1,1,1.0
function k to map,1,1,1.0
k to map the,1,1,1.0
to map the patterns,1,1,1.0
map the patterns to,1,1,1.0
the patterns to the,1,1,1.0
patterns to the feature,1,1,1.0
the feature space f,1,1,1.0
feature space f and,1,1,1.0
space f and thus,1,1,1.0
f and thus obtain,1,1,1.0
and thus obtain a,1,1,1.0
thus obtain a gram,1,1,1.0
obtain a gram matrix,1,1,1.0
a gram matrix k,1,1,1.0
gram matrix k with,1,1,1.0
matrix k with rank,1,1,1.0
k with rank r,1,1,1.0
with rank r r,1,1,1.0
rank r r the,1,1,1.0
r r the nonlinear,1,1,1.0
r the nonlinear map,1,1,1.0
the nonlinear map from,1,1,1.0
nonlinear map from the,1,1,1.0
map from the input,1,1,1.0
from the input space,1,1,1.0
the input space to,1,1,1.0
input space to the,1,1,1.0
space to the euclidean,1,1,1.0
to the euclidean space,1,1,1.0
the euclidean space φ,1,1,1.0
euclidean space φ e,1,1,1.0
space φ e r,1,1,1.0
φ e r x,1,1,1.0
e r x rr,1,1,1.0
r x rr which,1,1,1.0
x rr which preserves,1,1,1.0
rr which preserves the,1,1,1.0
which preserves the feature,1,1,1.0
preserves the feature space,1,1,1.0
feature space structure is,1,1,1.0
space structure is referred,1,1,1.0
structure is referred to,1,1,1.0
referred to as the,1,1,1.0
to as the empirical,1,1,1.0
as the empirical kernel,1,1,1.0
the empirical kernel map,9,1,9.0
empirical kernel map the,1,1,1.0
kernel map the efs,1,1,1.0
map the efs e,1,1,1.0
the efs e is,1,1,1.0
efs e is chosen,1,1,1.0
e is chosen so,1,1,1.0
is chosen so as,1,1,1.0
chosen so as to,1,1,1.0
so as to preserve,1,1,1.0
as to preserve the,1,1,1.0
to preserve the dot,1,1,1.0
preserve the dot product,1,1,1.0
the dot product information,1,1,1.0
dot product information about,1,1,1.0
product information about f,1,1,1.0
information about f contained,1,1,1.0
about f contained in,1,1,1.0
f contained in k,1,1,1.0
contained in k to,1,1,1.0
in k to be,1,1,1.0
k to be isometric,1,1,1.0
to be isometric isomorphic,1,1,1.0
be isometric isomorphic to,1,1,1.0
isometric isomorphic to the,1,1,1.0
isomorphic to the embedded,1,1,1.0
to the embedded feature,1,1,1.0
the embedded feature space,1,1,1.0
embedded feature space f,1,1,1.0
feature space f h,1,1,1.0
space f h in,1,1,1.0
f h in this,1,1,1.0
h in this sense,1,1,1.0
in this sense it,1,1,1.0
this sense it can,1,1,1.0
sense it can be,1,1,1.0
be said that the,2,1,2.0
said that the empirical,1,1,1.0
that the empirical kernel,1,1,1.0
empirical kernel map corresponds,1,1,1.0
kernel map corresponds to,1,1,1.0
map corresponds to a,1,1,1.0
corresponds to a bijective,1,1,1.0
to a bijective linear,1,1,1.0
bijective linear mapping ϕ,1,1,1.0
linear mapping ϕ f,1,1,1.0
mapping ϕ f e,1,1,1.0
ϕ f e a,1,1,1.0
f e a graphical,1,1,1.0
e a graphical representation,1,1,1.0
a graphical representation of,2,1,2.0
graphical representation of the,1,1,1.0
representation of the input,1,1,1.0
of the input space,3,1,3.0
the input space feature,1,1,1.0
input space feature space,2,1,2.0
space feature space efs,1,1,1.0
feature space efs and,1,1,1.0
space efs and mappings,1,1,1.0
efs and mappings between,1,1,1.0
and mappings between these,1,1,1.0
mappings between these spaces,1,1,1.0
between these spaces is,1,1,1.0
these spaces is shown,1,1,1.0
spaces is shown in,1,1,1.0
is shown in fig,1,1,1.0
shown in fig fig,1,1,1.0
in fig fig representation,1,1,1.0
fig fig representation of,1,1,1.0
fig representation of the,2,2,1.0
representation of the relation,1,1,1.0
of the relation and,1,1,1.0
the relation and mapping,1,1,1.0
relation and mapping between,1,1,1.0
and mapping between input,1,1,1.0
mapping between input space,1,1,1.0
between input space feature,1,1,1.0
space feature space and,1,1,1.0
feature space and empirical,1,1,1.0
space and empirical feature,1,1,1.0
and empirical feature space,2,1,2.0
empirical feature space any,1,1,1.0
feature space any given,1,1,1.0
space any given gram,1,1,1.0
any given gram matrix,1,1,1.0
given gram matrix k,1,1,1.0
gram matrix k of,1,1,1.0
matrix k of rank,2,1,2.0
k of rank r,2,1,2.0
of rank r can,1,1,1.0
rank r can be,1,1,1.0
r can be diagonalised,1,1,1.0
can be diagonalised as,1,1,1.0
be diagonalised as follows,1,1,1.0
diagonalised as follows λ,1,1,1.0
as follows λ pt,1,1,1.0
follows λ pt where,1,1,1.0
λ pt where t,1,1,1.0
pt where t is,1,1,1.0
where t is the,2,1,2.0
t is the transpose,1,1,1.0
is the transpose operation,1,1,1.0
the transpose operation λ,1,1,1.0
transpose operation λ is,1,1,1.0
operation λ is a,1,1,1.0
λ is a diagonal,1,1,1.0
is a diagonal matrix,1,1,1.0
a diagonal matrix containing,1,1,1.0
diagonal matrix containing the,1,1,1.0
matrix containing the r,1,1,1.0
containing the r nonzero,1,1,1.0
the r nonzero eigenvalues,1,1,1.0
r nonzero eigenvalues of,1,1,1.0
nonzero eigenvalues of k,1,1,1.0
eigenvalues of k in,1,1,1.0
of k in decreasing,1,1,1.0
k in decreasing order,1,1,1.0
in decreasing order λ,1,1,1.0
decreasing order λ r,1,1,1.0
order λ r and,1,1,1.0
λ r and p,1,1,1.0
r and p is,1,1,1.0
and p is a,1,1,1.0
p is a unitary,2,1,2.0
is a unitary matrix,2,1,2.0
a unitary matrix that,1,1,1.0
unitary matrix that consists,1,1,1.0
matrix that consists of,1,1,1.0
that consists of the,1,1,1.0
consists of the eigenvectors,1,1,1.0
of the eigenvectors associated,1,1,1.0
the eigenvectors associated to,1,1,1.0
eigenvectors associated to those,1,1,1.0
associated to those r,1,1,1.0
to those r eigenvalues,1,1,1.0
those r eigenvalues ur,1,1,1.0
r eigenvalues ur constituting,1,1,1.0
eigenvalues ur constituting an,1,1,1.0
ur constituting an orthonormal,1,1,1.0
constituting an orthonormal basis,1,1,1.0
an orthonormal basis of,2,1,2.0
orthonormal basis of rr,1,1,1.0
basis of rr then,1,1,1.0
of rr then the,1,1,1.0
rr then the empirical,1,1,1.0
then the empirical kernel,1,1,1.0
empirical kernel map is,1,1,1.0
kernel map is deﬁned,1,1,1.0
map is deﬁned as,1,1,1.0
is deﬁned as φ,1,1,1.0
deﬁned as φ e,1,1,1.0
as φ e r,1,1,1.0
φ e r xi,3,1,3.0
e r xi λ,1,1,1.0
r xi λ pt,1,1,1.0
xi λ pt k,1,1,1.0
λ pt k xi,1,1,1.0
pt k xi k,1,1,1.0
k xi k xi,1,1,1.0
xi k xi xm,1,1,1.0
k xi xm consider,1,1,1.0
xi xm consider the,1,1,1.0
xm consider the set,1,1,1.0
consider the set φ,1,1,1.0
the set φ e,1,1,1.0
set φ e r,1,1,1.0
φ e r φ,1,1,1.0
e r φ e,1,1,1.0
r φ e r,1,1,1.0
φ e r xm,1,1,1.0
e r xm of,1,1,1.0
r xm of the,1,1,1.0
xm of the efs,1,1,1.0
of the efs images,1,1,1.0
the efs images of,2,1,2.0
efs images of the,2,1,2.0
images of the training,2,1,2.0
of the training points,2,1,2.0
the training points let,1,1,1.0
training points let be,1,1,1.0
points let be the,1,1,1.0
let be the design,1,1,1.0
be the design matrix,1,1,1.0
design matrix storing φ,1,1,1.0
matrix storing φ e,1,1,1.0
storing φ e r,1,1,1.0
e r xi as,1,1,1.0
r xi as rows,1,1,1.0
xi as rows it,1,1,1.0
as rows it is,1,1,1.0
rows it is easy,1,1,1.0
it is easy to,1,1,1.0
is easy to check,1,1,1.0
easy to check that,1,1,1.0
to check that the,1,1,1.0
check that the standard,1,1,1.0
that the standard dot,1,1,1.0
the standard dot product,1,1,1.0
standard dot product matrix,1,1,1.0
dot product matrix of,1,1,1.0
product matrix of φ,1,1,1.0
matrix of φ e,1,1,1.0
of φ e r,1,1,1.0
e r xi i,1,1,1.0
r xi i m,1,1,1.0
xi i m evaluated,1,1,1.0
i m evaluated in,1,1,1.0
m evaluated in e,1,1,1.0
evaluated in e is,1,1,1.0
in e is k,1,1,1.0
e is k writing,1,1,1.0
is k writing z,1,1,1.0
k writing z λ,1,1,1.0
writing z λ pt,1,1,1.0
z λ pt k,1,1,1.0
λ pt k we,1,1,1.0
pt k we obtain,1,1,1.0
k we obtain ztz,1,1,1.0
we obtain ztz pλp,1,1,1.0
obtain ztz pλp tpλ,1,1,1.0
ztz pλp tpλ t,1,1,1.0
pλp tpλ t since,1,1,1.0
tpλ t since the,1,1,1.0
t since the distances,1,1,1.0
since the distances and,1,1,1.0
the distances and the,1,1,1.0
distances and the angles,1,1,1.0
and the angles of,1,1,1.0
the angles of the,1,1,1.0
angles of the m,1,1,1.0
of the m vectors,1,1,1.0
the m vectors φ,1,1,1.0
m vectors φ xi,1,1,1.0
vectors φ xi i,1,1,1.0
φ xi i m,1,1,1.0
xi i m in,1,1,1.0
i m in the,1,1,1.0
m in the feature,1,1,1.0
uniquely determined by the,1,1,1.0
determined by the note,1,1,1.0
by the note that,1,1,1.0
the note that p,1,1,1.0
note that p is,1,1,1.0
that p is a,1,1,1.0
a unitary matrix and,1,1,1.0
unitary matrix and k,1,1,1.0
matrix and k a,1,1,1.0
and k a symmetric,1,1,1.0
k a symmetric dot,1,1,1.0
a symmetric dot product,1,1,1.0
symmetric dot product xi,1,1,1.0
dot product xi xj,1,1,1.0
product xi xj k,1,1,1.0
xi xj k xi,2,1,2.0
xj k xi xi,1,1,1.0
k xi xi k,1,1,1.0
xi xi k xj,1,1,1.0
xi k xj xj,1,1,1.0
k xj xj xi,1,1,1.0
xj xj xi xj,1,1,1.0
xj xi xj the,1,1,1.0
xi xj the training,1,1,1.0
xj the training data,1,1,1.0
the training data have,1,1,1.0
training data have the,1,1,1.0
data have the same,1,1,1.0
have the same geometrical,1,1,1.0
the same geometrical ture,1,1,1.0
same geometrical ture in,1,1,1.0
geometrical ture in both,1,1,1.0
ture in both spaces,1,1,1.0
in both spaces f,1,1,1.0
both spaces f and,1,1,1.0
spaces f and however,1,1,1.0
f and however recall,1,1,1.0
and however recall that,1,1,1.0
however recall that the,1,1,1.0
recall that the map,1,1,1.0
that the map φ,1,1,1.0
the map φ into,1,1,1.0
map φ into the,1,1,1.0
φ into the feature,1,1,1.0
into the feature space,1,1,1.0
feature space is nonlinear,1,1,1.0
space is nonlinear therefore,1,1,1.0
is nonlinear therefore each,1,1,1.0
nonlinear therefore each point,1,1,1.0
therefore each point in,1,1,1.0
each point in the,1,1,1.0
point in the span,1,1,1.0
in the span of,1,1,1.0
the span of the,1,1,1.0
span of the mapped,1,1,1.0
of the mapped input,1,1,1.0
the mapped input data,1,1,1.0
mapped input data would,1,1,1.0
input data would not,1,1,1.0
data would not necessarily,1,1,1.0
would not necessarily be,1,1,1.0
not necessarily be the,1,1,1.0
necessarily be the image,1,1,1.0
be the image of,1,1,1.0
the image of some,1,1,1.0
image of some input,1,1,1.0
of some input pattern,1,1,1.0
some input pattern this,1,1,1.0
input pattern this is,1,1,1.0
pattern this is known,1,1,1.0
this is known as,1,1,1.0
known as the preimage,1,1,1.0
as the preimage problem,1,1,1.0
the preimage problem this,1,1,1.0
preimage problem this problem,1,1,1.0
problem this problem also,1,1,1.0
this problem also appears,1,1,1.0
problem also appears when,1,1,1.0
also appears when using,1,1,1.0
appears when using the,1,1,1.0
when using the empirical,1,1,1.0
using the empirical kernel,1,1,1.0
empirical kernel map because,1,1,1.0
kernel map because it,1,1,1.0
map because it also,1,1,1.0
because it also corresponds,1,1,1.0
it also corresponds to,1,1,1.0
also corresponds to a,1,1,1.0
corresponds to a nonlinear,1,1,1.0
to a nonlinear transformation,1,1,1.0
a nonlinear transformation note,1,1,1.0
nonlinear transformation note that,1,1,1.0
transformation note that this,1,1,1.0
note that this is,1,1,1.0
this is not a,1,1,1.0
is not a problem,2,1,2.0
not a problem for,1,1,1.0
a problem for the,1,1,1.0
problem for the of,1,1,1.0
for the of minority,1,1,1.0
the of minority class,1,1,1.0
of minority class since,1,1,1.0
minority class since the,1,1,1.0
class since the linear,1,1,1.0
since the linear decision,1,1,1.0
the linear decision boundary,1,1,1.0
linear decision boundary is,1,1,1.0
decision boundary is built,1,1,1.0
boundary is built in,1,1,1.0
is built in the,1,1,1.0
built in the feature,1,1,1.0
feature space and if,1,1,1.0
space and if the,1,1,1.0
and if the classes,1,1,1.0
if the classes are,1,1,1.0
the classes are almost,1,1,1.0
classes are almost linearly,1,1,1.0
are almost linearly separable,1,1,1.0
almost linearly separable in,1,1,1.0
linearly separable in the,1,1,1.0
separable in the feature,1,1,1.0
the feature space doing,1,1,1.0
feature space doing local,1,1,1.0
space doing local convex,1,1,1.0
doing local convex combination,1,1,1.0
local convex combination is,1,1,1.0
convex combination is reasonable,1,1,1.0
combination is reasonable whether,1,1,1.0
is reasonable whether the,1,1,1.0
reasonable whether the of,1,1,1.0
whether the of the,1,1,1.0
of the synthetic points,1,1,1.0
the synthetic points exist,1,1,1.0
synthetic points exist or,1,1,1.0
points exist or not,1,1,1.0
exist or not iii,1,1,1.0
or not iii s,1,1,1.0
not iii s y,1,1,1.0
iii s y n,1,1,1.0
s y n t,1,1,1.0
y n t h,1,1,1.0
n t h e,2,1,2.0
t h e t,1,1,1.0
h e t i,1,1,1.0
e t i c,1,1,1.0
t i c ov,1,1,1.0
i c ov e,1,1,1.0
c ov e r,1,1,1.0
ov e r a,3,1,3.0
e r a m,3,1,3.0
r a m p,3,1,3.0
a m p l,3,1,3.0
m p l i,3,1,3.0
p l i n,3,1,3.0
l i n g,3,1,3.0
i n g b,1,1,1.0
n g b y,1,1,1.0
g b y c,1,1,1.0
b y c o,1,1,1.0
y c o n,1,1,1.0
c o n v,1,1,1.0
o n v e,1,1,1.0
n v e x,1,1,1.0
v e x c,1,1,1.0
e x c o,1,1,1.0
x c o m,1,1,1.0
c o m b,1,1,1.0
o m b i,1,1,1.0
m b i nat,1,1,1.0
b i nat i,1,1,1.0
i nat i o,1,1,1.0
nat i o n,1,1,1.0
i o n i,1,1,1.0
o n i n,1,1,1.0
n i n t,1,1,1.0
i n t h,1,1,1.0
t h e efs,1,1,1.0
h e efs the,1,1,1.0
e efs the main,1,1,1.0
efs the main hypothesis,1,1,1.0
the main hypothesis in,1,1,1.0
main hypothesis in this,1,1,1.0
hypothesis in this section,1,1,1.0
in this section is,1,1,1.0
this section is that,1,1,1.0
section is that the,1,1,1.0
is that the efs,1,1,1.0
that the efs provide,1,1,1.0
the efs provide us,1,1,1.0
efs provide us with,1,1,1.0
provide us with a,2,1,2.0
us with a more,2,1,2.0
with a more suitable,1,1,1.0
a more suitable class,1,1,1.0
more suitable class distribution,1,1,1.0
suitable class distribution for,1,1,1.0
class distribution for it,1,1,1.0
distribution for it is,1,1,1.0
for it is clear,1,1,1.0
is clear that when,1,1,1.0
clear that when classes,1,1,1.0
that when classes are,1,1,1.0
when classes are nonlinearly,1,1,1.0
classes are nonlinearly separable,1,1,1.0
are nonlinearly separable which,1,1,1.0
nonlinearly separable which may,1,1,1.0
separable which may occur,1,1,1.0
which may occur in,1,1,1.0
may occur in the,1,1,1.0
occur in the input,1,1,1.0
the input space one,1,1,1.0
input space one should,1,1,1.0
space one should be,1,1,1.0
one should be very,1,1,1.0
should be very careful,1,1,1.0
be very careful when,1,1,1.0
very careful when creating,1,1,1.0
careful when creating synthetic,1,1,1.0
when creating synthetic patterns,1,1,1.0
creating synthetic patterns by,1,1,1.0
synthetic patterns by convex,1,1,1.0
patterns by convex combination,1,1,1.0
by convex combination because,1,1,1.0
convex combination because these,1,1,1.0
combination because these could,1,1,1.0
because these could lie,1,1,1.0
these could lie on,1,1,1.0
could lie on the,1,1,1.0
lie on the majority,1,1,1.0
majority class region however,1,1,1.0
class region however if,1,1,1.0
region however if the,1,1,1.0
however if the data,1,1,1.0
if the data are,1,1,1.0
the data are linearly,1,1,1.0
data are linearly separable,1,1,1.0
are linearly separable a,1,1,1.0
linearly separable a statement,1,1,1.0
separable a statement that,1,1,1.0
a statement that will,1,1,1.0
statement that will be,1,1,1.0
that will be true,1,1,1.0
will be true if,1,1,1.0
be true if the,1,1,1.0
true if the kernel,1,1,1.0
matches the underlying learning,1,1,1.0
the underlying learning problem,1,1,1.0
underlying learning problem by,1,1,1.0
learning problem by convex,1,1,1.0
problem by convex combination,1,1,1.0
combination of patterns is,1,1,1.0
of patterns is not,1,1,1.0
patterns is not a,1,1,1.0
not a problem t,1,1,1.0
a problem t o,1,1,1.0
problem t o illustrate,1,1,1.0
t o illustrate this,1,1,1.0
o illustrate this consider,1,1,1.0
illustrate this consider fig,1,1,1.0
this consider fig where,1,1,1.0
consider fig where a,1,1,1.0
fig where a toy,1,1,1.0
where a toy nonlinearly,1,1,1.0
a toy nonlinearly separable,1,1,1.0
toy nonlinearly separable dataset,1,1,1.0
nonlinearly separable dataset have,1,1,1.0
separable dataset have been,1,1,1.0
dataset have been represented,1,1,1.0
have been represented by,1,1,1.0
been represented by the,1,1,1.0
represented by the φ,1,1,1.0
by the φ e,1,1,1.0
the φ e transformation,1,1,1.0
φ e transformation using,1,1,1.0
e transformation using a,1,1,1.0
transformation using a gaussian,1,1,1.0
using a gaussian kernel,1,1,1.0
a gaussian kernel retaining,1,1,1.0
gaussian kernel retaining only,1,1,1.0
kernel retaining only two,1,1,1.0
retaining only two dominant,1,1,1.0
only two dominant dimensions,1,1,1.0
two dominant dimensions imbalanced,1,1,1.0
dominant dimensions imbalanced donut,1,1,1.0
dimensions imbalanced donut toy,1,1,1.0
imbalanced donut toy dataset,1,1,1.0
donut toy dataset separable,1,1,1.0
toy dataset separable projection,1,1,1.0
dataset separable projection of,1,1,1.0
separable projection of the,1,1,1.0
projection of the imbalanced,1,1,1.0
of the imbalanced toy,1,1,1.0
the imbalanced toy data,1,1,1.0
imbalanced toy data by,1,1,1.0
toy data by the,1,1,1.0
data by the empirical,1,1,1.0
by the empirical kernel,1,1,1.0
empirical kernel map linearly,1,1,1.0
kernel map linearly separable,1,1,1.0
map linearly separable fig,1,1,1.0
linearly separable fig synthethic,1,1,1.0
separable fig synthethic dataset,1,1,1.0
fig synthethic dataset representing,1,1,1.0
synthethic dataset representing a,1,1,1.0
dataset representing a linearly,1,1,1.0
representing a linearly separable,1,1,1.0
a linearly separable classiﬁcation,1,1,1.0
linearly separable classiﬁcation problem,1,1,1.0
separable classiﬁcation problem and,1,1,1.0
classiﬁcation problem and their,1,1,1.0
problem and their tion,1,1,1.0
and their tion to,1,1,1.0
their tion to the,1,1,1.0
tion to the dominant,1,1,1.0
to the dominant dimensions,2,1,2.0
the dominant dimensions of,2,1,2.0
dominant dimensions of the,3,1,3.0
dimensions of the efs,2,1,2.0
of the efs e,1,1,1.0
the efs e induced,1,1,1.0
efs e induced by,1,1,1.0
e induced by the,1,1,1.0
induced by the gaussian,2,1,2.0
by the gaussian kernel,2,1,2.0
the gaussian kernel function,2,1,2.0
gaussian kernel function linearly,2,1,2.0
kernel function linearly separable,2,1,2.0
function linearly separable problem,2,1,2.0
linearly separable problem reduced,1,1,1.0
separable problem reduced empirical,1,1,1.0
problem reduced empirical feature,1,1,1.0
reduced empirical feature space,3,1,3.0
empirical feature space in,2,1,2.0
feature space in this,1,1,1.0
space in this subsection,1,1,1.0
in this subsection we,2,1,2.0
this subsection we present,1,1,1.0
subsection we present a,1,1,1.0
we present a reduced,1,1,1.0
present a reduced version,1,1,1.0
a reduced version of,1,1,1.0
reduced version of the,1,1,1.0
version of the efs,1,1,1.0
of the efs where,1,1,1.0
the efs where we,1,1,1.0
efs where we select,1,1,1.0
where we select the,1,1,1.0
we select the q,1,1,1.0
select the q q,1,1,1.0
the q q r,1,1,1.0
q q r dominant,1,1,1.0
q r dominant dimensions,1,1,1.0
r dominant dimensions to,1,1,1.0
dominant dimensions to approximate,1,1,1.0
dimensions to approximate the,1,1,1.0
to approximate the kernel,1,1,1.0
approximate the kernel matrix,1,1,1.0
the kernel matrix in,2,2,1.0
kernel matrix in relation,1,1,1.0
matrix in relation to,1,1,1.0
in relation to classiﬁcation,1,1,1.0
relation to classiﬁcation it,1,1,1.0
to classiﬁcation it has,1,1,1.0
classiﬁcation it has been,1,1,1.0
it has been argued,1,1,1.0
has been argued that,1,1,1.0
been argued that most,1,1,1.0
argued that most decisive,1,1,1.0
that most decisive information,1,1,1.0
most decisive information can,1,1,1.0
decisive information can be,1,1,1.0
information can be contained,1,1,1.0
can be contained in,1,1,1.0
be contained in a,1,1,1.0
contained in a subspace,1,1,1.0
in a subspace of,1,1,1.0
a subspace of the,1,1,1.0
subspace of the feature,1,1,1.0
the feature space under,1,1,1.0
feature space under the,1,1,1.0
space under the assumption,1,1,1.0
under the assumption of,1,1,1.0
the assumption of smooth,1,1,1.0
assumption of smooth kernels,1,1,1.0
of smooth kernels matching,1,1,1.0
smooth kernels matching the,1,1,1.0
kernels matching the underlying,1,1,1.0
matching the underlying problem,1,1,1.0
the underlying problem however,1,1,1.0
underlying problem however for,1,1,1.0
problem however for the,1,1,1.0
however for the case,2,1,2.0
the case of svms,1,1,1.0
case of svms the,1,1,1.0
of svms the capacity,1,1,1.0
svms the capacity control,1,1,1.0
the capacity control inclusion,1,1,1.0
capacity control inclusion of,1,1,1.0
control inclusion of slacks,1,1,1.0
inclusion of slacks variables,1,1,1.0
of slacks variables and,1,1,1.0
slacks variables and dimensions,1,1,1.0
variables and dimensions associated,1,1,1.0
and dimensions associated with,1,1,1.0
dimensions associated with the,1,1,1.0
associated with the highest,1,1,1.0
with the highest eigenvalues,1,1,1.0
the highest eigenvalues of,1,1,1.0
highest eigenvalues of the,1,1,1.0
eigenvalues of the gram,1,1,1.0
of the gram matrix,1,1,1.0
the gram matrix parameter,1,1,1.0
gram matrix parameter for,1,1,1.0
matrix parameter for preventing,1,1,1.0
parameter for preventing is,1,1,1.0
for preventing is alent,1,1,1.0
preventing is alent to,1,1,1.0
is alent to some,1,1,1.0
alent to some form,1,1,1.0
to some form of,1,1,1.0
some form of regularisation,1,1,1.0
form of regularisation so,1,1,1.0
of regularisation so that,1,1,1.0
regularisation so that denoising,1,1,1.0
so that denoising is,1,1,1.0
that denoising is not,1,1,1.0
denoising is not necessary,1,1,1.0
is not necessary although,1,1,1.0
not necessary although it,1,1,1.0
necessary although it could,1,1,1.0
although it could be,1,1,1.0
it could be very,1,1,1.0
could be very useful,1,1,1.0
be very useful for,1,1,1.0
very useful for unregularised,1,1,1.0
useful for unregularised methods,1,1,1.0
for unregularised methods in,1,1,1.0
unregularised methods in this,1,1,1.0
methods in this section,1,1,1.0
this section we test,1,1,1.0
section we test whether,1,1,1.0
we test whether a,1,1,1.0
test whether a minority,1,1,1.0
whether a minority class,1,1,1.0
a minority class in,1,1,1.0
class in the reduced,1,1,1.0
in the reduced dimensionality,1,1,1.0
the reduced dimensionality efs,1,1,1.0
reduced dimensionality efs as,1,1,1.0
dimensionality efs as opposed,1,1,1.0
efs as opposed to,1,1,1.0
as opposed to in,1,1,1.0
opposed to in the,1,1,1.0
to in the full,1,1,1.0
in the full efs,1,1,1.0
the full efs can,1,1,1.0
full efs can be,1,1,1.0
efs can be beneﬁcial,1,1,1.0
can be beneﬁcial one,1,1,1.0
be beneﬁcial one motivation,1,1,1.0
beneﬁcial one motivation for,1,1,1.0
one motivation for in,1,1,1.0
motivation for in reduced,1,1,1.0
for in reduced dimensionality,1,1,1.0
in reduced dimensionality efs,1,1,1.0
reduced dimensionality efs is,1,1,1.0
dimensionality efs is that,1,1,1.0
efs is that the,1,1,1.0
is that the procedure,1,1,1.0
that the procedure relies,1,1,1.0
the procedure relies on,1,1,1.0
procedure relies on distances,1,1,1.0
relies on distances in,1,1,1.0
on distances in the,1,1,1.0
distances in the efs,1,1,1.0
in the efs to,1,1,1.0
the efs to perform,1,1,1.0
efs to perform the,1,1,1.0
to perform the neighbourhood,1,1,1.0
perform the neighbourhood analysis,1,1,1.0
the neighbourhood analysis roughly,1,1,1.0
neighbourhood analysis roughly speaking,1,1,1.0
analysis roughly speaking these,1,1,1.0
roughly speaking these distances,1,1,1.0
speaking these distances have,1,1,1.0
these distances have been,1,1,1.0
distances have been proven,1,1,1.0
have been proven to,1,1,1.0
been proven to be,1,1,1.0
proven to be misleading,1,1,1.0
to be misleading as,1,1,1.0
be misleading as the,1,1,1.0
misleading as the data,1,1,1.0
as the data dimensionality,1,1,1.0
the data dimensionality increases,1,1,1.0
data dimensionality increases making,1,1,1.0
dimensionality increases making more,1,1,1.0
increases making more probable,1,1,1.0
making more probable that,1,1,1.0
more probable that the,1,1,1.0
probable that the neighbours,1,1,1.0
that the neighbours are,1,1,1.0
the neighbours are chosen,1,1,1.0
neighbours are chosen in,1,1,1.0
are chosen in a,1,1,1.0
chosen in a random,1,1,1.0
in a random fashion,1,1,1.0
a random fashion it,1,1,1.0
random fashion it is,1,1,1.0
fashion it is that,1,1,1.0
it is that for,1,1,1.0
is that for any,1,1,1.0
that for any real,1,1,1.0
for any real symmetric,1,1,1.0
any real symmetric m,1,1,1.0
real symmetric m m,1,1,1.0
symmetric m m matrix,1,1,1.0
m m matrix k,1,1,1.0
m matrix k of,1,1,1.0
of rank r we,1,1,1.0
rank r we can,1,1,1.0
r we can ﬁnd,1,1,1.0
we can ﬁnd its,1,1,1.0
can ﬁnd its real,1,1,1.0
ﬁnd its real nonzero,1,1,1.0
its real nonzero eigenvalues,1,1,1.0
real nonzero eigenvalues λr,1,1,1.0
nonzero eigenvalues λr and,1,1,1.0
eigenvalues λr and the,1,1,1.0
λr and the corresponding,1,1,1.0
and the corresponding orthonormal,1,1,1.0
the corresponding orthonormal eigenvectors,1,1,1.0
corresponding orthonormal eigenvectors ur,1,1,1.0
orthonormal eigenvectors ur so,1,1,1.0
eigenvectors ur so that,1,1,1.0
ur so that k,1,1,1.0
so that k r,1,1,1.0
that k r λiuiut,1,1,1.0
k r λiuiut i,1,1,1.0
r λiuiut i in,1,1,1.0
λiuiut i in this,1,1,1.0
i in this case,1,1,1.0
in this case the,5,1,5.0
this case the best,1,1,1.0
case the best q,1,1,1.0
the best q r,1,1,1.0
best q r approximation,1,1,1.0
q r approximation to,1,1,1.0
r approximation to k,1,1,1.0
approximation to k is,1,1,1.0
to k is kq,1,1,1.0
k is kq q,1,1,1.0
is kq q λiuiut,1,1,1.0
kq q λiuiut i,1,1,1.0
q λiuiut i in,1,1,1.0
λiuiut i in the,1,1,1.0
i in the sense,1,1,1.0
in the sense that,2,1,2.0
the sense that it,2,1,2.0
sense that it minimises,1,1,1.0
that it minimises f,1,1,1.0
it minimises f over,1,1,1.0
minimises f over all,1,1,1.0
f over all q,1,1,1.0
over all q matrices,1,1,1.0
all q matrices where,1,1,1.0
q matrices where f,1,1,1.0
matrices where f denotes,1,1,1.0
where f denotes the,1,1,1.0
f denotes the frobenius,1,1,1.0
denotes the frobenius norm,1,1,1.0
the frobenius norm this,1,1,1.0
frobenius norm this concept,1,1,1.0
norm this concept can,1,1,1.0
this concept can be,1,1,1.0
concept can be said,1,1,1.0
can be said to,2,1,2.0
be said to be,2,1,2.0
said to be the,1,1,1.0
to be the main,1,1,1.0
be the main idea,1,1,1.0
the main idea for,1,1,1.0
main idea for the,1,1,1.0
idea for the reduced,1,1,1.0
for the reduced efs,1,1,1.0
the reduced efs instead,1,1,1.0
reduced efs instead of,1,1,1.0
efs instead of working,1,1,1.0
instead of working in,1,1,1.0
of working in the,1,1,1.0
working in the efs,1,1,1.0
in the efs e,3,1,3.0
the efs e we,1,1,1.0
efs e we can,1,1,1.0
e we can operate,1,1,1.0
we can operate in,1,1,1.0
can operate in its,1,1,1.0
operate in its lower,1,1,1.0
in its lower dimensional,1,1,1.0
its lower dimensional subspace,1,1,1.0
lower dimensional subspace e,1,1,1.0
dimensional subspace e q,1,1,1.0
subspace e q where,1,1,1.0
e q where the,1,1,1.0
q where the kernel,1,1,1.0
where the kernel matrix,1,1,1.0
the kernel matrix has,1,1,1.0
kernel matrix has the,1,1,1.0
matrix has the form,1,1,1.0
has the form k,1,1,1.0
the form k q,1,1,1.0
form k q p,1,1,1.0
k q p q,1,1,1.0
q p q λ,1,1,1.0
p q λ q,1,1,1.0
q λ q p,1,1,1.0
λ q p q,1,1,1.0
q p q t,1,1,1.0
p q t q,1,1,1.0
q t q r,1,1,1.0
t q r where,1,1,1.0
q r where p,1,1,1.0
r where p q,1,1,1.0
where p q and,1,1,1.0
p q and λ,1,1,1.0
q and λ q,1,1,1.0
and λ q consist,1,1,1.0
λ q consist of,1,1,1.0
q consist of the,1,1,1.0
consist of the ﬁrst,1,1,1.0
of the ﬁrst q,1,1,1.0
the ﬁrst q columns,1,1,1.0
ﬁrst q columns of,1,1,1.0
q columns of p,1,1,1.0
columns of p and,1,1,1.0
of p and λ,1,1,1.0
p and λ respectively,1,1,1.0
and λ respectively consider,1,1,1.0
λ respectively consider the,1,1,1.0
respectively consider the preimage,1,1,1.0
consider the preimage f,1,1,1.0
the preimage f q,1,1,1.0
preimage f q of,1,1,1.0
f q of e,1,1,1.0
q of e q,1,1,1.0
of e q under,1,1,1.0
e q under the,1,1,1.0
q under the isomorphism,1,1,1.0
under the isomorphism ϕ,1,1,1.0
the isomorphism ϕ let,1,1,1.0
isomorphism ϕ let uj,1,1,1.0
ϕ let uj q,1,1,1.0
let uj q be,1,1,1.0
uj q be an,1,1,1.0
q be an orthonormal,1,1,1.0
be an orthonormal basis,1,1,1.0
orthonormal basis of f,1,1,1.0
basis of f q,1,1,1.0
of f q given,1,1,1.0
f q given v,1,1,1.0
q given v f,1,1,1.0
given v f its,1,1,1.0
v f its projection,1,1,1.0
f its projection onto,1,1,1.0
its projection onto f,1,1,1.0
projection onto f q,1,1,1.0
onto f q is,1,1,1.0
f q is obtained,1,1,1.0
q is obtained as,1,1,1.0
is obtained as uj,1,1,1.0
obtained as uj q,1,1,1.0
as uj q the,1,1,1.0
uj q the isomorphism,1,1,1.0
q the isomorphism ϕ,1,1,1.0
the isomorphism ϕ from,1,1,1.0
isomorphism ϕ from f,1,1,1.0
ϕ from f to,1,1,1.0
from f to e,1,1,1.0
f to e carries,1,1,1.0
to e carries the,1,1,1.0
e carries the structure,1,1,1.0
carries the structure over,1,1,1.0
the structure over ϕ,1,1,1.0
structure over ϕ v,1,1,1.0
over ϕ v e,1,1,1.0
ϕ v e is,1,1,1.0
v e is projected,1,1,1.0
e is projected onto,1,1,1.0
is projected onto e,1,1,1.0
projected onto e q,1,1,1.0
onto e q as,1,1,1.0
e q as v,1,1,1.0
q as v ϕ,1,1,1.0
as v ϕ uj,1,1,1.0
v ϕ uj q,1,1,1.0
ϕ uj q moreover,1,1,1.0
uj q moreover for,1,1,1.0
q moreover for all,1,1,1.0
moreover for all j,1,1,1.0
for all j q,1,1,1.0
all j q uj,1,1,1.0
j q uj v,1,1,1.0
q uj v ϕ,1,1,1.0
uj v ϕ uj,1,1,1.0
v ϕ uj therefore,1,1,1.0
ϕ uj therefore we,1,1,1.0
uj therefore we could,1,1,1.0
therefore we could deﬁne,1,1,1.0
we could deﬁne the,1,1,1.0
could deﬁne the kernel,1,1,1.0
deﬁne the kernel associated,1,1,1.0
the kernel associated with,1,1,1.0
kernel associated with the,1,1,1.0
associated with the reduced,1,1,1.0
with the reduced efs,1,1,1.0
the reduced efs by,1,1,1.0
reduced efs by k,1,1,1.0
efs by k q,1,1,1.0
by k q xi,1,1,1.0
k q xi xj,1,1,1.0
q xi xj φ,1,1,1.0
xi xj φ e,1,1,1.0
xj φ e q,2,1,2.0
φ e q xi,10,1,10.0
e q xi φ,4,1,4.0
q xi φ e,4,1,4.0
xi φ e q,6,1,6.0
φ e q xj,2,1,2.0
e q xj e,1,1,1.0
q xj e which,1,1,1.0
xj e which for,1,1,1.0
e which for q,1,1,1.0
which for q being,1,1,1.0
for q being the,1,1,1.0
q being the rank,1,1,1.0
being the rank of,1,1,1.0
the rank of k,1,1,1.0
rank of k will,1,1,1.0
of k will correspond,1,1,1.0
k will correspond to,1,1,1.0
will correspond to synthetic,1,1,1.0
correspond to synthetic minority,1,1,1.0
to synthetic minority in,1,1,1.0
synthetic minority in the,1,1,1.0
minority in the reduced,1,1,1.0
in the reduced or,1,1,1.0
the reduced or rank,1,1,1.0
reduced or rank efs,1,1,1.0
or rank efs once,1,1,1.0
rank efs once that,1,1,1.0
efs once that the,1,1,1.0
once that the notion,1,1,1.0
that the notion of,1,1,1.0
the notion of efs,1,1,1.0
notion of efs has,1,1,1.0
of efs has been,1,1,1.0
efs has been introduced,1,1,1.0
has been introduced this,1,1,1.0
been introduced this subsection,1,1,1.0
introduced this subsection will,1,1,1.0
this subsection will show,1,1,1.0
subsection will show the,1,1,1.0
will show the main,1,1,1.0
show the main steps,1,1,1.0
the main steps to,1,1,1.0
main steps to extend,1,1,1.0
steps to extend a,1,1,1.0
to extend a algorithm,1,1,1.0
extend a algorithm to,1,1,1.0
a algorithm to this,1,1,1.0
algorithm to this space,1,1,1.0
to this space concerning,1,1,1.0
this space concerning the,1,1,1.0
space concerning the training,1,1,1.0
concerning the training phase,1,1,1.0
the training phase the,1,1,1.0
training phase the ﬁrst,1,1,1.0
phase the ﬁrst step,1,1,1.0
the ﬁrst step of,1,1,1.0
ﬁrst step of the,1,1,1.0
step of the proposed,1,1,1.0
of the proposed methodology,1,1,1.0
the proposed methodology corresponds,1,1,1.0
proposed methodology corresponds to,1,1,1.0
methodology corresponds to the,1,1,1.0
corresponds to the computation,1,1,1.0
to the computation of,1,1,1.0
the computation of the,2,1,2.0
computation of the training,1,1,1.0
of the training kernel,2,1,2.0
the training kernel matrix,2,1,2.0
training kernel matrix k,3,1,3.0
kernel matrix k through,1,1,1.0
matrix k through a,1,1,1.0
k through a predeﬁned,1,1,1.0
through a predeﬁned kernel,1,1,1.0
a predeﬁned kernel function,1,1,1.0
predeﬁned kernel function then,1,1,1.0
kernel function then the,1,1,1.0
function then the reduced,1,1,1.0
then the reduced or,1,1,1.0
the reduced or empirical,1,1,1.0
reduced or empirical kernel,1,1,1.0
or empirical kernel map,1,1,1.0
empirical kernel map φ,4,1,4.0
kernel map φ e,4,1,4.0
map φ e q,4,1,4.0
φ e q q,2,1,2.0
e q q r,2,1,2.0
q q r can,1,1,1.0
q r can be,1,1,1.0
r can be computed,1,1,1.0
can be computed via,1,1,1.0
be computed via the,1,1,1.0
computed via the eigenvector,1,1,1.0
via the eigenvector decomposition,1,1,1.0
the eigenvector decomposition of,1,1,1.0
eigenvector decomposition of this,1,1,1.0
decomposition of this training,1,1,1.0
of this training kernel,1,1,1.0
this training kernel matrix,1,1,1.0
kernel matrix k eq,1,1,1.0
matrix k eq as,1,1,1.0
k eq as said,1,1,1.0
eq as said let,1,1,1.0
as said let z,1,1,1.0
said let z be,1,1,1.0
let z be the,1,1,1.0
z be the set,1,1,1.0
be the set generated,1,1,1.0
the set generated by,1,1,1.0
set generated by applying,1,1,1.0
generated by applying the,1,1,1.0
by applying the φ,1,1,1.0
applying the φ e,1,1,1.0
the φ e q,1,1,1.0
φ e q transformation,1,1,1.0
e q transformation to,1,1,1.0
q transformation to the,1,1,1.0
transformation to the training,1,1,1.0
to the training patterns,1,1,1.0
the training patterns and,1,1,1.0
training patterns and the,1,1,1.0
patterns and the design,1,1,1.0
and the design matrix,2,1,2.0
storing points of z,1,1,1.0
points of z as,1,1,1.0
of z as rows,1,1,1.0
z as rows in,1,1,1.0
as rows in the,1,1,1.0
rows in the second,1,1,1.0
in the second step,1,1,1.0
the second step the,1,1,1.0
second step the process,1,1,1.0
step the process is,1,1,1.0
the process is w,1,1,1.0
process is w e,1,1,1.0
is w e assume,1,1,1.0
w e assume that,1,1,1.0
e assume that the,1,1,1.0
assume that the singular,1,1,1.0
that the singular values,1,1,1.0
the singular values are,1,1,1.0
singular values are performed,1,1,1.0
values are performed over,1,1,1.0
are performed over the,1,1,1.0
performed over the minority,1,1,1.0
the minority class images,1,1,1.0
minority class images of,1,1,1.0
class images of this,1,1,1.0
images of this z,1,1,1.0
of this z matrix,1,1,1.0
this z matrix resulting,1,1,1.0
z matrix resulting in,1,1,1.0
matrix resulting in the,1,1,1.0
resulting in the generation,1,1,1.0
the generation of n,1,1,1.0
generation of n new,1,1,1.0
of n new synthetic,1,1,1.0
n new synthetic images,1,1,1.0
new synthetic images arranged,1,1,1.0
synthetic images arranged in,1,1,1.0
images arranged in the,1,1,1.0
arranged in the set,1,1,1.0
in the set s,1,1,1.0
the set s and,1,1,1.0
set s and the,1,1,1.0
s and the design,1,1,1.0
the design matrix more,1,1,1.0
design matrix more speciﬁcally,1,1,1.0
matrix more speciﬁcally as,1,1,1.0
more speciﬁcally as the,1,1,1.0
speciﬁcally as the standard,1,1,1.0
as the standard smote,1,1,1.0
the standard smote algorithm,1,1,1.0
standard smote algorithm has,1,1,1.0
smote algorithm has been,1,1,1.0
algorithm has been chosen,1,1,1.0
has been chosen for,1,1,1.0
been chosen for each,1,1,1.0
chosen for each new,1,1,1.0
for each new synthetic,1,1,1.0
each new synthetic instance,1,1,1.0
new synthetic instance will,1,1,1.0
synthetic instance will be,1,1,1.0
instance will be generated,1,1,1.0
will be generated using,1,1,1.0
be generated using a,1,1,1.0
generated using a linear,1,1,1.0
using a linear interpolation,1,1,1.0
a linear interpolation between,1,1,1.0
linear interpolation between pattern,1,1,1.0
interpolation between pattern xi,1,1,1.0
between pattern xi and,1,1,1.0
pattern xi and one,1,1,1.0
xi and one of,1,1,1.0
the minority class at,1,1,1.0
minority class at every,1,1,1.0
class at every step,1,1,1.0
at every step j,1,1,1.0
every step j n,1,1,1.0
step j n we,1,1,1.0
j n we create,1,1,1.0
n we create a,1,1,1.0
we create a point,1,1,1.0
create a point sj,1,1,1.0
a point sj in,1,1,1.0
point sj in e,1,1,1.0
sj in e q,1,1,1.0
in e q by,1,1,1.0
e q by picking,1,1,1.0
q by picking at,1,1,1.0
by picking at random,1,1,1.0
picking at random a,1,1,1.0
at random a minority,1,1,1.0
random a minority class,1,1,1.0
a minority class point,1,1,1.0
minority class point xi,1,1,1.0
class point xi and,1,1,1.0
point xi and calculating,1,1,1.0
xi and calculating sj,1,1,1.0
and calculating sj φ,1,1,1.0
calculating sj φ e,1,1,1.0
sj φ e q,1,1,1.0
φ e q ˆ,4,1,4.0
e q ˆ xi,5,1,5.0
q ˆ xi φ,2,1,2.0
ˆ xi φ e,2,1,2.0
e q xi δ,2,1,2.0
q xi δ where,2,1,2.0
xi δ where φ,1,1,1.0
δ where φ e,1,1,1.0
where φ e q,1,1,1.0
q ˆ xi is,2,1,2.0
ˆ xi is one,2,1,2.0
xi is one of,2,1,2.0
one of the neighbours,2,1,2.0
of the neighbours for,1,1,1.0
the neighbours for φ,1,1,1.0
neighbours for φ e,1,1,1.0
for φ e q,1,1,1.0
e q xi in,2,1,2.0
q xi in the,2,1,2.0
xi in the efs,2,1,2.0
the efs e q,2,1,2.0
efs e q and,1,1,1.0
e q and δ,1,1,1.0
q and δ is,1,1,1.0
and δ is a,1,1,1.0
δ is a random,2,1,2.0
a random number generated,2,1,2.0
random number generated from,2,1,2.0
number generated from the,2,1,2.0
generated from the uniform,2,1,2.0
from the uniform distribution,2,1,2.0
the uniform distribution u,2,1,2.0
uniform distribution u for,1,1,1.0
distribution u for simplicity,1,1,1.0
u for simplicity we,1,1,1.0
for simplicity we the,1,1,1.0
simplicity we the minority,1,1,1.0
we the minority class,1,1,1.0
the minority class so,1,1,1.0
minority class so that,1,1,1.0
class so that the,1,1,1.0
so that the two,1,1,1.0
that the two classes,1,1,1.0
the two classes become,1,1,1.0
two classes become balanced,1,1,1.0
classes become balanced from,1,1,1.0
become balanced from the,1,1,1.0
balanced from the deﬁnition,1,1,1.0
from the deﬁnition of,1,1,1.0
the deﬁnition of the,1,1,1.0
deﬁnition of the efs,1,1,1.0
of the efs we,1,1,1.0
the efs we know,1,1,1.0
efs we know that,1,1,1.0
we know that ϕ,1,1,1.0
know that ϕ sj,1,1,1.0
that ϕ sj f,1,1,1.0
ϕ sj f q,1,1,1.0
sj f q the,1,1,1.0
f q the representation,1,1,1.0
q the representation of,1,1,1.0
the representation of the,2,1,2.0
representation of the new,1,1,1.0
of the new pattern,1,1,1.0
the new pattern in,1,1,1.0
new pattern in the,1,1,1.0
pattern in the feature,1,1,1.0
the feature space will,1,1,1.0
feature space will be,1,1,1.0
space will be unique,1,1,1.0
will be unique and,1,1,1.0
be unique and will,1,1,1.0
unique and will lie,1,1,1.0
and will lie on,1,1,1.0
lie on the line,1,1,1.0
on the line between,1,1,1.0
the line between ϕ,1,1,1.0
line between ϕ xi,1,1,1.0
between ϕ xi and,1,1,1.0
ϕ xi and ϕ,1,1,1.0
xi and ϕ ˆ,1,1,1.0
and ϕ ˆ xi,1,1,1.0
ϕ ˆ xi ϕ,1,1,1.0
ˆ xi ϕ is,1,1,1.0
xi ϕ is a,1,1,1.0
ϕ is a linear,1,1,1.0
is a linear map,1,1,1.0
a linear map recall,1,1,1.0
linear map recall that,1,1,1.0
map recall that the,1,1,1.0
recall that the norms,1,1,1.0
that the norms and,1,1,1.0
the norms and distances,1,1,1.0
norms and distances are,1,1,1.0
and distances are preserved,1,1,1.0
distances are preserved e,1,1,1.0
are preserved e q,1,1,1.0
preserved e q ˆ,1,1,1.0
e q xi ˆ,1,1,1.0
q xi ˆ xi,1,1,1.0
xi ˆ xi φ,1,1,1.0
ˆ xi φ xi,2,1,2.0
xi φ xi and,1,1,1.0
φ xi and so,1,1,1.0
xi and so are,1,1,1.0
and so are the,1,1,1.0
so are the angles,1,1,1.0
are the angles φ,1,1,1.0
the angles φ e,1,1,1.0
angles φ e q,1,1,1.0
q ˆ xi t,1,1,1.0
ˆ xi t φ,1,1,1.0
xi t φ e,1,1,1.0
t φ e q,1,1,1.0
e q xi sj,1,1,1.0
q xi sj xi,1,1,1.0
xi sj xi φ,1,1,1.0
sj xi φ ˆ,1,1,1.0
xi φ ˆ xi,1,1,1.0
φ ˆ xi φ,1,1,1.0
xi φ xi ϕ,1,1,1.0
φ xi ϕ sj,1,1,1.0
xi ϕ sj as,1,1,1.0
ϕ sj as a,1,1,1.0
sj as a consequence,1,1,1.0
as a consequence if,1,1,1.0
a consequence if φ,1,1,1.0
consequence if φ e,1,1,1.0
if φ e q,1,1,1.0
of the neighbours of,1,1,1.0
the neighbours of φ,1,1,1.0
neighbours of φ e,1,1,1.0
of φ e q,1,1,1.0
in the efs this,1,1,1.0
the efs this will,1,1,1.0
efs this will be,1,1,1.0
this will be so,1,1,1.0
will be so in,1,1,1.0
be so in the,1,1,1.0
so in the feature,1,1,1.0
the feature space as,1,1,1.0
feature space as well,1,1,1.0
space as well the,1,1,1.0
as well the third,1,1,1.0
well the third step,1,1,1.0
the third step is,1,1,1.0
third step is the,1,1,1.0
step is the execution,1,1,1.0
is the execution of,1,1,1.0
the execution of the,1,1,1.0
execution of the learning,1,1,1.0
of the learning machine,2,1,2.0
the learning machine over,1,1,1.0
learning machine over the,1,1,1.0
machine over the set,1,1,1.0
over the set ϕ,1,1,1.0
the set ϕ z,1,1,1.0
set ϕ z s,1,1,1.0
ϕ z s f,1,1,1.0
z s f q,1,1,1.0
s f q in,1,1,1.0
f q in this,1,1,1.0
q in this case,1,1,1.0
in this case there,1,1,1.0
this case there are,1,1,1.0
case there are two,1,1,1.0
there are two different,1,1,1.0
are two different possibilities,1,1,1.0
two different possibilities to,1,1,1.0
different possibilities to consider,1,1,1.0
possibilities to consider first,1,1,1.0
to consider first we,1,1,1.0
consider first we could,1,1,1.0
first we could employ,1,1,1.0
we could employ the,1,1,1.0
could employ the efs,1,1,1.0
employ the efs as,1,1,1.0
the efs as a,1,1,1.0
efs as a new,1,1,1.0
as a new representation,1,1,1.0
a new representation for,1,1,1.0
new representation for the,1,1,1.0
representation for the data,1,1,1.0
for the data and,1,1,1.0
the data and use,1,1,1.0
data and use the,2,2,1.0
and use the classiﬁcation,1,1,1.0
use the classiﬁcation algorithm,1,1,1.0
the classiﬁcation algorithm in,1,1,1.0
classiﬁcation algorithm in this,1,1,1.0
algorithm in this new,1,1,1.0
in this new space,1,1,1.0
this new space as,1,1,1.0
new space as done,1,1,1.0
space as done in,1,1,1.0
as done in other,3,1,3.0
done in other works,3,1,3.0
in other works this,1,1,1.0
other works this idea,1,1,1.0
works this idea will,1,1,1.0
this idea will provide,1,1,1.0
idea will provide us,1,1,1.0
will provide us with,1,1,1.0
with a more easily,1,1,1.0
a more easily separable,1,1,1.0
more easily separable and,1,1,1.0
easily separable and balanced,1,1,1.0
separable and balanced space,1,1,1.0
and balanced space than,1,1,1.0
balanced space than the,1,1,1.0
space than the input,1,1,1.0
than the input space,3,1,3.0
the input space which,1,1,1.0
input space which could,1,1,1.0
space which could indeed,1,1,1.0
which could indeed be,1,1,1.0
could indeed be used,1,1,1.0
indeed be used for,1,1,1.0
be used for any,2,1,2.0
used for any learning,1,1,1.0
for any learning machine,1,1,1.0
any learning machine independently,1,1,1.0
learning machine independently of,1,1,1.0
machine independently of being,1,1,1.0
independently of being kernelized,1,1,1.0
of being kernelized or,1,1,1.0
being kernelized or not,1,1,1.0
kernelized or not however,1,1,1.0
or not however when,1,1,1.0
not however when dealing,1,1,1.0
however when dealing with,1,1,1.0
when dealing with a,1,1,1.0
dealing with a kernel,1,1,1.0
with a kernel function,1,1,1.0
a kernel function it,1,1,1.0
kernel function it could,1,1,1.0
function it could actually,1,1,1.0
it could actually be,1,1,1.0
could actually be more,1,1,1.0
actually be more advisable,1,1,1.0
be more advisable to,1,1,1.0
more advisable to recompute,1,1,1.0
advisable to recompute the,1,1,1.0
to recompute the dot,1,1,1.0
recompute the dot products,1,1,1.0
the dot products between,1,1,1.0
dot products between patterns,1,1,1.0
products between patterns create,1,1,1.0
between patterns create a,1,1,1.0
patterns create a new,1,1,1.0
create a new sampled,1,1,1.0
a new sampled kernel,1,1,1.0
new sampled kernel matrix,1,1,1.0
sampled kernel matrix due,1,1,1.0
kernel matrix due to,1,1,1.0
matrix due to the,1,1,1.0
due to the high,1,1,1.0
to the high number,1,1,1.0
the high number of,1,1,1.0
high number of features,1,1,1.0
of features the dimensionality,1,1,1.0
features the dimensionality of,1,1,1.0
of the efs which,1,1,1.0
the efs which in,1,1,1.0
efs which in most,1,1,1.0
which in most of,1,1,1.0
in most of the,4,1,4.0
most of the cases,3,1,3.0
of the cases will,1,1,1.0
the cases will increase,1,1,1.0
cases will increase the,1,1,1.0
will increase the computational,1,1,1.0
increase the computational cost,1,1,1.0
the computational cost of,2,1,2.0
computational cost of the,1,1,1.0
cost of the learning,1,1,1.0
the learning machine considered,1,1,1.0
learning machine considered t,1,1,1.0
machine considered t o,1,1,1.0
considered t o do,1,1,1.0
t o do so,5,1,5.0
o do so synthetic,1,1,1.0
do so synthetic samples,1,1,1.0
so synthetic samples will,1,1,1.0
synthetic samples will be,1,1,1.0
samples will be used,1,1,1.0
will be used to,1,1,1.0
be used to complete,1,1,1.0
used to complete the,1,1,1.0
to complete the kernel,1,1,1.0
complete the kernel matrix,1,1,1.0
the kernel matrix by,1,1,1.0
kernel matrix by obtaining,1,1,1.0
matrix by obtaining their,1,1,1.0
by obtaining their dot,1,1,1.0
obtaining their dot product,1,1,1.0
their dot product in,1,1,1.0
dot product in the,1,1,1.0
product in the efs,1,1,1.0
in the efs with,1,1,1.0
the efs with respect,1,1,1.0
efs with respect to,1,1,1.0
respect to the rest,1,1,1.0
to the rest of,1,1,1.0
the rest of the,1,1,1.0
rest of the training,1,1,1.0
of the training patterns,3,1,3.0
the training patterns using,1,1,1.0
training patterns using this,1,1,1.0
patterns using this approach,1,1,1.0
using this approach the,1,1,1.0
this approach the sampled,1,1,1.0
approach the sampled training,1,1,1.0
the sampled training gram,1,1,1.0
sampled training gram matrix,1,1,1.0
training gram matrix will,1,1,1.0
gram matrix will be,2,1,2.0
matrix will be composed,2,1,2.0
will be composed as,2,1,2.0
be composed as follows,2,1,2.0
composed as follows z,1,1,1.0
as follows z zt,1,1,1.0
follows z zt z,1,1,1.0
z zt z st,1,1,1.0
zt z st s,1,1,1.0
z st s zt,1,1,1.0
st s zt s,1,1,1.0
s zt s st,1,1,1.0
zt s st note,1,1,1.0
s st note that,1,1,1.0
st note that for,1,1,1.0
note that for any,1,1,1.0
that for any number,1,1,1.0
for any number of,1,1,1.0
any number of dominant,1,1,1.0
number of dominant dimensions,3,1,3.0
of dominant dimensions q,1,1,1.0
dominant dimensions q for,1,1,1.0
dimensions q for the,1,1,1.0
q for the empirical,1,1,1.0
for the empirical kernel,1,1,1.0
φ e q the,1,1,1.0
e q the kernel,1,1,1.0
q the kernel matrix,1,1,1.0
the kernel matrix obtained,2,1,2.0
kernel matrix obtained will,1,1,1.0
matrix obtained will be,1,1,1.0
obtained will be positive,1,1,1.0
will be positive semideﬁnite,1,1,1.0
be positive semideﬁnite furthermore,1,1,1.0
positive semideﬁnite furthermore since,1,1,1.0
semideﬁnite furthermore since we,1,1,1.0
furthermore since we are,1,1,1.0
since we are generating,1,1,1.0
we are generating new,1,1,1.0
are generating new patterns,1,1,1.0
generating new patterns by,1,1,1.0
new patterns by a,1,1,1.0
patterns by a linear,1,1,1.0
by a linear combination,1,1,1.0
a linear combination of,1,1,1.0
linear combination of other,1,1,1.0
combination of other patterns,1,1,1.0
of other patterns in,1,1,1.0
other patterns in the,1,1,1.0
patterns in the dataset,1,1,1.0
in the dataset the,1,1,1.0
the dataset the empirical,1,1,1.0
dataset the empirical kernel,1,1,1.0
the empirical kernel maps,1,1,1.0
empirical kernel maps associated,1,1,1.0
kernel maps associated to,1,1,1.0
maps associated to ϕ,1,1,1.0
associated to ϕ z,1,1,1.0
to ϕ z and,1,1,1.0
ϕ z and to,1,1,1.0
z and to ϕ,1,1,1.0
and to ϕ z,1,1,1.0
to ϕ z s,1,1,1.0
ϕ z s can,1,1,1.0
z s can be,1,1,1.0
s can be said,1,1,1.0
said to be equivalent,1,1,1.0
to be equivalent for,1,1,1.0
be equivalent for the,1,1,1.0
equivalent for the generalisation,1,1,1.0
for the generalisation phase,1,1,1.0
the generalisation phase the,1,1,1.0
generalisation phase the same,1,1,1.0
phase the same steps,1,1,1.0
the same steps are,1,1,1.0
same steps are considered,1,1,1.0
steps are considered to,1,1,1.0
are considered to complete,1,1,1.0
considered to complete the,1,1,1.0
to complete the test,1,1,1.0
complete the test kernel,2,1,2.0
the test kernel matrix,2,1,2.0
test kernel matrix considering,1,1,1.0
kernel matrix considering that,1,1,1.0
matrix considering that the,1,1,1.0
considering that the efs,1,1,1.0
that the efs images,1,1,1.0
images of the test,1,1,1.0
of the test patterns,2,1,2.0
the test patterns are,1,1,1.0
test patterns are derived,1,1,1.0
patterns are derived using,1,1,1.0
are derived using the,1,1,1.0
derived using the same,1,1,1.0
using the same φ,1,1,1.0
the same φ e,1,1,1.0
same φ e q,1,1,1.0
φ e q map,1,1,1.0
e q map considering,1,1,1.0
q map considering only,1,1,1.0
map considering only the,1,1,1.0
considering only the training,1,1,1.0
only the training data,1,1,1.0
the training data note,1,1,1.0
training data note that,1,1,1.0
data note that in,1,1,1.0
note that in this,2,1,2.0
that in this case,2,1,2.0
in this case we,2,2,1.0
this case we will,1,1,1.0
case we will compute,1,1,1.0
we will compute the,1,1,1.0
will compute the dot,1,1,1.0
compute the dot product,1,1,1.0
the dot product between,3,1,3.0
dot product between train,1,1,1.0
product between train and,1,1,1.0
between train and test,1,1,1.0
train and test patterns,1,1,1.0
and test patterns and,1,1,1.0
test patterns and between,1,1,1.0
patterns and between test,1,1,1.0
and between test and,1,1,1.0
between test and synthetic,1,1,1.0
test and synthetic patterns,1,1,1.0
and synthetic patterns the,1,1,1.0
synthetic patterns the sampled,1,1,1.0
patterns the sampled test,1,1,1.0
the sampled test gram,1,1,1.0
sampled test gram matrix,1,1,1.0
test gram matrix will,1,1,1.0
composed as follows t,1,1,1.0
as follows t z,1,1,1.0
follows t z tt,1,1,1.0
t z tt s,1,1,1.0
z tt s tt,1,1,1.0
tt s tt where,1,1,1.0
s tt where t,1,1,1.0
tt where t is,1,1,1.0
t is the representation,1,1,1.0
is the representation in,1,1,1.0
the representation in the,1,1,1.0
representation in the efs,1,1,1.0
in the efs of,1,1,1.0
the efs of the,1,1,1.0
efs of the test,1,1,1.0
the test patterns and,1,1,1.0
test patterns and t,1,1,1.0
patterns and t corresponds,1,1,1.0
and t corresponds to,1,1,1.0
t corresponds to the,1,1,1.0
corresponds to the number,1,1,1.0
the number of test,1,1,1.0
number of test patterns,1,1,1.0
of test patterns note,1,1,1.0
test patterns note that,1,1,1.0
patterns note that these,1,1,1.0
note that these new,1,1,1.0
that these new kernel,1,1,1.0
these new kernel matrices,1,1,1.0
new kernel matrices and,1,1,1.0
kernel matrices and can,1,1,1.0
matrices and can be,1,1,1.0
and can be used,1,1,1.0
can be used for,1,1,1.0
used for any algorithm,1,1,1.0
for any algorithm a,1,1,1.0
any algorithm a summary,1,1,1.0
algorithm a summary of,1,1,1.0
a summary of this,1,1,1.0
summary of this method,1,1,1.0
of this method can,1,1,1.0
can be seen in,4,1,4.0
be seen in fig,2,1,2.0
seen in fig algorithm,1,1,1.0
in fig algorithm synthetic,1,1,1.0
fig algorithm synthetic in,1,1,1.0
algorithm synthetic in the,1,1,1.0
synthetic in the empirical,1,1,1.0
in the empirical feature,7,1,7.0
empirical feature space input,1,1,1.0
feature space input training,1,1,1.0
space input training patterns,1,1,1.0
input training patterns xtr,1,1,1.0
training patterns xtr training,1,1,1.0
patterns xtr training targets,1,1,1.0
xtr training targets ytr,1,1,1.0
training targets ytr and,1,1,1.0
targets ytr and testing,1,1,1.0
ytr and testing patterns,1,1,1.0
and testing patterns xts,1,1,1.0
testing patterns xts output,1,1,1.0
patterns xts output t,1,1,1.0
xts output t esting,1,1,1.0
output t esting targets,1,1,1.0
t esting targets yts,1,1,1.0
esting targets yts compute,1,1,1.0
targets yts compute kernel,1,1,1.0
yts compute kernel matrix,1,1,1.0
compute kernel matrix ktr,1,1,1.0
kernel matrix ktr for,1,1,1.0
matrix ktr for training,1,1,1.0
ktr for training patterns,1,1,1.0
for training patterns compute,1,1,1.0
training patterns compute the,1,1,1.0
patterns compute the empirical,1,1,1.0
compute the empirical kernel,1,1,1.0
φ e q via,1,1,1.0
e q via ktr,1,1,1.0
q via ktr map,1,1,1.0
via ktr map training,1,1,1.0
ktr map training patterns,1,1,1.0
map training patterns to,1,1,1.0
training patterns to the,1,1,1.0
patterns to the efs,2,1,2.0
to the efs using,2,1,2.0
the efs using φ,2,1,2.0
efs using φ e,2,1,2.0
using φ e q,2,1,2.0
φ e q and,2,1,2.0
e q and obtain,2,1,2.0
q and obtain their,2,1,2.0
and obtain their new,2,1,2.0
obtain their new tation,2,1,2.0
their new tation z,1,1,1.0
new tation z generate,1,1,1.0
tation z generate synthetic,1,1,1.0
z generate synthetic patterns,1,1,1.0
generate synthetic patterns s,1,1,1.0
synthetic patterns s using,1,1,1.0
patterns s using the,1,1,1.0
s using the new,1,1,1.0
using the new representation,1,1,1.0
the new representation z,1,1,1.0
new representation z of,1,1,1.0
representation z of the,1,1,1.0
z of the training,1,1,1.0
the training patterns complete,1,1,1.0
training patterns complete the,1,1,1.0
patterns complete the train,1,1,1.0
complete the train kernel,1,1,1.0
the train kernel matrix,1,1,1.0
train kernel matrix with,1,1,1.0
kernel matrix with the,3,1,3.0
matrix with the dot,2,1,2.0
with the dot product,2,1,2.0
dot product between patterns,2,1,2.0
product between patterns eq,2,1,2.0
between patterns eq train,1,1,1.0
patterns eq train the,1,1,1.0
eq train the learning,1,1,1.0
train the learning algorithm,1,1,1.0
the learning algorithm with,1,1,1.0
learning algorithm with kernel,1,1,1.0
algorithm with kernel matrix,1,1,1.0
with kernel matrix and,1,1,1.0
kernel matrix and obtain,1,1,1.0
matrix and obtain a,1,1,1.0
and obtain a plane,1,1,1.0
obtain a plane w,1,1,1.0
a plane w and,1,1,1.0
plane w and a,1,1,1.0
and a bias term,1,1,1.0
a bias term b,1,1,1.0
bias term b map,1,1,1.0
term b map testing,1,1,1.0
b map testing patterns,1,1,1.0
map testing patterns to,1,1,1.0
testing patterns to the,1,1,1.0
their new tation complete,1,1,1.0
new tation complete the,1,1,1.0
tation complete the test,1,1,1.0
test kernel matrix with,1,1,1.0
between patterns eq predict,1,1,1.0
patterns eq predict yts,1,1,1.0
eq predict yts using,1,1,1.0
predict yts using and,1,1,1.0
yts using and the,1,1,1.0
using and the model,1,1,1.0
and the model w,1,1,1.0
the model w b,1,1,1.0
model w b eq,1,1,1.0
w b eq fig,1,1,1.0
b eq fig different,1,1,1.0
eq fig different steps,1,1,1.0
fig different steps for,1,1,1.0
different steps for the,1,1,1.0
steps for the kernel,1,1,1.0
for the kernel algorithm,1,1,1.0
the kernel algorithm as,1,1,1.0
kernel algorithm as mentioned,1,1,1.0
algorithm as mentioned before,1,1,1.0
as mentioned before our,1,1,1.0
mentioned before our points,1,1,1.0
before our points in,1,1,1.0
our points in the,1,1,1.0
points in the feature,1,1,1.0
the feature space may,1,1,1.0
feature space may not,1,1,1.0
space may not have,1,1,1.0
may not have preimages,1,1,1.0
not have preimages in,1,1,1.0
have preimages in the,1,1,1.0
preimages in the input,1,1,1.0
space however this does,1,1,1.0
however this does not,1,1,1.0
this does not pose,1,1,1.0
does not pose a,1,1,1.0
not pose a methodological,1,1,1.0
pose a methodological problem,1,1,1.0
a methodological problem since,1,1,1.0
methodological problem since the,1,1,1.0
problem since the class,1,1,1.0
since the class separation,1,1,1.0
the class separation is,1,1,1.0
class separation is formulated,1,1,1.0
separation is formulated in,1,1,1.0
is formulated in the,1,1,1.0
formulated in the feature,1,1,1.0
the feature space iv,1,1,1.0
feature space iv o,1,1,1.0
space iv o p,1,1,1.0
iv o p t,1,1,1.0
o p t i,1,1,1.0
p t i m,1,1,1.0
t i m i,1,1,1.0
i m i s,1,1,1.0
m i s i,1,1,1.0
i s i n,1,1,1.0
s i n g,1,1,1.0
i n g t,1,1,1.0
n g t h,1,1,1.0
g t h e,1,1,1.0
t h e f,1,1,1.0
h e f e,1,1,1.0
e f e at,1,1,1.0
f e at u,1,1,1.0
e at u r,1,1,1.0
at u r e,1,1,1.0
u r e s,1,1,1.0
r e s pac,1,1,1.0
e s pac e,1,1,1.0
s pac e b,1,1,1.0
pac e b y,1,1,1.0
e b y k,1,1,1.0
b y k e,1,1,1.0
y k e r,1,1,1.0
k e r n,1,1,1.0
e r n e,1,1,1.0
r n e l,1,1,1.0
n e l l,1,1,1.0
e l l e,1,1,1.0
l l e a,1,1,1.0
l e a r,1,1,1.0
e a r n,1,1,1.0
a r n i,1,1,1.0
r n i n,1,1,1.0
n i n g,1,1,1.0
i n g f,1,1,1.0
n g f o,1,1,1.0
g f o r,1,1,1.0
f o r ov,1,1,1.0
o r ov e,1,1,1.0
r ov e r,1,1,1.0
i n g as,2,1,2.0
n g as stated,2,1,2.0
g as stated before,2,1,2.0
as stated before our,1,1,1.0
stated before our ﬁrst,1,1,1.0
before our ﬁrst hypothesis,1,1,1.0
our ﬁrst hypothesis was,1,1,1.0
ﬁrst hypothesis was that,1,1,1.0
hypothesis was that in,1,1,1.0
was that in the,1,1,1.0
that in the efs,2,1,2.0
in the efs was,1,1,1.0
the efs was more,1,1,1.0
efs was more advisable,1,1,1.0
was more advisable if,1,1,1.0
more advisable if the,1,1,1.0
advisable if the kernel,1,1,1.0
the kernel function matched,1,1,1.0
kernel function matched the,1,1,1.0
function matched the underlying,1,1,1.0
matched the underlying problem,1,1,1.0
the underlying problem in,1,1,1.0
underlying problem in the,1,1,1.0
problem in the sense,1,1,1.0
sense that it can,1,1,1.0
that it can asymptotically,1,1,1.0
it can asymptotically represent,1,1,1.0
can asymptotically represent the,1,1,1.0
asymptotically represent the function,1,1,1.0
represent the function to,1,1,1.0
the function to be,1,1,1.0
function to be learned,1,1,1.0
to be learned and,1,1,1.0
be learned and is,1,1,1.0
learned and is sufﬁciently,1,1,1.0
and is sufﬁciently smooth,1,1,1.0
is sufﬁciently smooth in,1,1,1.0
sufﬁciently smooth in this,1,1,1.0
smooth in this section,1,1,1.0
this section we propose,2,1,2.0
section we propose a,2,1,2.0
we propose a method,1,1,1.0
propose a method for,1,1,1.0
a method for kernel,1,1,1.0
method for kernel learning,1,1,1.0
for kernel learning that,1,1,1.0
kernel learning that would,1,1,1.0
learning that would ideally,1,1,1.0
that would ideally provide,1,1,1.0
would ideally provide a,1,1,1.0
ideally provide a clearer,1,1,1.0
provide a clearer class,1,1,1.0
a clearer class separation,1,1,1.0
clearer class separation in,1,1,1.0
class separation in the,1,1,1.0
separation in the feature,1,1,1.0
feature space to analyse,1,1,1.0
space to analyse its,1,1,1.0
to analyse its effect,1,1,1.0
analyse its effect in,1,1,1.0
its effect in the,1,1,1.0
effect in the method,1,1,1.0
in the method ideally,1,1,1.0
the method ideally we,1,1,1.0
method ideally we would,1,1,1.0
ideally we would like,1,1,1.0
we would like to,2,2,1.0
would like to ﬁnd,1,1,1.0
like to ﬁnd the,1,1,1.0
to ﬁnd the kernel,1,1,1.0
ﬁnd the kernel that,1,1,1.0
that minimises the true,1,1,1.0
minimises the true risk,1,1,1.0
the true risk of,1,1,1.0
true risk of a,1,1,1.0
risk of a classiﬁer,1,1,1.0
of a classiﬁer for,1,1,1.0
a classiﬁer for a,1,1,1.0
classiﬁer for a speciﬁc,1,1,1.0
for a speciﬁc dataset,1,1,1.0
a speciﬁc dataset unfortunately,1,1,1.0
speciﬁc dataset unfortunately the,1,1,1.0
dataset unfortunately the risk,1,1,1.0
unfortunately the risk is,1,1,1.0
the risk is not,1,1,1.0
risk is not accessible,1,1,1.0
is not accessible therefore,1,1,1.0
not accessible therefore different,1,1,1.0
accessible therefore different analytical,1,1,1.0
therefore different analytical bounds,1,1,1.0
different analytical bounds for,1,1,1.0
analytical bounds for the,1,1,1.0
bounds for the generalisation,1,1,1.0
for the generalisation error,1,1,1.0
the generalisation error have,1,1,1.0
generalisation error have been,1,1,1.0
error have been developed,1,1,1.0
have been developed in,2,1,2.0
been developed in the,2,1,2.0
developed in the machine,1,1,1.0
machine learning literature with,1,1,1.0
learning literature with the,1,1,1.0
literature with the aim,1,1,1.0
with the aim of,1,1,1.0
the aim of better,1,1,1.0
aim of better suiting,1,1,1.0
of better suiting a,1,1,1.0
better suiting a given,1,1,1.0
suiting a given dataset,1,1,1.0
a given dataset in,1,1,1.0
given dataset in the,1,1,1.0
dataset in the kernel,1,1,1.0
in the kernel machine,1,1,1.0
the kernel machine literature,1,1,1.0
kernel machine literature a,1,1,1.0
machine literature a considerable,1,1,1.0
literature a considerable interest,1,1,1.0
a considerable interest has,1,1,1.0
considerable interest has been,1,1,1.0
interest has been devoted,1,1,1.0
has been devoted to,1,1,1.0
been devoted to learning,1,1,1.0
devoted to learning the,1,1,1.0
to learning the optimal,1,1,1.0
learning the optimal kernel,1,1,1.0
the optimal kernel given,1,1,1.0
optimal kernel given a,1,1,1.0
kernel given a ular,1,1,1.0
given a ular classiﬁcation,1,1,1.0
a ular classiﬁcation task,1,1,1.0
ular classiﬁcation task as,1,1,1.0
classiﬁcation task as opposed,1,1,1.0
task as opposed to,1,1,1.0
as opposed to imposing,1,1,1.0
opposed to imposing them,1,1,1.0
to imposing them one,1,1,1.0
imposing them one of,1,1,1.0
them one of the,1,1,1.0
one of the prominent,1,1,1.0
of the prominent approaches,1,1,1.0
the prominent approaches in,1,1,1.0
prominent approaches in kernel,1,1,1.0
approaches in kernel learning,1,1,1.0
in kernel learning is,1,1,1.0
kernel learning is centred,1,1,1.0
learning is centred target,1,1,1.0
is centred target alignment,1,1,1.0
centred target alignment kt,1,1,1.0
target alignment kt a,1,1,1.0
alignment kt a centred,1,1,1.0
kt a centred kt,1,1,1.0
a centred kt a,1,1,1.0
centred kt a is,2,1,2.0
kt a is data,1,1,1.0
a is data distribution,1,1,1.0
is data distribution independent,1,1,1.0
data distribution independent making,1,1,1.0
distribution independent making it,1,1,1.0
independent making it particularly,1,1,1.0
making it particularly suitable,1,1,1.0
it particularly suitable for,1,1,1.0
particularly suitable for imbalanced,1,1,1.0
suitable for imbalanced classiﬁcation,1,1,1.0
for imbalanced classiﬁcation note,1,1,1.0
imbalanced classiﬁcation note that,1,1,1.0
classiﬁcation note that kt,1,1,1.0
note that kt a,1,1,1.0
that kt a is,1,1,1.0
kt a is related,1,1,1.0
a is related to,1,1,1.0
related to the fisher,2,1,2.0
to the fisher criterion,2,1,2.0
the fisher criterion which,1,1,1.0
fisher criterion which maximises,1,1,1.0
criterion which maximises the,1,1,1.0
the distance between different,1,1,1.0
distance between different classes,1,1,1.0
between different classes and,1,1,1.0
different classes and minimises,1,1,1.0
classes and minimises the,1,1,1.0
and minimises the within,1,1,1.0
minimises the within class,1,1,1.0
the within class distance,1,1,1.0
within class distance this,1,1,1.0
class distance this can,1,1,1.0
distance this can be,1,1,1.0
this can be a,1,1,1.0
can be a useful,1,1,1.0
be a useful property,1,1,1.0
a useful property of,1,1,1.0
useful property of the,1,1,1.0
property of the feature,1,1,1.0
the feature space in,1,1,1.0
feature space in which,1,1,1.0
space in which to,1,1,1.0
in which to perform,1,1,1.0
which to perform class,1,1,1.0
to perform class minority,1,1,1.0
perform class minority patterns,1,1,1.0
class minority patterns would,1,1,1.0
minority patterns would be,1,1,1.0
patterns would be far,1,1,1.0
would be far from,1,1,1.0
be far from the,1,1,1.0
far from the majority,1,1,1.0
majority class region and,1,1,1.0
class region and closely,1,1,1.0
region and closely clustered,1,1,1.0
and closely clustered together,1,1,1.0
closely clustered together kt,1,1,1.0
clustered together kt a,1,1,1.0
together kt a optimises,1,1,1.0
kt a optimises the,1,1,1.0
a optimises the kernel,1,1,1.0
optimises the kernel by,1,1,1.0
the kernel by aligning,1,1,1.0
kernel by aligning it,1,1,1.0
by aligning it to,1,1,1.0
aligning it to the,1,1,1.0
it to the ideal,1,1,1.0
to the ideal kernel,1,1,1.0
the ideal kernel matrix,2,1,2.0
ideal kernel matrix ki,2,1,2.0
kernel matrix ki which,1,1,1.0
matrix ki which will,1,1,1.0
ki which will submit,1,1,1.0
which will submit the,1,1,1.0
will submit the structure,1,1,1.0
submit the structure ki,1,1,1.0
the structure ki xi,1,1,1.0
structure ki xi xj,1,1,1.0
ki xi xj if,1,1,1.0
xi xj if yi,1,1,1.0
xj if yi yj,1,1,1.0
if yi yj otherwise,1,1,1.0
yi yj otherwise where,1,1,1.0
yj otherwise where yi,1,1,1.0
otherwise where yi is,1,1,1.0
where yi is the,1,1,1.0
yi is the target,1,1,1.0
is the target of,1,1,1.0
the target of pattern,1,1,1.0
target of pattern xi,1,1,1.0
of pattern xi xtr,1,1,1.0
pattern xi xtr in,1,1,1.0
xi xtr in this,1,1,1.0
xtr in this sense,1,1,1.0
in this sense ki,1,1,1.0
this sense ki will,1,1,1.0
sense ki will provide,1,1,1.0
ki will provide information,1,1,1.0
will provide information about,1,1,1.0
provide information about which,1,1,1.0
information about which patterns,1,1,1.0
about which patterns should,1,1,1.0
which patterns should be,3,1,3.0
patterns should be considered,1,1,1.0
should be considered to,1,1,1.0
considered to be similar,1,1,1.0
to be similar when,1,1,1.0
be similar when performing,1,1,1.0
similar when performing a,1,1,1.0
when performing a learning,1,1,1.0
performing a learning task,1,1,1.0
a learning task thus,1,1,1.0
learning task thus the,1,1,1.0
task thus the problem,1,1,1.0
thus the problem of,1,1,1.0
the problem of ﬁnding,1,1,1.0
problem of ﬁnding an,1,1,1.0
of ﬁnding an optimal,1,1,1.0
ﬁnding an optimal kernel,1,1,1.0
an optimal kernel k,1,1,1.0
optimal kernel k is,1,1,1.0
kernel k is changed,1,1,1.0
k is changed to,1,1,1.0
is changed to the,1,1,1.0
changed to the one,1,1,1.0
to the one of,1,1,1.0
the one of ﬁnding,1,1,1.0
one of ﬁnding a,1,1,1.0
of ﬁnding a good,1,1,1.0
ﬁnding a good approximation,1,1,1.0
a good approximation k,1,1,1.0
good approximation k for,1,1,1.0
approximation k for the,1,1,1.0
k for the ideal,1,1,1.0
for the ideal kernel,1,1,1.0
kernel matrix ki given,1,1,1.0
matrix ki given a,1,1,1.0
ki given a family,1,1,1.0
given a family of,1,1,1.0
a family of kernel,1,1,1.0
family of kernel functions,1,1,1.0
of kernel functions this,1,1,1.0
kernel functions this formulation,1,1,1.0
functions this formulation allows,1,1,1.0
this formulation allows to,1,1,1.0
formulation allows to separate,1,1,1.0
allows to separate the,1,1,1.0
to separate the optimisation,1,1,1.0
separate the optimisation from,1,1,1.0
the optimisation from kernel,1,1,1.0
optimisation from kernel machine,1,1,1.0
from kernel machine learning,1,1,1.0
kernel machine learning and,1,1,1.0
machine learning and to,1,1,1.0
learning and to reduce,1,1,1.0
and to reduce the,1,1,1.0
to reduce the increase,1,1,1.0
reduce the increase in,1,1,1.0
the increase in the,1,1,1.0
increase in the tional,1,1,1.0
in the tional cost,1,1,1.0
the tional cost of,1,1,1.0
tional cost of learning,1,1,1.0
cost of learning more,1,1,1.0
of learning more complex,1,1,1.0
learning more complex kernels,1,1,1.0
more complex kernels given,1,1,1.0
complex kernels given that,1,1,1.0
kernels given that the,1,1,1.0
given that the kernel,1,1,1.0
that the kernel machine,1,1,1.0
the kernel machine will,1,1,1.0
kernel machine will be,1,1,1.0
machine will be unaffected,1,1,1.0
will be unaffected by,1,1,1.0
be unaffected by this,1,1,1.0
unaffected by this higher,1,1,1.0
by this higher complexity,1,1,1.0
this higher complexity as,1,1,1.0
higher complexity as said,1,1,1.0
complexity as said before,1,1,1.0
as said before concerning,1,1,1.0
said before concerning imbalanced,1,1,1.0
before concerning imbalanced classiﬁcation,1,1,1.0
concerning imbalanced classiﬁcation ous,1,1,1.0
imbalanced classiﬁcation ous studies,1,1,1.0
classiﬁcation ous studies have,1,1,1.0
ous studies have noted,1,1,1.0
studies have noted several,1,1,1.0
have noted several issues,1,1,1.0
noted several issues in,1,1,1.0
several issues in kt,1,1,1.0
issues in kt a,1,1,1.0
in kt a for,1,1,1.0
kt a for different,1,1,1.0
a for different pattern,1,1,1.0
for different pattern distributions,1,1,1.0
different pattern distributions but,1,1,1.0
pattern distributions but a,1,1,1.0
distributions but a recent,1,1,1.0
but a recent study,1,1,1.0
a recent study has,1,1,1.0
recent study has shown,1,1,1.0
study has shown that,1,1,1.0
has shown that this,1,1,1.0
shown that this can,1,1,1.0
that this can be,1,1,1.0
this can be solved,1,1,1.0
can be solved by,1,1,1.0
be solved by the,1,1,1.0
solved by the use,1,1,1.0
by the use of,1,1,1.0
the use of centred,1,1,1.0
use of centred kernel,1,1,1.0
of centred kernel matrices,1,1,1.0
centred kernel matrices the,1,1,1.0
kernel matrices the notion,1,1,1.0
matrices the notion of,1,1,1.0
the notion of centred,1,1,1.0
notion of centred alignment,1,1,1.0
of centred alignment ac,1,1,1.0
centred alignment ac between,1,1,1.0
alignment ac between k,1,1,1.0
ac between k and,1,1,1.0
between k and ki,1,1,1.0
k and ki is,1,1,1.0
and ki is deﬁned,1,1,1.0
ki is deﬁned as,1,1,1.0
is deﬁned as ac,1,1,1.0
deﬁned as ac k,1,1,1.0
as ac k ki,1,1,1.0
ac k ki kic,1,1,1.0
k ki kic kic,1,1,1.0
ki kic kic where,1,1,1.0
kic kic where kc,1,1,1.0
kic where kc denotes,1,1,1.0
where kc denotes the,1,1,1.0
kc denotes the centred,1,1,1.0
denotes the centred version,1,1,1.0
the centred version of,1,1,1.0
centred version of kernel,1,1,1.0
version of kernel matrix,1,1,1.0
of kernel matrix k,1,1,1.0
kernel matrix k and,2,1,2.0
matrix k and is,2,1,2.0
k and is computed,1,1,1.0
and is computed as,1,1,1.0
is computed as kc,1,1,1.0
computed as kc k,1,1,1.0
as kc k m,1,1,1.0
kc k m m,1,1,1.0
k m m k,1,1,1.0
m m k m,1,1,1.0
m k m m,1,1,1.0
k m m being,1,1,1.0
m m being m,1,1,1.0
m being m a,1,1,1.0
being m a matrix,1,1,1.0
m a matrix with,1,1,1.0
a matrix with all,1,1,1.0
matrix with all elements,1,1,1.0
with all elements equal,1,1,1.0
all elements equal to,1,1,1.0
elements equal to m,1,1,1.0
equal to m centred,1,1,1.0
to m centred kt,1,1,1.0
m centred kt a,1,1,1.0
kt a is maximised,1,1,1.0
a is maximised when,1,1,1.0
is maximised when a,1,1,1.0
maximised when a kernel,1,1,1.0
when a kernel reﬂect,1,1,1.0
a kernel reﬂect the,1,1,1.0
kernel reﬂect the criminant,1,1,1.0
reﬂect the criminant properties,1,1,1.0
the criminant properties of,1,1,1.0
criminant properties of the,1,1,1.0
properties of the data,1,1,1.0
of the data used,1,1,1.0
the data used to,1,1,1.0
data used to deﬁne,1,1,1.0
used to deﬁne the,1,1,1.0
to deﬁne the ideal,1,1,1.0
deﬁne the ideal kernel,1,1,1.0
the ideal kernel consider,1,1,1.0
ideal kernel consider a,1,1,1.0
kernel consider a kernel,1,1,1.0
consider a kernel function,1,1,1.0
a kernel function depending,1,1,1.0
kernel function depending on,1,1,1.0
function depending on a,1,1,1.0
depending on a vector,1,1,1.0
on a vector of,1,1,1.0
a vector of eters,1,1,1.0
vector of eters α,1,1,1.0
of eters α because,1,1,1.0
eters α because of,1,1,1.0
α because of the,1,1,1.0
because of the differentiability,1,1,1.0
of the differentiability of,1,1,1.0
the differentiability of ac,1,1,1.0
differentiability of ac with,1,1,1.0
of ac with respect,1,1,1.0
ac with respect to,1,1,1.0
with respect to these,2,1,2.0
respect to these kernel,2,1,2.0
to these kernel parameters,2,1,2.0
these kernel parameters α,2,1,2.0
kernel parameters α a,1,1,1.0
parameters α a gradient,1,1,1.0
α a gradient ascent,1,1,1.0
a gradient ascent algorithm,1,1,1.0
gradient ascent algorithm can,1,1,1.0
ascent algorithm can be,1,1,1.0
algorithm can be used,1,1,1.0
be used to maximise,1,1,1.0
used to maximise the,1,1,1.0
to maximise the alignment,1,1,1.0
maximise the alignment between,1,1,1.0
the alignment between the,1,1,1.0
alignment between the kernel,1,1,1.0
between the kernel matrix,1,1,1.0
the kernel matrix constructed,1,1,1.0
kernel matrix constructed kα,1,1,1.0
matrix constructed kα and,1,1,1.0
constructed kα and the,1,1,1.0
kα and the ideal,1,1,1.0
and the ideal one,1,1,1.0
the ideal one ki,1,1,1.0
ideal one ki as,1,1,1.0
one ki as follows,1,1,1.0
ki as follows α,1,1,1.0
as follows α arg,1,1,1.0
follows α arg max,1,1,1.0
α arg max α,1,1,1.0
arg max α ac,1,1,1.0
max α ac kα,1,1,1.0
α ac kα ki,1,1,1.0
ac kα ki the,1,1,1.0
kα ki the alignment,1,1,1.0
ki the alignment derivative,1,1,1.0
the alignment derivative with,1,1,1.0
alignment derivative with respect,1,1,1.0
derivative with respect to,1,1,1.0
kernel parameters α is,1,1,1.0
parameters α is ac,1,1,1.0
α is ac kα,1,1,1.0
is ac kα ki,1,1,1.0
ac kα ki α,1,1,1.0
kα ki α kα,1,1,1.0
ki α kα α,1,1,1.0
α kα α kic,1,1,1.0
kα α kic c,1,1,1.0
α kic c kic,1,1,1.0
kic c kic c,1,1,1.0
c kic c kα,1,1,1.0
kic c kα α,1,1,1.0
c kα α c,1,1,1.0
kα α c f,1,1,1.0
α c f where,1,1,1.0
c f where and,1,1,1.0
f where and for,1,1,1.0
where and for arbitrary,1,1,1.0
and for arbitrary matrices,1,1,1.0
for arbitrary matrices and,1,1,1.0
arbitrary matrices and it,1,1,1.0
matrices and it holds,1,1,1.0
and it holds that,1,1,1.0
it holds that which,1,1,1.0
holds that which simpliﬁes,1,1,1.0
that which simpliﬁes the,1,1,1.0
which simpliﬁes the computation,1,1,1.0
simpliﬁes the computation in,1,1,1.0
the computation in this,1,1,1.0
computation in this paper,1,1,1.0
this paper we will,1,1,1.0
paper we will consider,1,1,1.0
we will consider a,1,1,1.0
will consider a generalised,1,1,1.0
consider a generalised gaussian,1,1,1.0
a generalised gaussian kernel,1,1,1.0
generalised gaussian kernel with,1,1,1.0
gaussian kernel with covariance,1,1,1.0
kernel with covariance structure,1,1,1.0
with covariance structure deﬁned,1,1,1.0
covariance structure deﬁned by,1,1,1.0
structure deﬁned by a,1,1,1.0
deﬁned by a positive,1,1,1.0
by a positive semideﬁnite,1,1,1.0
a positive semideﬁnite matrix,1,1,1.0
positive semideﬁnite matrix q,1,1,1.0
semideﬁnite matrix q k,1,1,1.0
matrix q k xi,1,1,1.0
q k xi xj,1,1,1.0
k xi xj exp,2,1,2.0
xi xj exp xi,2,1,2.0
xj exp xi xj,2,1,2.0
exp xi xj tq,1,1,1.0
xi xj tq xi,1,1,1.0
xj tq xi xj,1,1,1.0
tq xi xj as,1,1,1.0
xi xj as usual,1,1,1.0
xj as usual the,1,1,1.0
as usual the matrix,1,1,1.0
usual the matrix q,1,1,1.0
the matrix q will,1,1,1.0
matrix q will be,1,1,1.0
q will be parametrised,1,1,1.0
will be parametrised by,1,1,1.0
be parametrised by utu,1,1,1.0
parametrised by utu where,1,1,1.0
by utu where u,1,1,1.0
utu where u is,1,1,1.0
where u is a,1,1,1.0
u is a d,1,1,1.0
is a d d,1,1,1.0
a d d matrix,1,1,1.0
d d matrix d,1,1,1.0
d matrix d being,1,1,1.0
matrix d being the,1,1,1.0
d being the dimensionality,1,1,1.0
being the dimensionality of,1,1,1.0
dimensionality of the input,2,1,2.0
the input space therefore,1,1,1.0
input space therefore we,1,1,1.0
space therefore we can,1,1,1.0
therefore we can equivalently,1,1,1.0
we can equivalently restate,1,1,1.0
can equivalently restate our,1,1,1.0
equivalently restate our problem,1,1,1.0
restate our problem as,1,1,1.0
our problem as learning,1,1,1.0
problem as learning the,1,1,1.0
as learning the best,1,1,1.0
learning the best matrix,1,1,1.0
the best matrix u,1,1,1.0
best matrix u k,1,1,1.0
matrix u k xi,1,1,1.0
u k xi xj,1,1,1.0
exp xi xj tutu,1,1,1.0
xi xj tutu xi,1,1,1.0
xj tutu xi xj,1,1,1.0
tutu xi xj now,1,1,1.0
xi xj now we,1,1,1.0
xj now we can,1,1,1.0
now we can compute,1,1,1.0
we can compute the,1,1,1.0
can compute the derivative,1,1,1.0
compute the derivative of,1,1,1.0
the derivative of the,1,1,1.0
derivative of the kernel,1,1,1.0
of the kernel with,1,1,1.0
the kernel with respect,1,1,1.0
kernel with respect to,1,1,1.0
respect to the entries,1,1,1.0
to the entries of,1,1,1.0
the entries of the,2,1,2.0
entries of the u,2,1,2.0
of the u matrix,2,1,2.0
the u matrix xi,1,1,1.0
u matrix xi xj,1,1,1.0
matrix xi xj u,1,1,1.0
xi xj u xi,1,1,1.0
xj u xi xj,1,1,1.0
u xi xj t,1,1,1.0
xi xj t xi,1,1,1.0
xj t xi xj,1,1,1.0
t xi xj k,1,1,1.0
xj k xi xj,1,1,1.0
k xi xj therefore,1,1,1.0
xi xj therefore we,1,1,1.0
xj therefore we will,1,1,1.0
therefore we will optimise,1,1,1.0
we will optimise a,1,1,1.0
will optimise a vector,1,1,1.0
optimise a vector of,1,1,1.0
a vector of parameters,1,1,1.0
vector of parameters α,1,1,1.0
of parameters α composed,1,1,1.0
parameters α composed of,1,1,1.0
α composed of the,1,1,1.0
composed of the entries,1,1,1.0
of the entries of,1,1,1.0
the u matrix it,1,1,1.0
u matrix it is,1,1,1.0
matrix it is important,1,1,1.0
it is important to,1,1,1.0
is important to note,1,1,1.0
important to note that,1,1,1.0
to note that some,1,1,1.0
note that some attempts,1,1,1.0
that some attempts have,1,1,1.0
some attempts have been,1,1,1.0
attempts have been made,1,1,1.0
been made to establish,1,1,1.0
made to establish learning,1,1,1.0
to establish learning bounds,1,1,1.0
establish learning bounds for,1,1,1.0
learning bounds for the,1,1,1.0
bounds for the gaussian,1,1,1.0
for the gaussian kernel,1,1,1.0
the gaussian kernel with,1,1,1.0
gaussian kernel with several,1,1,1.0
kernel with several parameters,1,1,1.0
with several parameters when,1,1,1.0
several parameters when considering,1,1,1.0
parameters when considering large,1,1,1.0
when considering large margin,1,1,1.0
considering large margin classiﬁers,1,1,1.0
large margin classiﬁers these,1,1,1.0
margin classiﬁers these studies,1,1,1.0
classiﬁers these studies suggest,1,1,1.0
these studies suggest that,1,1,1.0
studies suggest that the,1,1,1.0
suggest that the interaction,1,1,1.0
that the interaction between,1,1,1.0
the interaction between the,1,1,1.0
interaction between the margin,1,1,1.0
between the margin and,1,1,1.0
the margin and the,1,1,1.0
margin and the complexity,1,1,1.0
and the complexity measure,1,1,1.0
the complexity measure of,1,1,1.0
complexity measure of the,1,1,1.0
measure of the kernel,1,1,1.0
of the kernel class,1,1,1.0
the kernel class is,1,1,1.0
kernel class is plicative,1,1,1.0
class is plicative thus,1,1,1.0
is plicative thus discouraging,1,1,1.0
plicative thus discouraging the,1,1,1.0
thus discouraging the development,1,1,1.0
discouraging the development of,1,1,1.0
the development of techniques,1,1,1.0
development of techniques for,1,1,1.0
of techniques for the,1,1,1.0
techniques for the optimisation,1,1,1.0
for the optimisation of,2,1,2.0
the optimisation of more,1,1,1.0
optimisation of more complex,1,1,1.0
of more complex kernels,1,1,1.0
more complex kernels however,1,1,1.0
complex kernels however recent,1,1,1.0
kernels however recent developments,1,1,1.0
however recent developments have,1,1,1.0
recent developments have shown,1,1,1.0
developments have shown that,1,1,1.0
have shown that this,1,1,1.0
shown that this interaction,1,1,1.0
that this interaction is,1,1,1.0
this interaction is additive,1,1,1.0
interaction is additive up,1,1,1.0
is additive up to,1,1,1.0
additive up to log,1,1,1.0
up to log factors,1,1,1.0
to log factors rather,1,1,1.0
log factors rather than,1,1,1.0
factors rather than multiplicative,1,1,1.0
rather than multiplicative yielding,1,1,1.0
than multiplicative yielding then,1,1,1.0
multiplicative yielding then stronger,1,1,1.0
yielding then stronger bounds,1,1,1.0
then stronger bounds therefore,1,1,1.0
stronger bounds therefore the,1,1,1.0
bounds therefore the number,1,1,1.0
therefore the number of,2,2,1.0
the number of patterns,3,1,3.0
number of patterns needed,1,1,1.0
of patterns needed to,1,1,1.0
patterns needed to obtain,1,1,1.0
needed to obtain the,1,1,1.0
to obtain the same,1,1,1.0
obtain the same estimation,1,1,1.0
the same estimation error,1,1,1.0
same estimation error with,1,1,1.0
estimation error with the,1,1,1.0
error with the same,1,1,1.0
with the same probability,1,1,1.0
the same probability for,1,1,1.0
same probability for a,1,1,1.0
probability for a kernel,1,1,1.0
for a kernel compared,1,1,1.0
a kernel compared to,1,1,1.0
kernel compared to a,1,1,1.0
compared to a spherical,1,1,1.0
to a spherical one,1,1,1.0
a spherical one grows,1,1,1.0
spherical one grows slowly,1,1,1.0
one grows slowly and,1,1,1.0
grows slowly and directly,1,1,1.0
slowly and directly depends,1,1,1.0
and directly depends on,1,1,1.0
directly depends on the,1,1,1.0
depends on the number,1,1,1.0
the number of parameters,1,1,1.0
number of parameters t,1,1,1.0
of parameters t o,1,1,1.0
parameters t o demonstrate,1,1,1.0
t o demonstrate the,1,1,1.0
o demonstrate the usefulness,1,1,1.0
demonstrate the usefulness of,1,1,1.0
the usefulness of learning,1,1,1.0
usefulness of learning the,1,1,1.0
of learning the kernels,1,1,1.0
learning the kernels we,1,1,1.0
the kernels we present,1,1,1.0
kernels we present in,1,1,1.0
we present in fig,1,1,1.0
present in fig a,1,1,1.0
in fig a graphical,1,1,1.0
fig a graphical representation,1,1,1.0
graphical representation of three,1,1,1.0
representation of three dimensional,1,1,1.0
of three dimensional toy,1,1,1.0
three dimensional toy datasets,1,1,1.0
dimensional toy datasets and,1,1,1.0
toy datasets and their,1,1,1.0
datasets and their mapping,1,1,1.0
and their mapping φ,1,1,1.0
their mapping φ e,1,1,1.0
mapping φ e using,1,1,1.0
φ e using a,1,1,1.0
e using a ical,1,1,1.0
using a ical gaussian,1,1,1.0
a ical gaussian kernel,1,1,1.0
ical gaussian kernel with,1,1,1.0
gaussian kernel with q,1,1,1.0
kernel with q id,1,1,1.0
with q id an,1,1,1.0
q id an optimised,1,1,1.0
id an optimised spherical,1,1,1.0
an optimised spherical gaussian,2,1,2.0
optimised spherical gaussian kernel,2,1,2.0
spherical gaussian kernel obtained,1,1,1.0
gaussian kernel obtained through,1,1,1.0
kernel obtained through centred,1,1,1.0
obtained through centred kt,1,1,1.0
through centred kt a,1,1,1.0
centred kt a and,1,1,1.0
kt a and an,1,1,1.0
a and an optimised,1,1,1.0
and an optimised generalised,1,1,1.0
an optimised generalised gaussian,2,1,2.0
optimised generalised gaussian kernel,2,1,2.0
generalised gaussian kernel summarising,1,1,1.0
gaussian kernel summarising kernel,1,1,1.0
kernel summarising kernel learning,1,1,1.0
summarising kernel learning will,1,1,1.0
kernel learning will be,1,1,1.0
learning will be applied,1,1,1.0
will be applied before,1,1,1.0
be applied before the,1,1,1.0
applied before the procedure,1,1,1.0
before the procedure to,1,1,1.0
the procedure to learn,1,1,1.0
procedure to learn a,1,1,1.0
to learn a suitable,1,1,1.0
learn a suitable kernel,1,1,1.0
a suitable kernel kα,1,1,1.0
suitable kernel kα for,1,1,1.0
kernel kα for the,1,1,1.0
kα for the data,1,1,1.0
for the data representation,1,1,1.0
the data representation after,1,1,1.0
data representation after this,1,1,1.0
representation after this the,1,1,1.0
after this the efs,1,1,1.0
this the efs φ,1,1,1.0
the efs φ e,1,1,1.0
efs φ e q,1,1,1.0
φ e q associated,1,1,1.0
e q associated to,1,1,1.0
q associated to this,1,1,1.0
associated to this kernel,1,1,1.0
to this kernel kα,1,1,1.0
this kernel kα will,1,1,1.0
kernel kα will be,1,1,1.0
kα will be computed,1,1,1.0
will be computed and,1,1,1.0
be computed and then,1,1,1.0
computed and then the,1,1,1.0
and then the images,1,1,1.0
then the images of,1,1,1.0
the training patterns for,1,1,1.0
training patterns for the,1,1,1.0
patterns for the minority,1,1,1.0
the minority class contained,1,1,1.0
minority class contained in,1,1,1.0
class contained in the,1,1,1.0
contained in the z,1,1,1.0
in the z matrix,1,1,1.0
the z matrix will,1,1,1.0
z matrix will be,1,1,1.0
matrix will be for,1,1,1.0
will be for comparison,1,1,1.0
be for comparison purposes,1,1,1.0
for comparison purposes we,1,1,1.0
comparison purposes we will,1,1,1.0
purposes we will also,1,1,1.0
we will also test,1,1,1.0
will also test the,1,1,1.0
also test the optimization,1,1,1.0
test the optimization of,1,1,1.0
the optimization of a,1,1,1.0
optimization of a spherical,1,1,1.0
of a spherical gaussian,2,1,2.0
a spherical gaussian kernel,3,1,3.0
spherical gaussian kernel with,1,1,1.0
gaussian kernel with one,1,1,1.0
kernel with one kernel,1,1,1.0
with one kernel parameter,1,1,1.0
one kernel parameter via,1,1,1.0
kernel parameter via alignment,1,1,1.0
parameter via alignment v,1,1,1.0
via alignment v u,1,1,1.0
alignment v u n,1,1,1.0
v u n i,1,1,1.0
u n i fi,1,1,1.0
n i fi e,1,1,1.0
i fi e d,1,1,1.0
fi e d f,1,1,1.0
e d f r,1,1,1.0
d f r a,1,1,1.0
f r a m,1,1,1.0
r a m e,1,1,1.0
a m e wo,1,1,1.0
m e wo r,1,1,1.0
e wo r k,1,1,1.0
wo r k f,1,1,1.0
r k f o,1,1,1.0
k f o r,1,1,1.0
f o r p,1,1,1.0
o r p r,1,1,1.0
r p r e,1,1,1.0
p r e f,1,1,1.0
r e f e,1,1,1.0
e f e r,1,1,1.0
f e r e,2,1,2.0
e r e n,2,1,2.0
r e n t,1,1,1.0
e n t i,1,1,1.0
n t i a,1,1,1.0
t i a l,1,1,1.0
i a l ov,1,1,1.0
a l ov e,1,1,1.0
l ov e r,1,1,1.0
as stated before several,1,1,1.0
stated before several approaches,1,1,1.0
before several approaches have,1,1,1.0
several approaches have been,1,1,1.0
approaches have been developed,1,1,1.0
developed in the literature,1,1,1.0
in the literature for,2,1,2.0
the literature for handling,1,1,1.0
literature for handling imbalanced,1,1,1.0
handling imbalanced data and,1,1,1.0
imbalanced data and a,1,1,1.0
data and a large,1,1,1.0
and a large number,1,1,1.0
large number of these,1,1,1.0
number of these contributions,1,1,1.0
of these contributions are,1,1,1.0
these contributions are based,1,1,1.0
contributions are based on,1,1,1.0
are based on analysing,1,1,1.0
based on analysing the,1,1,1.0
on analysing the patterns,1,1,1.0
analysing the patterns which,1,1,1.0
the patterns which could,1,1,1.0
patterns which could be,1,1,1.0
which could be more,1,1,1.0
could be more suitable,1,1,1.0
be more suitable for,1,1,1.0
more suitable for giving,1,1,1.0
suitable for giving rise,1,1,1.0
for giving rise to,1,1,1.0
giving rise to approaches,1,1,1.0
rise to approaches based,1,1,1.0
to approaches based on,1,1,1.0
approaches based on on,1,1,1.0
based on on the,1,1,1.0
on on the class,1,1,1.0
on the class boundary,2,1,2.0
the class boundary or,3,1,3.0
class boundary or in,1,1,1.0
boundary or in the,1,1,1.0
or in the within,1,1,1.0
in the within class,1,1,1.0
the within class safe,1,1,1.0
within class safe region,1,1,1.0
class safe region these,1,1,1.0
safe region these techniques,1,1,1.0
region these techniques are,1,1,1.0
these techniques are commonly,1,1,1.0
techniques are commonly referred,1,1,1.0
are commonly referred to,1,1,1.0
commonly referred to as,1,1,1.0
referred to as weighted,1,1,1.0
to as weighted however,1,1,1.0
as weighted however to,1,1,1.0
weighted however to our,1,1,1.0
however to our best,1,1,1.0
to our best knowledge,1,1,1.0
our best knowledge there,1,1,1.0
best knowledge there is,1,1,1.0
knowledge there is no,1,1,1.0
there is no principled,1,1,1.0
is no principled method,1,1,1.0
no principled method for,1,1,1.0
principled method for choosing,1,1,1.0
method for choosing the,1,1,1.0
for choosing the region,1,1,1.0
choosing the region of,1,1,1.0
the region of the,1,1,1.0
region of the minority,1,1,1.0
class to be used,1,1,1.0
to be used for,3,1,3.0
be used for sampling,1,1,1.0
used for sampling in,1,1,1.0
for sampling in this,1,1,1.0
sampling in this section,1,1,1.0
we propose a new,1,1,1.0
propose a new adaptive,1,1,1.0
a new adaptive weighted,1,1,1.0
new adaptive weighted technique,1,1,1.0
adaptive weighted technique that,1,1,1.0
weighted technique that naturally,1,1,1.0
technique that naturally spans,1,1,1.0
that naturally spans unweighted,1,1,1.0
naturally spans unweighted and,1,1,1.0
spans unweighted and weighted,1,1,1.0
unweighted and weighted methods,1,1,1.0
and weighted methods both,1,1,1.0
weighted methods both on,1,1,1.0
methods both on the,1,1,1.0
both on the boundary,1,1,1.0
on the boundary and,1,1,1.0
the boundary and within,1,1,1.0
boundary and within class,1,1,1.0
and within class t,1,1,1.0
within class t o,1,1,1.0
class t o do,1,1,1.0
o do so our,1,1,1.0
do so our approach,1,1,1.0
so our approach will,1,1,1.0
our approach will take,1,1,1.0
approach will take advantage,1,1,1.0
will take advantage of,1,1,1.0
take advantage of the,1,1,1.0
advantage of the spatial,1,1,1.0
of the spatial distribution,1,1,1.0
the spatial distribution of,3,1,3.0
spatial distribution of the,4,1,4.0
distribution of the patterns,4,1,4.0
of the patterns according,2,1,2.0
the patterns according to,3,1,3.0
according to the optimal,1,1,1.0
to the optimal hyperplane,2,1,2.0
the optimal hyperplane obtained,1,1,1.0
optimal hyperplane obtained from,1,1,1.0
hyperplane obtained from the,1,1,1.0
obtained from the svm,1,1,1.0
from the svm solution,1,1,1.0
the svm solution knowledge,1,1,1.0
svm solution knowledge extraction,1,1,1.0
solution knowledge extraction spatial,1,1,1.0
knowledge extraction spatial distribution,1,1,1.0
extraction spatial distribution of,1,1,1.0
of the patterns w,1,1,1.0
the patterns w eighted,1,1,1.0
patterns w eighted techniques,1,1,1.0
w eighted techniques are,1,1,1.0
eighted techniques are based,1,1,1.0
techniques are based on,1,1,1.0
are based on the,3,2,1.5
based on the idea,2,2,1.0
on the idea that,1,1,1.0
the idea that not,1,1,1.0
idea that not all,1,1,1.0
that not all the,1,1,1.0
not all the patterns,1,1,1.0
all the patterns of,1,1,1.0
the patterns of the,1,1,1.0
patterns of the dataset,1,1,1.0
of the dataset are,1,1,1.0
the dataset are equally,1,1,1.0
dataset are equally important,1,1,1.0
are equally important and,1,1,1.0
equally important and suitable,1,1,1.0
important and suitable for,1,1,1.0
and suitable for and,1,1,1.0
suitable for and therefore,1,1,1.0
for and therefore they,1,1,1.0
and therefore they should,1,1,1.0
therefore they should fig,1,1,1.0
they should fig synthethic,1,1,1.0
should fig synthethic datasets,1,1,1.0
fig synthethic datasets representing,1,1,1.0
synthethic datasets representing separable,1,1,1.0
datasets representing separable classiﬁcation,1,1,1.0
representing separable classiﬁcation problems,1,1,1.0
separable classiﬁcation problems and,1,1,1.0
classiﬁcation problems and their,1,1,1.0
problems and their transformation,1,1,1.0
and their transformation to,1,1,1.0
their transformation to the,1,1,1.0
transformation to the dominant,1,1,1.0
of the efs induced,1,1,1.0
the efs induced by,1,1,1.0
efs induced by the,1,1,1.0
linearly separable problem contribute,1,1,1.0
separable problem contribute equally,1,1,1.0
problem contribute equally to,1,1,1.0
contribute equally to the,1,1,1.0
equally to the new,1,1,1.0
to the new synthetic,1,1,1.0
the new synthetic data,1,1,1.0
new synthetic data one,1,1,1.0
synthetic data one of,1,1,1.0
data one of the,1,1,1.0
one of the ﬁrst,1,1,1.0
of the ﬁrst steps,1,1,1.0
the ﬁrst steps of,1,1,1.0
ﬁrst steps of these,1,1,1.0
steps of these methodologies,1,1,1.0
of these methodologies corresponds,1,1,1.0
these methodologies corresponds to,1,1,1.0
methodologies corresponds to the,1,1,1.0
corresponds to the identiﬁcation,1,1,1.0
to the identiﬁcation of,1,1,1.0
the identiﬁcation of the,1,1,1.0
identiﬁcation of the useful,1,1,1.0
of the useful patterns,1,1,1.0
the useful patterns to,1,1,1.0
useful patterns to be,1,1,1.0
patterns to be used,2,1,2.0
be used for most,1,1,1.0
used for most of,1,1,1.0
most of the approaches,1,1,1.0
of the approaches in,1,1,1.0
the approaches in the,1,1,1.0
in the literature do,1,1,1.0
the literature do so,1,1,1.0
literature do so by,1,1,1.0
do so by analysing,1,1,1.0
so by analysing local,1,1,1.0
by analysing local neighbourhood,1,1,1.0
analysing local neighbourhood of,1,1,1.0
local neighbourhood of points,1,1,1.0
neighbourhood of points in,1,1,1.0
of points in the,3,2,1.5
points in the minority,1,1,1.0
minority class in this,1,1,1.0
class in this paper,1,1,1.0
in this paper however,1,1,1.0
this paper however we,1,1,1.0
paper however we will,1,1,1.0
however we will derive,1,1,1.0
we will derive a,1,1,1.0
will derive a weighted,1,1,1.0
derive a weighted technique,1,1,1.0
a weighted technique considering,1,1,1.0
weighted technique considering the,1,1,1.0
technique considering the spatial,1,1,1.0
considering the spatial distribution,1,1,1.0
of the patterns with,1,1,1.0
the patterns with respect,1,1,1.0
patterns with respect to,2,1,2.0
respect to the optimal,1,1,1.0
to the optimal svm,1,1,1.0
optimal svm hyperplane in,1,1,1.0
svm hyperplane in particular,1,1,1.0
hyperplane in particular the,1,1,1.0
in particular the patterns,1,1,1.0
particular the patterns to,1,1,1.0
the patterns to be,1,1,1.0
be used for will,1,1,1.0
used for will be,1,1,1.0
for will be selected,1,1,1.0
will be selected based,1,1,1.0
be selected based on,1,1,1.0
selected based on their,2,1,2.0
based on their position,1,1,1.0
on their position and,1,1,1.0
their position and distance,1,1,1.0
position and distance to,1,1,1.0
and distance to the,1,1,1.0
distance to the optimal,1,1,1.0
the optimal hyperplane however,1,1,1.0
optimal hyperplane however as,1,1,1.0
hyperplane however as stated,1,1,1.0
however as stated before,1,1,1.0
as stated before the,1,1,1.0
stated before the optimisation,1,1,1.0
before the optimisation of,1,1,1.0
the optimisation of the,4,1,4.0
optimisation of the svm,1,1,1.0
of the svm paradigm,1,1,1.0
the svm paradigm poses,1,1,1.0
svm paradigm poses a,1,1,1.0
poses a serious problem,1,1,1.0
a serious problem for,1,1,1.0
serious problem for anced,1,1,1.0
problem for anced datasets,1,1,1.0
for anced datasets therefore,1,1,1.0
anced datasets therefore for,1,1,1.0
datasets therefore for the,1,1,1.0
therefore for the purpose,1,1,1.0
the purpose of weighted,1,1,1.0
purpose of weighted sampling,1,1,1.0
of weighted sampling we,1,1,1.0
weighted sampling we use,1,1,1.0
sampling we use the,1,1,1.0
we use the approach,1,1,1.0
use the approach giving,1,1,1.0
the approach giving more,1,1,1.0
approach giving more importance,1,1,1.0
giving more importance to,1,1,1.0
more importance to errors,1,1,1.0
importance to errors committed,1,1,1.0
to errors committed by,1,1,1.0
errors committed by patterns,1,1,1.0
committed by patterns belonging,1,1,1.0
by patterns belonging to,1,1,1.0
patterns belonging to the,2,1,2.0
minority class the svm,1,1,1.0
class the svm approach,1,1,1.0
the svm approach consists,1,1,1.0
svm approach consists of,1,1,1.0
approach consists of introducing,1,1,1.0
consists of introducing different,1,1,1.0
of introducing different penalty,1,1,1.0
introducing different penalty factors,1,1,1.0
different penalty factors and,1,1,1.0
penalty factors and for,1,1,1.0
factors and for the,1,1,1.0
and for the positive,1,1,1.0
for the positive and,1,1,1.0
positive and negative svm,1,1,1.0
and negative svm slack,1,1,1.0
negative svm slack variables,1,1,1.0
svm slack variables during,1,1,1.0
slack variables during training,1,1,1.0
variables during training the,1,1,1.0
during training the primal,1,1,1.0
training the primal svm,1,1,1.0
the primal svm problem,1,1,1.0
primal svm problem is,1,1,1.0
svm problem is transformed,1,1,1.0
problem is transformed into,1,1,1.0
is transformed into ξi,1,1,1.0
transformed into ξi ξi,1,1,1.0
into ξi ξi subject,1,1,1.0
ξi ξi subject to,1,1,1.0
ξi subject to the,1,1,1.0
ξ i m for,1,1,1.0
i m for simplicity,1,1,1.0
m for simplicity we,1,1,1.0
for simplicity we will,1,1,1.0
simplicity we will set,1,1,1.0
we will set where,1,1,1.0
will set where is,1,1,1.0
set where is assumed,1,1,1.0
where is assumed to,1,1,1.0
is assumed to be,1,1,1.0
assumed to be the,1,1,1.0
to be the minority,1,1,1.0
be the minority class,1,1,1.0
minority class is the,1,1,1.0
class is the number,1,1,1.0
number of patterns belonging,2,1,2.0
of patterns belonging to,2,1,2.0
patterns belonging to class,2,1,2.0
belonging to class and,1,1,1.0
to class and the,1,1,1.0
class and the number,1,1,1.0
and the number of,5,2,2.5
belonging to class the,1,1,1.0
to class the ratio,1,1,1.0
class the ratio is,1,1,1.0
the ratio is usually,1,1,1.0
ratio is usually known,1,1,1.0
is usually known as,1,1,1.0
usually known as the,1,1,1.0
known as the imbalanced,1,1,1.0
as the imbalanced ratio,1,1,1.0
the imbalanced ratio as,1,1,1.0
imbalanced ratio as stated,1,1,1.0
ratio as stated before,1,1,1.0
as stated before each,1,1,1.0
stated before each synthetically,1,1,1.0
before each synthetically generated,1,1,1.0
each synthetically generated point,1,1,1.0
synthetically generated point sz,1,1,1.0
generated point sz e,1,1,1.0
point sz e q,1,1,1.0
sz e q z,1,1,1.0
e q z n,1,1,1.0
q z n in,1,1,1.0
z n in the,1,1,1.0
n in the minority,1,1,1.0
the minority class represented,1,1,1.0
minority class represented by,1,1,1.0
class represented by training,1,1,1.0
represented by training samples,1,1,1.0
by training samples xtr,1,1,1.0
training samples xtr is,1,1,1.0
samples xtr is generated,1,1,1.0
xtr is generated by,1,1,1.0
is generated by ﬁrst,1,1,1.0
generated by ﬁrst picking,1,1,1.0
by ﬁrst picking a,1,1,1.0
ﬁrst picking a pair,1,1,1.0
picking a pair of,1,1,1.0
a pair of points,1,1,1.0
pair of points xi,1,1,1.0
of points xi and,1,1,1.0
points xi and xj,2,1,2.0
xi and xj from,1,1,1.0
and xj from xtr,1,1,1.0
xj from xtr and,1,1,1.0
from xtr and then,1,1,1.0
xtr and then constructing,1,1,1.0
and then constructing their,1,1,1.0
then constructing their convex,1,1,1.0
constructing their convex combination,1,1,1.0
their convex combination in,1,1,1.0
convex combination in the,1,1,1.0
combination in the efs,1,1,1.0
efs e q sz,1,1,1.0
e q sz φ,1,1,1.0
q sz φ e,1,1,1.0
sz φ e q,1,1,1.0
e q xj φ,1,1,1.0
q xj φ e,1,1,1.0
xi δ where δ,1,1,1.0
δ where δ is,1,1,1.0
where δ is a,1,1,1.0
uniform distribution u optimisation,1,1,1.0
distribution u optimisation of,1,1,1.0
u optimisation of the,1,1,1.0
optimisation of the procedure,1,1,1.0
of the procedure the,1,1,1.0
the procedure the points,1,1,1.0
procedure the points xi,1,1,1.0
the points xi and,1,1,1.0
xi and xj will,1,1,1.0
and xj will be,1,1,1.0
xj will be randomly,1,1,1.0
will be randomly selected,1,1,1.0
be randomly selected based,1,1,1.0
randomly selected based on,1,1,1.0
based on their relative,1,1,1.0
on their relative position,1,1,1.0
their relative position in,1,1,1.0
relative position in the,1,1,1.0
position in the feature,1,1,1.0
the feature space with,1,1,1.0
feature space with respect,1,1,1.0
space with respect to,1,1,1.0
respect to the separating,1,1,1.0
to the separating hyperplane,1,1,1.0
the separating hyperplane because,1,1,1.0
separating hyperplane because the,1,1,1.0
hyperplane because the norm,1,1,1.0
because the norm of,1,1,1.0
the norm of w,1,1,1.0
norm of w is,1,1,1.0
of w is the,1,1,1.0
w is the signed,1,1,1.0
is the signed distance,1,1,1.0
the signed distance of,1,1,1.0
signed distance of φ,1,1,1.0
distance of φ xi,1,1,1.0
of φ xi f,1,1,1.0
φ xi f q,1,1,1.0
xi f q from,1,1,1.0
f q from the,1,1,1.0
q from the hyperplane,1,1,1.0
from the hyperplane is,1,1,1.0
the hyperplane is given,1,1,1.0
hyperplane is given by,1,1,1.0
is given by f,1,1,1.0
given by f xi,1,1,1.0
by f xi xi,1,1,1.0
f xi xi note,1,1,1.0
xi xi note that,1,1,1.0
xi note that if,1,1,1.0
note that if φ,1,1,1.0
that if φ xi,1,1,1.0
if φ xi is,1,1,1.0
φ xi is on,1,1,1.0
xi is on the,1,1,1.0
is on the right,1,1,1.0
on the right side,1,1,1.0
the right side of,1,1,1.0
right side of the,1,1,1.0
side of the hyperplane,2,1,2.0
of the hyperplane f,1,1,1.0
the hyperplane f xi,1,1,1.0
hyperplane f xi is,1,1,1.0
f xi is positive,1,1,1.0
xi is positive otherwise,1,1,1.0
is positive otherwise it,1,1,1.0
positive otherwise it is,1,1,1.0
otherwise it is negative,1,1,1.0
it is negative w,1,1,1.0
is negative w e,1,1,1.0
negative w e will,1,1,1.0
w e will represent,1,1,1.0
e will represent the,1,1,1.0
will represent the selection,1,1,1.0
represent the selection process,1,1,1.0
the selection process as,1,1,1.0
selection process as draws,1,1,1.0
process as draws from,1,1,1.0
as draws from a,1,1,1.0
draws from a multinomial,1,1,1.0
from a multinomial distribution,1,1,1.0
a multinomial distribution over,1,1,1.0
multinomial distribution over xtr,1,1,1.0
distribution over xtr patterns,1,1,1.0
over xtr patterns belonging,1,1,1.0
xtr patterns belonging to,1,1,1.0
minority class with natural,1,1,1.0
class with natural parameters,1,1,1.0
with natural parameters µi,1,1,1.0
natural parameters µi f,1,1,1.0
parameters µi f xi,1,1,1.0
µi f xi where,1,1,1.0
f xi where β,1,1,1.0
xi where β r,1,1,1.0
where β r is,1,1,1.0
β r is a,1,1,1.0
r is a scale,1,1,1.0
is a scale parameter,1,1,1.0
a scale parameter using,1,1,1.0
scale parameter using the,1,1,1.0
parameter using the link,1,1,1.0
using the link function,1,1,1.0
the link function the,1,1,1.0
link function the probability,1,1,1.0
function the probability of,1,1,1.0
the probability of picking,1,1,1.0
probability of picking xi,1,1,1.0
of picking xi xtr,1,1,1.0
picking xi xtr is,1,1,1.0
xi xtr is p,1,1,1.0
xtr is p xi,1,1,1.0
is p xi exp,1,1,1.0
p xi exp xi,1,1,1.0
xi exp xi exp,1,1,1.0
exp xi exp x,1,1,1.0
xi exp x if,1,1,1.0
exp x if φ,1,1,1.0
x if φ xi,1,1,1.0
if φ xi lies,1,1,1.0
φ xi lies on,1,1,1.0
xi lies on the,1,1,1.0
lies on the separating,1,1,1.0
on the separating hyperplane,1,1,1.0
the separating hyperplane then,1,1,1.0
separating hyperplane then f,1,1,1.0
hyperplane then f xi,1,1,1.0
then f xi note,1,1,1.0
f xi note that,1,1,1.0
xi note that when,1,1,1.0
note that when β,1,1,1.0
that when β points,2,1,2.0
when β points deep,1,1,1.0
β points deep within,1,1,1.0
points deep within the,1,1,1.0
deep within the minority,1,1,1.0
within the minority class,2,1,2.0
feature space are more,2,1,2.0
space are more likely,2,1,2.0
likely to be picked,2,1,2.0
to be picked when,2,1,2.0
be picked when β,2,1,2.0
picked when β points,2,1,2.0
when β points closer,1,1,1.0
β points closer to,1,1,1.0
points closer to the,1,1,1.0
closer to the class,1,1,1.0
class boundary or lying,1,1,1.0
boundary or lying inside,1,1,1.0
or lying inside the,1,1,1.0
lying inside the opposite,1,1,1.0
inside the opposite class,1,1,1.0
the opposite class are,1,1,1.0
opposite class are preferred,1,1,1.0
class are preferred and,1,1,1.0
are preferred and when,2,1,2.0
preferred and when β,2,1,2.0
and when β all,2,1,2.0
when β all the,2,1,2.0
β all the points,2,1,2.0
all the points are,2,1,2.0
the points are equally,2,1,2.0
points are equally likely,2,1,2.0
are equally likely to,2,1,2.0
equally likely to be,3,2,1.5
likely to be chosen,2,1,2.0
to be chosen as,1,1,1.0
be chosen as this,1,1,1.0
chosen as this will,1,1,1.0
as this will correspond,1,1,1.0
this will correspond to,1,1,1.0
will correspond to the,1,1,1.0
correspond to the uniform,1,1,1.0
to the uniform distribution,1,1,1.0
the uniform distribution over,1,1,1.0
uniform distribution over xtr,1,1,1.0
distribution over xtr this,1,1,1.0
over xtr this approach,1,1,1.0
xtr this approach naturally,1,1,1.0
this approach naturally spans,1,1,1.0
approach naturally spans different,1,1,1.0
naturally spans different approaches,1,1,1.0
spans different approaches to,1,1,1.0
different approaches to weighted,1,1,1.0
approaches to weighted and,1,1,1.0
to weighted and unweighted,1,1,1.0
weighted and unweighted previously,1,1,1.0
and unweighted previously introduced,1,1,1.0
unweighted previously introduced in,1,1,1.0
previously introduced in the,1,1,1.0
introduced in the literature,1,1,1.0
the literature for selecting,1,1,1.0
literature for selecting the,1,1,1.0
for selecting the pairs,1,1,1.0
selecting the pairs xi,1,1,1.0
the pairs xi xj,1,1,1.0
pairs xi xj xtr,1,1,1.0
xi xj xtr we,1,1,1.0
xj xtr we could,1,1,1.0
xtr we could use,1,1,1.0
we could use two,1,1,1.0
could use two different,1,1,1.0
use two different ideas,1,1,1.0
two different ideas pick,1,1,1.0
different ideas pick xi,1,1,1.0
ideas pick xi and,1,1,1.0
pick xi and xj,1,1,1.0
xi and xj independently,2,1,2.0
and xj independently with,1,1,1.0
xj independently with respect,1,1,1.0
independently with respect to,1,1,1.0
respect to the bution,1,1,1.0
to the bution of,1,1,1.0
the bution of eq,1,1,1.0
bution of eq pick,1,1,1.0
of eq pick xi,1,1,1.0
eq pick xi according,1,1,1.0
pick xi according to,1,1,1.0
xi according to the,1,1,1.0
according to the distribution,1,1,1.0
to the distribution of,1,1,1.0
the distribution of eq,1,1,1.0
distribution of eq and,1,1,1.0
of eq and select,1,1,1.0
eq and select xj,1,1,1.0
and select xj using,1,1,1.0
select xj using neighbours,1,1,1.0
xj using neighbours method,1,1,1.0
using neighbours method in,1,1,1.0
neighbours method in most,1,1,1.0
method in most of,1,1,1.0
most of the weighted,1,1,1.0
of the weighted approaches,1,1,1.0
the weighted approaches in,1,1,1.0
weighted approaches in the,1,1,1.0
in the literature they,1,1,1.0
the literature they make,1,1,1.0
literature they make use,1,1,1.0
they make use of,1,1,1.0
make use of the,1,1,1.0
use of the neighbours,1,1,1.0
the neighbours method because,1,1,1.0
neighbours method because they,1,1,1.0
method because they obtain,1,1,1.0
because they obtain the,1,1,1.0
they obtain the spatial,1,1,1.0
obtain the spatial distribution,2,1,2.0
the spatial distribution information,1,1,1.0
spatial distribution information of,1,1,1.0
distribution information of the,1,1,1.0
information of the patterns,1,1,1.0
patterns according to their,1,1,1.0
according to their neighbourhood,1,1,1.0
to their neighbourhood however,1,1,1.0
their neighbourhood however for,1,1,1.0
neighbourhood however for this,1,1,1.0
however for this approach,1,1,1.0
for this approach note,1,1,1.0
this approach note that,1,1,1.0
approach note that it,1,1,1.0
note that it is,1,1,1.0
that it is actually,1,1,1.0
it is actually more,1,1,1.0
is actually more advisable,1,1,1.0
actually more advisable to,1,1,1.0
more advisable to select,1,1,1.0
advisable to select xi,1,1,1.0
to select xi and,1,1,1.0
select xi and xj,1,1,1.0
and xj independently according,1,1,1.0
xj independently according to,1,1,1.0
independently according to the,1,1,1.0
according to the probability,1,1,1.0
to the probability distribution,1,1,1.0
the probability distribution obtained,1,1,1.0
probability distribution obtained because,1,1,1.0
distribution obtained because otherwise,1,1,1.0
obtained because otherwise the,1,1,1.0
because otherwise the effect,1,1,1.0
otherwise the effect of,1,1,1.0
the effect of the,2,1,2.0
effect of the preferential,1,1,1.0
of the preferential learning,1,1,1.0
the preferential learning in,1,1,1.0
preferential learning in the,1,1,1.0
learning in the sampling,1,1,1.0
in the sampling process,1,1,1.0
the sampling process could,2,1,2.0
sampling process could be,2,1,2.0
process could be smoothed,1,1,1.0
could be smoothed picking,1,1,1.0
be smoothed picking points,1,1,1.0
smoothed picking points by,1,1,1.0
picking points by the,1,1,1.0
points by the neighbours,1,1,1.0
by the neighbours approach,1,1,1.0
the neighbours approach may,1,1,1.0
neighbours approach may differ,1,1,1.0
approach may differ to,1,1,1.0
may differ to a,1,1,1.0
differ to a large,1,1,1.0
to a large extent,1,1,1.0
a large extent to,1,1,1.0
large extent to the,1,1,1.0
extent to the selection,1,1,1.0
to the selection made,1,1,1.0
the selection made with,1,1,1.0
selection made with the,1,1,1.0
made with the probability,1,1,1.0
with the probability function,1,1,1.0
the probability function based,1,1,1.0
probability function based on,1,1,1.0
function based on the,1,1,1.0
based on the arguments,1,1,1.0
on the arguments in,1,1,1.0
the arguments in section,1,1,1.0
arguments in section iii,1,1,1.0
in section iii of,1,1,1.0
section iii of the,1,1,1.0
iii of the minority,1,1,1.0
feature space is done,1,1,1.0
space is done through,1,1,1.0
is done through sampling,1,1,1.0
done through sampling in,1,1,1.0
through sampling in the,1,1,1.0
in the efs note,1,1,1.0
the efs note that,1,1,1.0
efs note that the,1,1,1.0
note that the patterns,1,1,1.0
that the patterns preferred,1,1,1.0
the patterns preferred for,1,1,1.0
patterns preferred for sampling,1,1,1.0
preferred for sampling in,1,1,1.0
for sampling in the,2,1,2.0
sampling in the input,1,1,1.0
the input space could,1,1,1.0
input space could not,1,1,1.0
space could not be,1,1,1.0
could not be the,1,1,1.0
not be the ones,1,1,1.0
be the ones preferred,1,1,1.0
the ones preferred in,1,1,1.0
ones preferred in the,1,1,1.0
preferred in the feature,1,1,1.0
the feature space therefore,1,1,1.0
feature space therefore the,1,1,1.0
space therefore the use,1,1,1.0
therefore the use of,1,1,1.0
use of the efs,1,1,1.0
of the efs is,1,1,1.0
the efs is needed,1,1,1.0
efs is needed for,1,1,1.0
is needed for this,1,1,1.0
needed for this methodology,1,1,1.0
for this methodology as,1,1,1.0
this methodology as well,1,1,1.0
methodology as well t,1,1,1.0
as well t o,2,1,2.0
well t o optimise,1,1,1.0
t o optimise the,1,1,1.0
o optimise the β,1,1,1.0
optimise the β values,1,1,1.0
the β values as,1,1,1.0
β values as different,1,1,1.0
values as different β,1,1,1.0
as different β values,1,1,1.0
different β values will,1,1,1.0
β values will induce,1,1,1.0
values will induce different,1,1,1.0
will induce different synthetic,1,1,1.0
induce different synthetic patterns,1,1,1.0
different synthetic patterns we,1,1,1.0
synthetic patterns we will,1,1,1.0
patterns we will test,1,1,1.0
we will test two,1,1,1.0
will test two approaches,1,1,1.0
test two approaches the,1,1,1.0
two approaches the ﬁrst,1,1,1.0
approaches the ﬁrst idea,1,1,1.0
idea is to use,2,1,2.0
is to use a,1,1,1.0
to use a single,1,1,1.0
use a single value,1,1,1.0
a single value of,1,1,1.0
single value of β,1,1,1.0
value of β found,1,1,1.0
of β found by,1,1,1.0
β found by over,1,1,1.0
found by over a,1,1,1.0
by over a set,1,1,1.0
over a set of,1,1,1.0
a set of p,1,1,1.0
set of p predeﬁned,1,1,1.0
of p predeﬁned β,1,1,1.0
p predeﬁned β values,1,1,1.0
predeﬁned β values the,1,1,1.0
β values the second,1,1,1.0
values the second idea,1,1,1.0
the second idea is,1,1,1.0
second idea is to,1,1,1.0
is to use multiple,1,1,1.0
to use multiple β,1,1,1.0
use multiple β values,1,1,1.0
multiple β values within,1,1,1.0
β values within the,1,1,1.0
values within the framework,1,1,1.0
within the framework of,1,1,1.0
the framework of multiple,1,1,1.0
framework of multiple kernel,1,1,1.0
of multiple kernel learning,1,1,1.0
multiple kernel learning mkl,1,1,1.0
kernel learning mkl a,1,1,1.0
learning mkl a combination,1,1,1.0
mkl a combination of,1,1,1.0
a combination of different,1,1,1.0
combination of different kernel,1,1,1.0
of different kernel matrices,1,1,1.0
different kernel matrices for,1,1,1.0
kernel matrices for a,1,1,1.0
matrices for a particular,1,1,1.0
for a particular value,1,1,1.0
a particular value of,1,1,1.0
particular value of β,1,1,1.0
value of β we,1,1,1.0
of β we denote,1,1,1.0
β we denote by,1,1,1.0
we denote by the,1,1,1.0
denote by the kernel,1,1,1.0
by the kernel matrix,1,1,1.0
kernel matrix obtained on,1,1,1.0
matrix obtained on the,1,1,1.0
obtained on the extended,1,1,1.0
on the extended data,1,1,1.0
the extended data sample,1,1,1.0
extended data sample ing,1,1,1.0
data sample ing points,1,1,1.0
sample ing points obtained,1,1,1.0
ing points obtained using,1,1,1.0
points obtained using β,1,1,1.0
obtained using β w,1,1,1.0
using β w e,1,1,1.0
β w e ﬁx,1,1,1.0
w e ﬁx a,1,1,1.0
e ﬁx a set,1,1,1.0
ﬁx a set of,1,1,1.0
a set of β,1,1,1.0
set of β values,1,1,1.0
of β values β,1,1,1.0
β values β p,1,1,1.0
values β p and,1,1,1.0
β p and compute,1,1,1.0
p and compute the,1,1,1.0
and compute the kernel,1,1,1.0
compute the kernel matrices,1,1,1.0
the kernel matrices p,1,1,1.0
kernel matrices p then,1,1,1.0
matrices p then using,1,1,1.0
p then using kt,1,1,1.0
then using kt a,1,1,1.0
using kt a we,1,1,1.0
kt a we could,1,1,1.0
a we could derive,1,1,1.0
we could derive a,1,1,1.0
could derive a kernel,1,1,1.0
derive a kernel matrix,1,1,1.0
a kernel matrix p,1,1,1.0
kernel matrix p ωk,1,1,1.0
matrix p ωk k,1,1,1.0
p ωk k with,1,1,1.0
ωk k with ωk,1,1,1.0
k with ωk and,1,1,1.0
with ωk and p,1,1,1.0
ωk and p ωk,1,1,1.0
and p ωk convex,1,1,1.0
p ωk convex combination,1,1,1.0
ωk convex combination of,1,1,1.0
convex combination of kernel,1,1,1.0
combination of kernel matrices,2,1,2.0
of kernel matrices k,1,1,1.0
kernel matrices k by,1,1,1.0
matrices k by multiple,1,1,1.0
k by multiple kernel,1,1,1.0
by multiple kernel learning,1,1,1.0
multiple kernel learning techniques,2,1,2.0
kernel learning techniques thus,1,1,1.0
learning techniques thus this,1,1,1.0
techniques thus this strategy,1,1,1.0
thus this strategy will,1,1,1.0
this strategy will be,1,1,1.0
strategy will be more,1,1,1.0
will be more ﬂexible,1,1,1.0
be more ﬂexible than,1,1,1.0
more ﬂexible than the,1,1,1.0
ﬂexible than the validation,1,1,1.0
than the validation one,1,1,1.0
the validation one because,1,1,1.0
validation one because we,1,1,1.0
one because we can,1,1,1.0
because we can optimise,1,1,1.0
we can optimise a,1,1,1.0
can optimise a combination,1,1,1.0
optimise a combination of,1,1,1.0
a combination of kernel,1,1,1.0
of kernel matrices instead,1,1,1.0
kernel matrices instead of,1,1,1.0
matrices instead of restricting,1,1,1.0
instead of restricting the,1,1,1.0
of restricting the solution,1,1,1.0
restricting the solution to,1,1,1.0
the solution to only,1,1,1.0
solution to only choosing,1,1,1.0
to only choosing the,1,1,1.0
only choosing the best,1,1,1.0
choosing the best performing,1,1,1.0
the best performing one,1,1,1.0
best performing one for,1,1,1.0
performing one for the,1,1,1.0
one for the optimisation,1,1,1.0
for the optimisation we,1,1,1.0
the optimisation we will,1,1,1.0
optimisation we will need,1,1,1.0
we will need to,1,1,1.0
will need to deﬁne,1,1,1.0
need to deﬁne an,1,1,1.0
to deﬁne an extended,1,1,1.0
deﬁne an extended ideal,1,1,1.0
an extended ideal kernel,1,1,1.0
extended ideal kernel matrix,1,1,1.0
ideal kernel matrix by,1,1,1.0
kernel matrix by introducing,1,1,1.0
matrix by introducing the,1,1,1.0
by introducing the information,1,1,1.0
introducing the information of,1,1,1.0
the information of the,1,1,1.0
information of the new,1,1,1.0
of the new synthetic,1,1,1.0
the new synthetic patterns,1,1,1.0
new synthetic patterns recall,1,1,1.0
synthetic patterns recall that,1,1,1.0
patterns recall that all,1,1,1.0
recall that all these,1,1,1.0
that all these patterns,1,1,1.0
all these patterns will,1,1,1.0
these patterns will belong,1,1,1.0
patterns will belong to,1,1,1.0
will belong to the,1,1,1.0
minority class the optimisation,1,1,1.0
class the optimisation problem,1,1,1.0
the optimisation problem to,1,1,1.0
optimisation problem to solve,1,1,1.0
problem to solve in,1,1,1.0
to solve in this,1,1,1.0
solve in this case,1,1,1.0
in this case will,1,1,1.0
this case will be,1,1,1.0
case will be the,1,1,1.0
will be the following,1,1,1.0
be the following max,1,1,1.0
the following max ω,1,1,1.0
following max ω c,1,1,1.0
max ω c f,1,1,1.0
ω c f c,1,1,1.0
c f c where,1,1,1.0
f c where m,1,1,1.0
c where m ω,1,1,1.0
where m ω note,1,1,1.0
m ω note that,1,1,1.0
ω note that since,1,1,1.0
note that since we,1,1,1.0
that since we are,1,1,1.0
since we are trying,1,1,1.0
we are trying to,1,1,1.0
are trying to align,1,1,1.0
trying to align the,1,1,1.0
to align the real,1,1,1.0
align the real kernel,1,1,1.0
the real kernel matrix,1,1,1.0
real kernel matrix with,1,1,1.0
matrix with the ideal,1,1,1.0
with the ideal one,1,1,1.0
the ideal one the,1,1,1.0
ideal one the value,1,1,1.0
one the value of,1,1,1.0
the value of f,1,1,1.0
value of f does,1,1,1.0
of f does not,1,1,1.0
f does not change,1,1,1.0
does not change and,1,1,1.0
not change and it,1,1,1.0
change and it can,1,1,1.0
and it can be,1,1,1.0
it can be obviated,1,1,1.0
can be obviated in,1,1,1.0
be obviated in the,1,1,1.0
obviated in the optimisation,1,1,1.0
in the optimisation process,1,1,1.0
the optimisation process the,1,1,1.0
optimisation process the quadratic,1,1,1.0
process the quadratic programming,1,1,1.0
the quadratic programming qp,1,1,1.0
quadratic programming qp optimization,1,1,1.0
programming qp optimization problem,1,1,1.0
qp optimization problem associated,1,1,1.0
optimization problem associated can,1,1,1.0
problem associated can be,1,1,1.0
associated can be seen,1,1,1.0
seen in fig shows,1,1,1.0
in fig shows the,1,1,1.0
fig shows the representation,1,1,1.0
shows the representation of,1,1,1.0
representation of the training,1,1,1.0
the training data for,2,1,2.0
training data for the,2,1,2.0
data for the dataset,2,1,2.0
for the dataset in,1,1,1.0
the dataset in different,1,1,1.0
dataset in different efs,1,1,1.0
in different efs using,1,1,1.0
different efs using the,1,1,1.0
efs using the formation,1,1,1.0
using the formation φ,1,1,1.0
the formation φ e,1,1,1.0
formation φ e original,1,1,1.0
φ e original efs,1,1,1.0
e original efs efs,1,1,1.0
original efs efs for,1,1,1.0
efs efs for β,1,1,1.0
efs for β and,1,1,1.0
for β and β,1,1,1.0
β and β and,1,1,1.0
and β and optimised,1,1,1.0
β and optimised through,1,1,1.0
and optimised through mkl,1,1,1.0
optimised through mkl in,1,1,1.0
through mkl in this,1,1,1.0
mkl in this case,1,1,1.0
this case the difference,2,1,2.0
case the difference between,1,1,1.0
the difference between for,1,1,1.0
difference between for different,1,1,1.0
between for different β,1,1,1.0
for different β values,3,1,3.0
different β values could,1,1,1.0
β values could be,1,1,1.0
values could be difﬁcult,1,1,1.0
could be difﬁcult to,1,1,1.0
be difﬁcult to appreciate,1,1,1.0
difﬁcult to appreciate however,1,1,1.0
to appreciate however for,1,1,1.0
appreciate however for the,1,1,1.0
case of the optimised,1,1,1.0
of the optimised efs,1,1,1.0
the optimised efs one,1,1,1.0
optimised efs one can,1,1,1.0
efs one can note,1,1,1.0
one can note that,1,1,1.0
can note that the,1,1,1.0
note that the class,1,1,1.0
that the class separation,1,1,1.0
the class separation increases,1,1,1.0
class separation increases and,1,1,1.0
separation increases and the,1,1,1.0
increases and the within,1,1,1.0
and the within class,1,1,1.0
the within class decreases,1,1,1.0
within class decreases recall,1,1,1.0
class decreases recall that,1,1,1.0
decreases recall that kt,1,1,1.0
recall that kt a,1,1,1.0
that kt a was,1,1,1.0
kt a was related,1,1,1.0
a was related to,1,1,1.0
was related to the,1,1,1.0
the fisher criterion fig,1,1,1.0
fisher criterion fig empirical,1,1,1.0
criterion fig empirical feature,1,1,1.0
fig empirical feature spaces,2,1,2.0
empirical feature spaces for,2,1,2.0
feature spaces for the,2,1,2.0
spaces for the dataset,2,1,2.0
for the dataset associated,2,1,2.0
the dataset associated to,2,1,2.0
dataset associated to the,2,1,2.0
associated to the original,2,1,2.0
the original data for,2,1,2.0
original data for different,2,1,2.0
data for different β,2,1,2.0
different β values and,2,1,2.0
β values and optimised,2,1,2.0
values and optimised in,1,1,1.0
and optimised in the,1,1,1.0
optimised in the same,1,1,1.0
in the same vein,1,1,1.0
the same vein fig,1,1,1.0
same vein fig shows,1,1,1.0
vein fig shows the,1,1,1.0
fig shows the case,1,1,1.0
shows the case of,1,1,1.0
case of the training,1,1,1.0
for the dataset and,1,1,1.0
dataset and the transformation,1,1,1.0
and the transformation φ,1,1,1.0
the transformation φ e,1,1,1.0
transformation φ e in,1,1,1.0
φ e in this,1,1,1.0
e in this case,1,1,1.0
case the difference for,1,1,1.0
the difference for the,1,1,1.0
difference for the procedure,1,1,1.0
for the procedure when,1,1,1.0
the procedure when using,1,1,1.0
procedure when using different,1,1,1.0
when using different β,1,1,1.0
using different β values,1,1,1.0
different β values can,1,1,1.0
β values can be,1,1,1.0
values can be easily,1,1,1.0
can be easily appreciated,1,1,1.0
be easily appreciated vi,1,1,1.0
easily appreciated vi e,1,1,1.0
appreciated vi e x,1,1,1.0
vi e x p,1,1,1.0
e x p e,1,1,1.0
x p e r,1,1,1.0
p e r i,1,1,1.0
e r i m,1,1,1.0
r i m e,1,1,1.0
i m e n,1,1,1.0
m e n ta,1,1,1.0
e n ta l,1,1,1.0
n ta l r,1,1,1.0
ta l r e,1,1,1.0
l r e s,1,1,1.0
r e s u,1,1,1.0
e s u lt,1,1,1.0
s u lt s,1,1,1.0
u lt s the,1,1,1.0
lt s the proposed,1,1,1.0
s the proposed methodologies,1,1,1.0
the proposed methodologies have,1,1,1.0
proposed methodologies have been,1,1,1.0
methodologies have been tested,1,1,1.0
have been tested considering,1,1,1.0
been tested considering support,1,1,1.0
tested considering support v,1,1,1.0
considering support v ector,1,1,1.0
ector machines svm and,1,1,1.0
machines svm and the,1,1,1.0
svm and the smote,1,1,1.0
and the smote algorithm,1,1,1.0
the smote algorithm binary,1,1,1.0
smote algorithm binary datasets,1,1,1.0
algorithm binary datasets from,1,1,1.0
binary datasets from the,1,1,1.0
datasets from the uci,2,2,1.0
from the uci repository,1,1,1.0
the uci repository with,1,1,1.0
uci repository with different,1,1,1.0
repository with different imbalance,1,1,1.0
with different imbalance ratios,1,1,1.0
different imbalance ratios proportion,1,1,1.0
imbalance ratios proportion of,1,1,1.0
ratios proportion of majority,1,1,1.0
proportion of majority patterns,1,1,1.0
of majority patterns with,1,1,1.0
majority patterns with respect,1,1,1.0
with respect to minority,1,1,1.0
respect to minority ones,1,1,1.0
to minority ones have,1,1,1.0
minority ones have been,1,1,1.0
ones have been used,1,1,1.0
have been used to,1,1,1.0
been used to test,1,1,1.0
used to test the,1,1,1.0
to test the performance,1,1,1.0
test the performance of,1,1,1.0
performance of the methods,1,1,1.0
of the methods in,1,1,1.0
the methods in different,1,1,1.0
methods in different situations,1,1,1.0
in different situations the,1,1,1.0
different situations the characteristics,1,1,1.0
situations the characteristics of,1,1,1.0
the characteristics of these,1,1,1.0
characteristics of these datasets,1,1,1.0
of these datasets can,1,1,1.0
these datasets can be,1,1,1.0
datasets can be seen,1,1,1.0
be seen in t,1,1,1.0
seen in t able,1,1,1.0
in t able as,1,1,1.0
t able as done,1,1,1.0
able as done in,1,1,1.0
in other works some,1,1,1.0
other works some multiclass,1,1,1.0
works some multiclass datasets,1,1,1.0
some multiclass datasets have,1,1,1.0
multiclass datasets have also,1,1,1.0
datasets have also been,1,1,1.0
have also been considered,1,1,1.0
also been considered by,1,1,1.0
been considered by grouping,1,1,1.0
considered by grouping some,1,1,1.0
by grouping some classes,1,1,1.0
grouping some classes represents,1,1,1.0
some classes represents the,1,1,1.0
classes represents the ecoli,1,1,1.0
represents the ecoli dataset,1,1,1.0
the ecoli dataset when,1,1,1.0
ecoli dataset when considering,1,1,1.0
dataset when considering class,1,1,1.0
when considering class versus,1,1,1.0
considering class versus the,1,1,1.0
class versus the rest,1,1,1.0
versus the rest and,1,1,1.0
the rest and is,1,1,1.0
rest and is the,1,1,1.0
and is the yeast,1,1,1.0
is the yeast dataset,1,1,1.0
the yeast dataset when,1,1,1.0
yeast dataset when grouping,1,1,1.0
dataset when grouping classes,1,1,1.0
when grouping classes and,1,1,1.0
grouping classes and versus,1,1,1.0
classes and versus classes,1,1,1.0
and versus classes and,1,1,1.0
versus classes and in,1,1,1.0
classes and in order,1,1,1.0
and in order to,1,1,1.0
in order to obtain,2,2,1.0
order to obtain higher,1,1,1.0
to obtain higher imbalance,1,1,1.0
obtain higher imbalance ratio,1,1,1.0
higher imbalance ratio ir,1,1,1.0
imbalance ratio ir values,1,1,1.0
ratio ir values a,1,1,1.0
ir values a stratiﬁed,1,1,1.0
values a stratiﬁed dietterich,1,1,1.0
a stratiﬁed dietterich technique,1,1,1.0
stratiﬁed dietterich technique was,1,1,1.0
dietterich technique was performed,1,1,1.0
technique was performed to,1,1,1.0
was performed to divide,1,1,1.0
performed to divide the,1,1,1.0
to divide the data,1,1,1.0
divide the data and,1,1,1.0
the data and the,1,1,1.0
data and the results,1,1,1.0
and the results are,1,1,1.0
the results are taken,1,1,1.0
results are taken as,1,1,1.0
are taken as mean,1,1,1.0
taken as mean fig,1,1,1.0
as mean fig empirical,1,1,1.0
mean fig empirical feature,1,1,1.0
values and optimised standard,1,1,1.0
and optimised standard deviation,1,1,1.0
optimised standard deviation of,1,1,1.0
standard deviation of the,2,1,2.0
deviation of the selected,1,1,1.0
of the selected measures,1,1,1.0
the selected measures as,1,1,1.0
selected measures as done,1,1,1.0
measures as done elsewhere,1,1,1.0
as done elsewhere each,1,1,1.0
done elsewhere each experiment,1,1,1.0
elsewhere each experiment over,1,1,1.0
each experiment over each,1,1,1.0
experiment over each data,1,1,1.0
over each data partition,1,1,1.0
each data partition has,1,1,1.0
data partition has been,1,1,1.0
partition has been repeated,1,1,1.0
has been repeated times,1,1,1.0
been repeated times using,1,1,1.0
repeated times using a,1,1,1.0
times using a different,1,1,1.0
using a different seed,1,1,1.0
a different seed to,1,1,1.0
different seed to obtain,1,1,1.0
seed to obtain more,1,1,1.0
to obtain more robust,1,1,1.0
obtain more robust at,1,1,1.0
more robust at the,1,1,1.0
robust at the end,1,1,1.0
at the end of,1,1,1.0
the end of the,1,1,1.0
end of the execution,1,1,1.0
of the execution we,1,1,1.0
the execution we will,1,1,1.0
execution we will have,1,1,1.0
we will have results,1,1,1.0
will have results for,1,1,1.0
have results for each,1,1,1.0
results for each dataset,1,1,1.0
for each dataset the,1,1,1.0
each dataset the gaussian,1,1,1.0
dataset the gaussian kernel,1,1,1.0
the gaussian kernel was,1,1,1.0
gaussian kernel was used,1,1,1.0
kernel was used the,1,1,1.0
was used the kernel,1,1,1.0
used the kernel width,1,1,1.0
the kernel width and,1,1,1.0
kernel width and the,1,1,1.0
width and the cost,1,1,1.0
and the cost parameter,1,1,1.0
the cost parameter of,1,1,1.0
cost parameter of svm,1,1,1.0
parameter of svm were,1,1,1.0
of svm were selected,1,1,1.0
svm were selected within,1,1,1.0
were selected within the,1,1,1.0
selected within the values,1,1,1.0
within the values by,1,1,1.0
the values by means,1,1,1.0
values by means of,1,1,1.0
by means of a,1,1,1.0
means of a nested,1,1,1.0
of a nested method,1,1,1.0
a nested method applied,1,1,1.0
nested method applied to,1,1,1.0
method applied to the,1,1,1.0
applied to the training,1,1,1.0
to the training set,1,1,1.0
training set as done,1,1,1.0
set as done in,1,1,1.0
in other works the,1,1,1.0
other works the number,1,1,1.0
works the number of,1,1,1.0
the number of synthetic,9,2,4.5
number of synthetic patterns,1,1,1.0
of synthetic patterns generated,1,1,1.0
synthetic patterns generated was,1,1,1.0
patterns generated was that,1,1,1.0
generated was that needed,1,1,1.0
was that needed to,1,1,1.0
that needed to balance,1,1,1.0
needed to balance the,1,1,1.0
to balance the distributions,1,1,1.0
balance the distributions after,1,1,1.0
the distributions after applying,1,1,1.0
distributions after applying the,1,1,1.0
after applying the process,1,1,1.0
applying the process the,1,1,1.0
the process the number,1,1,1.0
process the number of,1,1,1.0
number of majority and,1,1,1.0
of majority and minority,1,1,1.0
majority and minority patterns,1,1,1.0
and minority patterns were,1,1,1.0
minority patterns were the,1,1,1.0
patterns were the same,1,1,1.0
were the same k,1,1,1.0
the same k nearest,1,1,1.0
same k nearest neighbours,1,1,1.0
k nearest neighbours were,1,1,1.0
nearest neighbours were evaluated,1,1,1.0
neighbours were evaluated to,1,1,1.0
were evaluated to generate,1,1,1.0
evaluated to generate synthetic,1,1,1.0
to generate synthetic samples,1,1,1.0
generate synthetic samples in,1,1,1.0
synthetic samples in order,1,1,1.0
samples in order to,1,1,1.0
in order to minimise,1,1,1.0
order to minimise the,1,1,1.0
to minimise the chance,1,1,1.0
minimise the chance that,1,1,1.0
the chance that synthetic,1,1,1.0
chance that synthetic patterns,1,1,1.0
that synthetic patterns are,2,1,2.0
synthetic patterns are generated,1,1,1.0
patterns are generated in,1,1,1.0
are generated in the,1,1,1.0
generated in the majority,1,1,1.0
majority class region when,1,1,1.0
class region when using,1,1,1.0
region when using the,1,1,1.0
when using the standard,1,1,1.0
using the standard smote,1,1,1.0
the standard smote technique,1,1,1.0
standard smote technique the,1,1,1.0
smote technique the results,1,1,1.0
technique the results have,1,1,1.0
the results have been,1,1,1.0
results have been reported,1,1,1.0
have been reported in,1,1,1.0
been reported in terms,1,1,1.0
reported in terms of,2,1,2.0
in terms of two,1,1,1.0
terms of two metrics,1,1,1.0
of two metrics one,1,1,1.0
two metrics one of,1,1,1.0
metrics one of them,1,1,1.0
one of them specially,1,1,1.0
of them specially designed,1,1,1.0
them specially designed to,1,1,1.0
specially designed to deal,1,1,1.0
designed to deal with,1,1,1.0
to deal with imbalanced,2,1,2.0
deal with imbalanced data,1,1,1.0
with imbalanced data the,1,1,1.0
imbalanced data the accuracy,1,1,1.0
data the accuracy metric,1,1,1.0
the accuracy metric acc,1,1,1.0
accuracy metric acc which,1,1,1.0
metric acc which sponds,1,1,1.0
acc which sponds to,1,1,1.0
which sponds to the,1,1,1.0
sponds to the ratio,1,1,1.0
to the ratio of,1,1,1.0
the ratio of correctly,1,1,1.0
ratio of correctly classiﬁed,2,1,2.0
of correctly classiﬁed patterns,2,1,2.0
correctly classiﬁed patterns and,1,1,1.0
classiﬁed patterns and measures,1,1,1.0
patterns and measures overall,1,1,1.0
and measures overall performance,1,1,1.0
measures overall performance for,1,1,1.0
overall performance for imbalanced,1,1,1.0
performance for imbalanced datasets,1,1,1.0
for imbalanced datasets this,1,1,1.0
imbalanced datasets this metric,1,1,1.0
datasets this metric may,1,1,1.0
this metric may not,1,1,1.0
metric may not be,1,1,1.0
not be the best,1,1,1.0
be the best option,1,1,1.0
the best option since,1,1,1.0
best option since the,1,1,1.0
option since the ﬁcation,1,1,1.0
since the ﬁcation of,1,1,1.0
the ﬁcation of the,1,1,1.0
ﬁcation of the minority,1,1,1.0
minority class may be,1,1,1.0
class may be compromised,1,1,1.0
may be compromised for,1,1,1.0
be compromised for the,1,1,1.0
compromised for the sake,1,1,1.0
for the sake of,2,1,2.0
the sake of the,1,1,1.0
sake of the majority,1,1,1.0
of the majority one,1,1,1.0
the majority one it,1,1,1.0
majority one it does,1,1,1.0
one it does not,1,1,1.0
it does not distinguish,1,1,1.0
does not distinguish between,1,1,1.0
not distinguish between the,1,1,1.0
distinguish between the numbers,1,1,1.0
between the numbers of,1,1,1.0
the numbers of correctly,1,1,1.0
numbers of correctly classiﬁed,1,1,1.0
of correctly classiﬁed examples,1,1,1.0
correctly classiﬁed examples of,1,1,1.0
classiﬁed examples of each,1,1,1.0
examples of each class,1,1,1.0
of each class and,1,1,1.0
each class and we,1,1,1.0
class and we could,1,1,1.0
and we could therefore,1,1,1.0
we could therefore obtain,1,1,1.0
could therefore obtain a,1,1,1.0
therefore obtain a trivial,1,1,1.0
a trivial classiﬁer always,1,1,1.0
trivial classiﬁer always outputting,1,1,1.0
classiﬁer always outputting the,1,1,1.0
always outputting the majority,1,1,1.0
outputting the majority class,1,1,1.0
the majority class the,1,1,1.0
majority class the geometric,1,1,1.0
class the geometric mean,1,1,1.0
the geometric mean of,1,1,1.0
geometric mean of the,1,1,1.0
mean of the sensitivities,1,1,1.0
of the sensitivities gm,1,1,1.0
the sensitivities gm sp,1,1,1.0
sensitivities gm sp sn,1,1,1.0
gm sp sn where,1,1,1.0
sp sn where sp,1,1,1.0
sn where sp is,1,1,1.0
where sp is the,1,1,1.0
sp is the sensitivity,1,1,1.0
is the sensitivity for,2,1,2.0
the sensitivity for the,2,1,2.0
sensitivity for the positive,1,1,1.0
for the positive class,1,1,1.0
the positive class ratio,1,1,1.0
positive class ratio of,1,1,1.0
class ratio of correctly,1,1,1.0
correctly classiﬁed patterns sidering,1,1,1.0
classiﬁed patterns sidering only,1,1,1.0
patterns sidering only this,1,1,1.0
sidering only this class,1,1,1.0
only this class and,1,1,1.0
this class and sn,1,1,1.0
class and sn is,1,1,1.0
and sn is the,1,1,1.0
sn is the sensitivity,1,1,1.0
sensitivity for the negative,1,1,1.0
for the negative one,1,1,1.0
the negative one the,1,1,1.0
negative one the measure,1,1,1.0
one the measure for,1,1,1.0
the measure for the,1,1,1.0
measure for the parameter,1,1,1.0
for the parameter selection,1,1,1.0
the parameter selection was,1,1,1.0
parameter selection was gm,1,1,1.0
selection was gm given,1,1,1.0
was gm given its,1,1,1.0
gm given its robustness,1,1,1.0
given its robustness and,1,1,1.0
its robustness and extended,1,1,1.0
robustness and extended use,1,1,1.0
and extended use for,1,1,1.0
extended use for imbalanced,1,1,1.0
use for imbalanced data,1,1,1.0
for imbalanced data note,1,1,1.0
imbalanced data note that,1,1,1.0
data note that this,1,1,1.0
note that this metric,1,1,1.0
that this metric gives,1,1,1.0
this metric gives much,1,1,1.0
metric gives much importance,1,1,1.0
gives much importance to,1,1,1.0
much importance to recall,1,1,1.0
importance to recall that,1,1,1.0
to recall that synthetic,1,1,1.0
recall that synthetic patterns,1,1,1.0
synthetic patterns are randomly,1,1,1.0
patterns are randomly generated,1,1,1.0
are randomly generated t,1,1,1.0
randomly generated t able,1,1,1.0
generated t able i,1,1,1.0
t able i datasets,1,1,1.0
able i datasets used,1,1,1.0
i datasets used for,1,1,1.0
datasets used for the,1,1,1.0
used for the experiments,1,1,1.0
for the experiments n,1,1,1.0
the experiments n corresponds,1,1,1.0
experiments n corresponds to,1,1,1.0
n corresponds to the,1,1,1.0
corresponds to the total,1,1,1.0
to the total number,1,1,1.0
the total number of,1,1,1.0
total number of patterns,1,1,1.0
number of patterns d,1,1,1.0
of patterns d to,1,1,1.0
patterns d to the,1,1,1.0
d to the dimensionality,1,1,1.0
to the dimensionality of,1,1,1.0
input space and ir,1,1,1.0
space and ir to,1,1,1.0
and ir to the,1,1,1.0
ir to the imbalance,1,1,1.0
to the imbalance ratio,1,1,1.0
the imbalance ratio dataset,1,1,1.0
imbalance ratio dataset n,1,1,1.0
ratio dataset n d,1,1,1.0
dataset n d ir,2,1,2.0
n d ir dataset,1,1,1.0
d ir dataset n,1,1,1.0
ir dataset n d,1,1,1.0
n d ir wisconsin,1,1,1.0
d ir wisconsin pima,1,1,1.0
ir wisconsin pima haberman,1,1,1.0
wisconsin pima haberman classes,1,1,1.0
pima haberman classes being,1,1,1.0
haberman classes being therefore,1,1,1.0
classes being therefore sensitive,1,1,1.0
being therefore sensitive to,1,1,1.0
therefore sensitive to trivial,1,1,1.0
sensitive to trivial classiﬁers,1,1,1.0
to trivial classiﬁers if,1,1,1.0
trivial classiﬁers if sp,1,1,1.0
classiﬁers if sp then,1,1,1.0
if sp then gm,1,1,1.0
sp then gm independently,1,1,1.0
then gm independently of,1,1,1.0
gm independently of the,1,1,1.0
independently of the value,1,1,1.0
of the value of,1,1,1.0
the value of sn,1,1,1.0
value of sn the,1,1,1.0
of sn the source,1,1,1.0
sn the source codes,1,1,1.0
the source codes in,1,1,1.0
source codes in matlab,1,1,1.0
codes in matlab for,1,1,1.0
in matlab for the,1,1,1.0
matlab for the methods,1,1,1.0
for the methods developed,1,1,1.0
the methods developed in,1,1,1.0
methods developed in this,1,1,1.0
developed in this paper,1,1,1.0
in this paper are,2,2,1.0
this paper are available,2,2,1.0
paper are available together,1,1,1.0
are available together with,1,1,1.0
available together with the,1,1,1.0
together with the datasets,1,1,1.0
with the datasets partitions,1,1,1.0
the datasets partitions and,1,1,1.0
datasets partitions and the,1,1,1.0
partitions and the results,1,1,1.0
and the results on,1,1,1.0
the results on the,2,2,1.0
results on the website,1,1,1.0
on the website associated,1,1,1.0
the website associated with,1,1,1.0
website associated with this,1,1,1.0
associated with this paper,1,1,1.0
with this paper the,1,1,1.0
this paper the purpose,1,1,1.0
paper the purpose of,1,1,1.0
the purpose of this,2,1,2.0
purpose of this section,1,1,1.0
of this section is,1,1,1.0
this section is the,1,1,1.0
section is the ﬁrst,1,1,1.0
is the ﬁrst iment,1,1,1.0
the ﬁrst iment is,1,1,1.0
ﬁrst iment is intended,1,1,1.0
iment is intended to,1,1,1.0
is intended to test,2,1,2.0
intended to test whether,1,1,1.0
test whether the empirical,1,1,1.0
whether the empirical kernel,1,1,1.0
empirical kernel map provides,1,1,1.0
kernel map provides a,1,1,1.0
map provides a more,1,1,1.0
suitable space for than,2,1,2.0
space for than the,2,1,2.0
for than the input,2,1,2.0
the input space when,1,1,1.0
input space when dealing,1,1,1.0
space when dealing with,1,1,1.0
with kernel methods and,1,1,1.0
kernel methods and analyses,1,1,1.0
methods and analyses the,1,1,1.0
and analyses the effect,1,1,1.0
analyses the effect of,1,1,1.0
effect of the number,1,1,1.0
of the number of,3,2,1.5
the number of dimensions,2,1,2.0
number of dimensions chosen,1,1,1.0
of dimensions chosen for,1,1,1.0
dimensions chosen for the,1,1,1.0
chosen for the inﬂuence,1,1,1.0
for the inﬂuence of,1,1,1.0
inﬂuence of the concentration,1,1,1.0
of the concentration of,1,1,1.0
the concentration of spectral,1,1,1.0
concentration of spectral properties,1,1,1.0
of spectral properties the,1,1,1.0
spectral properties the second,1,1,1.0
properties the second experimental,1,1,1.0
the second experimental subsection,1,1,1.0
second experimental subsection will,1,1,1.0
experimental subsection will complement,1,1,1.0
subsection will complement the,1,1,1.0
will complement the proach,1,1,1.0
complement the proach proposing,1,1,1.0
the proach proposing a,1,1,1.0
proach proposing a new,1,1,1.0
proposing a new kernel,1,1,1.0
a new kernel learning,1,1,1.0
new kernel learning algorithm,1,1,1.0
kernel learning algorithm to,1,1,1.0
learning algorithm to optimise,1,1,1.0
algorithm to optimise a,1,1,1.0
to optimise a more,1,1,1.0
optimise a more ﬂexible,1,1,1.0
a more ﬂexible kernel,4,1,4.0
more ﬂexible kernel function,1,1,1.0
ﬂexible kernel function which,1,1,1.0
kernel function which would,1,1,1.0
function which would ideally,1,1,1.0
which would ideally better,1,1,1.0
would ideally better ﬁt,1,1,1.0
ideally better ﬁt the,1,1,1.0
better ﬁt the data,1,1,1.0
ﬁt the data the,1,1,1.0
the data the purpose,1,1,1.0
data the purpose of,1,1,1.0
purpose of this experiment,1,1,1.0
of this experiment is,1,1,1.0
this experiment is to,1,1,1.0
experiment is to test,1,1,1.0
test whether the kernel,1,1,1.0
whether the kernel function,1,1,1.0
the kernel function chosen,1,1,1.0
kernel function chosen inﬂuences,1,1,1.0
function chosen inﬂuences the,1,1,1.0
chosen inﬂuences the results,1,1,1.0
inﬂuences the results and,1,1,1.0
the results and how,1,1,1.0
results and how optimising,1,1,1.0
and how optimising this,1,1,1.0
how optimising this kernel,1,1,1.0
optimising this kernel function,1,1,1.0
this kernel function the,1,1,1.0
kernel function the synthetic,1,1,1.0
function the synthetic generated,1,1,1.0
the synthetic generated data,1,1,1.0
synthetic generated data will,1,1,1.0
generated data will be,1,1,1.0
data will be better,1,1,1.0
will be better adapted,1,1,1.0
be better adapted to,1,1,1.0
better adapted to the,1,1,1.0
adapted to the classiﬁcation,1,1,1.0
to the classiﬁcation problem,1,1,1.0
the classiﬁcation problem finally,1,1,1.0
classiﬁcation problem finally the,1,1,1.0
problem finally the third,1,1,1.0
finally the third experiment,1,1,1.0
the third experiment focuses,1,1,1.0
third experiment focuses on,1,1,1.0
experiment focuses on the,1,1,1.0
focuses on the case,1,1,1.0
on the case of,1,1,1.0
the case of weighted,1,1,1.0
case of weighted or,1,1,1.0
of weighted or preferential,1,1,1.0
weighted or preferential to,1,1,1.0
or preferential to analyse,1,1,1.0
preferential to analyse which,1,1,1.0
analyse which patterns should,1,1,1.0
patterns should be more,2,1,2.0
should be more prone,2,1,2.0
be more prone to,2,1,2.0
more prone to be,2,1,2.0
prone to be and,1,1,1.0
to be and test,1,1,1.0
be and test a,1,1,1.0
and test a new,1,1,1.0
test a new multiple,1,1,1.0
a new multiple kernel,1,1,1.0
new multiple kernel learning,1,1,1.0
multiple kernel learning algorithm,1,1,1.0
kernel learning algorithm for,1,1,1.0
learning algorithm for optimising,1,1,1.0
algorithm for optimising the,1,1,1.0
for optimising the generated,1,1,1.0
optimising the generated patterns,1,1,1.0
the generated patterns t,1,1,1.0
generated patterns t able,1,1,1.0
patterns t able ii,1,1,1.0
t able ii contains,1,1,1.0
able ii contains information,1,1,1.0
ii contains information about,1,1,1.0
contains information about all,1,1,1.0
information about all of,1,1,1.0
about all of the,1,1,1.0
all of the methods,3,1,3.0
of the methods used,1,1,1.0
the methods used for,1,1,1.0
methods used for this,1,1,1.0
used for this experimentation,1,1,1.0
for this experimentation and,1,1,1.0
this experimentation and a,1,1,1.0
experimentation and a brief,1,1,1.0
and a brief summary,1,1,1.0
a brief summary mean,1,1,1.0
brief summary mean and,1,1,1.0
summary mean and standard,1,1,1.0
mean and standard deviation,2,1,2.0
and standard deviation of,1,1,1.0
deviation of the mean,1,1,1.0
of the mean results,1,1,1.0
the mean results obtained,1,1,1.0
mean results obtained along,1,1,1.0
results obtained along the,1,1,1.0
obtained along the datasets,1,1,1.0
along the datasets used,1,1,1.0
the datasets used apart,1,1,1.0
datasets used apart from,1,1,1.0
used apart from the,1,1,1.0
apart from the fact,1,1,1.0
from the fact that,1,1,1.0
fact that the svm,1,1,1.0
that the svm without,1,1,1.0
the svm without performs,1,1,1.0
svm without performs poorly,1,1,1.0
without performs poorly for,1,1,1.0
performs poorly for minority,1,1,1.0
poorly for minority classes,1,1,1.0
for minority classes it,1,1,1.0
minority classes it can,1,1,1.0
classes it can be,1,1,1.0
it can be seen,15,1,15.0
can be seen that,15,1,15.0
be seen that the,5,1,5.0
seen that the standard,1,1,1.0
that the standard deviation,1,1,1.0
the standard deviation is,1,1,1.0
standard deviation is very,1,1,1.0
deviation is very high,1,1,1.0
is very high in,1,1,1.0
very high in gm,1,1,1.0
high in gm indicating,1,1,1.0
in gm indicating large,1,1,1.0
gm indicating large ﬂuctuations,1,1,1.0
indicating large ﬂuctuations in,1,1,1.0
large ﬂuctuations in the,1,1,1.0
ﬂuctuations in the results,1,1,1.0
in the results one,1,1,1.0
the results one can,1,1,1.0
results one can also,1,1,1.0
one can also see,1,1,1.0
can also see that,1,1,1.0
also see that the,1,1,1.0
see that the optimisation,1,1,1.0
that the optimisation of,1,1,1.0
the optimisation of a,1,1,1.0
optimisation of a spherical,1,1,1.0
spherical gaussian kernel by,1,1,1.0
gaussian kernel by kt,1,1,1.0
kernel by kt a,1,1,1.0
by kt a osk,1,1,1.0
kt a osk does,1,1,1.0
a osk does not,1,1,1.0
osk does not lead,1,1,1.0
does not lead to,1,1,1.0
not lead to very,1,1,1.0
lead to very good,1,1,1.0
to very good results,1,1,1.0
very good results and,1,1,1.0
good results and a,1,1,1.0
results and a better,1,1,1.0
and a better option,1,1,1.0
a better option is,1,1,1.0
better option is to,1,1,1.0
option is to use,1,1,1.0
is to use instead,1,1,1.0
to use instead or,1,1,1.0
use instead or a,1,1,1.0
instead or a more,1,1,1.0
or a more ﬂexible,1,1,1.0
a more ﬂexible http,1,1,1.0
more ﬂexible http kernel,1,1,1.0
ﬂexible http kernel as,1,1,1.0
http kernel as the,1,1,1.0
kernel as the one,1,1,1.0
as the one used,2,1,2.0
the one used in,1,1,1.0
one used in ogk,1,1,1.0
used in ogk further,1,1,1.0
in ogk further information,1,1,1.0
ogk further information about,1,1,1.0
further information about the,1,1,1.0
information about the results,1,1,1.0
about the results will,1,1,1.0
the results will be,1,1,1.0
results will be extracted,1,1,1.0
will be extracted using,1,1,1.0
be extracted using statistical,1,1,1.0
extracted using statistical tests,1,1,1.0
using statistical tests the,1,1,1.0
statistical tests the complete,1,1,1.0
tests the complete results,1,1,1.0
the complete results for,1,1,1.0
complete results for all,1,1,1.0
results for all of,1,1,1.0
for all of the,4,2,2.0
of the methods can,1,1,1.0
the methods can be,1,1,1.0
methods can be seen,1,1,1.0
be seen in the,1,1,1.0
seen in the webpage,1,1,1.0
in the webpage associated,1,1,1.0
the webpage associated to,1,1,1.0
webpage associated to this,1,1,1.0
associated to this paper,1,1,1.0
to this paper including,1,1,1.0
this paper including the,1,1,1.0
paper including the individual,1,1,1.0
including the individual results,1,1,1.0
the individual results for,1,1,1.0
individual results for all,1,1,1.0
results for all the,1,1,1.0
for all the datasets,2,1,2.0
all the datasets for,1,1,1.0
datasets for the sake,1,1,1.0
the sake of comparison,1,1,1.0
sake of comparison we,1,1,1.0
of comparison we included,1,1,1.0
comparison we included the,1,1,1.0
we included the results,1,1,1.0
included the results obtained,1,1,1.0
the results obtained by,1,1,1.0
results obtained by a,1,1,1.0
obtained by a majority,1,1,1.0
by a majority class,1,1,1.0
a majority class rule,1,1,1.0
majority class rule mcr,1,1,1.0
class rule mcr classiﬁer,1,1,1.0
rule mcr classiﬁer as,1,1,1.0
mcr classiﬁer as a,1,1,1.0
classiﬁer as a baseline,1,1,1.0
as a baseline result,1,1,1.0
a baseline result a,1,1,1.0
baseline result a na,1,1,1.0
result a na rule,1,1,1.0
a na rule that,1,1,1.0
na rule that classify,1,1,1.0
rule that classify all,1,1,1.0
that classify all the,1,1,1.0
classify all the patterns,1,1,1.0
all the patterns as,1,1,1.0
the patterns as belonging,1,1,1.0
patterns as belonging to,1,1,1.0
as belonging to the,1,1,1.0
belonging to the majority,1,1,1.0
the majority class from,1,1,1.0
majority class from the,1,1,1.0
class from the results,1,1,1.0
from the results of,2,1,2.0
the results of mcr,1,1,1.0
results of mcr it,1,1,1.0
of mcr it can,1,1,1.0
mcr it can be,1,1,1.0
be seen that acc,1,1,1.0
seen that acc is,1,1,1.0
that acc is not,1,1,1.0
acc is not a,1,1,1.0
is not a suitable,1,1,1.0
not a suitable metric,1,1,1.0
a suitable metric to,1,1,1.0
suitable metric to take,1,1,1.0
metric to take into,1,1,1.0
to take into account,1,1,1.0
take into account since,1,1,1.0
into account since this,1,1,1.0
account since this trivial,1,1,1.0
since this trivial methodology,1,1,1.0
this trivial methodology achieves,1,1,1.0
trivial methodology achieves the,1,1,1.0
methodology achieves the best,1,1,1.0
achieves the best results,1,1,1.0
the best results in,1,1,1.0
best results in some,1,1,1.0
results in some cases,3,1,3.0
in some cases haberman,1,1,1.0
some cases haberman and,1,1,1.0
cases haberman and in,1,1,1.0
haberman and in the,1,1,1.0
and in the following,1,1,1.0
in the following subsections,1,1,1.0
the following subsections we,1,1,1.0
following subsections we will,1,1,1.0
subsections we will perform,1,1,1.0
we will perform three,1,1,1.0
will perform three differentiated,1,1,1.0
perform three differentiated statistical,1,1,1.0
three differentiated statistical tests,1,1,1.0
differentiated statistical tests to,1,1,1.0
statistical tests to validate,1,1,1.0
tests to validate the,1,1,1.0
to validate the previously,1,1,1.0
validate the previously stated,1,1,1.0
the previously stated hypotheses,1,1,1.0
previously stated hypotheses t,1,1,1.0
stated hypotheses t able,1,1,1.0
hypotheses t able ii,1,1,1.0
t able ii abbreviation,1,1,1.0
able ii abbreviation for,1,1,1.0
ii abbreviation for all,1,1,1.0
abbreviation for all the,1,1,1.0
for all the methods,1,1,1.0
all the methods considered,1,1,1.0
the methods considered for,1,1,1.0
methods considered for the,1,1,1.0
considered for the experimentation,1,1,1.0
for the experimentation and,1,1,1.0
the experimentation and mean,1,1,1.0
experimentation and mean and,1,1,1.0
and mean and standard,1,1,1.0
and standard deviation results,1,1,1.0
standard deviation results meansd,1,1,1.0
deviation results meansd for,1,1,1.0
results meansd for all,1,1,1.0
meansd for all of,1,1,1.0
all of the datasets,2,1,2.0
of the datasets algorithm,1,1,1.0
the datasets algorithm acc,1,1,1.0
datasets algorithm acc gm,1,1,1.0
algorithm acc gm majority,1,1,1.0
acc gm majority class,1,1,1.0
gm majority class rule,1,1,1.0
majority class rule classiﬁer,1,1,1.0
class rule classiﬁer mcr,1,1,1.0
rule classiﬁer mcr svm,1,1,1.0
classiﬁer mcr svm without,1,1,1.0
mcr svm without sv,1,1,1.0
svm without sv m,1,1,1.0
without sv m svm,1,1,1.0
sv m svm applying,1,1,1.0
m svm applying in,1,1,1.0
svm applying in the,1,1,1.0
applying in the input,1,1,1.0
the input space ois,2,1,2.0
input space ois svm,1,1,1.0
space ois svm with,1,1,1.0
ois svm with in,1,1,1.0
svm with in the,2,1,2.0
with in the empirical,1,1,1.0
empirical feature space oef,1,1,1.0
feature space oef s,1,1,1.0
space oef s svm,1,1,1.0
oef s svm with,1,1,1.0
s svm with in,1,1,1.0
with in the reduced,1,1,1.0
in the reduced empirical,2,1,2.0
the reduced empirical feature,2,1,2.0
empirical feature space oref,1,1,1.0
feature space oref s,1,1,1.0
space oref s svm,1,1,1.0
oref s svm with,1,1,1.0
s svm with an,1,1,1.0
svm with an optimised,4,1,4.0
with an optimised spherical,2,1,2.0
an optimised spherical kernel,1,1,1.0
optimised spherical kernel for,1,1,1.0
spherical kernel for sampling,1,1,1.0
kernel for sampling osk,1,1,1.0
for sampling osk svm,1,1,1.0
sampling osk svm with,1,1,1.0
osk svm with an,1,1,1.0
with an optimised generalised,2,1,2.0
an optimised generalised kernel,1,1,1.0
optimised generalised kernel for,1,1,1.0
generalised kernel for sampling,1,1,1.0
kernel for sampling ogk,1,1,1.0
for sampling ogk svm,1,1,1.0
sampling ogk svm with,1,1,1.0
ogk svm with via,1,1,1.0
svm with via tial,1,1,1.0
with via tial learning,1,1,1.0
via tial learning ocp,1,1,1.0
tial learning ocp l,1,1,1.0
learning ocp l svm,1,1,1.0
ocp l svm with,1,1,1.0
l svm with via,1,1,1.0
svm with via preferential,1,1,1.0
with via preferential multiple,1,1,1.0
via preferential multiple nel,1,1,1.0
preferential multiple nel learning,1,1,1.0
multiple nel learning op,1,1,1.0
nel learning op mkl,1,1,1.0
learning op mkl the,1,1,1.0
op mkl the best,1,1,1.0
mkl the best method,1,1,1.0
best method is in,1,1,1.0
method is in bold,1,1,1.0
is in bold face,1,1,1.0
in bold face and,1,1,1.0
bold face and the,1,1,1.0
face and the second,1,1,1.0
and the second one,2,1,2.0
the second one in,1,1,1.0
second one in italics,1,1,1.0
one in italics first,1,1,1.0
in italics first experiment,1,1,1.0
italics first experiment in,1,1,1.0
first experiment in the,1,1,1.0
experiment in the efs,1,1,1.0
efs in this subsection,1,1,1.0
this subsection we will,1,1,1.0
subsection we will validate,1,1,1.0
we will validate the,1,1,1.0
will validate the hypothesis,1,1,1.0
validate the hypothesis that,1,1,1.0
hypothesis that the efs,1,1,1.0
that the efs is,1,1,1.0
the efs is a,1,1,1.0
efs is a more,1,1,1.0
is a more suitable,1,1,1.0
the input space furthermore,1,1,1.0
input space furthermore we,1,1,1.0
space furthermore we will,1,1,1.0
furthermore we will test,1,1,1.0
we will test whether,1,1,1.0
will test whether by,1,1,1.0
test whether by optimising,1,1,1.0
whether by optimising the,1,1,1.0
by optimising the dimensionality,1,1,1.0
optimising the dimensionality of,1,1,1.0
the dimensionality of this,1,1,1.0
dimensionality of this space,1,1,1.0
of this space the,1,1,1.0
this space the generated,1,1,1.0
space the generated patterns,1,1,1.0
the generated patterns are,1,1,1.0
generated patterns are more,1,1,1.0
patterns are more adequate,1,1,1.0
are more adequate for,1,1,1.0
more adequate for the,1,1,1.0
adequate for the classiﬁcation,1,1,1.0
for the classiﬁcation problem,2,2,1.0
the classiﬁcation problem t,1,1,1.0
classiﬁcation problem t o,1,1,1.0
problem t o do,1,1,1.0
o do so we,3,1,3.0
do so we will,1,1,1.0
so we will test,1,1,1.0
we will test four,1,1,1.0
will test four different,1,1,1.0
test four different approaches,1,1,1.0
four different approaches sv,1,1,1.0
different approaches sv m,1,1,1.0
approaches sv m ois,1,1,1.0
sv m ois oef,3,1,3.0
m ois oef s,3,1,3.0
ois oef s and,2,1,2.0
oef s and oref,6,1,6.0
s and oref s,6,1,6.0
and oref s see,1,1,1.0
oref s see t,1,1,1.0
s see t able,1,1,1.0
see t able ii,1,1,1.0
t able ii for,1,1,1.0
able ii for the,1,1,1.0
ii for the meaning,1,1,1.0
for the meaning of,1,1,1.0
the meaning of the,1,1,1.0
meaning of the acronyms,1,1,1.0
of the acronyms as,1,1,1.0
the acronyms as said,1,1,1.0
acronyms as said before,1,1,1.0
as said before we,1,1,1.0
said before we discarded,1,1,1.0
before we discarded all,1,1,1.0
we discarded all dimensions,1,1,1.0
discarded all dimensions that,1,1,1.0
all dimensions that correspond,1,1,1.0
dimensions that correspond to,1,1,1.0
that correspond to zero,1,1,1.0
correspond to zero eigenvalues,1,1,1.0
to zero eigenvalues for,1,1,1.0
zero eigenvalues for the,1,1,1.0
eigenvalues for the computation,1,1,1.0
for the computation of,1,1,1.0
computation of the efs,1,1,1.0
of the efs for,3,1,3.0
the efs for oef,1,1,1.0
efs for oef s,1,1,1.0
for oef s furthermore,1,1,1.0
oef s furthermore we,1,1,1.0
s furthermore we performed,1,1,1.0
furthermore we performed a,1,1,1.0
we performed a nested,1,1,1.0
performed a nested validation,1,1,1.0
a nested validation over,1,1,1.0
nested validation over the,1,1,1.0
validation over the training,1,1,1.0
over the training sets,1,1,1.0
the training sets of,1,1,1.0
training sets of the,1,1,1.0
sets of the number,1,1,1.0
the number of dominant,2,1,2.0
of dominant dimensions when,1,1,1.0
dominant dimensions when considering,1,1,1.0
dimensions when considering in,1,1,1.0
when considering in the,1,1,1.0
considering in the reduced,1,1,1.0
in the reduced efs,1,1,1.0
the reduced efs oref,1,1,1.0
reduced efs oref s,1,1,1.0
efs oref s t,1,1,1.0
oref s t o,1,1,1.0
s t o do,1,1,1.0
do so we considered,1,1,1.0
so we considered the,1,1,1.0
we considered the following,1,1,1.0
considered the following values,1,1,1.0
the following values for,1,1,1.0
following values for the,1,1,1.0
values for the q,1,1,1.0
for the q value,1,1,1.0
the q value of,1,1,1.0
q value of the,1,1,1.0
value of the empirical,1,1,1.0
of the empirical kernel,1,1,1.0
q q r where,1,1,1.0
q r where r,1,1,1.0
r where r is,1,1,1.0
where r is the,1,1,1.0
r is the original,1,1,1.0
is the original rank,1,1,1.0
the original rank of,1,1,1.0
original rank of the,1,1,1.0
rank of the training,1,1,1.0
k and is the,1,1,1.0
and is the ﬂoor,1,1,1.0
is the ﬂoor function,1,1,1.0
the ﬂoor function it,1,1,1.0
ﬂoor function it can,1,1,1.0
function it can be,1,1,1.0
seen that the results,1,1,1.0
that the results in,1,1,1.0
the results in gm,1,1,1.0
results in gm for,1,1,1.0
in gm for sv,1,1,1.0
gm for sv m,1,1,1.0
for sv m are,1,1,1.0
sv m are in,1,1,1.0
m are in general,1,1,1.0
are in general very,1,1,1.0
in general very poor,1,1,1.0
general very poor analyse,1,1,1.0
very poor analyse for,1,1,1.0
poor analyse for example,1,1,1.0
analyse for example the,1,1,1.0
for example the case,1,1,1.0
example the case of,1,1,1.0
case of the haberman,1,1,1.0
of the haberman and,1,1,1.0
the haberman and datasets,1,1,1.0
haberman and datasets concerning,1,1,1.0
and datasets concerning the,1,1,1.0
datasets concerning the ois,1,1,1.0
concerning the ois method,1,1,1.0
the ois method it,1,1,1.0
ois method it can,1,1,1.0
be seen that in,2,1,2.0
seen that in some,1,1,1.0
that in some cases,1,1,1.0
in some cases the,1,1,1.0
some cases the results,2,1,2.0
cases the results of,1,1,1.0
the results of oef,1,1,1.0
results of oef s,1,1,1.0
of oef s are,1,1,1.0
oef s are much,1,1,1.0
s are much better,1,1,1.0
are much better analyse,1,1,1.0
much better analyse the,1,1,1.0
better analyse the result,1,1,1.0
analyse the result of,1,1,1.0
the result of the,2,1,2.0
result of the dataset,1,1,1.0
of the dataset where,1,1,1.0
the dataset where sv,1,1,1.0
dataset where sv m,1,1,1.0
where sv m even,1,1,1.0
sv m even obtained,1,1,1.0
m even obtained better,1,1,1.0
even obtained better results,1,1,1.0
obtained better results or,1,1,1.0
better results or the,1,1,1.0
results or the case,1,1,1.0
or the case of,1,1,1.0
case of the dataset,2,1,2.0
of the dataset in,1,1,1.0
the dataset in relation,1,1,1.0
dataset in relation to,1,1,1.0
relation to the effect,1,1,1.0
to the effect of,1,1,1.0
the effect of controlling,1,1,1.0
effect of controlling the,1,1,1.0
of controlling the dimensionality,1,1,1.0
controlling the dimensionality it,1,1,1.0
the dimensionality it can,1,1,1.0
dimensionality it can be,1,1,1.0
be seen that oref,1,1,1.0
seen that oref s,1,1,1.0
that oref s generally,1,1,1.0
oref s generally yielded,1,1,1.0
s generally yielded similar,1,1,1.0
generally yielded similar or,1,1,1.0
yielded similar or better,1,1,1.0
similar or better performance,1,1,1.0
or better performance than,1,1,1.0
better performance than oef,1,1,1.0
performance than oef s,1,1,1.0
than oef s see,1,1,1.0
oef s see the,1,1,1.0
s see the result,1,1,1.0
see the result of,1,1,1.0
result of the and,1,1,1.0
of the and datasets,2,1,2.0
the and datasets two,1,1,1.0
and datasets two examples,1,1,1.0
datasets two examples which,1,1,1.0
two examples which will,1,1,1.0
examples which will be,1,1,1.0
which will be afterwards,1,1,1.0
will be afterwards analysed,1,1,1.0
be afterwards analysed when,1,1,1.0
afterwards analysed when taking,1,1,1.0
analysed when taking acc,1,1,1.0
when taking acc into,1,1,1.0
taking acc into account,1,1,1.0
acc into account it,1,1,1.0
into account it can,1,1,1.0
account it can be,1,1,1.0
seen that the three,1,1,1.0
that the three methods,1,1,1.0
the three methods obtain,1,1,1.0
three methods obtain very,1,1,1.0
methods obtain very similar,2,1,2.0
obtain very similar values,1,1,1.0
very similar values although,1,1,1.0
similar values although oef,1,1,1.0
values although oef s,1,1,1.0
although oef s and,1,1,1.0
and oref s obtain,1,1,1.0
oref s obtain better,1,1,1.0
s obtain better results,1,1,1.0
obtain better results in,1,1,1.0
better results in some,1,1,1.0
in some cases t,1,1,1.0
some cases t able,1,1,1.0
cases t able iii,1,1,1.0
t able iii shows,1,1,1.0
able iii shows the,1,1,1.0
iii shows the test,1,1,1.0
shows the test mean,1,1,1.0
the test mean rankings,1,1,1.0
test mean rankings for,1,1,1.0
mean rankings for the,1,1,1.0
rankings for the best,1,1,1.0
for the best method,1,1,1.0
the best method and,1,1,1.0
best method and for,1,1,1.0
method and for the,1,1,1.0
and for the worst,1,1,1.0
for the worst for,1,1,1.0
the worst for the,1,1,1.0
worst for the methods,1,1,1.0
for the methods considered,1,1,1.0
the methods considered in,1,1,1.0
methods considered in this,2,1,2.0
considered in this experiment,1,1,1.0
in this experiment along,1,1,1.0
this experiment along all,1,1,1.0
experiment along all of,1,1,1.0
along all of the,1,1,1.0
of the datasets in,1,1,1.0
the datasets in terms,1,1,1.0
datasets in terms of,1,1,1.0
in terms of acc,1,1,1.0
terms of acc and,1,1,1.0
of acc and gm,1,1,1.0
acc and gm the,1,1,1.0
and gm the results,1,1,1.0
gm the results show,1,1,1.0
results show that sv,1,1,1.0
show that sv m,1,1,1.0
that sv m is,1,1,1.0
sv m is the,1,1,1.0
m is the best,1,1,1.0
is the best performing,1,1,1.0
the best performing method,1,1,1.0
best performing method for,1,1,1.0
performing method for acc,1,1,1.0
method for acc but,1,1,1.0
for acc but the,1,1,1.0
acc but the worst,1,1,1.0
but the worst performing,1,1,1.0
the worst performing when,1,1,1.0
worst performing when considering,1,1,1.0
performing when considering a,1,1,1.0
when considering a metric,1,1,1.0
considering a metric that,1,1,1.0
a metric that takes,1,1,1.0
metric that takes into,1,1,1.0
that takes into account,1,1,1.0
takes into account the,1,1,1.0
into account the imbalanced,1,1,1.0
account the imbalanced nature,1,1,1.0
the imbalanced nature of,2,1,2.0
imbalanced nature of the,2,1,2.0
nature of the data,2,1,2.0
of the data gm,1,1,1.0
the data gm furthermore,1,1,1.0
data gm furthermore it,1,1,1.0
gm furthermore it is,1,1,1.0
furthermore it is shown,1,1,1.0
it is shown that,1,1,1.0
is shown that both,1,1,1.0
shown that both approaches,1,1,1.0
that both approaches for,1,1,1.0
both approaches for sampling,1,1,1.0
approaches for sampling in,1,1,1.0
in the efs oef,1,1,1.0
the efs oef s,1,1,1.0
efs oef s and,1,1,1.0
and oref s outperfomed,1,1,1.0
oref s outperfomed the,1,1,1.0
s outperfomed the results,1,1,1.0
outperfomed the results obtained,1,1,1.0
the results obtained when,1,1,1.0
results obtained when in,1,1,1.0
obtained when in the,1,1,1.0
when in the input,1,1,1.0
input space ois finally,1,1,1.0
space ois finally it,1,1,1.0
ois finally it can,1,1,1.0
finally it can be,2,1,2.0
be seen that controlling,1,1,1.0
seen that controlling the,1,1,1.0
that controlling the efs,1,1,1.0
controlling the efs dimensionality,1,1,1.0
the efs dimensionality we,1,1,1.0
efs dimensionality we improve,1,1,1.0
dimensionality we improve the,1,1,1.0
we improve the results,1,1,1.0
improve the results in,1,1,1.0
the results in most,1,1,1.0
results in most cases,1,1,1.0
in most cases as,1,1,1.0
most cases as the,1,1,1.0
cases as the oref,1,1,1.0
as the oref s,1,1,1.0
the oref s method,3,1,3.0
oref s method obtained,2,1,2.0
s method obtained better,1,1,1.0
method obtained better mean,1,1,1.0
obtained better mean results,1,1,1.0
better mean results than,1,1,1.0
mean results than oef,1,1,1.0
results than oef s,2,1,2.0
than oef s t,1,1,1.0
oef s t o,1,1,1.0
s t o quantify,1,1,1.0
t o quantify whether,1,1,1.0
o quantify whether a,1,1,1.0
quantify whether a statistical,1,1,1.0
whether a statistical difference,1,1,1.0
a statistical difference exists,1,1,1.0
statistical difference exists among,1,1,1.0
difference exists among the,1,1,1.0
exists among the algorithms,1,1,1.0
among the algorithms a,1,1,1.0
the algorithms a procedure,1,1,1.0
algorithms a procedure is,1,1,1.0
a procedure is employed,1,1,1.0
procedure is employed to,1,1,1.0
is employed to compare,1,1,1.0
employed to compare multiple,1,1,1.0
to compare multiple classiﬁers,1,1,1.0
compare multiple classiﬁers in,1,1,1.0
multiple classiﬁers in multiple,1,1,1.0
classiﬁers in multiple datasets,1,1,1.0
in multiple datasets t,1,1,1.0
multiple datasets t able,1,1,1.0
datasets t able iii,1,1,1.0
t able iii also,1,1,1.0
able iii also shows,1,1,1.0
iii also shows the,1,1,1.0
also shows the result,1,1,1.0
shows the result of,1,1,1.0
the result of applying,2,1,2.0
result of applying the,2,1,2.0
of applying the statistical,1,1,1.0
applying the statistical friedman,1,1,1.0
the statistical friedman s,1,1,1.0
statistical friedman s test,1,1,1.0
friedman s test for,1,1,1.0
s test for a,1,1,1.0
test for a signiﬁcance,1,1,1.0
for a signiﬁcance level,1,1,1.0
a signiﬁcance level of,2,2,1.0
signiﬁcance level of α,2,2,1.0
level of α to,1,1,1.0
of α to the,1,1,1.0
α to the mean,1,1,1.0
to the mean acc,1,1,1.0
the mean acc and,1,1,1.0
mean acc and gm,1,1,1.0
acc and gm rankings,1,1,1.0
and gm rankings the,1,1,1.0
gm rankings the test,1,1,1.0
rankings the test rejects,1,1,1.0
the test rejects the,1,1,1.0
test rejects the that,1,1,1.0
rejects the that all,1,1,1.0
the that all algorithms,1,1,1.0
that all algorithms perform,1,1,1.0
all algorithms perform similarly,1,1,1.0
algorithms perform similarly in,1,1,1.0
perform similarly in mean,1,1,1.0
similarly in mean ranking,1,1,1.0
in mean ranking for,1,1,1.0
mean ranking for both,1,1,1.0
ranking for both metrics,1,1,1.0
for both metrics note,1,1,1.0
both metrics note that,1,1,1.0
metrics note that for,1,1,1.0
note that for gm,1,1,1.0
that for gm the,1,1,1.0
for gm the differences,1,1,1.0
gm the differences are,1,1,1.0
the differences are larger,1,1,1.0
differences are larger t,1,1,1.0
are larger t able,1,1,1.0
larger t able iii,1,1,1.0
t able iii mean,1,1,1.0
able iii mean ranking,1,1,1.0
iii mean ranking results,1,1,1.0
mean ranking results for,3,1,3.0
ranking results for sv,1,1,1.0
results for sv m,1,1,1.0
for sv m ois,1,1,1.0
and oref s ranking,1,1,1.0
oref s ranking sv,1,1,1.0
s ranking sv m,1,1,1.0
ranking sv m ois,1,1,1.0
ois oef s oref,1,1,1.0
oef s oref s,1,1,1.0
s oref s acc,1,1,1.0
oref s acc gm,2,1,2.0
s acc gm friedman,1,1,1.0
acc gm friedman s,3,1,3.0
gm friedman s test,3,1,3.0
friedman s test conﬁdence,3,1,3.0
s test conﬁdence interval,3,1,3.0
test conﬁdence interval f,3,1,3.0
conﬁdence interval f α,3,1,3.0
interval f α gm,3,1,3.0
f α gm on,1,1,1.0
α gm on the,1,1,1.0
gm on the basis,1,1,1.0
on the basis of,2,1,2.0
the basis of this,1,1,1.0
basis of this rejection,1,1,1.0
of this rejection and,1,1,1.0
this rejection and following,1,1,1.0
rejection and following the,1,1,1.0
and following the guidelines,1,1,1.0
following the guidelines of,1,1,1.0
the guidelines of we,1,1,1.0
guidelines of we consider,1,1,1.0
of we consider the,1,1,1.0
we consider the best,1,1,1.0
consider the best performing,1,1,1.0
the best performing methods,1,1,1.0
best performing methods in,1,1,1.0
performing methods in gm,1,1,1.0
methods in gm the,1,1,1.0
in gm the two,1,1,1.0
gm the two proposals,1,1,1.0
the two proposals oef,1,1,1.0
two proposals oef s,1,1,1.0
proposals oef s and,1,1,1.0
and oref s as,2,1,2.0
oref s as control,2,1,2.0
s as control methods,2,1,2.0
as control methods for,1,1,1.0
control methods for the,1,1,1.0
methods for the test,1,1,1.0
for the test and,1,1,1.0
the test and we,1,1,1.0
test and we compare,1,1,1.0
and we compare them,1,1,1.0
we compare them to,1,1,1.0
compare them to the,1,1,1.0
them to the rest,1,1,1.0
to the rest according,1,1,1.0
the rest according to,1,1,1.0
rest according to their,1,1,1.0
according to their rankings,1,1,1.0
to their rankings it,1,1,1.0
their rankings it has,1,1,1.0
rankings it has been,1,1,1.0
it has been noted,1,1,1.0
has been noted that,1,1,1.0
been noted that the,1,1,1.0
noted that the approach,1,1,1.0
that the approach of,1,1,1.0
the approach of paring,1,1,1.0
approach of paring all,1,1,1.0
of paring all classiﬁers,1,1,1.0
paring all classiﬁers to,1,1,1.0
all classiﬁers to each,1,1,1.0
classiﬁers to each other,1,1,1.0
to each other in,1,1,1.0
each other in a,1,1,1.0
other in a test,1,1,1.0
in a test is,1,1,1.0
a test is not,1,1,1.0
test is not as,1,1,1.0
is not as sensitive,1,1,1.0
not as sensitive as,1,1,1.0
as sensitive as the,1,1,1.0
sensitive as the approach,1,1,1.0
as the approach of,1,1,1.0
the approach of comparing,1,1,1.0
approach of comparing all,1,1,1.0
of comparing all classiﬁers,1,1,1.0
comparing all classiﬁers to,1,1,1.0
all classiﬁers to a,1,1,1.0
classiﬁers to a given,1,1,1.0
to a given classiﬁer,1,1,1.0
a given classiﬁer control,1,1,1.0
given classiﬁer control method,1,1,1.0
classiﬁer control method one,1,1,1.0
control method one approach,1,1,1.0
method one approach to,1,1,1.0
one approach to this,1,1,1.0
approach to this latter,1,1,1.0
to this latter type,1,1,1.0
this latter type of,1,1,1.0
latter type of comparison,1,1,1.0
type of comparison is,1,1,1.0
of comparison is the,1,1,1.0
comparison is the holm,1,1,1.0
is the holm s,1,1,1.0
the holm s test,3,1,3.0
holm s test the,1,1,1.0
s test the test,2,1,2.0
test the test statistics,1,1,1.0
the test statistics for,1,1,1.0
test statistics for comparing,1,1,1.0
statistics for comparing the,1,1,1.0
for comparing the and,1,1,1.0
comparing the and method,1,1,1.0
the and method using,1,1,1.0
and method using this,1,1,1.0
method using this procedure,1,1,1.0
using this procedure is,1,1,1.0
this procedure is z,1,1,1.0
procedure is z k,1,1,1.0
is z k where,1,1,1.0
z k where k,1,1,1.0
k where k is,1,1,1.0
where k is the,1,1,1.0
k is the number,1,1,1.0
the number of algorithms,1,1,1.0
number of algorithms n,1,1,1.0
of algorithms n is,1,1,1.0
algorithms n is the,1,1,1.0
n is the number,1,1,1.0
number of datasets and,1,1,1.0
of datasets and ri,1,1,1.0
datasets and ri is,1,1,1.0
and ri is the,1,1,1.0
ri is the mean,1,1,1.0
is the mean ranking,1,1,1.0
the mean ranking of,1,1,1.0
mean ranking of the,1,1,1.0
ranking of the method,1,1,1.0
of the method the,1,1,1.0
the method the z,1,1,1.0
method the z value,1,1,1.0
the z value is,1,1,1.0
z value is used,1,1,1.0
value is used to,1,1,1.0
is used to ﬁnd,1,1,1.0
used to ﬁnd the,1,1,1.0
to ﬁnd the corresponding,1,1,1.0
ﬁnd the corresponding probability,1,1,1.0
the corresponding probability from,1,1,1.0
corresponding probability from the,1,1,1.0
probability from the table,1,1,1.0
from the table of,1,1,1.0
the table of normal,1,1,1.0
table of normal distribution,1,1,1.0
of normal distribution which,1,1,1.0
normal distribution which is,1,1,1.0
distribution which is compared,1,1,1.0
which is compared with,1,1,1.0
is compared with an,1,1,1.0
compared with an appropriate,1,1,1.0
with an appropriate level,1,1,1.0
an appropriate level of,1,1,1.0
appropriate level of signiﬁcance,1,1,1.0
level of signiﬁcance α,1,1,1.0
of signiﬁcance α holm,1,1,1.0
signiﬁcance α holm s,1,1,1.0
α holm s test,1,1,1.0
holm s test adjusts,1,1,1.0
s test adjusts the,1,1,1.0
test adjusts the α,1,1,1.0
adjusts the α value,1,1,1.0
the α value in,1,1,1.0
α value in order,1,1,1.0
value in order to,1,1,1.0
to compensate for multiple,1,1,1.0
compensate for multiple comparisons,1,1,1.0
for multiple comparisons this,1,1,1.0
multiple comparisons this is,1,1,1.0
comparisons this is done,1,1,1.0
is done in a,1,1,1.0
done in a procedure,1,1,1.0
in a procedure that,1,1,1.0
a procedure that sequentially,1,1,1.0
procedure that sequentially tests,1,1,1.0
that sequentially tests the,1,1,1.0
sequentially tests the hypotheses,1,1,1.0
tests the hypotheses ordered,1,1,1.0
the hypotheses ordered by,1,1,1.0
hypotheses ordered by their,1,1,1.0
ordered by their signiﬁcance,1,1,1.0
by their signiﬁcance w,1,1,1.0
their signiﬁcance w e,1,1,1.0
signiﬁcance w e will,1,1,1.0
w e will denote,1,1,1.0
e will denote the,1,1,1.0
will denote the ordered,1,1,1.0
denote the ordered by,1,1,1.0
the ordered by p,1,1,1.0
ordered by p p,1,1,1.0
by p p k,1,1,1.0
p p k so,1,1,1.0
p k so that,1,1,1.0
k so that pk,1,1,1.0
so that pk holm,1,1,1.0
that pk holm s,1,1,1.0
pk holm s test,1,1,1.0
holm s test compares,1,1,1.0
s test compares each,1,1,1.0
test compares each pi,1,1,1.0
compares each pi with,1,1,1.0
each pi with α,1,1,1.0
pi with α holm,1,1,1.0
with α holm from,1,1,1.0
α holm from the,1,1,1.0
holm from the most,1,1,1.0
from the most signiﬁcant,1,1,1.0
the most signiﬁcant p,1,1,1.0
most signiﬁcant p value,1,1,1.0
signiﬁcant p value if,1,1,1.0
p value if is,1,1,1.0
value if is below,1,1,1.0
if is below k,1,1,1.0
is below k the,1,1,1.0
below k the hypothesis,1,1,1.0
k the hypothesis is,1,1,1.0
the hypothesis is rejected,1,1,1.0
hypothesis is rejected and,1,1,1.0
is rejected and we,1,1,1.0
rejected and we compare,1,1,1.0
and we compare with,1,1,1.0
we compare with k,1,1,1.0
compare with k if,1,1,1.0
with k if the,1,1,1.0
k if the second,1,1,1.0
if the second hypothesis,1,1,1.0
the second hypothesis is,1,1,1.0
second hypothesis is rejected,1,1,1.0
hypothesis is rejected the,1,1,1.0
is rejected the test,1,1,1.0
rejected the test proceeds,1,1,1.0
the test proceeds with,1,1,1.0
test proceeds with the,1,1,1.0
proceeds with the third,1,1,1.0
with the third and,1,1,1.0
the third and so,1,1,1.0
third and so on,1,1,1.0
and so on as,1,1,1.0
so on as soon,1,1,1.0
on as soon as,1,1,1.0
as soon as a,1,1,1.0
soon as a certain,1,1,1.0
as a certain null,1,1,1.0
a certain null hypothesis,1,1,1.0
certain null hypothesis can,1,1,1.0
null hypothesis can not,1,1,1.0
hypothesis can not be,1,1,1.0
can not be rejected,1,1,1.0
not be rejected all,1,1,1.0
be rejected all the,1,1,1.0
rejected all the remaining,1,1,1.0
all the remaining hypotheses,1,1,1.0
the remaining hypotheses are,1,1,1.0
remaining hypotheses are retained,1,1,1.0
hypotheses are retained as,1,1,1.0
are retained as well,1,1,1.0
retained as well t,1,1,1.0
well t o analyse,1,1,1.0
t o analyse the,2,1,2.0
o analyse the results,1,1,1.0
analyse the results obtained,1,1,1.0
results obtained from the,1,1,1.0
obtained from the holm,1,1,1.0
from the holm s,1,1,1.0
holm s test see,1,1,1.0
s test see t,1,1,1.0
test see t able,1,1,1.0
see t able iv,1,1,1.0
t able iv for,1,1,1.0
able iv for the,1,1,1.0
iv for the oef,1,1,1.0
for the oef s,1,1,1.0
the oef s method,1,1,1.0
oef s method the,1,1,1.0
s method the test,1,1,1.0
method the test concluded,1,1,1.0
the test concluded that,3,1,3.0
test concluded that there,3,1,3.0
concluded that there were,2,1,2.0
that there were statistically,2,1,2.0
there were statistically signiﬁcant,2,1,2.0
were statistically signiﬁcant differences,2,1,2.0
statistically signiﬁcant differences with,1,1,1.0
signiﬁcant differences with sv,1,1,1.0
differences with sv m,1,1,1.0
with sv m for,1,1,1.0
sv m for acc,1,1,1.0
m for acc note,1,1,1.0
for acc note that,1,1,1.0
acc note that in,1,1,1.0
in this case sv,1,1,1.0
this case sv m,1,1,1.0
case sv m obtained,1,1,1.0
sv m obtained better,2,1,2.0
m obtained better results,1,1,1.0
obtained better results sv,1,1,1.0
better results sv m,1,1,1.0
results sv m for,1,1,1.0
sv m for gm,1,1,1.0
m for gm and,1,1,1.0
for gm and ois,1,1,1.0
gm and ois for,1,1,1.0
and ois for gm,1,1,1.0
ois for gm as,1,1,1.0
for gm as well,1,1,1.0
gm as well this,1,1,1.0
as well this indicates,1,1,1.0
well this indicates that,1,1,1.0
this indicates that although,1,1,1.0
indicates that although oef,1,1,1.0
that although oef s,1,1,1.0
although oef s obtained,1,1,1.0
oef s obtained worst,1,1,1.0
s obtained worst results,1,1,1.0
obtained worst results for,1,1,1.0
worst results for acc,1,1,1.0
results for acc in,1,1,1.0
for acc in comparison,1,1,1.0
acc in comparison with,1,1,1.0
in comparison with sv,1,1,1.0
comparison with sv m,1,1,1.0
with sv m the,1,1,1.0
sv m the results,1,1,1.0
m the results for,1,1,1.0
the results for gm,1,1,1.0
results for gm are,1,1,1.0
for gm are signiﬁcantly,1,1,1.0
gm are signiﬁcantly better,1,1,1.0
are signiﬁcantly better with,1,1,1.0
signiﬁcantly better with comparison,1,1,1.0
better with comparison to,1,1,1.0
with comparison to sv,1,1,1.0
comparison to sv m,1,1,1.0
to sv m and,2,1,2.0
sv m and ois,2,1,2.0
m and ois therefore,1,1,1.0
and ois therefore giving,1,1,1.0
ois therefore giving evidence,1,1,1.0
therefore giving evidence that,1,1,1.0
giving evidence that the,1,1,1.0
evidence that the efs,1,1,1.0
that the efs provides,1,1,1.0
suitable space for sampling,1,1,1.0
space for sampling by,1,1,1.0
for sampling by convex,1,1,1.0
sampling by convex combination,1,1,1.0
combination of patterns concerning,1,1,1.0
of patterns concerning the,1,1,1.0
patterns concerning the oref,1,1,1.0
concerning the oref s,1,1,1.0
oref s method the,1,1,1.0
s method the same,1,1,1.0
method the same results,1,1,1.0
the same results are,1,1,1.0
same results are obtained,1,1,1.0
results are obtained but,1,1,1.0
are obtained but there,1,1,1.0
obtained but there are,1,1,1.0
but there are also,1,1,1.0
there are also signiﬁcant,1,1,1.0
are also signiﬁcant differences,1,1,1.0
also signiﬁcant differences when,1,1,1.0
signiﬁcant differences when considering,1,1,1.0
differences when considering the,1,1,1.0
when considering the ois,1,1,1.0
considering the ois method,1,1,1.0
the ois method for,1,1,1.0
ois method for acc,1,1,1.0
method for acc which,1,1,1.0
for acc which could,1,1,1.0
acc which could indicate,1,1,1.0
which could indicate that,1,1,1.0
could indicate that in,1,1,1.0
indicate that in the,1,1,1.0
that in the empirical,1,1,1.0
empirical feature space can,1,1,1.0
feature space can be,1,1,1.0
space can be beneﬁcial,1,1,1.0
can be beneﬁcial with,1,1,1.0
be beneﬁcial with other,1,1,1.0
beneﬁcial with other purposes,1,1,1.0
with other purposes for,1,1,1.0
other purposes for example,1,1,1.0
purposes for example for,1,1,1.0
for example for ensuring,1,1,1.0
example for ensuring the,1,1,1.0
for ensuring the class,1,1,1.0
ensuring the class boundaries,1,1,1.0
the class boundaries t,1,1,1.0
class boundaries t able,1,1,1.0
boundaries t able iv,1,1,1.0
t able iv results,1,1,1.0
able iv results of,1,1,1.0
iv results of the,1,1,1.0
results of the holm,3,1,3.0
of the holm procedure,3,1,3.0
the holm procedure using,3,1,3.0
holm procedure using oef,1,1,1.0
procedure using oef s,1,1,1.0
using oef s and,1,1,1.0
as control methods cms,1,1,1.0
control methods cms when,1,1,1.0
methods cms when compared,1,1,1.0
cms when compared to,1,1,1.0
when compared to sv,1,1,1.0
compared to sv m,1,1,1.0
m and ois corrected,1,1,1.0
and ois corrected α,1,1,1.0
ois corrected α values,1,1,1.0
corrected α values compared,3,1,3.0
α values compared method,3,1,3.0
values compared method and,3,1,3.0
compared method and all,2,1,2.0
method and all of,2,1,2.0
and all of them,2,1,2.0
all of them ordered,2,1,2.0
of them ordered by,2,1,2.0
them ordered by the,2,1,2.0
ordered by the number,3,1,3.0
by the number of,3,1,3.0
the number of comparison,3,1,3.0
number of comparison i,3,1,3.0
of comparison i cm,3,1,3.0
comparison i cm oef,1,1,1.0
i cm oef s,1,1,1.0
cm oef s acc,1,1,1.0
oef s acc gm,1,1,1.0
s acc gm i,2,1,2.0
acc gm i α,2,1,2.0
gm i α method,5,1,5.0
i α method pi,5,1,5.0
α method pi method,2,1,2.0
method pi method pi,2,1,2.0
pi method pi sv,2,1,2.0
method pi sv m,2,1,2.0
pi sv m sv,2,1,2.0
sv m sv m,2,1,2.0
m sv m ois,2,1,2.0
sv m ois ois,2,1,2.0
m ois ois oref,1,1,1.0
ois ois oref s,1,1,1.0
ois oref s oref,1,1,1.0
oref s oref s,1,1,1.0
s oref s cm,1,1,1.0
oref s cm oref,1,1,1.0
s cm oref s,1,1,1.0
cm oref s acc,1,1,1.0
m ois ois oef,1,1,1.0
ois ois oef s,1,1,1.0
ois oef s oef,1,1,1.0
oef s oef s,1,1,1.0
s oef s win,1,1,1.0
oef s win or,2,1,2.0
s win or lose,2,1,2.0
win or lose with,4,1,4.0
or lose with statistical,4,1,4.0
lose with statistical signiﬁcant,3,1,3.0
with statistical signiﬁcant difference,2,1,2.0
statistical signiﬁcant difference for,2,1,2.0
signiﬁcant difference for α,2,1,2.0
difference for α in,1,1,1.0
for α in relation,1,1,1.0
α in relation to,1,1,1.0
relation to the optimal,1,1,1.0
to the optimal dimensionality,1,1,1.0
the optimal dimensionality of,2,1,2.0
optimal dimensionality of the,3,1,3.0
of the efs it,1,1,1.0
the efs it can,1,1,1.0
efs it can be,1,1,1.0
said that the decay,1,1,1.0
that the decay rate,1,1,1.0
the decay rate of,1,1,1.0
decay rate of the,1,1,1.0
rate of the eigenvalues,1,1,1.0
of the eigenvalues is,1,1,1.0
the eigenvalues is related,1,1,1.0
eigenvalues is related to,1,1,1.0
related to the smoothness,1,1,1.0
to the smoothness of,1,1,1.0
the smoothness of the,1,1,1.0
smoothness of the kernel,1,1,1.0
of the kernel and,1,1,1.0
the kernel and the,2,1,2.0
kernel and the number,1,1,1.0
the number of necessary,1,1,1.0
number of necessary dimensions,1,1,1.0
of necessary dimensions depends,1,1,1.0
necessary dimensions depends on,1,1,1.0
dimensions depends on the,1,1,1.0
depends on the interplay,1,1,1.0
on the interplay between,1,1,1.0
the interplay between the,1,1,1.0
interplay between the kernel,1,1,1.0
between the kernel and,1,1,1.0
kernel and the dataset,1,1,1.0
and the dataset in,1,1,1.0
dataset in this case,1,1,1.0
this case the mean,1,1,1.0
case the mean value,1,1,1.0
the mean value obtained,1,1,1.0
mean value obtained from,1,1,1.0
obtained from the step,1,1,1.0
from the step for,1,1,1.0
the step for the,1,1,1.0
step for the number,1,1,1.0
for the number of,2,2,1.0
number of dimensions was,1,1,1.0
of dimensions was more,1,1,1.0
dimensions was more speciﬁcally,1,1,1.0
was more speciﬁcally fig,1,1,1.0
more speciﬁcally fig shows,1,1,1.0
speciﬁcally fig shows the,1,1,1.0
fig shows the histogram,2,1,2.0
shows the histogram of,2,1,2.0
the histogram of the,1,1,1.0
histogram of the optimal,1,1,1.0
of the optimal dimensionality,1,1,1.0
the efs for all,2,1,2.0
efs for all the,1,1,1.0
all the datasets tested,1,1,1.0
the datasets tested where,1,1,1.0
datasets tested where it,1,1,1.0
tested where it can,1,1,1.0
where it can be,1,1,1.0
seen that in most,1,1,1.0
that in most of,1,1,1.0
of the cases is,1,1,1.0
the cases is enough,1,1,1.0
cases is enough to,1,1,1.0
is enough to contain,1,1,1.0
enough to contain all,1,1,1.0
to contain all the,1,1,1.0
contain all the relevant,1,1,1.0
all the relevant information,1,1,1.0
the relevant information about,1,1,1.0
relevant information about the,1,1,1.0
information about the dataset,1,1,1.0
about the dataset as,1,1,1.0
the dataset as said,1,1,1.0
dataset as said before,1,1,1.0
as said before one,1,1,1.0
said before one of,1,1,1.0
before one of the,1,1,1.0
one of the hypothesis,1,1,1.0
of the hypothesis for,1,1,1.0
the hypothesis for controlling,1,1,1.0
hypothesis for controlling the,1,1,1.0
for controlling the mensionality,1,1,1.0
controlling the mensionality of,1,1,1.0
the mensionality of the,1,1,1.0
mensionality of the efs,1,1,1.0
of the efs was,1,1,1.0
the efs was that,1,1,1.0
efs was that our,1,1,1.0
was that our algorithm,1,1,1.0
that our algorithm relies,1,1,1.0
our algorithm relies on,1,1,1.0
algorithm relies on distances,1,1,1.0
relies on distances computed,1,1,1.0
on distances computed in,1,1,1.0
distances computed in the,1,1,1.0
computed in the efs,1,1,1.0
in the efs for,1,1,1.0
the efs for computing,1,1,1.0
efs for computing nearest,1,1,1.0
for computing nearest neighbours,1,1,1.0
computing nearest neighbours and,1,1,1.0
nearest neighbours and choosing,1,1,1.0
neighbours and choosing which,1,1,1.0
and choosing which patterns,1,1,1.0
choosing which patterns to,1,1,1.0
which patterns to distances,1,1,1.0
patterns to distances which,1,1,1.0
to distances which may,1,1,1.0
distances which may bear,1,1,1.0
which may bear less,1,1,1.0
may bear less neighbourhood,1,1,1.0
bear less neighbourhood information,1,1,1.0
less neighbourhood information as,1,1,1.0
neighbourhood information as the,1,1,1.0
information as the efs,1,1,1.0
as the efs dimensionality,1,1,1.0
the efs dimensionality increases,1,1,1.0
efs dimensionality increases fig,1,1,1.0
dimensionality increases fig shows,1,1,1.0
increases fig shows the,1,1,1.0
the histogram of distances,1,1,1.0
histogram of distances between,2,1,2.0
of distances between pairs,1,1,1.0
distances between pairs of,1,1,1.0
between pairs of patterns,1,1,1.0
pairs of patterns for,1,1,1.0
of patterns for different,2,1,2.0
patterns for different values,1,1,1.0
for different values of,1,1,1.0
different values of the,1,1,1.0
values of the dimensionality,1,1,1.0
of the dimensionality of,2,1,2.0
of the efs and,1,1,1.0
the efs and for,1,1,1.0
efs and for two,1,1,1.0
and for two datasets,1,1,1.0
for two datasets where,1,1,1.0
two datasets where the,1,1,1.0
datasets where the oref,1,1,1.0
where the oref s,1,1,1.0
s method obtained much,1,1,1.0
method obtained much better,1,1,1.0
obtained much better results,1,1,1.0
much better results than,1,1,1.0
better results than oef,1,1,1.0
than oef s and,1,1,1.0
oef s and where,1,1,1.0
s and where this,1,1,1.0
and where this spectral,1,1,1.0
where this spectral erties,1,1,1.0
this spectral erties phenomenon,1,1,1.0
spectral erties phenomenon can,1,1,1.0
erties phenomenon can be,1,1,1.0
phenomenon can be appreciated,1,1,1.0
can be appreciated note,1,1,1.0
be appreciated note that,1,1,1.0
appreciated note that for,1,1,1.0
note that for the,1,1,1.0
that for the case,1,1,1.0
of the dataset using,1,1,1.0
the dataset using all,1,1,1.0
dataset using all of,1,1,1.0
using all of the,1,1,1.0
all of the dimensions,1,1,1.0
of the dimensions corresponds,1,1,1.0
the dimensions corresponds to,1,1,1.0
dimensions corresponds to in,1,1,1.0
corresponds to in an,1,1,1.0
to in an almost,1,1,1.0
in an almost randomly,1,1,1.0
an almost randomly fashion,1,1,1.0
almost randomly fashion r,1,1,1.0
randomly fashion r fig,1,1,1.0
fashion r fig histogram,1,1,1.0
r fig histogram of,1,1,1.0
fig histogram of the,2,1,2.0
histogram of the mean,2,1,2.0
of the mean optimal,1,1,1.0
the mean optimal dimensionality,1,1,1.0
mean optimal dimensionality of,1,1,1.0
efs for all datasets,1,1,1.0
for all datasets the,1,1,1.0
all datasets the abscissa,1,1,1.0
datasets the abscissa axis,1,1,1.0
the abscissa axis represents,2,1,2.0
abscissa axis represents the,2,1,2.0
axis represents the mean,1,1,1.0
represents the mean value,1,1,1.0
the mean value over,1,1,1.0
mean value over the,1,1,1.0
value over the results,1,1,1.0
over the results for,1,1,1.0
the results for the,2,2,1.0
results for the rate,1,1,1.0
for the rate of,1,1,1.0
the rate of the,1,1,1.0
rate of the rank,1,1,1.0
of the rank of,1,1,1.0
the rank of the,1,1,1.0
rank of the kernel,1,1,1.0
of the kernel matrix,1,1,1.0
the kernel matrix the,1,1,1.0
kernel matrix the ordinate,1,1,1.0
matrix the ordinate axis,1,1,1.0
the ordinate axis shows,1,1,1.0
ordinate axis shows the,1,1,1.0
axis shows the number,1,1,1.0
shows the number of,1,1,1.0
number of datasets where,2,1,2.0
of datasets where this,1,1,1.0
datasets where this value,1,1,1.0
where this value was,1,1,1.0
this value was selected,1,1,1.0
value was selected from,2,1,2.0
was selected from the,2,1,2.0
selected from the step,1,1,1.0
from the step as,1,1,1.0
the step as the,1,1,1.0
step as the neighbours,1,1,1.0
as the neighbours rule,1,1,1.0
the neighbours rule will,1,1,1.0
neighbours rule will not,1,1,1.0
rule will not be,1,1,1.0
will not be very,1,1,1.0
not be very precise,1,1,1.0
be very precise since,1,1,1.0
very precise since most,1,1,1.0
precise since most of,1,1,1.0
since most of the,1,1,1.0
most of the distances,1,1,1.0
of the distances between,1,1,1.0
the distances between pair,1,1,1.0
distances between pair of,2,1,2.0
between pair of patterns,2,1,2.0
pair of patterns are,1,1,1.0
of patterns are similar,1,1,1.0
patterns are similar fig,1,1,1.0
are similar fig histogram,1,1,1.0
similar fig histogram of,1,1,1.0
fig histogram of distances,1,1,1.0
of distances between pair,1,1,1.0
pair of patterns for,1,1,1.0
patterns for different dimensionality,1,1,1.0
for different dimensionality values,1,1,1.0
different dimensionality values of,1,1,1.0
dimensionality values of the,1,1,1.0
values of the efs,1,1,1.0
the efs the abscissa,1,1,1.0
efs the abscissa axis,1,1,1.0
axis represents the distance,1,1,1.0
represents the distance between,1,1,1.0
the distance between two,1,1,1.0
distance between two patterns,1,1,1.0
between two patterns and,1,1,1.0
two patterns and the,1,1,1.0
patterns and the ordinate,1,1,1.0
and the ordinate axis,1,1,1.0
the ordinate axis the,1,1,1.0
ordinate axis the occurrence,1,1,1.0
axis the occurrence of,1,1,1.0
the occurrence of each,1,1,1.0
occurrence of each distance,1,1,1.0
of each distance from,1,1,1.0
each distance from the,1,1,1.0
distance from the results,1,1,1.0
from the results several,1,1,1.0
the results several conclusions,1,1,1.0
results several conclusions can,1,1,1.0
several conclusions can be,2,1,2.0
conclusions can be drawn,2,1,2.0
can be drawn firstly,1,1,1.0
be drawn firstly by,1,1,1.0
drawn firstly by convex,1,1,1.0
firstly by convex combination,1,1,1.0
by convex combination is,1,1,1.0
convex combination is more,1,1,1.0
combination is more suitable,1,1,1.0
is more suitable in,1,1,1.0
more suitable in an,1,1,1.0
suitable in an ideally,1,1,1.0
in an ideally linearly,1,1,1.0
an ideally linearly separable,1,1,1.0
ideally linearly separable space,1,1,1.0
linearly separable space such,1,1,1.0
separable space such as,1,1,1.0
space such as the,1,1,1.0
such as the efs,1,1,1.0
as the efs the,1,1,1.0
the efs the method,1,1,1.0
efs the method obtains,1,1,1.0
the method obtains better,2,1,2.0
method obtains better results,1,1,1.0
obtains better results in,1,1,1.0
better results in metrics,1,1,1.0
results in metrics that,1,1,1.0
in metrics that consider,1,1,1.0
metrics that consider the,1,1,1.0
that consider the imbalanced,1,1,1.0
consider the imbalanced nature,1,1,1.0
of the data without,1,1,1.0
the data without compromising,1,1,1.0
data without compromising the,1,1,1.0
without compromising the overall,1,1,1.0
compromising the overall accuracy,1,1,1.0
the overall accuracy however,1,1,1.0
overall accuracy however in,1,1,1.0
accuracy however in the,1,1,1.0
however in the input,1,1,1.0
the input space does,1,1,1.0
input space does not,1,1,1.0
space does not achieve,1,1,1.0
does not achieve this,1,1,1.0
not achieve this balance,1,1,1.0
achieve this balance indicating,1,1,1.0
this balance indicating that,1,1,1.0
balance indicating that a,1,1,1.0
indicating that a convex,1,1,1.0
that a convex combination,1,1,1.0
combination of patterns in,1,1,1.0
of patterns in a,1,1,1.0
patterns in a possibly,1,1,1.0
in a possibly nonlinearly,1,1,1.0
a possibly nonlinearly separable,1,1,1.0
possibly nonlinearly separable space,1,1,1.0
nonlinearly separable space could,1,1,1.0
separable space could generate,1,1,1.0
space could generate patterns,1,1,1.0
could generate patterns in,1,1,1.0
generate patterns in unwanted,1,1,1.0
patterns in unwanted areas,1,1,1.0
in unwanted areas concerning,1,1,1.0
unwanted areas concerning the,1,1,1.0
areas concerning the optimisation,1,1,1.0
concerning the optimisation of,1,1,1.0
the optimisation of dominant,1,1,1.0
optimisation of dominant dimensions,1,1,1.0
of dominant dimensions for,1,1,1.0
dominant dimensions for the,1,1,1.0
dimensions for the feature,1,1,1.0
for the feature space,1,1,1.0
the feature space this,1,1,1.0
feature space this methodology,1,1,1.0
space this methodology improves,1,1,1.0
this methodology improves the,1,1,1.0
methodology improves the results,1,1,1.0
improves the results in,1,1,1.0
the results in some,1,1,1.0
in some cases thus,1,1,1.0
some cases thus encouraging,1,1,1.0
cases thus encouraging further,1,1,1.0
thus encouraging further development,1,1,1.0
encouraging further development of,1,1,1.0
further development of an,1,1,1.0
development of an analytical,1,1,1.0
of an analytical method,1,1,1.0
an analytical method to,1,1,1.0
analytical method to do,1,1,1.0
method to do so,1,1,1.0
to do so second,1,1,1.0
do so second experiment,1,1,1.0
so second experiment inﬂuence,1,1,1.0
second experiment inﬂuence of,1,1,1.0
experiment inﬂuence of the,1,1,1.0
inﬂuence of the kernel,1,1,1.0
of the kernel function,2,1,2.0
the kernel function for,1,1,1.0
kernel function for this,1,1,1.0
function for this experiment,1,1,1.0
this experiment we compare,1,1,1.0
experiment we compare three,1,1,1.0
we compare three different,1,1,1.0
compare three different proposals,1,1,1.0
three different proposals ﬁrstly,1,1,1.0
different proposals ﬁrstly oef,1,1,1.0
proposals ﬁrstly oef s,1,1,1.0
ﬁrstly oef s which,1,1,1.0
oef s which will,1,1,1.0
s which will be,1,1,1.0
which will be used,1,1,1.0
will be used as,1,1,1.0
be used as a,1,1,1.0
used as a baseline,1,1,1.0
as a baseline method,1,1,1.0
a baseline method to,1,1,1.0
baseline method to if,1,1,1.0
method to if the,1,1,1.0
to if the optimisation,1,1,1.0
if the optimisation of,1,1,1.0
optimisation of the kernel,1,1,1.0
the kernel function leads,1,1,1.0
kernel function leads to,1,1,1.0
function leads to better,1,1,1.0
leads to better results,1,1,1.0
to better results secondly,1,1,1.0
better results secondly svm,1,1,1.0
results secondly svm with,1,1,1.0
secondly svm with an,1,1,1.0
spherical gaussian kernel the,1,1,1.0
gaussian kernel the same,1,1,1.0
kernel the same kernel,1,1,1.0
the same kernel than,1,1,1.0
same kernel than for,1,1,1.0
kernel than for oef,1,1,1.0
than for oef s,1,1,1.0
for oef s but,1,1,1.0
oef s but optimised,1,1,1.0
s but optimised through,1,1,1.0
but optimised through kt,1,1,1.0
optimised through kt a,1,1,1.0
through kt a for,1,1,1.0
kt a for performing,1,1,1.0
a for performing the,1,1,1.0
for performing the in,1,1,1.0
performing the in the,1,1,1.0
the in the empirical,1,1,1.0
empirical feature space osk,1,1,1.0
feature space osk and,1,1,1.0
space osk and ﬁnally,1,1,1.0
osk and ﬁnally svm,1,1,1.0
and ﬁnally svm with,1,1,1.0
ﬁnally svm with an,1,1,1.0
generalised gaussian kernel in,1,1,1.0
gaussian kernel in the,1,1,1.0
kernel in the empirical,2,1,2.0
empirical feature space ogk,1,1,1.0
feature space ogk in,1,1,1.0
space ogk in this,1,1,1.0
ogk in this work,1,1,1.0
in this work the,1,1,1.0
this work the irprop,1,1,1.0
work the irprop algorithm,1,1,1.0
the irprop algorithm is,1,1,1.0
irprop algorithm is used,1,1,1.0
algorithm is used to,1,1,1.0
is used to optimise,1,1,1.0
used to optimise the,1,1,1.0
to optimise the aforementioned,1,1,1.0
optimise the aforementioned centred,1,1,1.0
the aforementioned centred kt,1,1,1.0
aforementioned centred kt a,1,1,1.0
centred kt a because,1,1,1.0
kt a because of,1,1,1.0
a because of its,1,1,1.0
because of its robustness,1,1,1.0
of its robustness the,1,1,1.0
its robustness the gradient,1,1,1.0
robustness the gradient norm,1,1,1.0
the gradient norm stopping,1,1,1.0
gradient norm stopping criterion,1,1,1.0
norm stopping criterion was,1,1,1.0
stopping criterion was set,1,1,1.0
criterion was set to,1,1,1.0
was set to and,1,1,1.0
set to and the,1,1,1.0
to and the maximum,1,1,1.0
and the maximum number,1,1,1.0
the maximum number of,1,1,1.0
maximum number of conjugate,1,1,1.0
number of conjugate gradient,1,1,1.0
of conjugate gradient steps,1,1,1.0
conjugate gradient steps to,1,1,1.0
gradient steps to for,1,1,1.0
steps to for the,1,1,1.0
to for the optimisation,1,1,1.0
the optimisation of ogk,1,1,1.0
optimisation of ogk we,1,1,1.0
of ogk we also,1,1,1.0
ogk we also included,1,1,1.0
we also included a,1,1,1.0
also included a γ,1,1,1.0
included a γ parameter,1,1,1.0
a γ parameter as,1,1,1.0
γ parameter as an,1,1,1.0
parameter as an additional,1,1,1.0
as an additional parameter,1,1,1.0
an additional parameter in,1,1,1.0
additional parameter in the,1,1,1.0
parameter in the generalised,1,1,1.0
in the generalised gaussian,1,1,1.0
the generalised gaussian kernel,2,1,2.0
generalised gaussian kernel which,1,1,1.0
gaussian kernel which will,1,1,1.0
kernel which will indeed,1,1,1.0
which will indeed make,1,1,1.0
will indeed make the,1,1,1.0
indeed make the parameters,1,1,1.0
make the parameters initialisation,1,1,1.0
the parameters initialisation easier,1,1,1.0
parameters initialisation easier the,1,1,1.0
initialisation easier the initial,1,1,1.0
easier the initial point,1,1,1.0
the initial point for,1,1,1.0
initial point for γ,1,1,1.0
point for γ for,1,1,1.0
for γ for all,1,1,1.0
γ for all of,1,1,1.0
of the methods tested,1,1,1.0
the methods tested was,1,1,1.0
methods tested was chosen,1,1,1.0
tested was chosen from,1,1,1.0
was chosen from the,1,1,1.0
chosen from the set,1,1,1.0
from the set analysing,1,1,1.0
the set analysing the,1,1,1.0
set analysing the best,1,1,1.0
analysing the best result,1,1,1.0
the best result in,1,1,1.0
best result in alignment,1,1,1.0
result in alignment for,1,1,1.0
in alignment for the,1,1,1.0
alignment for the three,1,1,1.0
for the three values,1,1,1.0
the three values the,1,1,1.0
three values the q,1,1,1.0
values the q matrix,1,1,1.0
the q matrix for,1,1,1.0
q matrix for the,1,1,1.0
matrix for the generalised,1,1,1.0
for the generalised gaussian,1,1,1.0
generalised gaussian kernel is,1,1,1.0
gaussian kernel is initialised,1,1,1.0
kernel is initialised as,1,1,1.0
is initialised as the,1,1,1.0
initialised as the pseudoinverse,1,1,1.0
as the pseudoinverse of,1,1,1.0
the pseudoinverse of the,1,1,1.0
pseudoinverse of the covariance,1,1,1.0
of the covariance of,1,1,1.0
the covariance of the,1,1,1.0
covariance of the training,1,1,1.0
the training points q,1,1,1.0
training points q cov,1,1,1.0
points q cov xtr,1,1,1.0
q cov xtr to,1,1,1.0
cov xtr to address,1,1,1.0
xtr to address the,1,1,1.0
to address the problem,1,1,1.0
address the problem of,1,1,1.0
the problem of covariance,1,1,1.0
problem of covariance matrices,1,1,1.0
of covariance matrices once,1,1,1.0
covariance matrices once the,1,1,1.0
matrices once the kernel,1,1,1.0
once the kernel has,1,1,1.0
the kernel has been,1,1,1.0
kernel has been optimised,1,1,1.0
has been optimised via,1,1,1.0
been optimised via kt,1,1,1.0
optimised via kt a,1,1,1.0
via kt a we,1,1,1.0
kt a we optimise,1,1,1.0
a we optimise the,1,1,1.0
we optimise the c,1,1,1.0
optimise the c parameter,1,1,1.0
the c parameter using,1,1,1.0
c parameter using validation,1,1,1.0
parameter using validation within,1,1,1.0
using validation within the,1,1,1.0
validation within the values,1,1,1.0
within the values this,1,1,1.0
the values this two,1,1,1.0
values this two stage,1,1,1.0
this two stage optimisation,1,1,1.0
two stage optimisation method,1,1,1.0
stage optimisation method is,1,1,1.0
optimisation method is also,1,1,1.0
method is also referred,1,1,1.0
is also referred in,1,1,1.0
also referred in the,1,1,1.0
referred in the literature,1,1,1.0
in the literature as,1,1,1.0
the literature as method,1,1,1.0
literature as method from,1,1,1.0
as method from the,1,1,1.0
method from the results,1,1,1.0
from the results that,1,1,1.0
the results that can,1,1,1.0
results that can be,1,1,1.0
that can be found,1,1,1.0
be found in the,1,1,1.0
found in the website,1,1,1.0
in the website one,1,1,1.0
the website one can,1,1,1.0
website one can see,1,1,1.0
one can see that,6,2,3.0
can see that osk,1,1,1.0
see that osk and,1,1,1.0
that osk and ogk,1,1,1.0
osk and ogk obtained,1,1,1.0
and ogk obtained in,1,1,1.0
ogk obtained in some,1,1,1.0
obtained in some cases,1,1,1.0
in some cases better,1,1,1.0
some cases better results,1,1,1.0
cases better results in,1,1,1.0
better results in acc,1,1,1.0
results in acc than,1,1,1.0
in acc than sv,1,1,1.0
acc than sv m,1,1,1.0
than sv m this,1,1,1.0
sv m this could,1,1,1.0
m this could be,1,1,1.0
this could be due,1,1,1.0
could be due to,1,1,1.0
be due to the,1,1,1.0
application of the kernel,1,1,1.0
of the kernel optimisation,1,1,1.0
the kernel optimisation through,1,1,1.0
kernel optimisation through kt,1,1,1.0
optimisation through kt a,1,1,1.0
through kt a which,1,1,1.0
kt a which selected,1,1,1.0
a which selected a,1,1,1.0
which selected a more,1,1,1.0
selected a more optimal,1,1,1.0
a more optimal kernel,1,1,1.0
more optimal kernel than,1,1,1.0
optimal kernel than the,1,1,1.0
kernel than the method,1,1,1.0
than the method analysing,1,1,1.0
the method analysing gm,1,1,1.0
method analysing gm it,1,1,1.0
analysing gm it can,1,1,1.0
gm it can be,1,1,1.0
seen that the performance,1,1,1.0
that the performance of,2,2,1.0
performance of the spherical,1,1,1.0
of the spherical gaussian,1,1,1.0
the spherical gaussian kernel,2,1,2.0
spherical gaussian kernel is,1,1,1.0
gaussian kernel is not,1,1,1.0
kernel is not satisfactory,1,1,1.0
is not satisfactory in,1,1,1.0
not satisfactory in optimising,1,1,1.0
satisfactory in optimising the,1,1,1.0
in optimising the spherical,1,1,1.0
optimising the spherical kernel,1,1,1.0
the spherical kernel a,1,1,1.0
spherical kernel a methodology,1,1,1.0
kernel a methodology should,1,1,1.0
a methodology should be,1,1,1.0
methodology should be preferred,1,1,1.0
should be preferred to,1,1,1.0
be preferred to kt,1,1,1.0
preferred to kt t,1,1,1.0
to kt t o,1,1,1.0
kt t o see,1,1,1.0
t o see this,1,1,1.0
o see this analyse,1,1,1.0
see this analyse the,1,1,1.0
this analyse the case,1,1,1.0
analyse the case of,1,1,1.0
case of the and,1,1,1.0
the and datasets where,1,1,1.0
and datasets where although,1,1,1.0
datasets where although osk,1,1,1.0
where although osk incorporates,1,1,1.0
although osk incorporates a,1,1,1.0
osk incorporates a stage,1,1,1.0
incorporates a stage sv,1,1,1.0
a stage sv m,1,1,1.0
stage sv m obtained,1,1,1.0
m obtained better gm,1,1,1.0
obtained better gm results,1,1,1.0
better gm results finally,1,1,1.0
gm results finally it,1,1,1.0
results finally it can,1,1,1.0
be seen that ogk,1,1,1.0
seen that ogk yielded,1,1,1.0
that ogk yielded a,1,1,1.0
ogk yielded a much,1,1,1.0
yielded a much better,1,1,1.0
a much better mance,1,1,1.0
much better mance in,1,1,1.0
better mance in most,1,1,1.0
mance in most of,1,1,1.0
of the cases analyse,1,1,1.0
the cases analyse the,1,1,1.0
cases analyse the dataset,1,1,1.0
analyse the dataset demonstrating,1,1,1.0
the dataset demonstrating therefore,1,1,1.0
dataset demonstrating therefore that,1,1,1.0
demonstrating therefore that a,1,1,1.0
therefore that a more,1,1,1.0
that a more ﬂexible,1,1,1.0
more ﬂexible kernel combined,1,1,1.0
ﬂexible kernel combined with,1,1,1.0
kernel combined with kernel,1,1,1.0
combined with kernel learning,1,1,1.0
with kernel learning techniques,1,1,1.0
kernel learning techniques could,2,1,2.0
learning techniques could optimise,1,1,1.0
techniques could optimise the,1,1,1.0
could optimise the separation,1,1,1.0
optimise the separation of,1,1,1.0
the separation of the,1,1,1.0
separation of the classes,1,1,1.0
of the classes in,1,1,1.0
the classes in the,1,1,1.0
classes in the feature,1,1,1.0
the feature space a,1,1,1.0
feature space a necessary,1,1,1.0
space a necessary condition,1,1,1.0
a necessary condition for,1,1,1.0
necessary condition for by,1,1,1.0
condition for by convex,1,1,1.0
combination of patterns as,1,1,1.0
of patterns as done,1,1,1.0
patterns as done before,1,1,1.0
as done before t,1,1,1.0
done before t able,1,1,1.0
before t able v,1,1,1.0
t able v shows,1,1,1.0
able v shows the,1,1,1.0
v shows the mean,1,1,1.0
shows the mean ranking,2,1,2.0
the mean ranking results,2,1,2.0
ranking results for the,1,1,1.0
results for the three,1,1,1.0
for the three methods,1,1,1.0
the three methods considered,1,1,1.0
three methods considered in,1,1,1.0
considered in this subsection,1,1,1.0
in this subsection and,1,1,1.0
this subsection and the,1,1,1.0
subsection and the result,1,1,1.0
and the result of,1,1,1.0
of applying the friedman,1,1,1.0
applying the friedman s,1,1,1.0
the friedman s test,2,1,2.0
friedman s test the,1,1,1.0
test the test accepted,1,1,1.0
the test accepted the,1,1,1.0
test accepted the that,2,1,2.0
accepted the that all,1,1,1.0
the that all of,1,1,1.0
that all of the,1,1,1.0
all of the algorithms,1,1,1.0
of the algorithms perform,1,1,1.0
the algorithms perform similarly,1,1,1.0
algorithms perform similarly for,1,1,1.0
perform similarly for acc,1,1,1.0
similarly for acc and,1,1,1.0
for acc and rejected,2,1,2.0
acc and rejected it,2,1,2.0
and rejected it for,2,1,2.0
rejected it for gm,2,1,2.0
it for gm from,2,1,2.0
for gm from the,1,1,1.0
gm from the results,1,1,1.0
from the results obtained,1,1,1.0
the results obtained it,2,1,2.0
results obtained it can,2,1,2.0
obtained it can be,3,2,1.5
be seen that when,1,1,1.0
seen that when using,1,1,1.0
that when using a,1,1,1.0
when using a spherical,1,1,1.0
using a spherical gaussian,1,1,1.0
spherical gaussian kernel as,1,1,1.0
gaussian kernel as in,1,1,1.0
kernel as in oef,1,1,1.0
as in oef s,1,1,1.0
in oef s optimised,1,1,1.0
oef s optimised through,1,1,1.0
s optimised through and,1,1,1.0
optimised through and osk,1,1,1.0
through and osk optimised,1,1,1.0
and osk optimised by,1,1,1.0
osk optimised by kt,1,1,1.0
optimised by kt a,1,1,1.0
by kt a the,1,1,1.0
kt a the results,1,1,1.0
a the results are,1,1,1.0
the results are comparable,1,1,1.0
results are comparable and,1,1,1.0
are comparable and the,1,1,1.0
comparable and the methods,1,1,1.0
and the methods obtain,1,1,1.0
the methods obtain very,1,1,1.0
obtain very similar mean,1,1,1.0
very similar mean ranking,1,1,1.0
similar mean ranking results,1,1,1.0
mean ranking results in,1,1,1.0
ranking results in this,1,1,1.0
results in this case,1,1,1.0
in this case it,1,1,1.0
this case it is,1,1,1.0
case it is clear,1,1,1.0
is clear that the,1,1,1.0
clear that the method,1,1,1.0
that the method obtains,1,1,1.0
method obtains better gm,1,1,1.0
obtains better gm results,1,1,1.0
better gm results as,1,1,1.0
gm results as this,1,1,1.0
results as this is,1,1,1.0
as this is the,1,1,1.0
this is the metric,1,1,1.0
is the metric used,1,1,1.0
the metric used for,1,1,1.0
metric used for the,1,1,1.0
used for the parameters,1,1,1.0
for the parameters selection,1,1,1.0
the parameters selection stage,1,1,1.0
parameters selection stage however,1,1,1.0
selection stage however when,1,1,1.0
stage however when using,1,1,1.0
however when using a,1,1,1.0
when using a more,1,1,1.0
using a more ﬂexible,1,1,1.0
more ﬂexible kernel such,2,1,2.0
ﬂexible kernel such as,2,1,2.0
kernel such as the,2,1,2.0
such as the one,2,1,2.0
as the one considered,1,1,1.0
the one considered in,1,1,1.0
one considered in the,1,1,1.0
considered in the ogk,1,1,1.0
in the ogk method,1,1,1.0
the ogk method the,1,1,1.0
ogk method the results,1,1,1.0
method the results can,1,1,1.0
the results can be,1,1,1.0
results can be signiﬁcantly,1,1,1.0
can be signiﬁcantly improved,1,1,1.0
be signiﬁcantly improved note,1,1,1.0
signiﬁcantly improved note that,1,1,1.0
improved note that applying,1,1,1.0
note that applying to,1,1,1.0
that applying to the,1,1,1.0
applying to the generalised,1,1,1.0
to the generalised kernel,1,1,1.0
the generalised kernel could,1,1,1.0
generalised kernel could possibly,1,1,1.0
kernel could possibly improve,1,1,1.0
could possibly improve gm,1,1,1.0
possibly improve gm results,1,1,1.0
improve gm results but,1,1,1.0
gm results but the,1,1,1.0
results but the computational,1,1,1.0
but the computational task,1,1,1.0
the computational task required,1,1,1.0
computational task required would,1,1,1.0
task required would be,1,1,1.0
required would be infeasible,1,1,1.0
would be infeasible on,1,1,1.0
be infeasible on the,1,1,1.0
infeasible on the basis,1,1,1.0
the basis of friedman,1,1,1.0
basis of friedman s,1,1,1.0
of friedman s test,1,1,1.0
friedman s test rejection,1,1,1.0
s test rejection the,1,1,1.0
test rejection the holm,1,1,1.0
rejection the holm test,1,1,1.0
the holm test t,1,1,1.0
holm test t able,1,1,1.0
test t able v,1,1,1.0
t able v mean,1,1,1.0
able v mean ranking,1,1,1.0
v mean ranking results,1,1,1.0
ranking results for oef,1,1,1.0
results for oef s,1,1,1.0
for oef s osk,1,1,1.0
oef s osk and,1,1,1.0
s osk and ogk,1,1,1.0
osk and ogk ranking,1,1,1.0
and ogk ranking oef,1,1,1.0
ogk ranking oef s,1,1,1.0
ranking oef s osk,1,1,1.0
oef s osk ogk,1,1,1.0
s osk ogk acc,1,1,1.0
osk ogk acc gm,1,1,1.0
ogk acc gm friedman,1,1,1.0
f α gm for,1,1,1.0
α gm for multiple,1,1,1.0
gm for multiple comparisons,1,1,1.0
for multiple comparisons has,2,1,2.0
multiple comparisons has been,2,1,2.0
comparisons has been applied,1,1,1.0
has been applied see,1,1,1.0
been applied see t,1,1,1.0
applied see t able,1,1,1.0
see t able vi,1,1,1.0
t able vi and,1,1,1.0
able vi and the,1,1,1.0
vi and the test,1,1,1.0
and the test concluded,1,1,1.0
statistically signiﬁcant differences for,4,1,4.0
signiﬁcant differences for gm,2,1,2.0
differences for gm when,2,1,2.0
for gm when considering,1,1,1.0
gm when considering osk,1,1,1.0
when considering osk and,1,1,1.0
considering osk and oef,1,1,1.0
osk and oef s,2,1,2.0
and oef s as,1,1,1.0
oef s as said,1,1,1.0
s as said there,1,1,1.0
as said there were,1,1,1.0
said there were no,1,1,1.0
there were no statistically,1,1,1.0
were no statistically signiﬁcant,1,1,1.0
no statistically signiﬁcant differences,1,1,1.0
signiﬁcant differences for acc,1,1,1.0
differences for acc t,1,1,1.0
for acc t able,1,1,1.0
acc t able vi,1,1,1.0
t able vi results,1,1,1.0
able vi results of,1,1,1.0
vi results of the,1,1,1.0
holm procedure using ogk,1,1,1.0
procedure using ogk as,1,1,1.0
using ogk as the,1,1,1.0
ogk as the control,1,1,1.0
as the control method,1,1,1.0
the control method when,1,1,1.0
control method when compared,1,1,1.0
method when compared to,1,1,1.0
when compared to osk,1,1,1.0
compared to osk and,1,1,1.0
to osk and oef,1,1,1.0
and oef s corrected,1,1,1.0
oef s corrected α,1,1,1.0
s corrected α values,1,1,1.0
comparison i cm ogk,1,1,1.0
i cm ogk gm,1,1,1.0
cm ogk gm i,1,1,1.0
ogk gm i α,1,1,1.0
α method pi osk,1,1,1.0
method pi osk oef,1,1,1.0
pi osk oef s,1,1,1.0
osk oef s win,1,1,1.0
with statistical signiﬁcant differences,1,1,1.0
statistical signiﬁcant differences for,1,1,1.0
signiﬁcant differences for α,2,1,2.0
differences for α the,1,1,1.0
for α the results,1,1,1.0
α the results in,1,1,1.0
the results in this,1,1,1.0
results in this subsection,1,1,1.0
in this subsection show,1,1,1.0
this subsection show that,1,1,1.0
subsection show that in,1,1,1.0
show that in the,1,1,1.0
in the efs is,1,1,1.0
the efs is affected,1,1,1.0
efs is affected by,1,1,1.0
is affected by the,1,1,1.0
affected by the kernel,1,1,1.0
the kernel function although,1,1,1.0
kernel function although spherical,1,1,1.0
function although spherical gaussian,1,1,1.0
although spherical gaussian kernel,1,1,1.0
spherical gaussian kernel has,2,1,2.0
gaussian kernel has been,2,1,2.0
kernel has been proven,1,1,1.0
has been proven to,1,1,1.0
been proven to show,1,1,1.0
proven to show promising,1,1,1.0
to show promising results,1,1,1.0
show promising results in,1,1,1.0
promising results in the,1,1,1.0
results in the previous,1,1,1.0
in the previous subsection,1,1,1.0
the previous subsection kernel,1,1,1.0
previous subsection kernel which,1,1,1.0
subsection kernel which is,1,1,1.0
kernel which is indeed,1,1,1.0
which is indeed a,1,1,1.0
is indeed a complex,1,1,1.0
indeed a complex issue,1,1,1.0
a complex issue shows,1,1,1.0
complex issue shows much,1,1,1.0
issue shows much better,1,1,1.0
shows much better results,1,1,1.0
much better results when,1,1,1.0
better results when ploying,1,1,1.0
results when ploying a,1,1,1.0
when ploying a more,1,1,1.0
ploying a more ﬂexible,1,1,1.0
the one used therefore,1,1,1.0
one used therefore different,1,1,1.0
used therefore different kernel,1,1,1.0
therefore different kernel learning,1,1,1.0
different kernel learning techniques,1,1,1.0
learning techniques could be,1,1,1.0
techniques could be explored,1,1,1.0
could be explored in,1,1,1.0
be explored in the,2,1,2.0
explored in the future,2,1,2.0
in the future for,1,1,1.0
the future for the,1,1,1.0
future for the purpose,1,1,1.0
the purpose of in,1,1,1.0
purpose of in the,1,1,1.0
in the efs third,1,1,1.0
the efs third experiment,1,1,1.0
efs third experiment preferential,1,1,1.0
third experiment preferential this,1,1,1.0
experiment preferential this experimental,1,1,1.0
preferential this experimental subsection,1,1,1.0
this experimental subsection is,1,1,1.0
experimental subsection is intended,1,1,1.0
subsection is intended to,1,1,1.0
intended to test if,1,1,1.0
to test if there,1,1,1.0
test if there are,1,1,1.0
if there are patterns,1,1,1.0
there are patterns which,1,1,1.0
are patterns which are,1,1,1.0
patterns which are more,1,1,1.0
which are more suitable,1,1,1.0
more suitable for and,1,1,1.0
suitable for and if,1,1,1.0
for and if a,1,1,1.0
and if a general,1,1,1.0
if a general adaptive,1,1,1.0
a general adaptive approach,1,1,1.0
general adaptive approach yielding,1,1,1.0
adaptive approach yielding solutions,1,1,1.0
approach yielding solutions based,1,1,1.0
yielding solutions based on,1,1,1.0
solutions based on unweighted,1,1,1.0
based on unweighted borderline,1,1,1.0
on unweighted borderline weighted,1,1,1.0
unweighted borderline weighted or,1,1,1.0
borderline weighted or safe,1,1,1.0
weighted or safe level,1,1,1.0
or safe level weighted,1,1,1.0
safe level weighted could,1,1,1.0
level weighted could achieve,1,1,1.0
weighted could achieve better,1,1,1.0
could achieve better results,1,1,1.0
achieve better results than,1,1,1.0
better results than standard,1,1,1.0
results than standard unweighted,1,1,1.0
than standard unweighted t,1,1,1.0
standard unweighted t o,1,1,1.0
unweighted t o do,1,1,1.0
do so we compare,1,1,1.0
so we compare oef,1,1,1.0
we compare oef s,1,1,1.0
compare oef s to,1,1,1.0
oef s to two,1,1,1.0
s to two different,1,1,1.0
to two different approaches,1,1,1.0
two different approaches the,1,1,1.0
different approaches the ﬁrst,1,1,1.0
approaches the ﬁrst one,1,1,1.0
the ﬁrst one based,1,1,1.0
ﬁrst one based on,1,1,1.0
one based on a,1,1,1.0
based on a strategy,1,1,1.0
on a strategy ocp,1,1,1.0
a strategy ocp l,1,1,1.0
strategy ocp l and,1,1,1.0
ocp l and the,1,1,1.0
l and the second,1,1,1.0
the second one based,1,1,1.0
second one based on,1,1,1.0
one based on kernel,1,1,1.0
based on kernel learning,1,1,1.0
on kernel learning techniques,1,1,1.0
kernel learning techniques op,2,1,2.0
learning techniques op mkl,2,1,2.0
techniques op mkl as,1,1,1.0
op mkl as said,1,1,1.0
mkl as said before,1,1,1.0
as said before to,1,1,1.0
said before to test,1,1,1.0
before to test this,1,1,1.0
to test this idea,1,1,1.0
test this idea we,1,1,1.0
this idea we ﬁrst,1,1,1.0
idea we ﬁrst obtain,1,1,1.0
we ﬁrst obtain the,1,1,1.0
ﬁrst obtain the spatial,1,1,1.0
of the patterns based,1,1,1.0
the patterns based on,1,1,1.0
patterns based on a,1,1,1.0
based on a svm,1,1,1.0
on a svm hyperplane,1,1,1.0
a svm hyperplane and,1,1,1.0
svm hyperplane and we,1,1,1.0
hyperplane and we use,1,1,1.0
and we use a,1,1,1.0
we use a parametrised,1,1,1.0
use a parametrised link,1,1,1.0
a parametrised link function,1,1,1.0
parametrised link function eq,1,1,1.0
link function eq to,1,1,1.0
function eq to assign,1,1,1.0
eq to assign different,1,1,1.0
to assign different probabilities,1,1,1.0
assign different probabilities of,1,1,1.0
different probabilities of being,1,1,1.0
probabilities of being to,1,1,1.0
of being to the,1,1,1.0
being to the patterns,1,1,1.0
to the patterns according,1,1,1.0
patterns according to this,1,1,1.0
according to this spatial,1,1,1.0
to this spatial distribution,1,1,1.0
this spatial distribution this,1,1,1.0
spatial distribution this parametrisation,1,1,1.0
distribution this parametrisation is,1,1,1.0
this parametrisation is made,1,1,1.0
parametrisation is made using,1,1,1.0
is made using a,1,1,1.0
made using a β,1,1,1.0
using a β scale,1,1,1.0
a β scale parameter,1,1,1.0
β scale parameter which,1,1,1.0
scale parameter which will,1,1,1.0
parameter which will be,1,1,1.0
which will be optimised,1,1,1.0
will be optimised through,1,1,1.0
be optimised through ocp,1,1,1.0
optimised through ocp l,1,1,1.0
through ocp l within,1,1,1.0
ocp l within a,1,1,1.0
l within a set,1,1,1.0
within a set of,1,1,1.0
a set of values,1,1,1.0
set of values and,1,1,1.0
of values and through,1,1,1.0
values and through kernel,1,1,1.0
and through kernel learning,1,1,1.0
through kernel learning techniques,1,1,1.0
techniques op mkl for,1,1,1.0
op mkl for the,1,1,1.0
mkl for the experiments,1,1,1.0
for the experiments we,1,1,1.0
the experiments we select,1,1,1.0
experiments we select the,1,1,1.0
we select the set,1,1,1.0
select the set β,1,1,1.0
the set β analysing,1,1,1.0
set β analysing the,1,1,1.0
β analysing the results,1,1,1.0
analysing the results obtained,1,1,1.0
be seen that both,2,1,2.0
seen that both ocp,1,1,1.0
that both ocp l,1,1,1.0
both ocp l and,2,1,2.0
ocp l and op,4,1,4.0
l and op mkl,4,1,4.0
and op mkl obtain,1,1,1.0
op mkl obtain very,1,1,1.0
mkl obtain very competitive,1,1,1.0
obtain very competitive results,1,1,1.0
very competitive results both,1,1,1.0
competitive results both for,1,1,1.0
results both for acc,1,1,1.0
both for acc and,1,1,1.0
for acc and gm,1,1,1.0
acc and gm for,1,1,1.0
and gm for some,1,1,1.0
gm for some cases,1,1,1.0
for some cases the,1,1,1.0
cases the results obtained,1,1,1.0
the results obtained are,1,1,1.0
results obtained are equal,1,1,1.0
obtained are equal since,1,1,1.0
are equal since op,1,1,1.0
equal since op mkl,1,1,1.0
since op mkl also,1,1,1.0
op mkl also includes,1,1,1.0
mkl also includes the,1,1,1.0
also includes the solutions,1,1,1.0
includes the solutions of,1,1,1.0
the solutions of ocp,1,1,1.0
solutions of ocp once,1,1,1.0
of ocp once again,1,1,1.0
ocp once again t,1,1,1.0
once again t able,1,1,1.0
again t able vii,1,1,1.0
t able vii shows,1,1,1.0
able vii shows the,1,1,1.0
vii shows the mean,1,1,1.0
mean ranking results when,1,1,1.0
ranking results when comparing,1,1,1.0
results when comparing these,1,1,1.0
when comparing these two,1,1,1.0
comparing these two approaches,1,1,1.0
these two approaches to,1,1,1.0
two approaches to the,1,1,1.0
approaches to the standard,1,1,1.0
to the standard posed,1,1,1.0
the standard posed technique,1,1,1.0
standard posed technique oef,1,1,1.0
posed technique oef s,1,1,1.0
technique oef s in,1,1,1.0
oef s in this,1,1,1.0
s in this case,1,1,1.0
this case the friedman,1,1,1.0
case the friedman s,1,1,1.0
friedman s test accepted,1,1,1.0
s test accepted the,1,1,1.0
accepted the that the,1,1,1.0
the that the algorithms,1,1,1.0
that the algorithms perform,1,1,1.0
the algorithms perform larly,1,1,1.0
algorithms perform larly for,1,1,1.0
perform larly for acc,1,1,1.0
larly for acc and,1,1,1.0
for gm from these,1,1,1.0
gm from these results,1,1,1.0
from these results it,2,1,2.0
these results it can,2,1,2.0
results it can be,2,1,2.0
seen that both methods,1,1,1.0
that both methods outperform,1,1,1.0
both methods outperform the,1,1,1.0
methods outperform the standard,1,1,1.0
outperform the standard proposal,1,1,1.0
the standard proposal or,1,1,1.0
standard proposal or at,1,1,1.0
proposal or at least,1,1,1.0
or at least yield,1,1,1.0
at least yield similar,1,1,1.0
least yield similar performance,1,1,1.0
yield similar performance when,1,1,1.0
similar performance when considering,1,1,1.0
performance when considering acc,1,1,1.0
when considering acc t,1,1,1.0
considering acc t able,1,1,1.0
acc t able vii,1,1,1.0
t able vii mean,1,1,1.0
able vii mean ranking,1,1,1.0
vii mean ranking results,1,1,1.0
mean ranking results obtained,1,1,1.0
ranking results obtained by,1,1,1.0
results obtained by oef,1,1,1.0
obtained by oef s,1,1,1.0
by oef s ocp,1,1,1.0
oef s ocp l,3,1,3.0
s ocp l and,1,1,1.0
and op mkl ranking,1,1,1.0
op mkl ranking oef,1,1,1.0
mkl ranking oef s,1,1,1.0
ranking oef s ocp,1,1,1.0
s ocp l op,1,1,1.0
ocp l op mkl,1,1,1.0
l op mkl acc,1,1,1.0
op mkl acc gm,1,1,1.0
mkl acc gm friedman,1,1,1.0
f α gm the,1,1,1.0
α gm the holm,1,1,1.0
gm the holm s,1,1,1.0
holm s test for,1,1,1.0
s test for multiple,1,1,1.0
test for multiple comparisons,1,1,1.0
comparisons has been also,1,1,1.0
has been also plied,1,1,1.0
been also plied see,1,1,1.0
also plied see t,1,1,1.0
plied see t able,1,1,1.0
see t able viii,1,1,1.0
t able viii for,1,1,1.0
able viii for both,1,1,1.0
viii for both ocp,1,1,1.0
for both ocp l,1,1,1.0
and op mkl the,1,1,1.0
op mkl the test,1,1,1.0
mkl the test concluded,1,1,1.0
concluded that there are,1,1,1.0
that there are statistically,1,1,1.0
there are statistically signiﬁcant,2,1,2.0
are statistically signiﬁcant differences,2,1,2.0
for gm when compared,1,1,1.0
gm when compared to,1,1,1.0
when compared to oef,1,1,1.0
compared to oef s,1,1,1.0
to oef s indicating,1,1,1.0
oef s indicating that,1,1,1.0
s indicating that preferential,1,1,1.0
indicating that preferential is,1,1,1.0
that preferential is preferable,1,1,1.0
preferential is preferable over,1,1,1.0
is preferable over the,1,1,1.0
preferable over the uniform,1,1,1.0
over the uniform one,1,1,1.0
the uniform one although,1,1,1.0
uniform one although the,1,1,1.0
one although the strategy,1,1,1.0
although the strategy obtains,1,1,1.0
the strategy obtains very,1,1,1.0
strategy obtains very good,1,1,1.0
obtains very good results,1,1,1.0
very good results the,1,1,1.0
good results the multiple,1,1,1.0
results the multiple kernel,1,1,1.0
the multiple kernel strategy,1,1,1.0
multiple kernel strategy yields,1,1,1.0
kernel strategy yields slightly,1,1,1.0
strategy yields slightly better,1,1,1.0
yields slightly better performance,1,1,1.0
slightly better performance there,1,1,1.0
better performance there are,1,1,1.0
performance there are statistically,1,1,1.0
differences for α t,1,1,1.0
for α t able,1,1,1.0
α t able viii,1,1,1.0
t able viii results,1,1,1.0
able viii results of,1,1,1.0
viii results of the,1,1,1.0
holm procedure using ocp,1,1,1.0
procedure using ocp l,1,1,1.0
using ocp l and,1,1,1.0
and op mkl as,1,1,1.0
op mkl as control,1,1,1.0
mkl as control methods,1,1,1.0
as control methods when,1,1,1.0
control methods when compared,1,1,1.0
methods when compared to,1,1,1.0
when compared to other,3,2,1.5
compared to other methods,2,2,1.0
to other methods corrected,1,1,1.0
other methods corrected α,1,1,1.0
methods corrected α values,1,1,1.0
compared method and ordered,1,1,1.0
method and ordered by,1,1,1.0
and ordered by the,1,1,1.0
comparison i cm op,1,1,1.0
i cm op mkl,1,1,1.0
cm op mkl gm,1,1,1.0
op mkl gm i,1,1,1.0
mkl gm i α,1,1,1.0
α method pi oef,2,1,2.0
method pi oef s,2,1,2.0
pi oef s ocp,1,1,1.0
s ocp l cm,1,1,1.0
ocp l cm ocp,1,1,1.0
l cm ocp l,1,1,1.0
cm ocp l gm,1,1,1.0
ocp l gm i,1,1,1.0
l gm i α,1,1,1.0
pi oef s op,1,1,1.0
oef s op mkl,1,1,1.0
s op mkl win,1,1,1.0
op mkl win or,1,1,1.0
mkl win or lose,1,1,1.0
lose with statistical cant,1,1,1.0
with statistical cant difference,1,1,1.0
statistical cant difference for,1,1,1.0
cant difference for α,1,1,1.0
difference for α win,1,1,1.0
for α win or,1,1,1.0
α win or lose,1,1,1.0
difference for α t,1,1,1.0
for α t o,1,1,1.0
α t o analyse,1,1,1.0
o analyse the most,1,1,1.0
analyse the most appropriate,1,1,1.0
the most appropriate region,1,1,1.0
most appropriate region for,1,1,1.0
appropriate region for we,1,1,1.0
region for we analyse,1,1,1.0
for we analyse the,1,1,1.0
we analyse the optimal,1,1,1.0
analyse the optimal β,1,1,1.0
the optimal β values,1,1,1.0
optimal β values obtained,1,1,1.0
β values obtained from,1,1,1.0
values obtained from see,1,1,1.0
obtained from see fig,1,1,1.0
from see fig for,1,1,1.0
see fig for the,1,1,1.0
fig for the histogram,1,1,1.0
for the histogram recall,1,1,1.0
the histogram recall that,1,1,1.0
histogram recall that when,1,1,1.0
recall that when β,1,1,1.0
when β points within,1,1,1.0
β points within the,1,1,1.0
points within the minority,1,1,1.0
when β points on,1,1,1.0
β points on the,1,1,1.0
points on the class,1,1,1.0
class boundary or even,1,1,1.0
boundary or even on,1,1,1.0
or even on the,1,1,1.0
even on the other,1,1,1.0
on the other side,1,1,1.0
the other side of,1,1,1.0
other side of the,1,1,1.0
of the hyperplane are,1,1,1.0
the hyperplane are preferred,1,1,1.0
hyperplane are preferred and,1,1,1.0
to be chosen it,1,1,1.0
be chosen it can,1,1,1.0
chosen it can be,1,1,1.0
be seen that for,1,1,1.0
seen that for most,1,1,1.0
that for most datasets,1,1,1.0
for most datasets within,1,1,1.0
most datasets within interior,1,1,1.0
datasets within interior of,1,1,1.0
within interior of the,1,1,1.0
interior of the minority,1,1,1.0
minority class is preferable,1,1,1.0
class is preferable moreover,1,1,1.0
is preferable moreover note,1,1,1.0
preferable moreover note that,1,1,1.0
moreover note that for,1,1,1.0
note that for a,1,1,1.0
that for a relatively,1,1,1.0
for a relatively large,1,1,1.0
a relatively large number,1,1,1.0
relatively large number of,1,1,1.0
large number of datasets,1,1,1.0
number of datasets the,1,1,1.0
of datasets the choice,1,1,1.0
datasets the choice is,1,1,1.0
the choice is uniform,1,1,1.0
choice is uniform however,1,1,1.0
is uniform however this,1,1,1.0
uniform however this could,1,1,1.0
however this could mean,1,1,1.0
this could mean that,1,1,1.0
could mean that uniform,1,1,1.0
mean that uniform sampling,1,1,1.0
that uniform sampling could,1,1,1.0
uniform sampling could be,1,1,1.0
sampling could be feasible,1,1,1.0
could be feasible in,1,1,1.0
be feasible in some,1,1,1.0
feasible in some cases,1,1,1.0
in some cases for,1,1,1.0
some cases for in,1,1,1.0
cases for in the,1,1,1.0
for in the feature,1,1,1.0
the feature space because,1,1,1.0
feature space because of,1,1,1.0
space because of the,1,1,1.0
because of the improved,1,1,1.0
of the improved data,1,1,1.0
the improved data separation,1,1,1.0
improved data separation finally,1,1,1.0
data separation finally to,1,1,1.0
separation finally to study,1,1,1.0
finally to study the,1,1,1.0
to study the computational,1,1,1.0
study the computational cost,1,1,1.0
computational cost of preferential,1,1,1.0
cost of preferential we,1,1,1.0
of preferential we included,1,1,1.0
preferential we included a,1,1,1.0
we included a small,1,1,1.0
included a small comparison,1,1,1.0
a small comparison of,1,1,1.0
small comparison of oefs,1,1,1.0
comparison of oefs and,1,1,1.0
of oefs and opmkl,1,1,1.0
oefs and opmkl using,1,1,1.0
and opmkl using only,1,1,1.0
opmkl using only a,1,1,1.0
using only a data,1,1,1.0
only a data partition,1,1,1.0
a data partition the,1,1,1.0
data partition the dataset,1,1,1.0
partition the dataset chosen,1,1,1.0
the dataset chosen is,1,1,1.0
dataset chosen is haberman,1,1,1.0
chosen is haberman and,1,1,1.0
is haberman and the,1,1,1.0
haberman and the time,1,1,1.0
and the time is,1,1,1.0
the time is reported,1,1,1.0
time is reported in,1,1,1.0
is reported in terms,1,1,1.0
in terms of seconds,1,1,1.0
terms of seconds fig,1,1,1.0
of seconds fig histogram,1,1,1.0
seconds fig histogram of,1,1,1.0
of the mean values,1,1,1.0
the mean values for,1,1,1.0
mean values for the,1,1,1.0
values for the beta,1,1,1.0
for the beta parameter,1,1,1.0
the beta parameter used,1,1,1.0
beta parameter used in,1,1,1.0
parameter used in the,1,1,1.0
used in the process,1,1,1.0
in the process the,1,1,1.0
the process the x,1,1,1.0
process the x coordinate,1,1,1.0
the x coordinate represents,1,1,1.0
x coordinate represents the,1,1,1.0
coordinate represents the different,1,1,1.0
represents the different mean,1,1,1.0
the different mean β,1,1,1.0
different mean β values,1,1,1.0
mean β values chosen,1,1,1.0
β values chosen for,1,1,1.0
values chosen for each,1,1,1.0
chosen for each dataset,1,1,1.0
for each dataset related,1,1,1.0
each dataset related to,1,1,1.0
dataset related to preferential,1,1,1.0
related to preferential and,1,1,1.0
to preferential and the,1,1,1.0
preferential and the y,1,1,1.0
and the y the,1,1,1.0
the y the number,1,1,1.0
y the number of,1,1,1.0
of datasets where the,1,1,1.0
datasets where the value,1,1,1.0
where the value was,1,1,1.0
the value was selected,1,1,1.0
selected from the process,1,1,1.0
from the process needed,1,1,1.0
the process needed to,1,1,1.0
process needed to the,1,1,1.0
needed to the data,1,1,1.0
to the data of,1,1,1.0
the data of parameters,1,1,1.0
data of parameters is,1,1,1.0
of parameters is not,1,1,1.0
parameters is not considered,1,1,1.0
is not considered according,1,1,1.0
not considered according to,1,1,1.0
considered according to this,1,1,1.0
according to this the,1,1,1.0
to this the results,1,1,1.0
this the results are,1,1,1.0
the results are the,1,1,1.0
results are the following,1,1,1.0
are the following for,1,1,1.0
the following for oefs,1,1,1.0
following for oefs and,1,1,1.0
for oefs and for,1,1,1.0
oefs and for opmkl,1,1,1.0
and for opmkl from,1,1,1.0
for opmkl from these,1,1,1.0
opmkl from these results,1,1,1.0
seen that the computational,1,1,1.0
that the computational time,2,2,1.0
the computational time is,1,1,1.0
computational time is affordable,1,1,1.0
time is affordable vii,1,1,1.0
is affordable vii c,1,1,1.0
affordable vii c o,1,1,1.0
vii c o n,1,1,1.0
c o n c,1,1,1.0
o n c l,1,1,1.0
n c l u,1,1,1.0
c l u s,1,1,1.0
l u s i,1,1,1.0
u s i o,1,1,1.0
s i o n,1,1,1.0
i o n s,1,1,1.0
o n s this,1,1,1.0
n s this paper,1,1,1.0
s this paper explores,1,1,1.0
this paper explores the,1,1,1.0
paper explores the notion,1,1,1.0
explores the notion of,1,1,1.0
the notion of in,1,1,1.0
notion of in the,1,1,1.0
of in the feature,1,1,1.0
a kernel function to,1,1,1.0
kernel function to deal,1,1,1.0
function to deal with,1,1,1.0
deal with imbalanced classiﬁcation,1,1,1.0
with imbalanced classiﬁcation problems,1,1,1.0
imbalanced classiﬁcation problems since,1,1,1.0
classiﬁcation problems since the,1,1,1.0
problems since the feature,1,1,1.0
not directly accessible the,1,1,1.0
directly accessible the empirical,1,1,1.0
accessible the empirical feature,1,1,1.0
empirical feature space is,2,1,2.0
feature space is used,1,1,1.0
space is used a,1,1,1.0
is used a euclidean,1,1,1.0
used a euclidean space,1,1,1.0
a euclidean space that,1,1,1.0
euclidean space that preserves,1,1,1.0
space that preserves the,1,1,1.0
that preserves the structure,1,1,1.0
preserves the structure of,1,1,1.0
the structure of the,1,1,1.0
original feature space is,1,1,1.0
feature space is tackled,1,1,1.0
space is tackled by,1,1,1.0
is tackled by convex,1,1,1.0
tackled by convex tion,1,1,1.0
by convex tion of,1,1,1.0
convex tion of patterns,1,1,1.0
tion of patterns as,1,1,1.0
of patterns as usually,1,1,1.0
patterns as usually done,1,1,1.0
as usually done in,1,1,1.0
usually done in the,1,1,1.0
done in the and,1,1,1.0
in the and we,1,1,1.0
the and we focus,1,1,1.0
and we focus on,1,1,1.0
we focus on the,1,1,1.0
focus on the paradigm,1,1,1.0
on the paradigm of,1,1,1.0
the paradigm of kernel,1,1,1.0
paradigm of kernel methods,1,1,1.0
of kernel methods w,1,1,1.0
kernel methods w e,1,1,1.0
methods w e explore,1,1,1.0
e explore the ideas,1,1,1.0
explore the ideas of,1,1,1.0
the ideas of in,1,1,1.0
ideas of in the,1,1,1.0
of in the full,1,1,1.0
empirical feature space the,1,1,1.0
feature space the optimisation,1,1,1.0
space the optimisation of,1,1,1.0
optimisation of the feature,2,1,2.0
the feature space by,1,1,1.0
feature space by kernel,1,1,1.0
space by kernel learning,1,1,1.0
by kernel learning and,1,1,1.0
kernel learning and the,1,1,1.0
learning and the notion,1,1,1.0
and the notion of,1,1,1.0
the notion of preferential,1,1,1.0
notion of preferential which,1,1,1.0
of preferential which analyses,1,1,1.0
preferential which analyses which,1,1,1.0
which analyses which patterns,1,1,1.0
analyses which patterns should,1,1,1.0
prone to be from,1,1,1.0
to be from the,1,1,1.0
be from the results,1,1,1.0
the results of a,1,1,1.0
results of a thorough,1,1,1.0
of a thorough set,1,1,1.0
set of experiments over,1,1,1.0
of experiments over imbalanced,1,1,1.0
over imbalanced datasets several,1,1,1.0
imbalanced datasets several conclusions,1,1,1.0
datasets several conclusions can,1,1,1.0
can be drawn ﬁrstly,1,1,1.0
be drawn ﬁrstly in,1,1,1.0
drawn ﬁrstly in the,1,1,1.0
ﬁrstly in the empirical,1,1,1.0
feature space is seen,1,1,1.0
space is seen to,1,1,1.0
is seen to yield,1,1,1.0
seen to yield better,1,1,1.0
to yield better performance,1,1,1.0
yield better performance than,1,1,1.0
better performance than in,1,1,1.0
performance than in the,1,1,1.0
than in the input,1,1,1.0
the input space secondly,1,1,1.0
input space secondly the,1,1,1.0
space secondly the control,1,1,1.0
secondly the control of,1,1,1.0
the control of the,1,1,1.0
control of the dimensionality,1,1,1.0
dimensionality of the empirical,1,1,1.0
empirical feature space could,2,1,2.0
feature space could lead,1,1,1.0
space could lead to,1,1,1.0
could lead to better,1,1,1.0
lead to better results,1,1,1.0
to better results thirdly,1,1,1.0
better results thirdly the,1,1,1.0
results thirdly the kernel,1,1,1.0
thirdly the kernel used,1,1,1.0
the kernel used inﬂuence,1,1,1.0
kernel used inﬂuence the,1,1,1.0
used inﬂuence the solution,1,1,1.0
inﬂuence the solution to,1,1,1.0
the solution to a,1,1,1.0
solution to a great,1,1,1.0
to a great extent,1,1,1.0
a great extent making,1,1,1.0
great extent making advisable,1,1,1.0
extent making advisable the,1,1,1.0
making advisable the optimisation,1,1,1.0
advisable the optimisation of,1,1,1.0
feature space structure although,1,1,1.0
space structure although the,1,1,1.0
structure although the spherical,1,1,1.0
although the spherical gaussian,1,1,1.0
kernel has been shown,1,1,1.0
been shown to perform,1,1,1.0
shown to perform well,1,1,1.0
to perform well for,1,1,1.0
perform well for several,1,1,1.0
well for several cases,1,1,1.0
for several cases and,1,1,1.0
several cases and ﬁnally,1,1,1.0
cases and ﬁnally that,1,1,1.0
and ﬁnally that there,1,1,1.0
ﬁnally that there exist,1,1,1.0
that there exist some,1,1,1.0
there exist some regions,1,1,1.0
exist some regions of,1,1,1.0
regions of the dataset,1,1,1.0
of the dataset which,1,1,1.0
the dataset which should,1,1,1.0
dataset which should be,1,1,1.0
which should be preferred,1,1,1.0
should be preferred for,1,1,1.0
be preferred for and,1,1,1.0
preferred for and that,1,1,1.0
for and that multiple,1,1,1.0
and that multiple kernel,1,1,1.0
that multiple kernel learning,1,1,1.0
kernel learning techniques should,1,1,1.0
learning techniques should be,1,1,1.0
techniques should be explored,1,1,1.0
should be explored in,1,1,1.0
in the future with,1,1,1.0
the future with the,1,1,1.0
future with the purpose,1,1,1.0
with the purpose of,2,1,2.0
the purpose of the,1,1,1.0
purpose of the authors,1,1,1.0
of the authors would,1,1,1.0
the authors would also,1,1,1.0
authors would also like,1,1,1.0
would also like to,2,2,1.0
also like to stress,1,1,1.0
like to stress several,1,1,1.0
to stress several lines,1,1,1.0
stress several lines of,1,1,1.0
several lines of future,1,1,1.0
lines of future work,1,1,1.0
of future work firstly,1,1,1.0
future work firstly an,1,1,1.0
work firstly an analytical,1,1,1.0
firstly an analytical methodology,1,1,1.0
an analytical methodology for,1,1,1.0
analytical methodology for optimising,1,1,1.0
methodology for optimising the,1,1,1.0
for optimising the number,1,1,1.0
optimising the number of,1,1,1.0
of dominant dimensions of,1,1,1.0
dimensions of the empirical,1,1,1.0
feature space could be,1,1,1.0
space could be developed,1,1,1.0
could be developed with,1,1,1.0
be developed with the,1,1,1.0
developed with the purpose,1,1,1.0
the purpose of secondly,1,1,1.0
purpose of secondly considering,1,1,1.0
of secondly considering a,1,1,1.0
secondly considering a unique,1,1,1.0
considering a unique methodology,1,1,1.0
a unique methodology combining,1,1,1.0
unique methodology combining the,1,1,1.0
methodology combining the techniques,1,1,1.0
combining the techniques proposed,1,1,1.0
the techniques proposed in,1,1,1.0
techniques proposed in this,1,1,1.0
proposed in this paper,1,1,1.0
in this paper could,1,1,1.0
this paper could be,1,1,1.0
paper could be accomplished,1,1,1.0
could be accomplished to,1,1,1.0
be accomplished to analyse,1,1,1.0
accomplished to analyse how,1,1,1.0
to analyse how these,1,1,1.0
analyse how these methods,1,1,1.0
how these methods could,1,1,1.0
these methods could complement,1,1,1.0
methods could complement each,1,1,1.0
could complement each other,1,1,1.0
complement each other furthermore,1,1,1.0
each other furthermore in,1,1,1.0
other furthermore in the,1,1,1.0
furthermore in the context,1,1,1.0
the context of kernel,1,1,1.0
context of kernel learning,1,1,1.0
of kernel learning the,1,1,1.0
kernel learning the sampling,1,1,1.0
learning the sampling process,1,1,1.0
process could be incorporated,1,1,1.0
could be incorporated in,1,1,1.0
be incorporated in the,1,1,1.0
incorporated in the kernel,1,1,1.0
in the kernel learning,1,1,1.0
the kernel learning stage,1,1,1.0
kernel learning stage to,1,1,1.0
learning stage to search,1,1,1.0
stage to search the,1,1,1.0
to search the more,1,1,1.0
search the more suitable,1,1,1.0
the more suitable representation,1,1,1.0
more suitable representation for,1,1,1.0
suitable representation for performing,1,1,1.0
representation for performing the,1,1,1.0
for performing the not,1,1,1.0
performing the not only,1,1,1.0
the not only the,1,1,1.0
not only the better,1,1,1.0
only the better class,1,1,1.0
the better class separation,1,1,1.0
better class separation finally,1,1,1.0
class separation finally other,1,1,1.0
separation finally other intelligent,1,1,1.0
finally other intelligent optimisation,1,1,1.0
other intelligent optimisation techniques,1,1,1.0
intelligent optimisation techniques could,1,1,1.0
optimisation techniques could be,1,1,1.0
techniques could be developed,1,1,1.0
could be developed for,1,1,1.0
be developed for the,1,1,1.0
developed for the generation,1,1,1.0
the generation of the,1,1,1.0
generation of the synthetic,1,1,1.0
of the synthetic patterns,1,1,1.0
the synthetic patterns re,1,1,1.0
synthetic patterns re f,1,1,1.0
patterns re f e,1,1,1.0
re f e r,1,1,1.0
r e n c,1,1,1.0
e n c e,1,1,1.0
n c e s,1,1,1.0
c e s he,1,1,1.0
e s he and,1,1,1.0
s he and garcia,1,1,1.0
he and garcia learning,2,2,1.0
and garcia learning from,2,2,1.0
and data engineering vol,3,1,3.0
data engineering vol pp,3,1,3.0
engineering vol pp japkowicz,1,1,1.0
vol pp japkowicz and,1,1,1.0
pp japkowicz and stephen,1,1,1.0
japkowicz and stephen the,1,1,1.0
and stephen the class,1,1,1.0
stephen the class imbalance,1,1,1.0
class imbalance problem a,1,1,1.0
imbalance problem a tematic,1,1,1.0
problem a tematic study,1,1,1.0
a tematic study intelligent,1,1,1.0
tematic study intelligent data,1,1,1.0
study intelligent data analysis,1,1,1.0
intelligent data analysis vol,1,1,1.0
data analysis vol no,1,1,1.0
analysis vol no pp,1,1,1.0
vol no pp v,2,2,1.0
no pp v chawla,2,2,1.0
pp v chawla w,1,1,1.0
v chawla w bowyer,1,1,1.0
chawla w bowyer hall,1,1,1.0
w bowyer hall and,1,1,1.0
bowyer hall and w,1,1,1.0
hall and w p,1,1,1.0
and w p kegelmeyer,1,1,1.0
w p kegelmeyer smote,1,1,1.0
p kegelmeyer smote synthetic,1,1,1.0
technique journal of artiﬁcial,1,1,1.0
journal of artiﬁcial intelligence,1,1,1.0
of artiﬁcial intelligence research,1,1,1.0
artiﬁcial intelligence research vol,1,1,1.0
intelligence research vol no,1,1,1.0
research vol no pp,3,1,3.0
vol no pp y,2,2,1.0
no pp y t,1,1,1.0
pp y t ang,1,1,1.0
y t ang y,1,1,1.0
t ang y zhang,1,1,1.0
ang y zhang v,1,1,1.0
y zhang v chawla,1,1,1.0
zhang v chawla and,1,1,1.0
v chawla and krasser,1,1,1.0
chawla and krasser svms,1,1,1.0
and krasser svms modeling,1,1,1.0
krasser svms modeling for,1,1,1.0
svms modeling for highly,1,1,1.0
modeling for highly imbalanced,1,1,1.0
for highly imbalanced classiﬁcation,1,1,1.0
highly imbalanced classiﬁcation ieee,1,1,1.0
imbalanced classiﬁcation ieee transactions,1,1,1.0
classiﬁcation ieee transactions on,2,1,2.0
man and cybernetics p,2,1,2.0
and cybernetics p art,2,1,2.0
cybernetics p art b,1,1,1.0
p art b cybernetics,1,1,1.0
art b cybernetics vol,1,1,1.0
b cybernetics vol pp,1,1,1.0
cybernetics vol pp nguyen,1,1,1.0
vol pp nguyen w,1,1,1.0
pp nguyen w cooper,1,1,1.0
nguyen w cooper and,1,1,1.0
w cooper and kamei,1,1,1.0
cooper and kamei borderline,1,1,1.0
and kamei borderline for,1,1,1.0
kamei borderline for imbalanced,1,1,1.0
borderline for imbalanced data,1,1,1.0
for imbalanced data classiﬁcation,1,1,1.0
imbalanced data classiﬁcation international,1,1,1.0
data classiﬁcation international journal,1,1,1.0
classiﬁcation international journal of,1,1,1.0
and soft data p,1,1,1.0
soft data p aradigms,1,1,1.0
data p aradigms vol,1,1,1.0
p aradigms vol no,1,1,1.0
aradigms vol no pp,1,1,1.0
vol no pp bunkhumpornpat,1,1,1.0
no pp bunkhumpornpat sinapiromsaran,1,1,1.0
pp bunkhumpornpat sinapiromsaran and,1,1,1.0
bunkhumpornpat sinapiromsaran and lursinsap,1,1,1.0
sinapiromsaran and lursinsap minority,1,1,1.0
and lursinsap minority technique,1,1,1.0
class imbalanced problem in,1,1,1.0
imbalanced problem in proceedings,1,1,1.0
proceedings of the p,2,1,2.0
of the p conference,1,1,1.0
the p conference on,1,1,1.0
p conference on advances,1,1,1.0
and data mining bangkok,1,1,1.0
data mining bangkok thailand,1,1,1.0
mining bangkok thailand april,1,1,1.0
bangkok thailand april pp,1,1,1.0
thailand april pp he,1,1,1.0
april pp he y,1,1,1.0
pp he y bai,2,2,1.0
he y bai garcia,2,2,1.0
y bai garcia and,2,2,1.0
bai garcia and li,2,2,1.0
garcia and li adasyn,2,2,1.0
and li adasyn adaptive,2,2,1.0
imbalanced learning in proceedings,1,1,1.0
on neural networks hong,1,1,1.0
neural networks hong kong,1,1,1.0
networks hong kong china,1,1,1.0
hong kong china pp,2,1,2.0
kong china pp barua,1,1,1.0
china pp barua islam,1,1,1.0
pp barua islam and,1,1,1.0
barua islam and murase,1,1,1.0
islam and murase a,1,1,1.0
and murase a novel,1,1,1.0
murase a novel synthetic,1,1,1.0
a novel synthetic minority,1,1,1.0
novel synthetic minority oversampling,1,1,1.0
minority oversampling technique for,2,1,2.0
oversampling technique for imbalanced,2,1,2.0
technique for imbalanced data,2,1,2.0
for imbalanced data set,2,1,2.0
imbalanced data set learning,2,1,2.0
data set learning in,1,1,1.0
set learning in ings,1,1,1.0
learning in ings of,1,1,1.0
ings of the international,2,2,1.0
international conference on neural,5,1,5.0
conference on neural information,5,1,5.0
on neural information processing,5,1,5.0
neural information processing shanghai,1,1,1.0
information processing shanghai china,1,1,1.0
processing shanghai china nov,1,1,1.0
shanghai china nov pp,1,1,1.0
china nov pp barua,1,1,1.0
nov pp barua islam,1,1,1.0
pp barua islam y,1,1,1.0
barua islam y ao,1,1,1.0
islam y ao and,1,1,1.0
y ao and murase,1,1,1.0
ao and murase mwmote,1,1,1.0
and murase mwmote majority,1,1,1.0
murase mwmote majority weighted,1,1,1.0
mwmote majority weighted minority,1,1,1.0
majority weighted minority oversampling,1,1,1.0
weighted minority oversampling technique,1,1,1.0
data set learning ieee,1,1,1.0
set learning ieee transactions,1,1,1.0
learning ieee transactions on,1,1,1.0
engineering vol pp galar,1,1,1.0
vol pp galar fern,1,1,1.0
pp galar fern barrenechea,2,1,2.0
galar fern barrenechea bustince,1,1,1.0
fern barrenechea bustince and,1,1,1.0
barrenechea bustince and f,1,1,1.0
bustince and f herrera,1,1,1.0
and f herrera a,1,1,1.0
f herrera a review,1,1,1.0
cybernetics p art c,1,1,1.0
p art c vol,1,1,1.0
art c vol pp,1,1,1.0
c vol pp jul,1,1,1.0
vol pp jul gantner,1,1,1.0
pp jul gantner and,1,1,1.0
jul gantner and learning,1,1,1.0
gantner and learning methods,1,1,1.0
and learning methods for,1,1,1.0
learning methods for imbalanced,1,1,1.0
methods for imbalanced data,1,1,1.0
for imbalanced data in,1,1,1.0
imbalanced data in proceedings,1,1,1.0
proceedings of the national,1,1,1.0
of the national joint,1,1,1.0
the national joint conference,1,1,1.0
national joint conference on,2,2,1.0
on neural networks barcelona,1,1,1.0
neural networks barcelona spain,1,1,1.0
networks barcelona spain july,1,1,1.0
barcelona spain july pp,1,1,1.0
spain july pp boser,1,1,1.0
july pp boser guyon,1,1,1.0
pp boser guyon and,1,1,1.0
boser guyon and v,1,1,1.0
guyon and v v,1,1,1.0
and v v apnik,2,1,2.0
v v apnik a,1,1,1.0
v apnik a training,1,1,1.0
apnik a training algorithm,1,1,1.0
a training algorithm for,1,1,1.0
training algorithm for optimal,1,1,1.0
algorithm for optimal margin,1,1,1.0
for optimal margin classiﬁers,1,1,1.0
optimal margin classiﬁers in,1,1,1.0
margin classiﬁers in proceedings,1,1,1.0
classiﬁers in proceedings of,1,1,1.0
of the fifth annual,1,1,1.0
the fifth annual acm,1,1,1.0
fifth annual acm w,1,1,1.0
annual acm w orkshop,1,1,1.0
acm w orkshop on,1,1,1.0
w orkshop on computational,1,1,1.0
orkshop on computational learning,1,1,1.0
on computational learning theory,1,1,1.0
computational learning theory pittsburgh,1,1,1.0
learning theory pittsburgh p,1,1,1.0
theory pittsburgh p a,1,1,1.0
pittsburgh p a july,1,1,1.0
p a july pp,1,1,1.0
a july pp cortes,1,1,1.0
july pp cortes and,1,1,1.0
pp cortes and v,1,1,1.0
cortes and v v,1,1,1.0
v v apnik networks,1,1,1.0
v apnik networks machine,1,1,1.0
apnik networks machine learning,1,1,1.0
networks machine learning vol,1,1,1.0
machine learning vol no,1,1,1.0
learning vol no pp,1,1,1.0
vol no pp zeng,1,1,1.0
no pp zeng and,1,1,1.0
pp zeng and gao,1,1,1.0
zeng and gao improving,1,1,1.0
and gao improving svm,1,1,1.0
gao improving svm classiﬁcation,1,1,1.0
improving svm classiﬁcation with,1,1,1.0
svm classiﬁcation with imbalance,1,1,1.0
classiﬁcation with imbalance data,1,1,1.0
with imbalance data set,1,1,1.0
imbalance data set in,1,1,1.0
data set in proceedings,1,1,1.0
set in proceedings of,1,1,1.0
neural information processing bangkok,1,1,1.0
information processing bangkok thailand,1,1,1.0
processing bangkok thailand pp,1,1,1.0
bangkok thailand pp scholkopf,1,1,1.0
thailand pp scholkopf and,1,1,1.0
pp scholkopf and smola,1,1,1.0
scholkopf and smola learning,1,1,1.0
and smola learning with,1,1,1.0
smola learning with kernels,1,1,1.0
learning with kernels support,1,1,1.0
with kernels support v,1,1,1.0
kernels support v ector,1,1,1.0
v ector machines regularization,1,1,1.0
ector machines regularization optimization,1,1,1.0
machines regularization optimization and,1,1,1.0
regularization optimization and beyond,1,1,1.0
optimization and beyond cambridge,1,1,1.0
and beyond cambridge ma,1,1,1.0
beyond cambridge ma usa,1,1,1.0
cambridge ma usa mit,1,1,1.0
ma usa mit press,1,1,1.0
usa mit press sch,1,1,1.0
mit press sch mika,1,1,1.0
press sch mika burges,1,1,1.0
sch mika burges p,1,1,1.0
mika burges p knirsch,1,1,1.0
burges p knirsch m,1,1,1.0
p knirsch m r,1,1,1.0
knirsch m r and,1,1,1.0
m r and smola,1,1,1.0
r and smola input,1,1,1.0
and smola input space,1,1,1.0
smola input space versus,1,1,1.0
input space versus feature,1,1,1.0
space versus feature space,1,1,1.0
versus feature space in,1,1,1.0
feature space in based,1,1,1.0
space in based methods,1,1,1.0
in based methods ieee,1,1,1.0
based methods ieee transactions,1,1,1.0
methods ieee transactions on,2,1,2.0
on neural networks vol,14,2,7.0
neural networks vol pp,3,1,3.0
networks vol pp xiong,1,1,1.0
vol pp xiong swamy,1,1,1.0
pp xiong swamy and,1,1,1.0
xiong swamy and ahmad,1,1,1.0
swamy and ahmad optimizing,1,1,1.0
and ahmad optimizing the,1,1,1.0
ahmad optimizing the kernel,1,1,1.0
optimizing the kernel in,1,1,1.0
the kernel in the,1,1,1.0
empirical feature space ieee,1,1,1.0
feature space ieee transactions,1,1,1.0
space ieee transactions on,1,1,1.0
networks vol pp march,1,1,1.0
vol pp march cristianini,1,1,1.0
pp march cristianini kandola,1,1,1.0
march cristianini kandola elisseeff,1,1,1.0
cristianini kandola elisseeff and,1,1,1.0
kandola elisseeff and aylor,1,1,1.0
elisseeff and aylor on,1,1,1.0
and aylor on target,1,1,1.0
aylor on target alignment,1,1,1.0
on target alignment in,1,1,1.0
target alignment in proceedings,1,1,1.0
alignment in proceedings of,1,1,1.0
information processing systems v,2,1,2.0
processing systems v ancouver,2,1,2.0
systems v ancouver canada,2,1,2.0
v ancouver canada pp,1,1,1.0
ancouver canada pp cortes,1,1,1.0
canada pp cortes mohri,1,1,1.0
pp cortes mohri and,1,1,1.0
cortes mohri and rostamizadeh,1,1,1.0
mohri and rostamizadeh algorithms,1,1,1.0
and rostamizadeh algorithms for,1,1,1.0
rostamizadeh algorithms for learning,1,1,1.0
algorithms for learning kernels,1,1,1.0
for learning kernels based,1,1,1.0
learning kernels based on,1,1,1.0
kernels based on centered,1,1,1.0
based on centered alignment,1,1,1.0
on centered alignment journal,1,1,1.0
centered alignment journal of,1,1,1.0
alignment journal of machine,1,1,1.0
machine learning research vol,4,1,4.0
learning research vol no,2,1,2.0
vol no pp akbani,1,1,1.0
no pp akbani kwek,1,1,1.0
pp akbani kwek and,1,1,1.0
akbani kwek and japkowicz,1,1,1.0
kwek and japkowicz applying,1,1,1.0
and japkowicz applying support,1,1,1.0
japkowicz applying support vector,1,1,1.0
applying support vector machines,1,1,1.0
support vector machines to,1,1,1.0
vector machines to imbalanced,1,1,1.0
machines to imbalanced datasets,1,1,1.0
to imbalanced datasets in,1,1,1.0
imbalanced datasets in proceedings,1,1,1.0
datasets in proceedings of,1,1,1.0
proceedings of the european,1,1,1.0
of the european conference,1,1,1.0
the european conference on,1,1,1.0
on machine learning pisa,1,1,1.0
machine learning pisa italy,1,1,1.0
learning pisa italy pp,1,1,1.0
pisa italy pp y,1,1,1.0
italy pp y liu,1,1,1.0
pp y liu an,1,1,1.0
y liu an and,1,1,1.0
liu an and huang,1,1,1.0
an and huang boosting,1,1,1.0
and huang boosting prediction,1,1,1.0
huang boosting prediction accuracy,1,1,1.0
boosting prediction accuracy on,1,1,1.0
prediction accuracy on balanced,1,1,1.0
accuracy on balanced datasets,1,1,1.0
on balanced datasets with,1,1,1.0
balanced datasets with svm,1,1,1.0
datasets with svm ensembles,1,1,1.0
svm ensembles in proceedings,1,1,1.0
ensembles in proceedings of,1,1,1.0
of the p asia,1,1,1.0
the p asia conference,1,1,1.0
p asia conference on,1,1,1.0
asia conference on advances,1,1,1.0
and data mining singapore,1,1,1.0
data mining singapore april,1,1,1.0
mining singapore april pp,1,1,1.0
singapore april pp p,1,1,1.0
april pp p kang,1,1,1.0
pp p kang and,1,1,1.0
p kang and cho,1,1,1.0
kang and cho eus,1,1,1.0
and cho eus svms,1,1,1.0
cho eus svms ensemble,1,1,1.0
eus svms ensemble of,1,1,1.0
svms ensemble of svms,1,1,1.0
ensemble of svms for,1,1,1.0
of svms for data,1,1,1.0
svms for data imbalance,1,1,1.0
for data imbalance problems,1,1,1.0
data imbalance problems in,1,1,1.0
imbalance problems in proceedings,1,1,1.0
problems in proceedings of,1,1,1.0
neural information processing hong,1,1,1.0
information processing hong kong,1,1,1.0
processing hong kong china,1,1,1.0
kong china pp hong,1,1,1.0
china pp hong chen,1,1,1.0
pp hong chen and,2,2,1.0
hong chen and harris,2,2,1.0
chen and harris a,2,2,1.0
and harris a classiﬁer,2,2,1.0
harris a classiﬁer for,2,2,1.0
a classiﬁer for imbalanced,2,2,1.0
classiﬁer for imbalanced data,2,2,1.0
imbalanced data sets ieee,2,2,1.0
data sets ieee transactions,1,1,1.0
sets ieee transactions on,1,1,1.0
networks vol pp wu,1,1,1.0
vol pp wu and,1,1,1.0
pp wu and y,2,2,1.0
wu and y chang,3,2,1.5
and y chang kba,2,2,1.0
y chang kba kernel,2,2,1.0
chang kba kernel boundary,2,2,1.0
kba kernel boundary alignment,2,2,1.0
kernel boundary alignment ering,1,1,1.0
boundary alignment ering imbalanced,1,1,1.0
alignment ering imbalanced data,1,1,1.0
ering imbalanced data distribution,1,1,1.0
imbalanced data distribution ieee,2,2,1.0
data distribution ieee transactions,1,1,1.0
distribution ieee transactions on,1,1,1.0
engineering vol pp june,1,1,1.0
vol pp june adaptive,1,1,1.0
pp june adaptive conformal,1,1,1.0
june adaptive conformal transformation,1,1,1.0
adaptive conformal transformation for,1,1,1.0
conformal transformation for anced,1,1,1.0
transformation for anced data,1,1,1.0
for anced data learning,1,1,1.0
anced data learning in,1,1,1.0
data learning in proceedings,1,1,1.0
proceedings of the t,1,1,1.0
of the t wentieth,1,1,1.0
the t wentieth international,1,1,1.0
t wentieth international conference,1,1,1.0
wentieth international conference on,1,1,1.0
on machine learning w,1,1,1.0
machine learning w ashington,1,1,1.0
learning w ashington usa,1,1,1.0
w ashington usa pp,1,1,1.0
ashington usa pp y,1,1,1.0
usa pp y uan,1,1,1.0
pp y uan li,1,1,1.0
y uan li and,1,1,1.0
uan li and zhang,1,1,1.0
li and zhang learning,2,2,1.0
and zhang learning concepts,2,2,1.0
zhang learning concepts from,2,2,1.0
learning concepts from large,2,2,1.0
concepts from large scale,2,2,1.0
from large scale imbalanced,2,2,1.0
large scale imbalanced data,2,2,1.0
scale imbalanced data sets,2,2,1.0
imbalanced data sets using,2,2,1.0
data sets using support,2,2,1.0
sets using support cluster,2,2,1.0
using support cluster machines,2,2,1.0
support cluster machines in,2,2,1.0
cluster machines in proceedings,1,1,1.0
proceedings of the annual,2,1,2.0
of the annual acm,1,1,1.0
the annual acm international,1,1,1.0
annual acm international conference,1,1,1.0
acm international conference on,1,1,1.0
international conference on multimedia,1,1,1.0
conference on multimedia new,1,1,1.0
on multimedia new y,1,1,1.0
multimedia new y ork,1,1,1.0
new y ork usa,1,1,1.0
y ork usa pp,1,1,1.0
ork usa pp t,1,1,1.0
usa pp t cover,1,1,1.0
pp t cover and,1,1,1.0
t cover and p,1,1,1.0
cover and p hart,1,1,1.0
and p hart nearest,1,1,1.0
p hart nearest neighbor,1,1,1.0
hart nearest neighbor pattern,1,1,1.0
nearest neighbor pattern classiﬁcation,1,1,1.0
neighbor pattern classiﬁcation ieee,1,1,1.0
pattern classiﬁcation ieee transactions,1,1,1.0
on information theory vol,1,1,1.0
information theory vol pp,1,1,1.0
theory vol pp w,1,1,1.0
vol pp w johnson,1,1,1.0
pp w johnson and,1,1,1.0
w johnson and riess,1,1,1.0
johnson and riess numerical,1,1,1.0
and riess numerical analysis,1,1,1.0
riess numerical analysis reading,1,1,1.0
numerical analysis reading mass,1,1,1.0
analysis reading mass esley,1,1,1.0
reading mass esley pub,1,1,1.0
mass esley pub t,1,1,1.0
esley pub t y,1,1,1.0
pub t y kwok,1,1,1.0
t y kwok and,1,1,1.0
y kwok and w,1,1,1.0
kwok and w tsang,1,1,1.0
and w tsang the,1,1,1.0
w tsang the problem,1,1,1.0
tsang the problem in,1,1,1.0
the problem in kernel,1,1,1.0
problem in kernel methods,1,1,1.0
in kernel methods ieee,1,1,1.0
kernel methods ieee transactions,1,1,1.0
neural networks vol no,12,2,6.0
networks vol no pp,2,1,2.0
vol no pp nov,1,1,1.0
no pp nov braun,1,1,1.0
pp nov braun buhmann,1,1,1.0
nov braun buhmann and,1,1,1.0
braun buhmann and m,1,1,1.0
buhmann and m on,1,1,1.0
and m on relevant,1,1,1.0
m on relevant dimensions,1,1,1.0
on relevant dimensions in,1,1,1.0
relevant dimensions in kernel,1,1,1.0
dimensions in kernel feature,1,1,1.0
in kernel feature spaces,1,1,1.0
kernel feature spaces journal,1,1,1.0
feature spaces journal of,1,1,1.0
spaces journal of machine,1,1,1.0
vol no pp smola,1,1,1.0
no pp smola sch,1,1,1.0
pp smola sch and,1,1,1.0
smola sch and m,1,1,1.0
sch and m the,1,1,1.0
and m the connection,1,1,1.0
m the connection between,1,1,1.0
the connection between regularization,1,1,1.0
connection between regularization operators,1,1,1.0
between regularization operators and,1,1,1.0
regularization operators and support,1,1,1.0
operators and support vector,1,1,1.0
and support vector kernels,1,1,1.0
support vector kernels neural,1,1,1.0
vector kernels neural networks,1,1,1.0
kernels neural networks vol,1,1,1.0
vol no pp jun,8,2,4.0
no pp jun ledoux,1,1,1.0
pp jun ledoux the,1,1,1.0
jun ledoux the concentration,1,1,1.0
ledoux the concentration of,1,1,1.0
the concentration of measure,1,1,1.0
concentration of measure phenomenon,1,1,1.0
of measure phenomenon ser,1,1,1.0
measure phenomenon ser ical,1,1,1.0
phenomenon ser ical surveys,1,1,1.0
ser ical surveys and,1,1,1.0
ical surveys and monographs,1,1,1.0
surveys and monographs american,1,1,1.0
and monographs american mathematical,1,1,1.0
monographs american mathematical society,1,1,1.0
american mathematical society giannopoulos,1,1,1.0
mathematical society giannopoulos and,1,1,1.0
society giannopoulos and v,1,1,1.0
giannopoulos and v milman,1,1,1.0
and v milman concentration,1,1,1.0
v milman concentration property,1,1,1.0
milman concentration property on,1,1,1.0
concentration property on probability,1,1,1.0
property on probability spaces,1,1,1.0
on probability spaces advances,1,1,1.0
probability spaces advances in,1,1,1.0
spaces advances in mathematics,1,1,1.0
advances in mathematics vol,1,1,1.0
in mathematics vol no,1,1,1.0
mathematics vol no pp,1,1,1.0
vol no pp abe,1,1,1.0
no pp abe and,1,1,1.0
pp abe and onishi,1,1,1.0
abe and onishi sparse,1,1,1.0
and onishi sparse least,1,1,1.0
onishi sparse least squares,1,1,1.0
sparse least squares support,1,1,1.0
least squares support vector,1,1,1.0
squares support vector regressors,1,1,1.0
support vector regressors trained,1,1,1.0
vector regressors trained in,1,1,1.0
regressors trained in the,1,1,1.0
trained in the reduced,1,1,1.0
feature space in proceedings,2,1,2.0
space in proceedings of,2,1,2.0
international conference on artiﬁcial,1,1,1.0
conference on artiﬁcial neural,1,1,1.0
on artiﬁcial neural networks,1,1,1.0
artiﬁcial neural networks san,1,1,1.0
neural networks san sebasti,1,1,1.0
networks san sebasti spain,1,1,1.0
san sebasti spain june,1,1,1.0
sebasti spain june pp,1,1,1.0
spain june pp xiong,1,1,1.0
june pp xiong a,1,1,1.0
pp xiong a uniﬁed,1,1,1.0
xiong a uniﬁed framework,1,1,1.0
uniﬁed framework for kernelization,1,1,1.0
framework for kernelization the,1,1,1.0
for kernelization the empirical,1,1,1.0
kernelization the empirical kernel,1,1,1.0
the empirical kernel feature,1,1,1.0
empirical kernel feature space,1,1,1.0
kernel feature space in,1,1,1.0
proceedings of the chinese,1,1,1.0
of the chinese conference,1,1,1.0
the chinese conference on,1,1,1.0
chinese conference on p,1,1,1.0
conference on p attern,1,1,1.0
on p attern recognition,1,1,1.0
p attern recognition nanjing,1,1,1.0
attern recognition nanjing china,1,1,1.0
recognition nanjing china nov,1,1,1.0
nanjing china nov pp,1,1,1.0
china nov pp ramona,1,1,1.0
nov pp ramona richard,1,1,1.0
pp ramona richard and,1,1,1.0
ramona richard and david,1,1,1.0
richard and david multiclass,1,1,1.0
and david multiclass feature,1,1,1.0
david multiclass feature selection,1,1,1.0
multiclass feature selection with,1,1,1.0
feature selection with kernel,1,1,1.0
selection with kernel criteria,1,1,1.0
with kernel criteria ieee,1,1,1.0
kernel criteria ieee transactions,1,1,1.0
criteria ieee transactions on,1,1,1.0
and learning systems vol,1,1,1.0
learning systems vol pp,1,1,1.0
systems vol pp lanckriet,1,1,1.0
vol pp lanckriet cristianini,1,1,1.0
pp lanckriet cristianini p,1,1,1.0
lanckriet cristianini p bartlett,1,1,1.0
cristianini p bartlett ghaoui,1,1,1.0
p bartlett ghaoui and,1,1,1.0
bartlett ghaoui and jordan,1,1,1.0
ghaoui and jordan learning,1,1,1.0
and jordan learning the,1,1,1.0
jordan learning the kernel,1,1,1.0
learning the kernel matrix,1,1,1.0
the kernel matrix with,1,1,1.0
kernel matrix with semideﬁnite,1,1,1.0
matrix with semideﬁnite programming,1,1,1.0
with semideﬁnite programming journal,1,1,1.0
semideﬁnite programming journal of,1,1,1.0
programming journal of machine,1,1,1.0
learning research vol pp,2,1,2.0
research vol pp srebro,1,1,1.0
vol pp srebro and,1,1,1.0
pp srebro and learning,1,1,1.0
srebro and learning bounds,1,1,1.0
and learning bounds for,1,1,1.0
learning bounds for support,1,1,1.0
bounds for support vector,1,1,1.0
for support vector chines,1,1,1.0
support vector chines with,1,1,1.0
vector chines with learned,1,1,1.0
chines with learned kernels,1,1,1.0
with learned kernels in,1,1,1.0
learned kernels in proceedings,1,1,1.0
kernels in proceedings of,1,1,1.0
of the annual conference,1,1,1.0
the annual conference on,1,1,1.0
annual conference on learning,1,1,1.0
conference on learning theory,1,1,1.0
on learning theory pittsburgh,1,1,1.0
learning theory pittsburgh usa,1,1,1.0
theory pittsburgh usa june,1,1,1.0
pittsburgh usa june pp,1,1,1.0
usa june pp lichman,1,1,1.0
june pp lichman uci,1,1,1.0
pp lichman uci machine,1,1,1.0
machine learning repository online,2,2,1.0
learning repository online a,1,1,1.0
repository online a vailable,1,1,1.0
online a vailable http,1,1,1.0
a vailable http kubat,1,1,1.0
vailable http kubat and,1,1,1.0
http kubat and matwin,1,1,1.0
imbalanced training sets selection,2,2,1.0
training sets selection in,2,2,1.0
sets selection in proceedings,1,1,1.0
of the international ence,1,1,1.0
the international ence on,1,1,1.0
international ence on machine,1,1,1.0
ence on machine learning,1,1,1.0
machine learning nashville usa,1,1,1.0
learning nashville usa july,1,1,1.0
nashville usa july pp,1,1,1.0
usa july pp barandela,1,1,1.0
july pp barandela s,1,1,1.0
pp barandela s v,1,1,1.0
barandela s v garc,1,1,1.0
s v garc and,1,1,1.0
v garc and rangel,1,1,1.0
garc and rangel strategies,1,1,1.0
class imbalance problems p,1,1,1.0
imbalance problems p attern,1,1,1.0
problems p attern recognition,1,1,1.0
p attern recognition vol,2,1,2.0
attern recognition vol no,2,1,2.0
recognition vol no pp,2,1,2.0
vol no pp galar,1,1,1.0
no pp galar fern,1,1,1.0
galar fern barrenechea and,1,1,1.0
fern barrenechea and f,1,1,1.0
barrenechea and f herrera,1,1,1.0
and f herrera eusboost,1,1,1.0
f herrera eusboost enhancing,1,1,1.0
herrera eusboost enhancing ensembles,1,1,1.0
eusboost enhancing ensembles for,1,1,1.0
enhancing ensembles for highly,1,1,1.0
ensembles for highly imbalanced,1,1,1.0
for highly imbalanced by,1,1,1.0
highly imbalanced by evolutionary,1,1,1.0
imbalanced by evolutionary undersampling,1,1,1.0
by evolutionary undersampling p,1,1,1.0
evolutionary undersampling p attern,1,1,1.0
undersampling p attern recognition,1,1,1.0
vol no pp demsar,1,1,1.0
no pp demsar statistical,1,1,1.0
pp demsar statistical comparisons,1,1,1.0
demsar statistical comparisons of,1,1,1.0
statistical comparisons of classiﬁers,2,2,1.0
comparisons of classiﬁers over,2,2,1.0
of classiﬁers over multiple,2,2,1.0
classiﬁers over multiple data,2,2,1.0
over multiple data sets,2,2,1.0
multiple data sets journal,1,1,1.0
data sets journal of,1,1,1.0
sets journal of machine,1,1,1.0
research vol pp igel,1,1,1.0
vol pp igel and,1,1,1.0
pp igel and h,1,1,1.0
igel and h empirical,1,1,1.0
and h empirical evaluation,1,1,1.0
h empirical evaluation of,1,1,1.0
empirical evaluation of the,1,1,1.0
evaluation of the improved,1,1,1.0
of the improved rprop,1,1,1.0
the improved rprop learning,1,1,1.0
improved rprop learning algorithms,1,1,1.0
rprop learning algorithms neurocomputing,1,1,1.0
learning algorithms neurocomputing vol,1,1,1.0
algorithms neurocomputing vol pp,1,1,1.0
neurocomputing vol pp chapelle,1,1,1.0
vol pp chapelle and,1,1,1.0
pp chapelle and rakotomamonjy,1,1,1.0
chapelle and rakotomamonjy second,1,1,1.0
and rakotomamonjy second order,1,1,1.0
rakotomamonjy second order optimization,1,1,1.0
second order optimization of,1,1,1.0
order optimization of kernel,1,1,1.0
optimization of kernel parameters,1,1,1.0
of kernel parameters in,1,1,1.0
kernel parameters in in,1,1,1.0
parameters in in proceedings,1,1,1.0
in in proceedings of,1,1,1.0
networks vol no october,10,1,10.0
vol no october ramoboost,1,1,1.0
no october ramoboost ranked,1,1,1.0
october ramoboost ranked minority,1,1,1.0
ramoboost ranked minority oversampling,1,1,1.0
ranked minority oversampling in,10,1,10.0
minority oversampling in boosting,10,1,10.0
oversampling in boosting sheng,1,1,1.0
in boosting sheng chen,1,1,1.0
boosting sheng chen student,1,1,1.0
sheng chen student member,1,1,1.0
chen student member ieee,1,1,1.0
student member ieee haibo,1,1,1.0
member ieee haibo he,1,1,1.0
ieee haibo he member,1,1,1.0
haibo he member ieee,1,1,1.0
he member ieee and,1,1,1.0
member ieee and edwardo,1,1,1.0
ieee and edwardo garcia,1,1,1.0
and edwardo garcia in,1,1,1.0
edwardo garcia in recent,1,1,1.0
garcia in recent years,1,1,1.0
in recent years learning,1,1,1.0
recent years learning from,1,1,1.0
years learning from imbalanced,1,1,1.0
from imbalanced data has,1,1,1.0
imbalanced data has attracted,1,1,1.0
data has attracted growing,1,1,1.0
has attracted growing attention,1,1,1.0
attracted growing attention from,1,1,1.0
growing attention from both,1,1,1.0
attention from both academia,1,1,1.0
from both academia and,1,1,1.0
both academia and industry,1,1,1.0
academia and industry due,1,1,1.0
and industry due to,1,1,1.0
industry due to the,1,1,1.0
due to the explosive,1,1,1.0
to the explosive growth,1,1,1.0
the explosive growth of,1,1,1.0
explosive growth of applications,1,1,1.0
growth of applications that,1,1,1.0
of applications that use,1,1,1.0
applications that use and,1,1,1.0
that use and produce,1,1,1.0
use and produce imbalanced,1,1,1.0
and produce imbalanced data,1,1,1.0
produce imbalanced data however,1,1,1.0
imbalanced data however because,1,1,1.0
data however because of,1,1,1.0
however because of the,1,1,1.0
because of the complex,1,1,1.0
of the complex characteristics,1,1,1.0
the complex characteristics of,1,1,1.0
complex characteristics of imbalanced,1,1,1.0
characteristics of imbalanced data,1,1,1.0
of imbalanced data many,1,1,1.0
imbalanced data many solutions,1,1,1.0
data many solutions struggle,1,1,1.0
many solutions struggle to,1,1,1.0
solutions struggle to provide,1,1,1.0
struggle to provide robust,1,1,1.0
to provide robust efﬁciency,1,1,1.0
provide robust efﬁciency in,1,1,1.0
robust efﬁciency in applications,1,1,1.0
efﬁciency in applications in,1,1,1.0
in applications in an,1,1,1.0
applications in an effort,1,1,1.0
in an effort to,1,1,1.0
an effort to address,1,1,1.0
effort to address this,1,1,1.0
to address this problem,1,1,1.0
address this problem this,1,1,1.0
this problem this paper,1,1,1.0
problem this paper presents,1,1,1.0
this paper presents ranked,1,1,1.0
paper presents ranked minority,1,1,1.0
presents ranked minority sampling,1,1,1.0
ranked minority sampling in,1,1,1.0
minority sampling in boosting,1,1,1.0
sampling in boosting ramoboost,1,1,1.0
in boosting ramoboost which,1,1,1.0
boosting ramoboost which is,1,1,1.0
ramoboost which is a,2,1,2.0
which is a ramo,2,1,2.0
is a ramo technique,2,1,2.0
a ramo technique based,1,1,1.0
ramo technique based on,1,1,1.0
technique based on the,1,1,1.0
on the idea of,1,1,1.0
the idea of adaptive,1,1,1.0
idea of adaptive synthetic,1,1,1.0
of adaptive synthetic data,1,1,1.0
adaptive synthetic data generation,1,1,1.0
synthetic data generation in,2,1,2.0
data generation in an,1,1,1.0
generation in an ensemble,1,1,1.0
in an ensemble learning,1,1,1.0
an ensemble learning system,1,1,1.0
ensemble learning system brieﬂy,1,1,1.0
learning system brieﬂy ramoboost,1,1,1.0
system brieﬂy ramoboost adaptively,1,1,1.0
brieﬂy ramoboost adaptively ranks,1,1,1.0
ramoboost adaptively ranks minority,1,1,1.0
adaptively ranks minority class,1,1,1.0
ranks minority class instances,1,1,1.0
minority class instances at,1,1,1.0
class instances at each,1,1,1.0
instances at each learning,1,1,1.0
at each learning iteration,1,1,1.0
each learning iteration according,1,1,1.0
learning iteration according to,1,1,1.0
iteration according to a,1,1,1.0
according to a sampling,1,1,1.0
to a sampling probability,1,1,1.0
a sampling probability distribution,1,1,1.0
sampling probability distribution that,1,1,1.0
probability distribution that is,1,1,1.0
distribution that is based,1,1,1.0
that is based on,1,1,1.0
based on the underlying,1,1,1.0
on the underlying data,1,1,1.0
the underlying data distribution,1,1,1.0
underlying data distribution and,1,1,1.0
data distribution and can,1,1,1.0
distribution and can adaptively,1,1,1.0
and can adaptively shift,1,1,1.0
can adaptively shift the,2,1,2.0
adaptively shift the decision,2,1,2.0
shift the decision boundary,3,1,3.0
the decision boundary toward,4,1,4.0
decision boundary toward minori,1,1,1.0
boundary toward minori ty,1,1,1.0
toward minori ty and,1,1,1.0
minori ty and majority,1,1,1.0
ty and majority class,1,1,1.0
and majority class instances,1,1,1.0
majority class instances by,1,1,1.0
class instances by using,1,1,1.0
instances by using a,1,1,1.0
by using a hypothesis,1,1,1.0
using a hypothesis assessment,1,1,1.0
a hypothesis assessment procedure,1,1,1.0
hypothesis assessment procedure simulation,1,1,1.0
assessment procedure simulation analysis,1,1,1.0
procedure simulation analysis on,1,1,1.0
simulation analysis on datasets,1,1,1.0
analysis on datasets assessed,1,1,1.0
on datasets assessed over,1,1,1.0
datasets assessed over various,1,1,1.0
assessed over various overall,1,1,1.0
over various overall accuracy,1,1,1.0
various overall accuracy precision,1,1,1.0
overall accuracy precision recall,1,1,1.0
accuracy precision recall and,1,1,1.0
precision recall and receiver,1,1,1.0
recall and receiver operation,1,1,1.0
and receiver operation characteristic,1,1,1.0
receiver operation characteristic used,1,1,1.0
operation characteristic used to,1,1,1.0
characteristic used to illustrate,1,1,1.0
used to illustrate the,1,1,1.0
to illustrate the effectiveness,2,1,2.0
illustrate the effectiveness of,2,1,2.0
the effectiveness of this,1,1,1.0
effectiveness of this method,1,1,1.0
of this method index,1,1,1.0
this method index t,1,1,1.0
method index t adaptive,1,1,1.0
index t adaptive boosting,1,1,1.0
t adaptive boosting data,1,1,1.0
adaptive boosting data mining,1,1,1.0
boosting data mining ensemble,1,1,1.0
data mining ensemble ing,1,1,1.0
mining ensemble ing imbalanced,1,1,1.0
ensemble ing imbalanced data,1,1,1.0
ing imbalanced data i,1,1,1.0
imbalanced data i ntroduction,1,1,1.0
data i ntroduction l,1,1,1.0
i ntroduction l earning,1,1,1.0
ntroduction l earning from,1,1,1.0
l earning from imbalanced,1,1,1.0
earning from imbalanced da,1,1,1.0
from imbalanced da ta,1,1,1.0
imbalanced da ta imbalanced,1,1,1.0
da ta imbalanced learning,1,1,1.0
ta imbalanced learning has,1,1,1.0
imbalanced learning has become,1,1,1.0
learning has become a,1,1,1.0
has become a criti,1,1,1.0
become a criti cal,1,1,1.0
a criti cal and,1,1,1.0
criti cal and signiﬁcant,1,1,1.0
cal and signiﬁcant research,1,1,1.0
and signiﬁcant research issue,1,1,1.0
signiﬁcant research issue in,1,1,1.0
research issue in many,1,1,1.0
issue in many of,1,1,1.0
in many of today,1,1,1.0
many of today s,1,1,1.0
of today s applications,1,1,1.0
today s applications such,1,1,1.0
s applications such as,1,1,1.0
applications such as ﬁnancial,1,1,1.0
such as ﬁnancial engineering,1,1,1.0
as ﬁnancial engineering anom,1,1,1.0
ﬁnancial engineering anom aly,1,1,1.0
engineering anom aly detection,1,1,1.0
anom aly detection biomedical,1,1,1.0
aly detection biomedical data,1,1,1.0
detection biomedical data analysis,1,1,1.0
biomedical data analysis and,1,1,1.0
data analysis and many,1,1,1.0
analysis and many others,1,1,1.0
and many others the,1,1,1.0
many others the amount,1,1,1.0
others the amount and,1,1,1.0
the amount and complexity,1,1,1.0
amount and complexity of,1,1,1.0
and complexity of raw,1,1,1.0
complexity of raw data,1,1,1.0
of raw data that,1,1,1.0
raw data that is,1,1,1.0
data that is captured,1,1,1.0
that is captured to,1,1,1.0
is captured to monitor,1,1,1.0
captured to monitor analyze,1,1,1.0
to monitor analyze and,1,1,1.0
monitor analyze and support,1,1,1.0
analyze and support making,1,1,1.0
and support making processes,1,1,1.0
support making processes continue,1,1,1.0
making processes continue to,1,1,1.0
processes continue to grow,1,1,1.0
continue to grow at,1,1,1.0
to grow at an,1,1,1.0
grow at an incredible,1,1,1.0
at an incredible rate,1,1,1.0
an incredible rate consequently,1,1,1.0
incredible rate consequently this,1,1,1.0
rate consequently this enhances,1,1,1.0
consequently this enhances the,1,1,1.0
this enhances the capacity,1,1,1.0
enhances the capacity for,1,1,1.0
the capacity for computationally,1,1,1.0
capacity for computationally intelligent,1,1,1.0
for computationally intelligent methods,1,1,1.0
computationally intelligent methods to,1,1,1.0
intelligent methods to play,1,1,1.0
methods to play an,1,1,1.0
to play an essential,1,1,1.0
play an essential role,1,1,1.0
an essential role in,1,1,1.0
essential role in applications,1,1,1.0
role in applications involving,1,1,1.0
in applications involving large,1,1,1.0
applications involving large amounts,1,1,1.0
involving large amounts of,1,1,1.0
large amounts of data,1,1,1.0
amounts of data on,1,1,1.0
of data on the,1,1,1.0
data on the other,1,1,1.0
the other hand these,1,1,1.0
other hand these opportunities,1,1,1.0
hand these opportunities also,1,1,1.0
these opportunities also raise,1,1,1.0
opportunities also raise many,1,1,1.0
also raise many new,1,1,1.0
raise many new challenges,1,1,1.0
many new challenges for,1,1,1.0
new challenges for the,1,1,1.0
challenges for the research,1,1,1.0
for the research community,1,1,1.0
the research community in,1,1,1.0
research community in general,1,1,1.0
community in general generally,1,1,1.0
in general generally speaking,1,1,1.0
general generally speaking any,1,1,1.0
generally speaking any dataset,1,1,1.0
speaking any dataset that,1,1,1.0
any dataset that exhibits,1,1,1.0
dataset that exhibits an,1,1,1.0
that exhibits an unequal,1,1,1.0
exhibits an unequal distribution,1,1,1.0
unequal distribution between its,1,1,1.0
distribution between its classes,1,1,1.0
between its classes can,1,1,1.0
its classes can be,1,1,1.0
classes can be considered,1,1,1.0
can be considered imbalanced,1,1,1.0
be considered imbalanced in,1,1,1.0
considered imbalanced in applications,1,1,1.0
imbalanced in applications datasets,1,1,1.0
in applications datasets exhibiting,1,1,1.0
applications datasets exhibiting severe,1,1,1.0
datasets exhibiting severe ances,1,1,1.0
exhibiting severe ances are,1,1,1.0
severe ances are of,1,1,1.0
ances are of great,1,1,1.0
are of great interest,1,1,1.0
of great interest since,1,1,1.0
great interest since they,1,1,1.0
interest since they generally,1,1,1.0
since they generally present,1,1,1.0
they generally present icant,1,1,1.0
generally present icant difﬁculties,1,1,1.0
present icant difﬁculties for,1,1,1.0
icant difﬁculties for learning,1,1,1.0
difﬁculties for learning mechanisms,1,1,1.0
for learning mechanisms typical,1,1,1.0
learning mechanisms typical imbalance,1,1,1.0
mechanisms typical imbalance manuscript,1,1,1.0
typical imbalance manuscript received,1,1,1.0
imbalance manuscript received february,1,1,1.0
manuscript received february revised,1,1,1.0
received february revised november,1,1,1.0
february revised november march,1,1,1.0
revised november march august,1,1,1.0
november march august and,1,1,1.0
march august and august,1,1,1.0
august and august accepted,1,1,1.0
and august accepted august,1,1,1.0
august accepted august date,1,1,1.0
accepted august date of,1,1,1.0
august date of publication,1,1,1.0
date of publication august,1,1,1.0
of publication august date,1,1,1.0
publication august date of,1,1,1.0
august date of current,1,1,1.0
date of current version,1,1,1.0
of current version october,1,1,1.0
current version october chen,1,1,1.0
version october chen and,1,1,1.0
october chen and garcia,1,1,1.0
chen and garcia are,1,1,1.0
and garcia are with,1,1,1.0
garcia are with the,1,1,1.0
the department of electrical,3,1,3.0
department of electrical and,2,1,2.0
of electrical and computer,3,1,3.0
electrical and computer engineering,2,1,2.0
and computer engineering stevens,2,1,2.0
computer engineering stevens institute,2,1,2.0
engineering stevens institute of,2,1,2.0
stevens institute of technology,5,1,5.0
institute of technology hoboken,4,1,4.0
of technology hoboken nj,4,1,4.0
technology hoboken nj usa,1,1,1.0
hoboken nj usa egarcia,1,1,1.0
nj usa egarcia he,1,1,1.0
usa egarcia he is,1,1,1.0
egarcia he is with,1,1,1.0
he is with the,1,1,1.0
is with the department,1,1,1.0
department of electrical computer,1,1,1.0
of electrical computer and,2,1,2.0
electrical computer and biomedical,2,1,2.0
computer and biomedical engineering,2,1,2.0
and biomedical engineering university,2,1,2.0
biomedical engineering university of,2,1,2.0
engineering university of rhode,2,1,2.0
university of rhode island,2,1,2.0
of rhode island kingston,2,1,2.0
rhode island kingston ri,1,1,1.0
island kingston ri usa,1,1,1.0
kingston ri usa he,1,1,1.0
ri usa he color,1,1,1.0
usa he color versions,1,1,1.0
he color versions of,1,1,1.0
color versions of one,1,1,1.0
versions of one or,1,1,1.0
of one or more,1,1,1.0
or more of the,1,1,1.0
more of the ﬁgures,1,1,1.0
of the ﬁgures in,1,1,1.0
the ﬁgures in this,1,1,1.0
ﬁgures in this paper,1,1,1.0
paper are available online,1,1,1.0
are available online at,1,1,1.0
available online at http,1,1,1.0
online at http digital,1,1,1.0
at http digital object,1,1,1.0
http digital object identiﬁer,1,1,1.0
digital object identiﬁer ratios,1,1,1.0
object identiﬁer ratios can,1,1,1.0
identiﬁer ratios can range,1,1,1.0
ratios can range from,1,1,1.0
can range from in,1,1,1.0
range from in fraud,1,1,1.0
from in fraud detection,1,1,1.0
in fraud detection problems,1,1,1.0
fraud detection problems to,1,1,1.0
detection problems to in,1,1,1.0
problems to in physics,1,1,1.0
to in physics event,1,1,1.0
in physics event classiﬁcation,1,1,1.0
physics event classiﬁcation however,1,1,1.0
event classiﬁcation however imbalances,1,1,1.0
classiﬁcation however imbalances of,1,1,1.0
however imbalances of this,1,1,1.0
imbalances of this form,1,1,1.0
of this form are,1,1,1.0
this form are just,1,1,1.0
form are just one,1,1,1.0
are just one aspect,1,1,1.0
just one aspect of,1,1,1.0
one aspect of the,1,1,1.0
aspect of the imbalanced,1,1,1.0
of the imbalanced learning,1,1,1.0
the imbalanced learning problem,2,1,2.0
imbalanced learning problem the,1,1,1.0
learning problem the imbalance,1,1,1.0
problem the imbalance learning,1,1,1.0
the imbalance learning problem,1,1,1.0
imbalance learning problem generally,1,1,1.0
learning problem generally manifests,1,1,1.0
problem generally manifests itself,1,1,1.0
generally manifests itself in,1,1,1.0
manifests itself in two,1,1,1.0
itself in two forms,1,1,1.0
in two forms relative,1,1,1.0
two forms relative imbalances,1,1,1.0
forms relative imbalances and,1,1,1.0
relative imbalances and absolute,1,1,1.0
imbalances and absolute imbalances,1,1,1.0
and absolute imbalances absolute,1,1,1.0
absolute imbalances absolute imbalances,1,1,1.0
imbalances absolute imbalances arise,1,1,1.0
absolute imbalances arise in,1,1,1.0
imbalances arise in datasets,1,1,1.0
arise in datasets where,1,1,1.0
in datasets where minority,1,1,1.0
datasets where minority exampl,1,1,1.0
where minority exampl es,1,1,1.0
minority exampl es are,1,1,1.0
exampl es are deﬁnitively,1,1,1.0
es are deﬁnitively scarce,1,1,1.0
are deﬁnitively scarce and,1,1,1.0
deﬁnitively scarce and underrepresented,1,1,1.0
scarce and underrepresented whereas,1,1,1.0
and underrepresented whereas relative,1,1,1.0
underrepresented whereas relative imbalances,1,1,1.0
whereas relative imbalances are,1,1,1.0
relative imbalances are indicative,1,1,1.0
imbalances are indicative of,1,1,1.0
are indicative of datasets,1,1,1.0
indicative of datasets in,1,1,1.0
of datasets in which,1,1,1.0
datasets in which minority,1,1,1.0
in which minority examples,1,1,1.0
which minority examples are,1,1,1.0
minority examples are well,1,1,1.0
examples are well represented,1,1,1.0
are well represented but,1,1,1.0
well represented but remain,1,1,1.0
represented but remain severely,1,1,1.0
but remain severely outnumbere,1,1,1.0
remain severely outnumbere d,1,1,1.0
severely outnumbere d by,1,1,1.0
outnumbere d by majority,1,1,1.0
d by majority class,1,1,1.0
by majority class examples,1,1,1.0
majority class examples some,1,1,1.0
class examples some studies,1,1,1.0
examples some studies have,1,1,1.0
some studies have shown,1,1,1.0
studies have shown that,1,1,1.0
have shown that the,1,1,1.0
shown that the degradation,1,1,1.0
that the degradation of,1,1,1.0
the degradation of classiﬁcation,1,1,1.0
degradation of classiﬁcation performance,1,1,1.0
of classiﬁcation performance attributed,1,1,1.0
classiﬁcation performance attributed to,1,1,1.0
performance attributed to imbalanced,1,1,1.0
attributed to imbalanced data,1,1,1.0
to imbalanced data is,1,1,1.0
imbalanced data is not,1,1,1.0
data is not necessarily,1,1,1.0
is not necessarily the,1,1,1.0
not necessarily the result,1,1,1.0
necessarily the result of,1,1,1.0
the result of relative,1,1,1.0
result of relative imbalances,1,1,1.0
of relative imbalances but,1,1,1.0
relative imbalances but rather,1,1,1.0
imbalances but rather due,1,1,1.0
but rather due to,1,1,1.0
rather due to the,1,1,1.0
due to the lack,1,1,1.0
to the lack of,1,1,1.0
the lack of representative,1,1,1.0
lack of representative examples,1,1,1.0
of representative examples absolute,1,1,1.0
representative examples absolute imbalances,1,1,1.0
examples absolute imbalances in,1,1,1.0
absolute imbalances in particular,1,1,1.0
imbalances in particular for,1,1,1.0
in particular for a,1,1,1.0
particular for a given,1,1,1.0
a given dataset that,1,1,1.0
given dataset that contains,1,1,1.0
dataset that contains several,1,1,1.0
that contains several the,1,1,1.0
contains several the distribution,1,1,1.0
several the distribution of,1,1,1.0
the distribution of minority,1,1,1.0
distribution of minority examples,1,1,1.0
of minority examples over,1,1,1.0
minority examples over the,1,1,1.0
examples over the minority,1,1,1.0
the minority class concepts,1,1,1.0
minority class concepts may,1,1,1.0
class concepts may yield,1,1,1.0
concepts may yield clusters,1,1,1.0
may yield clusters with,1,1,1.0
yield clusters with insufﬁcient,1,1,1.0
clusters with insufﬁcient representative,1,1,1.0
with insufﬁcient representative exa,1,1,1.0
insufﬁcient representative exa mples,1,1,1.0
representative exa mples to,1,1,1.0
exa mples to form,1,1,1.0
mples to form a,1,1,1.0
to form a classiﬁcation,1,1,1.0
form a classiﬁcation rule,1,1,1.0
a classiﬁcation rule this,1,1,1.0
classiﬁcation rule this problem,1,1,1.0
rule this problem of,1,1,1.0
this problem of concept,1,1,1.0
problem of concept data,1,1,1.0
of concept data representation,1,1,1.0
concept data representation within,1,1,1.0
data representation within a,1,1,1.0
representation within a class,1,1,1.0
a class is also,1,1,1.0
class is also known,1,1,1.0
is also known as,1,1,1.0
also known as the,1,1,1.0
known as the imbalance,1,1,1.0
as the imbalance problem,1,1,1.0
the imbalance problem and,1,1,1.0
imbalance problem and was,1,1,1.0
problem and was veriﬁed,1,1,1.0
and was veriﬁed to,1,1,1.0
was veriﬁed to be,1,1,1.0
veriﬁed to be more,1,1,1.0
to be more difﬁcult,1,1,1.0
be more difﬁcult to,1,1,1.0
more difﬁcult to handle,1,1,1.0
difﬁcult to handle than,1,1,1.0
to handle than datasets,1,1,1.0
handle than datasets with,1,1,1.0
than datasets with only,1,1,1.0
datasets with only homogeneous,1,1,1.0
with only homogeneous concepts,1,1,1.0
only homogeneous concepts for,1,1,1.0
homogeneous concepts for each,1,1,1.0
concepts for each class,1,1,1.0
for each class logically,1,1,1.0
each class logically it,1,1,1.0
class logically it would,1,1,1.0
logically it would follow,1,1,1.0
it would follow that,1,1,1.0
would follow that solutions,1,1,1.0
follow that solutions targeted,1,1,1.0
that solutions targeted at,1,1,1.0
solutions targeted at both,1,1,1.0
targeted at both relative,1,1,1.0
at both relative and,1,1,1.0
both relative and absolute,1,1,1.0
relative and absolute imbalances,1,1,1.0
and absolute imbalances would,1,1,1.0
absolute imbalances would be,1,1,1.0
imbalances would be more,1,1,1.0
would be more adept,1,1,1.0
be more adept to,1,1,1.0
more adept to handling,1,1,1.0
adept to handling a,1,1,1.0
to handling a wide,1,1,1.0
handling a wide spectrum,1,1,1.0
a wide spectrum of,1,1,1.0
wide spectrum of imbalanced,1,1,1.0
spectrum of imbalanced learning,1,1,1.0
of imbalanced learning problems,1,1,1.0
imbalanced learning problems to,2,1,2.0
learning problems to this,1,1,1.0
problems to this end,1,1,1.0
to this end this,1,1,1.0
this end this paper,1,1,1.0
end this paper proposes,1,1,1.0
this paper proposes ramoboost,1,1,1.0
paper proposes ramoboost which,1,1,1.0
proposes ramoboost which is,1,1,1.0
a ramo technique embedded,1,1,1.0
ramo technique embedded with,1,1,1.0
technique embedded with a,1,1,1.0
embedded with a boosting,1,1,1.0
with a boosting procedure,1,1,1.0
a boosting procedure to,1,1,1.0
boosting procedure to facilitate,1,1,1.0
procedure to facilitate learning,1,1,1.0
to facilitate learning from,2,1,2.0
facilitate learning from imbalanced,1,1,1.0
learning from imbalanced datase,1,1,1.0
from imbalanced datase ts,1,1,1.0
imbalanced datase ts based,1,1,1.0
datase ts based on,1,1,1.0
ts based on an,1,1,1.0
based on an integration,1,1,1.0
on an integration of,1,1,1.0
an integration of oversampling,1,1,1.0
integration of oversampling and,1,1,1.0
of oversampling and ensemble,1,1,1.0
oversampling and ensemble learning,1,1,1.0
and ensemble learning ramoboost,1,1,1.0
ensemble learning ramoboost tematically,1,1,1.0
learning ramoboost tematically generates,1,1,1.0
ramoboost tematically generates synthetic,1,1,1.0
tematically generates synthetic instances,1,1,1.0
generates synthetic instances by,2,1,2.0
synthetic instances by considering,1,1,1.0
instances by considering the,1,1,1.0
by considering the class,1,1,1.0
considering the class ratios,1,1,1.0
the class ratios of,1,1,1.0
class ratios of surrounding,1,1,1.0
ratios of surrounding nearest,1,1,1.0
of surrounding nearest neighbors,1,1,1.0
surrounding nearest neighbors of,1,1,1.0
nearest neighbors of each,5,1,5.0
neighbors of each minority,5,1,5.0
of each minority class,1,1,1.0
each minority class example,1,1,1.0
minority class example in,1,1,1.0
class example in the,1,1,1.0
example in the underlying,1,1,1.0
in the underlying training,1,1,1.0
the underlying training data,1,1,1.0
underlying training data distribution,1,1,1.0
training data distribution unlike,1,1,1.0
data distribution unlike many,1,1,1.0
distribution unlike many existing,1,1,1.0
unlike many existing approaches,1,1,1.0
many existing approaches that,1,1,1.0
existing approaches that use,1,1,1.0
approaches that use uniform,1,1,1.0
that use uniform sampling,1,1,1.0
use uniform sampling distributions,1,1,1.0
uniform sampling distributions ramoboost,1,1,1.0
sampling distributions ramoboost adaptively,1,1,1.0
distributions ramoboost adaptively adjusts,1,1,1.0
ramoboost adaptively adjusts the,1,1,1.0
adaptively adjusts the sampling,1,1,1.0
adjusts the sampling weights,1,1,1.0
the sampling weights of,1,1,1.0
sampling weights of minority,1,1,1.0
weights of minority class,1,1,1.0
minority class examples according,1,1,1.0
class examples according to,1,1,1.0
examples according to their,1,1,1.0
according to their data,1,1,1.0
to their data distributions,1,1,1.0
their data distributions moreover,1,1,1.0
data distributions moreover by,1,1,1.0
distributions moreover by integrating,1,1,1.0
moreover by integrating the,1,1,1.0
by integrating the ensemble,1,1,1.0
integrating the ensemble learning,1,1,1.0
the ensemble learning methodology,1,1,1.0
ensemble learning methodology ramoboost,1,1,1.0
learning methodology ramoboost adopts,1,1,1.0
methodology ramoboost adopts an,1,1,1.0
ramoboost adopts an iterative,1,1,1.0
adopts an iterative learning,1,1,1.0
an iterative learning dure,1,1,1.0
iterative learning dure that,1,1,1.0
learning dure that assesses,1,1,1.0
dure that assesses the,1,1,1.0
that assesses the hypothesis,1,1,1.0
assesses the hypothesis developed,1,1,1.0
the hypothesis developed at,1,1,1.0
hypothesis developed at each,1,1,1.0
developed at each boosting,1,1,1.0
at each boosting iteration,2,1,2.0
each boosting iteration to,1,1,1.0
boosting iteration to adaptively,1,1,1.0
iteration to adaptively shift,1,1,1.0
to adaptively shift the,1,1,1.0
the decision boundary to,2,1,2.0
decision boundary to focus,1,1,1.0
boundary to focus more,1,1,1.0
focus more on those,1,1,1.0
more on those instances,1,1,1.0
on those instances of,1,1,1.0
those instances of both,1,1,1.0
instances of both the,1,1,1.0
of both the majority,1,1,1.0
both the majority and,1,1,1.0
the majority and the,1,1,1.0
majority and the minority,1,1,1.0
and the minority classes,1,1,1.0
the minority classes we,1,1,1.0
minority classes we organize,1,1,1.0
classes we organize the,1,1,1.0
we organize the remainder,1,1,1.0
organize the remainder of,1,1,1.0
of this paper as,1,1,1.0
this paper as follows,1,1,1.0
paper as follows in,1,1,1.0
as follows in section,1,1,1.0
follows in section ii,1,1,1.0
in section ii we,1,1,1.0
section ii we present,1,1,1.0
ii we present a,1,1,1.0
we present a brief,1,1,1.0
present a brief review,1,1,1.0
a brief review of,1,1,1.0
brief review of the,1,1,1.0
review of the art,1,1,1.0
of the art techniques,1,1,1.0
the art techniques proposed,1,1,1.0
art techniques proposed in,1,1,1.0
techniques proposed in the,1,1,1.0
proposed in the community,1,1,1.0
in the community to,1,1,1.0
the community to address,1,1,1.0
community to address the,1,1,1.0
to address the imbalanced,1,1,1.0
address the imbalanced learning,1,1,1.0
imbalanced learning problem in,1,1,1.0
learning problem in section,1,1,1.0
problem in section iii,1,1,1.0
in section iii we,1,1,1.0
section iii we discuss,1,1,1.0
iii we discuss the,1,1,1.0
we discuss the motivation,1,1,1.0
discuss the motivation behind,1,1,1.0
the motivation behind the,1,1,1.0
motivation behind the ramoboost,1,1,1.0
behind the ramoboost framework,1,1,1.0
the ramoboost framework and,1,1,1.0
ramoboost framework and present,1,1,1.0
framework and present ieeechen,1,1,1.0
and present ieeechen et,1,1,1.0
present ieeechen et al,1,1,1.0
ieeechen et al ranked,1,1,1.0
et al ranked minority,9,1,9.0
al ranked minority oversampling,9,1,9.0
oversampling in boosting the,2,1,2.0
in boosting the algorithm,1,1,1.0
boosting the algorithm in,1,1,1.0
the algorithm in detail,1,1,1.0
algorithm in detail the,1,1,1.0
in detail the computational,1,1,1.0
detail the computational complexity,1,1,1.0
the computational complexity analysis,1,1,1.0
computational complexity analysis of,1,1,1.0
complexity analysis of the,1,1,1.0
analysis of the proposed,1,1,1.0
of the proposed ramoboost,2,1,2.0
the proposed ramoboost algorithms,1,1,1.0
proposed ramoboost algorithms is,1,1,1.0
ramoboost algorithms is also,1,1,1.0
algorithms is also presented,1,1,1.0
is also presented in,1,1,1.0
also presented in this,2,1,2.0
presented in this section,4,1,4.0
in this section in,1,1,1.0
this section in section,1,1,1.0
section in section iv,1,1,1.0
in section iv simulation,1,1,1.0
section iv simulation analysis,1,1,1.0
iv simulation analysis on,1,1,1.0
simulation analysis on world,1,1,1.0
analysis on world machine,1,1,1.0
on world machine learning,1,1,1.0
world machine learning datasets,1,1,1.0
machine learning datasets are,1,1,1.0
learning datasets are provided,1,1,1.0
datasets are provided to,1,1,1.0
are provided to illustrate,1,1,1.0
provided to illustrate the,1,1,1.0
the effectiveness of the,1,1,1.0
effectiveness of the proposed,1,1,1.0
of the proposed method,2,1,2.0
the proposed method details,1,1,1.0
proposed method details of,1,1,1.0
method details of experimental,1,1,1.0
details of experimental parameters,1,1,1.0
of experimental parameters and,1,1,1.0
experimental parameters and evaluation,1,1,1.0
parameters and evaluation metrics,1,1,1.0
and evaluation metrics are,1,1,1.0
evaluation metrics are also,1,1,1.0
metrics are also presented,1,1,1.0
are also presented in,1,1,1.0
in this section finally,1,1,1.0
this section finally a,1,1,1.0
section finally a conclusion,1,1,1.0
finally a conclusion and,1,1,1.0
a conclusion and brief,1,1,1.0
conclusion and brief discussion,1,1,1.0
and brief discussion on,1,1,1.0
brief discussion on future,1,1,1.0
discussion on future research,1,1,1.0
on future research directions,1,1,1.0
future research directions are,1,1,1.0
research directions are presented,1,1,1.0
directions are presented in,1,1,1.0
are presented in section,1,1,1.0
presented in section ii,1,1,1.0
in section ii r,1,1,1.0
section ii r elated,1,1,1.0
ii r elated works,1,1,1.0
r elated works a,1,1,1.0
elated works a comprehensive,1,1,1.0
works a comprehensive review,1,1,1.0
a comprehensive review of,1,1,1.0
comprehensive review of the,1,1,1.0
review of the development,1,1,1.0
of the development of,1,1,1.0
the development of research,1,1,1.0
development of research in,1,1,1.0
of research in learning,1,1,1.0
research in learning from,1,1,1.0
from imbalanced data including,1,1,1.0
imbalanced data including the,1,1,1.0
data including the nature,1,1,1.0
including the nature of,1,1,1.0
nature of the problem,1,1,1.0
of the problem the,1,1,1.0
the problem the rt,1,1,1.0
problem the rt approaches,1,1,1.0
the rt approaches the,1,1,1.0
rt approaches the assessment,1,1,1.0
approaches the assessment metrics,1,1,1.0
the assessment metrics and,1,1,1.0
assessment metrics and the,1,1,1.0
metrics and the major,1,1,1.0
and the major opportunities,1,1,1.0
the major opportunities and,1,1,1.0
major opportunities and challenges,1,1,1.0
opportunities and challenges has,1,1,1.0
and challenges has been,1,1,1.0
challenges has been presented,1,1,1.0
has been presented in,1,1,1.0
been presented in interested,1,1,1.0
presented in interested readers,1,1,1.0
in interested readers can,1,1,1.0
interested readers can refer,1,1,1.0
readers can refer to,1,1,1.0
can refer to that,1,1,1.0
refer to that article,1,1,1.0
to that article for,1,1,1.0
that article for details,1,1,1.0
article for details in,1,1,1.0
for details in this,1,1,1.0
details in this section,1,1,1.0
this section we provide,1,1,1.0
section we provide a,1,1,1.0
we provide a focused,1,1,1.0
provide a focused review,1,1,1.0
a focused review of,1,1,1.0
focused review of four,1,1,1.0
review of four major,1,1,1.0
of four major categories,1,1,1.0
four major categories of,1,1,1.0
major categories of research,1,1,1.0
categories of research activity,1,1,1.0
of research activity in,1,1,1.0
research activity in imbalanced,1,1,1.0
activity in imbalanced learning,1,1,1.0
in imbalanced learning sampling,1,1,1.0
imbalanced learning sampling methods,1,1,1.0
learning sampling methods building,1,1,1.0
sampling methods building on,1,1,1.0
methods building on the,1,1,1.0
building on the foundation,1,1,1.0
on the foundation of,1,1,1.0
the foundation of the,1,1,1.0
foundation of the random,1,1,1.0
of the random simple,1,1,1.0
the random simple pling,1,1,1.0
random simple pling and,1,1,1.0
simple pling and undersampling,1,1,1.0
pling and undersampling techniques,1,1,1.0
and undersampling techniques researchers,1,1,1.0
undersampling techniques researchers have,1,1,1.0
techniques researchers have oped,1,1,1.0
researchers have oped advanced,1,1,1.0
have oped advanced sampling,1,1,1.0
oped advanced sampling methods,1,1,1.0
advanced sampling methods to,1,1,1.0
sampling methods to address,1,1,1.0
methods to address the,1,1,1.0
to address the shortcomings,1,1,1.0
address the shortcomings of,1,1,1.0
the shortcomings of these,1,1,1.0
shortcomings of these basic,1,1,1.0
of these basic techniques,1,1,1.0
these basic techniques such,1,1,1.0
basic techniques such as,1,1,1.0
techniques such as overﬁtting,1,1,1.0
such as overﬁtting and,1,1,1.0
as overﬁtting and information,1,1,1.0
overﬁtting and information loss,1,1,1.0
and information loss for,1,1,1.0
information loss for example,1,1,1.0
loss for example the,1,1,1.0
for example the synthetic,1,1,1.0
example the synthetic minority,1,1,1.0
the synthetic minority oversampling,1,1,1.0
synthetic minority oversampling nique,1,1,1.0
minority oversampling nique smote,1,1,1.0
oversampling nique smote algorithm,1,1,1.0
nique smote algorithm was,1,1,1.0
smote algorithm was proposed,1,1,1.0
algorithm was proposed to,1,1,1.0
was proposed to search,1,1,1.0
proposed to search for,1,1,1.0
to search for the,1,1,1.0
for the nearest neighbors,1,1,1.0
nearest neighbors of every,1,1,1.0
neighbors of every minority,1,1,1.0
of every minority instance,1,1,1.0
every minority instance and,1,1,1.0
minority instance and generates,1,1,1.0
instance and generates synthetic,1,1,1.0
and generates synthetic minority,1,1,1.0
generates synthetic minority data,1,1,1.0
synthetic minority data by,1,1,1.0
minority data by calculating,1,1,1.0
data by calculating linear,1,1,1.0
by calculating linear interpolations,1,1,1.0
calculating linear interpolations between,1,1,1.0
linear interpolations between an,1,1,1.0
interpolations between an original,1,1,1.0
between an original minority,1,1,1.0
an original minority class,1,1,1.0
original minority class instance,1,1,1.0
minority class instance and,1,1,1.0
class instance and a,1,1,1.0
instance and a randomly,1,1,1.0
and a randomly selected,1,1,1.0
a randomly selected neighbor,1,1,1.0
randomly selected neighbor expanding,1,1,1.0
selected neighbor expanding on,1,1,1.0
neighbor expanding on the,1,1,1.0
expanding on the smote,1,1,1.0
on the smote framework,1,1,1.0
the smote framework the,1,1,1.0
smote framework the algorithm,1,1,1.0
framework the algorithm locates,1,1,1.0
the algorithm locates those,1,1,1.0
algorithm locates those minority,1,1,1.0
locates those minority class,1,1,1.0
those minority class examples,1,1,1.0
class examples that reside,1,1,1.0
examples that reside along,1,1,1.0
that reside along the,1,1,1.0
reside along the borders,1,1,1.0
along the borders between,1,1,1.0
the borders between majority,1,1,1.0
borders between majority and,1,1,1.0
and minority classes other,1,1,1.0
minority classes other popular,1,1,1.0
classes other popular approaches,1,1,1.0
other popular approaches include,1,1,1.0
popular approaches include the,1,1,1.0
approaches include the imbalanced,1,1,1.0
include the imbalanced learning,1,1,1.0
the imbalanced learning adaboost,1,1,1.0
imbalanced learning adaboost in,1,1,1.0
learning adaboost in junction,1,1,1.0
adaboost in junction with,1,1,1.0
in junction with and,1,1,1.0
junction with and jittering,1,1,1.0
with and jittering of,1,1,1.0
and jittering of the,1,1,1.0
jittering of the data,1,1,1.0
of the data approach,1,1,1.0
the data approach and,1,1,1.0
data approach and the,1,1,1.0
approach and the integration,1,1,1.0
and the integration of,1,1,1.0
the integration of smote,1,1,1.0
integration of smote with,1,1,1.0
of smote with tomek,1,1,1.0
smote with tomek links,1,1,1.0
with tomek links and,1,1,1.0
tomek links and edited,1,1,1.0
links and edited nearest,1,1,1.0
and edited nearest neighbor,1,1,1.0
edited nearest neighbor since,1,1,1.0
nearest neighbor since many,1,1,1.0
neighbor since many of,1,1,1.0
since many of the,1,1,1.0
many of the aforementioned,1,1,1.0
of the aforementioned sampling,1,1,1.0
the aforementioned sampling algorithms,1,1,1.0
aforementioned sampling algorithms solely,1,1,1.0
sampling algorithms solely focus,1,1,1.0
algorithms solely focus on,1,1,1.0
solely focus on relative,1,1,1.0
focus on relative imbalances,1,1,1.0
on relative imbalances various,1,1,1.0
relative imbalances various approaches,1,1,1.0
imbalances various approaches were,1,1,1.0
various approaches were proposed,1,1,1.0
approaches were proposed to,1,1,1.0
were proposed to explicitly,1,1,1.0
proposed to explicitly address,1,1,1.0
to explicitly address the,1,1,1.0
explicitly address the imbalance,1,1,1.0
address the imbalance issue,1,1,1.0
the imbalance issue some,1,1,1.0
imbalance issue some of,1,1,1.0
issue some of the,1,1,1.0
some of the important,1,1,1.0
of the important works,1,1,1.0
the important works include,1,1,1.0
important works include the,1,1,1.0
works include the oversampling,1,1,1.0
include the oversampling algorithm,1,1,1.0
the oversampling algorithm support,1,1,1.0
oversampling algorithm support cluster,1,1,1.0
algorithm support cluster machines,1,1,1.0
support cluster machines and,1,1,1.0
cluster machines and others,1,1,1.0
machines and others learning,1,1,1.0
and others learning methods,2,1,2.0
others learning methods learning,1,1,1.0
learning methods learning methods,1,1,1.0
methods learning methods typically,1,1,1.0
learning methods typically employ,1,1,1.0
methods typically employ the,1,1,1.0
typically employ the use,1,1,1.0
employ the use of,1,1,1.0
the use of cost,1,1,1.0
use of cost matrices,1,1,1.0
of cost matrices to,1,1,1.0
cost matrices to estimate,1,1,1.0
matrices to estimate the,1,1,1.0
to estimate the costs,1,1,1.0
estimate the costs of,1,1,1.0
the costs of different,1,1,1.0
costs of different classiﬁcation,1,1,1.0
of different classiﬁcation errors,1,1,1.0
different classiﬁcation errors these,1,1,1.0
classiﬁcation errors these techniques,1,1,1.0
errors these techniques have,1,1,1.0
these techniques have shown,1,1,1.0
techniques have shown great,1,1,1.0
have shown great success,1,1,1.0
shown great success when,1,1,1.0
great success when applied,1,1,1.0
success when applied to,1,1,1.0
when applied to imbalanced,1,1,1.0
applied to imbalanced learning,1,1,1.0
to imbalanced learning problems,1,1,1.0
imbalanced learning problems for,1,1,1.0
learning problems for example,1,1,1.0
problems for example in,1,1,1.0
for example in an,1,1,1.0
example in an method,1,1,1.0
in an method was,1,1,1.0
an method was presented,1,1,1.0
method was presented to,1,1,1.0
was presented to induce,1,1,1.0
presented to induce trees,1,1,1.0
to induce trees in,1,1,1.0
induce trees in and,1,1,1.0
trees in and the,1,1,1.0
in and the asymmetric,1,1,1.0
and the asymmetric adaboost,1,1,1.0
the asymmetric adaboost method,1,1,1.0
asymmetric adaboost method is,1,1,1.0
adaboost method is proposed,1,1,1.0
method is proposed to,1,1,1.0
is proposed to handle,1,1,1.0
proposed to handle face,1,1,1.0
to handle face detection,1,1,1.0
handle face detection problems,1,1,1.0
face detection problems where,1,1,1.0
detection problems where the,1,1,1.0
problems where the skewed,1,1,1.0
where the skewed class,1,1,1.0
the skewed class ratio,1,1,1.0
skewed class ratio can,1,1,1.0
class ratio can be,1,1,1.0
ratio can be quite,1,1,1.0
can be quite high,1,1,1.0
be quite high the,1,1,1.0
quite high the adacost,1,1,1.0
high the adacost method,1,1,1.0
the adacost method proposed,1,1,1.0
adacost method proposed in,1,1,1.0
method proposed in combines,1,1,1.0
proposed in combines learning,1,1,1.0
in combines learning with,1,1,1.0
combines learning with ing,1,1,1.0
learning with ing by,1,1,1.0
with ing by referring,1,1,1.0
ing by referring to,1,1,1.0
by referring to the,1,1,1.0
referring to the matrix,1,1,1.0
to the matrix adacost,1,1,1.0
the matrix adacost assigns,1,1,1.0
matrix adacost assigns different,1,1,1.0
adacost assigns different cost,1,1,1.0
assigns different cost values,1,1,1.0
different cost values to,1,1,1.0
cost values to misclassiﬁed,1,1,1.0
values to misclassiﬁed minority,1,1,1.0
to misclassiﬁed minority and,1,1,1.0
misclassiﬁed minority and majority,1,1,1.0
minority and majority examples,2,1,2.0
and majority examples by,1,1,1.0
majority examples by the,1,1,1.0
examples by the trained,1,1,1.0
by the trained hypothesis,1,1,1.0
the trained hypothesis at,1,1,1.0
trained hypothesis at each,1,1,1.0
hypothesis at each iteration,2,1,2.0
at each iteration loop,2,1,2.0
each iteration loop other,1,1,1.0
iteration loop other examples,1,1,1.0
loop other examples of,1,1,1.0
other examples of learning,1,1,1.0
examples of learning include,1,1,1.0
of learning include the,1,1,1.0
learning include the cost,1,1,1.0
include the cost framework,1,1,1.0
the cost framework neural,1,1,1.0
cost framework neural network,1,1,1.0
framework neural network sensitive,1,1,1.0
neural network sensitive support,1,1,1.0
network sensitive support vector,1,1,1.0
sensitive support vector machines,1,1,1.0
support vector machines svms,1,1,1.0
vector machines svms and,1,1,1.0
machines svms and others,1,1,1.0
svms and others learning,1,1,1.0
others learning methods methods,1,1,1.0
learning methods methods have,1,1,1.0
methods methods have recently,1,1,1.0
methods have recently become,1,1,1.0
have recently become very,1,1,1.0
recently become very popular,1,1,1.0
become very popular across,1,1,1.0
very popular across various,1,1,1.0
popular across various ﬁelds,1,1,1.0
across various ﬁelds including,1,1,1.0
various ﬁelds including imbalanced,1,1,1.0
ﬁelds including imbalanced learning,1,1,1.0
including imbalanced learning in,1,1,1.0
imbalanced learning in eral,1,1,1.0
learning in eral methods,1,1,1.0
in eral methods fac,1,1,1.0
eral methods fac ilitate,1,1,1.0
methods fac ilitate learning,1,1,1.0
fac ilitate learning by,1,1,1.0
ilitate learning by maximizing,1,1,1.0
learning by maximizing the,1,1,1.0
by maximizing the separation,1,1,1.0
maximizing the separation margin,1,1,1.0
the separation margin between,1,1,1.0
separation margin between concepts,1,1,1.0
margin between concepts in,1,1,1.0
between concepts in linearly,1,1,1.0
concepts in linearly separable,1,1,1.0
in linearly separable feature,1,1,1.0
linearly separable feature spaces,1,1,1.0
separable feature spaces for,1,1,1.0
feature spaces for instance,1,1,1.0
spaces for instance the,1,1,1.0
for instance the alignment,1,1,1.0
instance the alignment gorithm,1,1,1.0
the alignment gorithm was,1,1,1.0
alignment gorithm was proposed,1,1,1.0
gorithm was proposed in,1,1,1.0
was proposed in and,2,1,2.0
proposed in and in,1,1,1.0
in and in which,1,1,1.0
and in which imbalanced,1,1,1.0
in which imbalanced data,1,1,1.0
which imbalanced data is,1,1,1.0
imbalanced data is used,1,1,1.0
data is used as,1,1,1.0
is used as information,1,1,1.0
used as information prior,1,1,1.0
as information prior to,1,1,1.0
information prior to adjusting,1,1,1.0
prior to adjusting the,1,1,1.0
to adjusting the kernel,1,1,1.0
adjusting the kernel matrix,1,1,1.0
kernel matrix in order,1,1,1.0
matrix in order to,1,1,1.0
in order to facilitate,1,1,1.0
order to facilitate svm,1,1,1.0
to facilitate svm learning,1,1,1.0
facilitate svm learning for,1,1,1.0
svm learning for improved,1,1,1.0
learning for improved prediction,1,1,1.0
for improved prediction accuracy,1,1,1.0
improved prediction accuracy another,1,1,1.0
prediction accuracy another example,1,1,1.0
accuracy another example of,1,1,1.0
another example of learning,1,1,1.0
example of learning presents,1,1,1.0
of learning presents a,1,1,1.0
learning presents a kernel,1,1,1.0
presents a kernel classiﬁer,1,1,1.0
a kernel classiﬁer construction,1,1,1.0
kernel classiﬁer construction algorithm,1,1,1.0
classiﬁer construction algorithm using,1,1,1.0
construction algorithm using orthogonal,1,1,1.0
algorithm using orthogonal ward,1,1,1.0
using orthogonal ward selection,1,1,1.0
orthogonal ward selection to,1,1,1.0
ward selection to optimize,1,1,1.0
selection to optimize the,1,1,1.0
to optimize the generalization,1,1,1.0
optimize the generalization model,1,1,1.0
the generalization model for,1,1,1.0
generalization model for class,1,1,1.0
model for class imbalanced,1,1,1.0
for class imbalanced learning,1,1,1.0
class imbalanced learning problems,1,1,1.0
imbalanced learning problems this,1,1,1.0
learning problems this is,1,1,1.0
problems this is accomplished,1,1,1.0
this is accomplished by,2,1,2.0
is accomplished by using,1,1,1.0
accomplished by using the,1,1,1.0
by using the regularized,1,1,1.0
using the regularized orthogonal,1,1,1.0
the regularized orthogonal weighted,1,1,1.0
regularized orthogonal weighted least,1,1,1.0
orthogonal weighted least squares,1,1,1.0
weighted least squares method,1,1,1.0
least squares method and,1,1,1.0
squares method and a,1,1,1.0
method and a model,1,1,1.0
and a model selection,1,1,1.0
a model selection criterion,1,1,1.0
model selection criterion of,1,1,1.0
selection criterion of maximal,1,1,1.0
criterion of maximal area,1,1,1.0
of maximal area under,1,1,1.0
maximal area under curve,1,1,1.0
under curve auc of,1,1,1.0
curve auc of the,1,1,1.0
auc of the receiver,1,1,1.0
of the receiver operating,1,1,1.0
the receiver operating characteristic,1,1,1.0
receiver operating characteristic roc,1,1,1.0
operating characteristic roc graph,1,1,1.0
characteristic roc graph active,1,1,1.0
roc graph active learning,1,1,1.0
graph active learning methods,1,1,1.0
active learning methods active,1,1,1.0
learning methods active learning,1,1,1.0
methods active learning methods,1,1,1.0
active learning methods were,1,1,1.0
learning methods were originally,1,1,1.0
methods were originally developed,1,1,1.0
were originally developed for,1,1,1.0
originally developed for learning,1,1,1.0
developed for learning from,1,1,1.0
for learning from datasets,1,1,1.0
learning from datasets with,1,1,1.0
from datasets with unl,1,1,1.0
datasets with unl abeled,1,1,1.0
with unl abeled instances,1,1,1.0
unl abeled instances recently,1,1,1.0
abeled instances recently tive,1,1,1.0
instances recently tive learning,1,1,1.0
recently tive learning methods,1,1,1.0
tive learning methods have,1,1,1.0
learning methods have found,1,1,1.0
methods have found increased,1,1,1.0
have found increased use,1,1,1.0
found increased use in,1,1,1.0
increased use in imbalanced,1,1,1.0
use in imbalanced learning,1,1,1.0
in imbalanced learning applications,1,1,1.0
imbalanced learning applications fo,1,1,1.0
learning applications fo r,1,1,1.0
applications fo r example,1,1,1.0
fo r example an,1,1,1.0
r example an active,1,1,1.0
example an active learning,1,1,1.0
an active learning approach,1,1,1.0
active learning approach for,1,1,1.0
learning approach for imbalanced,1,1,1.0
approach for imbalanced datasets,1,1,1.0
for imbalanced datasets was,1,1,1.0
imbalanced datasets was proposed,1,1,1.0
datasets was proposed in,1,1,1.0
proposed in and this,1,1,1.0
in and this algorithm,1,1,1.0
and this algorithm locates,1,1,1.0
this algorithm locates the,1,1,1.0
algorithm locates the most,1,1,1.0
locates the most informative,1,1,1.0
the most informative sample,1,1,1.0
most informative sample by,1,1,1.0
informative sample by evaluating,1,1,1.0
sample by evaluating a,1,1,1.0
by evaluating a small,1,1,1.0
evaluating a small ﬁxed,1,1,1.0
a small ﬁxed number,1,1,1.0
small ﬁxed number of,1,1,1.0
ﬁxed number of randomly,1,1,1.0
number of randomly selected,1,1,1.0
of randomly selected examples,1,1,1.0
randomly selected examples instead,1,1,1.0
selected examples instead of,1,1,1.0
examples instead of the,1,1,1.0
instead of the entire,1,1,1.0
of the entire dataset,1,1,1.0
the entire dataset in,1,1,1.0
entire dataset in the,1,1,1.0
dataset in the stopping,1,1,1.0
in the stopping condition,1,1,1.0
the stopping condition for,1,1,1.0
stopping condition for active,1,1,1.0
condition for active l,1,1,1.0
for active l earning,1,1,1.0
active l earning applications,1,1,1.0
l earning applications in,1,1,1.0
earning applications in word,1,1,1.0
applications in word sense,1,1,1.0
in word sense disambiguation,1,1,1.0
word sense disambiguation wsd,1,1,1.0
sense disambiguation wsd domains,1,1,1.0
disambiguation wsd domains was,1,1,1.0
wsd domains was investigated,1,1,1.0
domains was investigated to,1,1,1.0
was investigated to alleviate,1,1,1.0
investigated to alleviate the,1,1,1.0
to alleviate the complications,1,1,1.0
alleviate the complications introduced,1,1,1.0
the complications introduced by,1,1,1.0
complications introduced by ances,1,1,1.0
introduced by ances the,1,1,1.0
by ances the authors,1,1,1.0
ances the authors proposed,1,1,1.0
the authors proposed a,1,1,1.0
authors proposed a oversampling,1,1,1.0
proposed a oversampling technique,1,1,1.0
a oversampling technique to,1,1,1.0
oversampling technique to improve,1,1,1.0
technique to improve active,1,1,1.0
to improve active learning,1,1,1.0
improve active learning performance,1,1,1.0
active learning performance for,1,1,1.0
learning performance for anced,1,1,1.0
performance for anced wsd,1,1,1.0
for anced wsd applications,1,1,1.0
anced wsd applications iii,1,1,1.0
wsd applications iii ramob,1,1,1.0
applications iii ramob oost,1,1,1.0
iii ramob oost framework,1,1,1.0
ramob oost framework i,1,1,1.0
oost framework i ntegration,1,1,1.0
framework i ntegration of,1,1,1.0
i ntegration of data,1,1,1.0
ntegration of data generation,1,1,1.0
of data generation and,1,1,1.0
data generation and boosting,1,1,1.0
generation and boosting ensemble,1,1,1.0
and boosting ensemble learning,1,1,1.0
boosting ensemble learning preliminaries,1,1,1.0
ensemble learning preliminaries for,1,1,1.0
learning preliminaries for ramoboost,1,1,1.0
preliminaries for ramoboost various,1,1,1.0
for ramoboost various techniques,1,1,1.0
ramoboost various techniques exist,1,1,1.0
various techniques exist for,1,1,1.0
techniques exist for quantizing,1,1,1.0
exist for quantizing the,1,1,1.0
for quantizing the learning,1,1,1.0
quantizing the learning culty,1,1,1.0
the learning culty level,1,1,1.0
learning culty level of,1,1,1.0
culty level of an,1,1,1.0
level of an example,1,1,1.0
of an example accord,1,1,1.0
an example accord ing,1,1,1.0
example accord ing to,1,1,1.0
accord ing to the,1,1,1.0
ing to the distance,1,1,1.0
to the distance between,1,1,1.0
the distance between minority,1,1,1.0
distance between minority and,1,1,1.0
minority and majority cases,1,1,1.0
and majority cases an,1,1,1.0
majority cases an intuitive,1,1,1.0
cases an intuitive method,1,1,1.0
an intuitive method can,1,1,1.0
intuitive method can be,1,1,1.0
method can be formulated,1,1,1.0
can be formulated as,1,1,1.0
be formulated as follows,1,1,1.0
formulated as follows first,1,1,1.0
as follows first l,1,1,1.0
follows first l ocate,1,1,1.0
first l ocate the,1,1,1.0
l ocate the center,1,1,1.0
ocate the center mean,1,1,1.0
the center mean of,1,1,1.0
center mean of the,1,1,1.0
mean of the majority,1,1,1.0
of the majority examples,1,1,1.0
the majority examples in,1,1,1.0
majority examples in the,2,1,2.0
examples in the featur,1,1,1.0
in the featur e,1,1,1.0
the featur e space,1,1,1.0
featur e space then,1,1,1.0
e space then calculate,1,1,1.0
space then calculate the,1,1,1.0
then calculate the euclidean,1,1,1.0
calculate the euclidean distance,1,1,1.0
the euclidean distance between,2,1,2.0
euclidean distance between each,1,1,1.0
distance between each minority,1,1,1.0
between each minority example,1,1,1.0
each minority example and,4,1,4.0
minority example and the,1,1,1.0
example and the center,1,1,1.0
and the center the,1,1,1.0
the center the set,1,1,1.0
center the set of,1,1,1.0
the set of minimal,1,1,1.0
set of minimal distanced,1,1,1.0
of minimal distanced minority,1,1,1.0
minimal distanced minority examples,1,1,1.0
distanced minority examples up,1,1,1.0
minority examples up to,1,1,1.0
examples up to some,1,1,1.0
up to some threshold,1,1,1.0
to some threshold then,1,1,1.0
some threshold then would,1,1,1.0
threshold then would represent,1,1,1.0
then would represent the,1,1,1.0
would represent the most,1,1,1.0
represent the most informative,1,1,1.0
the most informative examples,1,1,1.0
most informative examples fig,1,1,1.0
informative examples fig a,1,1,1.0
examples fig a illustrates,1,1,1.0
fig a illustrates this,1,1,1.0
a illustrates this idea,1,1,1.0
illustrates this idea in,1,1,1.0
this idea in this,1,1,1.0
idea in this ﬁgure,1,1,1.0
in this ﬁgure the,1,1,1.0
this ﬁgure the stars,1,1,1.0
ﬁgure the stars and,1,1,1.0
the stars and circles,1,1,1.0
stars and circles represent,1,1,1.0
and circles represent the,1,1,1.0
circles represent the minority,1,1,1.0
represent the minority and,1,1,1.0
and majority classes respectively,1,1,1.0
majority classes respectively and,1,1,1.0
classes respectively and the,1,1,1.0
respectively and the triangle,1,1,1.0
and the triangle within,1,1,1.0
the triangle within the,1,1,1.0
triangle within the majority,1,1,1.0
within the majority data,1,1,1.0
the majority data sents,1,1,1.0
majority data sents the,1,1,1.0
data sents the approximate,1,1,1.0
sents the approximate center,1,1,1.0
the approximate center the,1,1,1.0
approximate center the euclidean,1,1,1.0
center the euclidean distances,1,1,1.0
the euclidean distances between,1,1,1.0
euclidean distances between minority,1,1,1.0
distances between minority examples,1,1,1.0
between minority examples a,1,1,1.0
minority examples a n,1,1,1.0
examples a n d,1,1,1.0
a n d and,1,1,1.0
n d and the,1,1,1.0
d and the majority,1,1,1.0
the majority class center,1,1,1.0
majority class center would,1,1,1.0
class center would be,1,1,1.0
center would be calculated,1,1,1.0
would be calculated respectively,1,1,1.0
be calculated respectively as,1,1,1.0
calculated respectively as efﬁcient,1,1,1.0
respectively as efﬁcient as,1,1,1.0
as efﬁcient as it,1,1,1.0
efﬁcient as it appears,1,1,1.0
as it appears this,1,1,1.0
it appears this method,1,1,1.0
appears this method exhibits,1,1,1.0
this method exhibits a,1,1,1.0
method exhibits a critical,1,1,1.0
exhibits a critical ﬂaw,1,1,1.0
a critical ﬂaw it,1,1,1.0
critical ﬂaw it assumes,1,1,1.0
ﬂaw it assumes that,1,1,1.0
it assumes that there,1,1,1.0
assumes that there are,1,1,1.0
there are no disjuncts,1,1,1.0
are no disjuncts within,1,1,1.0
no disjuncts within the,1,1,1.0
disjuncts within the majority,1,1,1.0
within the majority class,1,1,1.0
the majority class concept,1,1,1.0
majority class concept otherwise,1,1,1.0
class concept otherwise there,1,1,1.0
concept otherwise there may,1,1,1.0
otherwise there may ex,1,1,1.0
there may ex ist,1,1,1.0
may ex ist several,1,1,1.0
ex ist several for,1,1,1.0
ist several for the,1,1,1.0
several for the majority,1,1,1.0
class and the task,1,1,1.0
and the task of,1,1,1.0
the task of computing,1,1,1.0
task of computing the,1,1,1.0
of computing the euclidean,1,1,1.0
computing the euclidean distance,1,1,1.0
the euclidean distance against,1,1,1.0
euclidean distance against the,1,1,1.0
distance against the centers,1,1,1.0
against the centers beco,1,1,1.0
the centers beco mes,1,1,1.0
centers beco mes complicated,1,1,1.0
beco mes complicated and,1,1,1.0
mes complicated and requires,1,1,1.0
complicated and requires careful,1,1,1.0
and requires careful examination,1,1,1.0
requires careful examination we,1,1,1.0
careful examination we highlight,1,1,1.0
examination we highlight this,1,1,1.0
we highlight this issue,1,1,1.0
highlight this issue in,1,1,1.0
this issue in fig,1,1,1.0
issue in fig b,1,1,1.0
in fig b ieee,1,1,1.0
fig b ieee transactions,1,1,1.0
b ieee transactions on,1,1,1.0
vol no october a,2,1,2.0
no october a b,2,1,2.0
october a b c,2,1,2.0
a b c majority,1,1,1.0
b c majority cluster,1,1,1.0
c majority cluster majority,1,1,1.0
majority cluster majority cluster,1,1,1.0
cluster majority cluster fig,1,1,1.0
majority cluster fig cluster,1,1,1.0
cluster fig cluster and,1,1,1.0
fig cluster and distance,1,1,1.0
cluster and distance calculation,1,1,1.0
and distance calculation based,1,1,1.0
distance calculation based on,1,1,1.0
calculation based on euclidean,1,1,1.0
based on euclidean distance,1,1,1.0
on euclidean distance a,1,1,1.0
euclidean distance a int,1,1,1.0
distance a int uitive,1,1,1.0
a int uitive approach,1,1,1.0
int uitive approach to,1,1,1.0
uitive approach to decide,1,1,1.0
approach to decide the,1,1,1.0
to decide the infor,1,1,1.0
decide the infor mative,1,1,1.0
the infor mative data,1,1,1.0
infor mative data examples,1,1,1.0
mative data examples based,1,1,1.0
data examples based on,1,1,1.0
examples based on euclid,1,1,1.0
based on euclid ean,1,1,1.0
on euclid ean distance,1,1,1.0
euclid ean distance b,1,1,1.0
ean distance b potential,1,1,1.0
distance b potential dilemma,1,1,1.0
b potential dilemma by,1,1,1.0
potential dilemma by applying,1,1,1.0
dilemma by applying this,1,1,1.0
by applying this in,1,1,1.0
applying this in tuitive,1,1,1.0
this in tuitive approach,1,1,1.0
in tuitive approach c,1,1,1.0
tuitive approach c proposed,1,1,1.0
approach c proposed approach,1,1,1.0
c proposed approach by,1,1,1.0
proposed approach by using,1,1,1.0
approach by using k,1,1,1.0
by using k nearest,1,1,1.0
using k nearest neighbors,1,1,1.0
k nearest neighbors of,5,1,5.0
of each minority example,9,1,9.0
minority example and using,1,1,1.0
example and using this,1,1,1.0
and using this value,1,1,1.0
using this value to,1,1,1.0
this value to decide,1,1,1.0
value to decide the,1,1,1.0
to decide the weight,1,1,1.0
decide the weight to,1,1,1.0
the weight to ﬁnd,1,1,1.0
weight to ﬁnd the,1,1,1.0
to ﬁnd the informative,1,1,1.0
ﬁnd the informative data,1,1,1.0
the informative data examples,1,1,1.0
informative data examples generated,1,1,1.0
data examples generated synthetic,1,1,1.0
examples generated synthetic instance,1,1,1.0
generated synthetic instance xi,1,1,1.0
synthetic instance xi xiˆ,1,1,1.0
instance xi xiˆ fig,1,1,1.0
xi xiˆ fig synthetic,1,1,1.0
xiˆ fig synthetic data,1,1,1.0
fig synthetic data generation,1,1,1.0
synthetic data generation based,1,1,1.0
data generation based on,1,1,1.0
generation based on the,1,1,1.0
based on the smote,1,1,1.0
on the smote algorithm,1,1,1.0
smote algorithm the majority,1,1,1.0
algorithm the majority class,1,1,1.0
the majority class contains,1,1,1.0
majority class contains two,1,1,1.0
class contains two clusters,1,1,1.0
contains two clusters and,1,1,1.0
two clusters and each,1,1,1.0
clusters and each minority,1,1,1.0
and each minority example,1,1,1.0
each minority example has,2,1,2.0
minority example has to,1,1,1.0
example has to locate,1,1,1.0
has to locate its,1,1,1.0
to locate its respective,1,1,1.0
locate its respective closest,1,1,1.0
its respective closest cluster,1,1,1.0
respective closest cluster center,1,1,1.0
closest cluster center before,1,1,1.0
cluster center before calculating,1,1,1.0
center before calculating the,1,1,1.0
before calculating the dist,1,1,1.0
calculating the dist ance,1,1,1.0
the dist ance to,1,1,1.0
dist ance to the,1,1,1.0
ance to the majority,1,1,1.0
to the majority center,1,1,1.0
the majority center ramoboost,1,1,1.0
majority center ramoboost targets,1,1,1.0
center ramoboost targets this,1,1,1.0
ramoboost targets this ﬂaw,1,1,1.0
targets this ﬂaw in,1,1,1.0
this ﬂaw in the,1,1,1.0
ﬂaw in the following,1,1,1.0
in the following way,1,1,1.0
the following way stead,1,1,1.0
following way stead of,1,1,1.0
way stead of searching,1,1,1.0
stead of searching for,1,1,1.0
of searching for the,1,1,1.0
searching for the center,1,1,1.0
for the center s,1,1,1.0
the center s of,1,1,1.0
center s of the,1,1,1.0
s of the majority,1,1,1.0
of the majority dataset,1,1,1.0
the majority dataset ramoboost,1,1,1.0
majority dataset ramoboost determines,1,1,1.0
dataset ramoboost determines the,1,1,1.0
ramoboost determines the number,1,1,1.0
determines the number of,1,1,1.0
number of majority examples,2,1,2.0
of majority examples that,1,1,1.0
majority examples that are,2,1,2.0
examples that are within,2,1,2.0
that are within the,2,1,2.0
are within the k,1,1,1.0
within the k nearest,2,1,2.0
the k nearest neighbors,5,1,5.0
minority example and assigns,1,1,1.0
example and assigns this,1,1,1.0
and assigns this value,1,1,1.0
assigns this value as,1,1,1.0
this value as the,1,1,1.0
value as the information,1,1,1.0
as the information weight,1,1,1.0
the information weight we,1,1,1.0
information weight we illustrate,1,1,1.0
weight we illustrate this,1,1,1.0
we illustrate this idea,1,1,1.0
illustrate this idea in,1,1,1.0
this idea in fig,1,1,1.0
idea in fig c,1,1,1.0
in fig c here,1,1,1.0
fig c here the,1,1,1.0
c here the highlighted,1,1,1.0
here the highlighted areas,1,1,1.0
the highlighted areas surrounded,1,1,1.0
highlighted areas surrounded by,1,1,1.0
areas surrounded by dashed,1,1,1.0
surrounded by dashed circles,1,1,1.0
by dashed circles represents,1,1,1.0
dashed circles represents the,1,1,1.0
circles represents the k,1,1,1.0
represents the k nearest,1,1,1.0
the k nearest neighbor,1,1,1.0
k nearest neighbor search,1,1,1.0
nearest neighbor search area,1,1,1.0
neighbor search area for,1,1,1.0
search area for each,1,1,1.0
area for each minority,1,1,1.0
for each minority example,9,1,9.0
each minority example a,1,1,1.0
minority example a n,1,1,1.0
example a n d,1,1,1.0
a n d k,1,1,1.0
n d k n,1,1,1.0
d k n this,1,1,1.0
k n this case,1,1,1.0
n this case accordingly,1,1,1.0
this case accordingly ther,1,1,1.0
case accordingly ther e,1,1,1.0
accordingly ther e are,1,1,1.0
ther e are and,1,1,1.0
e are and majority,1,1,1.0
are and majority examples,1,1,1.0
and majority examples that,1,1,1.0
are within the four,1,1,1.0
within the four nearest,1,1,1.0
the four nearest neighbors,1,1,1.0
four nearest neighbors of,1,1,1.0
nearest neighbors of a,1,1,1.0
neighbors of a n,1,1,1.0
of a n d,1,1,1.0
a n d respectively,1,1,1.0
n d respectively this,1,1,1.0
d respectively this means,1,1,1.0
respectively this means the,1,1,1.0
this means the minority,1,1,1.0
means the minority example,1,1,1.0
the minority example is,1,1,1.0
minority example is more,1,1,1.0
example is more likely,1,1,1.0
is more likely to,1,1,1.0
likely to be located,1,1,1.0
to be located near,1,1,1.0
be located near the,1,1,1.0
located near the decision,1,1,1.0
near the decision boundary,2,1,2.0
the decision boundary is,1,1,1.0
decision boundary is a,1,1,1.0
boundary is a example,1,1,1.0
is a example and,1,1,1.0
a example and therefore,1,1,1.0
example and therefore more,1,1,1.0
and therefore more synthetic,1,1,1.0
therefore more synthetic samples,1,1,1.0
more synthetic samples should,1,1,1.0
synthetic samples should be,1,1,1.0
samples should be generated,1,1,1.0
should be generated for,1,1,1.0
be generated for to,2,1,2.0
generated for to force,1,1,1.0
for to force the,1,1,1.0
to force the ﬁnal,1,1,1.0
force the ﬁnal learning,1,1,1.0
the ﬁnal learning hypothesis,1,1,1.0
ﬁnal learning hypothesis to,1,1,1.0
learning hypothesis to be,1,1,1.0
hypothesis to be more,1,1,1.0
to be more focused,2,1,2.0
be more focused on,3,1,3.0
more focused on the,2,1,2.0
focused on the difﬁcult,2,1,2.0
on the difﬁcult learning,1,1,1.0
the difﬁcult learning regions,1,1,1.0
difﬁcult learning regions our,1,1,1.0
learning regions our previous,1,1,1.0
regions our previous work,1,1,1.0
our previous work adaptive,1,1,1.0
previous work adaptive synthetic,1,1,1.0
work adaptive synthetic adasyn,1,1,1.0
adaptive synthetic adasyn also,1,1,1.0
synthetic adasyn also applies,1,1,1.0
adasyn also applies a,1,1,1.0
also applies a similar,1,1,1.0
applies a similar idea,1,1,1.0
a similar idea to,1,1,1.0
similar idea to assess,1,1,1.0
idea to assess the,1,1,1.0
to assess the difﬁculty,1,1,1.0
assess the difﬁculty level,1,1,1.0
the difﬁculty level of,1,1,1.0
difﬁculty level of minority,1,1,1.0
level of minority data,1,1,1.0
of minority data the,1,1,1.0
minority data the difference,1,1,1.0
data the difference is,1,1,1.0
the difference is that,1,1,1.0
difference is that instead,1,1,1.0
is that instead of,1,1,1.0
that instead of directly,1,1,1.0
instead of directly specifying,1,1,1.0
of directly specifying the,1,1,1.0
directly specifying the number,1,1,1.0
specifying the number of,1,1,1.0
number of synthetic instances,8,1,8.0
of synthetic instances generated,3,1,3.0
synthetic instances generated for,1,1,1.0
instances generated for each,1,1,1.0
generated for each minority,3,1,3.0
each minority example as,1,1,1.0
minority example as in,1,1,1.0
example as in adasyn,1,1,1.0
as in adasyn ramoboost,1,1,1.0
in adasyn ramoboost adopts,1,1,1.0
adasyn ramoboost adopts this,1,1,1.0
ramoboost adopts this mechanism,1,1,1.0
adopts this mechanism to,1,1,1.0
this mechanism to determi,1,1,1.0
mechanism to determi ne,1,1,1.0
to determi ne the,1,1,1.0
determi ne the chance,1,1,1.0
ne the chance of,1,1,1.0
the chance of each,1,1,1.0
chance of each minority,1,1,1.0
each minority example for,3,1,3.0
minority example for generating,2,1,2.0
example for generating the,1,1,1.0
for generating the synthetic,2,1,2.0
generating the synthetic instances,2,1,2.0
the synthetic instances before,1,1,1.0
synthetic instances before presenting,1,1,1.0
instances before presenting the,1,1,1.0
before presenting the details,1,1,1.0
presenting the details of,1,1,1.0
the details of the,1,1,1.0
details of the ramoboost,1,1,1.0
of the ramoboost algorithm,1,1,1.0
the ramoboost algorithm we,2,1,2.0
ramoboost algorithm we brieﬂy,1,1,1.0
algorithm we brieﬂy discuss,1,1,1.0
we brieﬂy discuss the,1,1,1.0
brieﬂy discuss the data,1,1,1.0
discuss the data generation,1,1,1.0
the data generation mechanisms,1,1,1.0
data generation mechanisms of,1,1,1.0
generation mechanisms of smote,1,1,1.0
mechanisms of smote and,1,1,1.0
of smote and adasyn,3,1,3.0
smote and adasyn which,1,1,1.0
and adasyn which motivated,1,1,1.0
adasyn which motivated the,1,1,1.0
which motivated the work,1,1,1.0
motivated the work presented,1,1,1.0
the work presented in,1,1,1.0
work presented in this,1,1,1.0
presented in this paper,1,1,1.0
in this paper and,2,1,2.0
this paper and can,1,1,1.0
paper and can provide,1,1,1.0
and can provide a,1,1,1.0
can provide a better,1,1,1.0
provide a better understanding,1,1,1.0
a better understanding of,1,1,1.0
better understanding of ramoboost,1,1,1.0
understanding of ramoboost in,1,1,1.0
of ramoboost in the,1,1,1.0
ramoboost in the smote,1,1,1.0
in the smote method,1,1,1.0
the smote method the,1,1,1.0
smote method the k,1,1,1.0
method the k nearest,1,1,1.0
neighbors of the minority,2,1,2.0
of the minority data,3,1,3.0
the minority data where,1,1,1.0
minority data where k,1,1,1.0
data where k is,1,1,1.0
where k is a,1,1,1.0
k is a parameter,1,1,1.0
is a parameter are,1,1,1.0
a parameter are adasyn,1,1,1.0
parameter are adasyn smote,1,1,1.0
are adasyn smote fig,1,1,1.0
adasyn smote fig representation,1,1,1.0
smote fig representation of,1,1,1.0
representation of the synthetic,1,1,1.0
of the synthetic instance,1,1,1.0
the synthetic instance proportions,1,1,1.0
synthetic instance proportions for,1,1,1.0
instance proportions for smote,1,1,1.0
proportions for smote and,1,1,1.0
for smote and adasyn,1,1,1.0
smote and adasyn identiﬁed,1,1,1.0
and adasyn identiﬁed for,1,1,1.0
adasyn identiﬁed for each,1,1,1.0
identiﬁed for each minority,1,1,1.0
each minority example the,3,1,3.0
minority example the are,1,1,1.0
example the are deﬁned,1,1,1.0
the are deﬁned as,1,1,1.0
are deﬁned as the,1,1,1.0
deﬁned as the set,1,1,1.0
as the set of,1,1,1.0
the set of k,1,1,1.0
set of k instances,1,1,1.0
of k instances such,1,1,1.0
k instances such that,1,1,1.0
instances such that the,1,1,1.0
such that the euclidean,1,1,1.0
that the euclidean distance,2,1,2.0
euclidean distance between the,1,1,1.0
distance between the minority,1,1,1.0
between the minority example,1,1,1.0
the minority example under,7,1,7.0
minority example under consideration,7,1,7.0
example under consideration and,2,1,2.0
under consideration and the,1,1,1.0
consideration and the examples,1,1,1.0
and the examples of,1,1,1.0
of the minority set,1,1,1.0
the minority set are,1,1,1.0
minority set are of,1,1,1.0
set are of minimal,1,1,1.0
are of minimal length,1,1,1.0
of minimal length along,1,1,1.0
minimal length along the,1,1,1.0
length along the feature,1,1,1.0
along the feature space,1,1,1.0
the feature space synthetic,1,1,1.0
feature space synthetic samples,1,1,1.0
space synthetic samples are,1,1,1.0
synthetic samples are then,1,1,1.0
samples are then created,1,1,1.0
are then created by,1,1,1.0
then created by randomly,1,1,1.0
created by randomly selecting,1,1,1.0
by randomly selecting one,1,1,1.0
randomly selecting one of,1,1,1.0
selecting one of the,1,1,1.0
one of the k,1,1,1.0
of the k nearest,1,1,1.0
k nearest neighbors and,1,1,1.0
nearest neighbors and multiplying,1,1,1.0
neighbors and multiplying the,1,1,1.0
and multiplying the corresponding,1,1,1.0
multiplying the corresponding feature,1,1,1.0
the corresponding feature vector,1,1,1.0
corresponding feature vector difference,1,1,1.0
feature vector difference with,1,1,1.0
vector difference with a,1,1,1.0
difference with a random,1,1,1.0
with a random number,1,1,1.0
random number between xnew,1,1,1.0
number between xnew xi,1,1,1.0
between xnew xi ˆxi,1,1,1.0
xnew xi ˆxi xi,1,1,1.0
xi ˆxi xi where,1,1,1.0
ˆxi xi where xi,1,1,1.0
xi where xi is,1,1,1.0
where xi is the,1,1,1.0
xi is the minority,1,1,1.0
is the minority example,1,1,1.0
example under consideration ˆxi,1,1,1.0
under consideration ˆxi is,1,1,1.0
consideration ˆxi is one,1,1,1.0
ˆxi is one of,1,1,1.0
one of the neighbors,1,1,1.0
of the neighbors minority,1,1,1.0
the neighbors minority class,1,1,1.0
neighbors minority class for,1,1,1.0
minority class for xi,1,1,1.0
class for xi a,1,1,1.0
for xi a n,1,1,1.0
xi a n is,1,1,1.0
a n is a,1,1,1.0
n is a random,1,1,1.0
a random number therefore,1,1,1.0
random number therefore the,1,1,1.0
number therefore the resulting,1,1,1.0
therefore the resulting synthetic,1,1,1.0
the resulting synthetic instance,1,1,1.0
resulting synthetic instance according,1,1,1.0
synthetic instance according to,1,1,1.0
instance according to is,1,1,1.0
according to is a,1,1,1.0
to is a point,1,1,1.0
is a point along,1,1,1.0
a point along the,1,1,1.0
point along the line,1,1,1.0
along the line segment,1,1,1.0
the line segment joining,1,1,1.0
line segment joining the,1,1,1.0
segment joining the example,1,1,1.0
joining the example under,1,1,1.0
the example under consideration,2,1,2.0
example under consideration xi,1,1,1.0
under consideration xi and,1,1,1.0
consideration xi and the,1,1,1.0
xi and the randomly,1,1,1.0
and the randomly selected,1,1,1.0
the randomly selected k,1,1,1.0
randomly selected k nearest,1,1,1.0
selected k nearest neighbor,1,1,1.0
k nearest neighbor ˆxi,1,1,1.0
nearest neighbor ˆxi fig,1,1,1.0
neighbor ˆxi fig illustrates,1,1,1.0
ˆxi fig illustrates this,1,1,1.0
fig illustrates this procedure,1,1,1.0
illustrates this procedure in,1,1,1.0
this procedure in this,1,1,1.0
procedure in this way,1,1,1.0
in this way smote,1,1,1.0
this way smote generates,1,1,1.0
way smote generates the,1,1,1.0
smote generates the same,1,1,1.0
generates the same number,1,1,1.0
same number of synthetic,1,1,1.0
of synthetic instances for,4,1,4.0
synthetic instances for each,3,1,3.0
instances for each minority,3,1,3.0
for each minority exam,1,1,1.0
each minority exam ple,1,1,1.0
minority exam ple uniform,1,1,1.0
exam ple uniform di,1,1,1.0
ple uniform di stribution,1,1,1.0
uniform di stribution adasyn,1,1,1.0
di stribution adasyn also,1,1,1.0
stribution adasyn also uses,1,1,1.0
adasyn also uses feature,1,1,1.0
also uses feature int,1,1,1.0
uses feature int erpolation,1,1,1.0
feature int erpolation to,1,1,1.0
int erpolation to generate,1,1,1.0
erpolation to generate thetic,1,1,1.0
to generate thetic instances,1,1,1.0
generate thetic instances the,1,1,1.0
thetic instances the difference,1,1,1.0
instances the difference from,1,1,1.0
the difference from smote,1,1,1.0
difference from smote is,1,1,1.0
from smote is instead,1,1,1.0
smote is instead of,1,1,1.0
is instead of applying,1,1,1.0
instead of applying a,1,1,1.0
of applying a uniform,1,1,1.0
applying a uniform distribution,1,1,1.0
a uniform distribution for,1,1,1.0
uniform distribution for data,1,1,1.0
distribution for data generation,1,1,1.0
for data generation like,1,1,1.0
data generation like smote,1,1,1.0
generation like smote adasyn,1,1,1.0
like smote adasyn uses,1,1,1.0
smote adasyn uses a,1,1,1.0
adasyn uses a density,1,1,1.0
uses a density distribution,1,1,1.0
a density distribution ˆri,1,1,1.0
density distribution ˆri as,1,1,1.0
distribution ˆri as a,1,1,1.0
ˆri as a rion,1,1,1.0
as a rion to,1,1,1.0
a rion to automatically,1,1,1.0
rion to automatically decide,1,1,1.0
to automatically decide the,1,1,1.0
automatically decide the number,1,1,1.0
decide the number of,1,1,1.0
number of synthetic samples,1,1,1.0
of synthetic samples that,1,1,1.0
synthetic samples that need,1,1,1.0
samples that need to,1,1,1.0
to be generated for,2,1,2.0
be generated for each,1,1,1.0
minority example the density,1,1,1.0
example the density distribution,1,1,1.0
the density distribution criteria,1,1,1.0
density distribution criteria is,1,1,1.0
distribution criteria is deﬁned,1,1,1.0
criteria is deﬁned as,1,1,1.0
is deﬁned as the,1,1,1.0
deﬁned as the normalized,1,1,1.0
as the normalized number,1,1,1.0
the normalized number of,1,1,1.0
normalized number of majority,1,1,1.0
number of majority cases,6,1,6.0
of majority cases within,2,1,2.0
majority cases within the,2,1,2.0
cases within the neighbor,1,1,1.0
within the neighbor of,1,1,1.0
the neighbor of each,1,1,1.0
neighbor of each minority,1,1,1.0
each minority example fig,1,1,1.0
minority example fig shows,1,1,1.0
example fig shows the,1,1,1.0
fig shows the proportion,1,1,1.0
shows the proportion of,1,1,1.0
the proportion of synthetic,1,1,1.0
proportion of synthetic instances,1,1,1.0
instances generated by smote,1,1,1.0
generated by smote and,1,1,1.0
by smote and adasyn,1,1,1.0
smote and adasyn based,1,1,1.0
and adasyn based on,1,1,1.0
adasyn based on neighbors,1,1,1.0
based on neighbors for,1,1,1.0
on neighbors for minority,1,1,1.0
neighbors for minority examples,1,1,1.0
for minority examples through,1,1,1.0
minority examples through from,1,1,1.0
examples through from fig,1,1,1.0
through from fig it,1,1,1.0
from fig it is,1,1,1.0
fig it is clear,1,1,1.0
is clear that smote,1,1,1.0
clear that smote uniformly,1,1,1.0
that smote uniformly assigns,1,1,1.0
smote uniformly assigns the,1,1,1.0
uniformly assigns the number,1,1,1.0
assigns the number of,1,1,1.0
of synthetic instances to,1,1,1.0
synthetic instances to be,1,1,1.0
instances to be generated,1,1,1.0
generated for to and,1,1,1.0
for to and adasyn,1,1,1.0
to and adasyn uses,1,1,1.0
and adasyn uses a,1,1,1.0
adasyn uses a differen,1,1,1.0
uses a differen t,1,1,1.0
a differen t distribution,1,1,1.0
differen t distribution to,1,1,1.0
t distribution to determinechen,1,1,1.0
distribution to determinechen et,1,1,1.0
to determinechen et al,1,1,1.0
determinechen et al ranked,1,1,1.0
in boosting the number,1,1,1.0
boosting the number of,1,1,1.0
synthetic instances for to,1,1,1.0
instances for to d,1,1,1.0
for to d a,1,1,1.0
to d a s,1,1,1.0
d a s y,1,1,1.0
a s y n,1,1,1.0
s y n ﬁrst,1,1,1.0
y n ﬁrst calculates,1,1,1.0
n ﬁrst calculates the,1,1,1.0
ﬁrst calculates the number,1,1,1.0
calculates the number of,1,1,1.0
the number of m,1,1,1.0
number of m ajority,1,1,1.0
of m ajority cases,1,1,1.0
m ajority cases within,1,1,1.0
ajority cases within the,1,1,1.0
cases within the nearest,1,1,1.0
within the nearest neighbors,1,1,1.0
nearest neighbors of to,1,1,1.0
neighbors of to ﬁrst,1,1,1.0
of to ﬁrst which,1,1,1.0
to ﬁrst which is,1,1,1.0
ﬁrst which is in,1,1,1.0
which is in this,1,1,1.0
is in this example,1,1,1.0
in this example and,1,1,1.0
this example and normalizes,1,1,1.0
example and normalizes this,1,1,1.0
and normalizes this into,1,1,1.0
normalizes this into the,1,1,1.0
this into the density,1,1,1.0
into the density distribution,1,1,1.0
the density distribution which,1,1,1.0
density distribution which is,1,1,1.0
distribution which is used,1,1,1.0
is used to bias,1,1,1.0
used to bias the,1,1,1.0
to bias the data,1,1,1.0
bias the data generation,1,1,1.0
the data generation pro,1,1,1.0
data generation pro cess,1,1,1.0
generation pro cess algorithm,1,1,1.0
pro cess algorithm adasyn,1,1,1.0
cess algorithm adasyn one,1,1,1.0
algorithm adasyn one issue,1,1,1.0
adasyn one issue worth,1,1,1.0
one issue worth noting,1,1,1.0
issue worth noting is,1,1,1.0
worth noting is that,1,1,1.0
noting is that adasyn,1,1,1.0
is that adasyn does,1,1,1.0
that adasyn does not,1,1,1.0
adasyn does not generate,1,1,1.0
does not generate instances,1,1,1.0
not generate instances for,1,1,1.0
generate instances for minority,1,1,1.0
instances for minority examples,1,1,1.0
for minority examples with,1,1,1.0
minority examples with no,1,1,1.0
examples with no majority,1,1,1.0
with no majority cases,1,1,1.0
no majority cases in,1,1,1.0
majority cases in their,1,1,1.0
cases in their neighbors,1,1,1.0
in their neighbors like,1,1,1.0
their neighbors like in,1,1,1.0
neighbors like in fig,1,1,1.0
like in fig ramoboost,1,1,1.0
in fig ramoboost learning,1,1,1.0
fig ramoboost learning algorithm,1,1,1.0
ramoboost learning algorithm the,1,1,1.0
learning algorithm the objective,1,1,1.0
algorithm the objective of,1,1,1.0
the objective of ramoboost,1,1,1.0
objective of ramoboost is,1,1,1.0
of ramoboost is twofold,1,1,1.0
ramoboost is twofold to,1,1,1.0
is twofold to reduce,1,1,1.0
twofold to reduce the,1,1,1.0
to reduce the induction,1,1,1.0
reduce the induction biases,1,1,1.0
the induction biases introduced,1,1,1.0
induction biases introduced from,1,1,1.0
biases introduced from imbalanced,1,1,1.0
introduced from imbalanced data,1,1,1.0
from imbalanced data and,1,1,1.0
imbalanced data and to,1,1,1.0
data and to adaptively,1,1,1.0
and to adaptively learn,1,1,1.0
to adaptively learn information,1,1,1.0
adaptively learn information from,1,1,1.0
learn information from the,1,1,1.0
information from the data,1,1,1.0
from the data distribution,1,1,1.0
the data distribution this,1,1,1.0
data distribution this is,1,1,1.0
distribution this is achieved,1,1,1.0
this is achieved in,1,1,1.0
is achieved in two,1,1,1.0
achieved in two respects,1,1,1.0
in two respects first,1,1,1.0
two respects first an,1,1,1.0
respects first an adaptive,1,1,1.0
first an adaptive weight,1,1,1.0
an adaptive weight adjustment,1,1,1.0
adaptive weight adjustment procedure,1,1,1.0
weight adjustment procedure is,1,1,1.0
adjustment procedure is embedded,1,1,1.0
procedure is embedded in,1,1,1.0
is embedded in ramoboost,1,1,1.0
embedded in ramoboost that,1,1,1.0
in ramoboost that shifts,1,1,1.0
ramoboost that shifts the,1,1,1.0
that shifts the decision,1,1,1.0
shifts the decision boundary,1,1,1.0
decision boundary toward the,2,1,2.0
boundary toward the examples,2,1,2.0
toward the examples from,1,1,1.0
the examples from both,1,1,1.0
examples from both the,1,1,1.0
from both the minority,1,1,1.0
both the minority and,1,1,1.0
and majority classes second,1,1,1.0
majority classes second a,1,1,1.0
classes second a ranked,1,1,1.0
second a ranked sampling,1,1,1.0
a ranked sampling probability,1,1,1.0
ranked sampling probability distribution,1,1,1.0
sampling probability distribution is,1,1,1.0
probability distribution is used,1,1,1.0
distribution is used to,1,1,1.0
is used to generate,1,1,1.0
to generate synthetic minority,1,1,1.0
generate synthetic minority instances,2,1,2.0
synthetic minority instances to,1,1,1.0
minority instances to balance,1,1,1.0
instances to balance the,1,1,1.0
to balance the skewed,1,1,1.0
balance the skewed distribution,1,1,1.0
the skewed distribution motivated,1,1,1.0
skewed distribution motivated by,1,1,1.0
distribution motivated by the,1,1,1.0
motivated by the smote,1,1,1.0
by the smote smoteboost,1,1,1.0
the smote smoteboost and,1,1,1.0
smote smoteboost and adasyn,1,1,1.0
smoteboost and adasyn rithms,1,1,1.0
and adasyn rithms ramoboost,1,1,1.0
adasyn rithms ramoboost facilitates,1,1,1.0
rithms ramoboost facilitates imbalanced,1,1,1.0
ramoboost facilitates imbalanced learning,1,1,1.0
facilitates imbalanced learning by,1,1,1.0
imbalanced learning by tively,1,1,1.0
learning by tively developing,1,1,1.0
by tively developing an,1,1,1.0
tively developing an ensemble,1,1,1.0
developing an ensemble of,1,1,1.0
an ensemble of hypotheses,1,1,1.0
ensemble of hypotheses however,1,1,1.0
of hypotheses however unlike,1,1,1.0
hypotheses however unlike smote,1,1,1.0
however unlike smote which,1,1,1.0
unlike smote which samples,1,1,1.0
smote which samples minority,1,1,1.0
which samples minority examples,1,1,1.0
samples minority examples indiscriminately,1,1,1.0
minority examples indiscriminately and,1,1,1.0
examples indiscriminately and uniformly,1,1,1.0
indiscriminately and uniformly ramoboost,1,1,1.0
and uniformly ramoboost evaluates,1,1,1.0
uniformly ramoboost evaluates the,1,1,1.0
ramoboost evaluates the potential,1,1,1.0
evaluates the potential learning,1,1,1.0
the potential learning contribution,1,1,1.0
potential learning contribution of,1,1,1.0
learning contribution of each,1,1,1.0
contribution of each minority,1,1,1.0
minority example and determines,1,1,1.0
example and determines their,1,1,1.0
and determines their sampling,1,1,1.0
determines their sampling weights,1,1,1.0
their sampling weights accordingly,1,1,1.0
sampling weights accordingly this,1,1,1.0
weights accordingly this is,1,1,1.0
accordingly this is achieved,1,1,1.0
this is achieved by,1,1,1.0
is achieved by calculating,1,1,1.0
achieved by calculating the,1,1,1.0
by calculating the distance,1,1,1.0
calculating the distance of,1,1,1.0
the distance of any,1,1,1.0
distance of any single,1,1,1.0
of any single minority,1,1,1.0
any single minority example,1,1,1.0
single minority example from,1,1,1.0
minority example from the,1,1,1.0
example from the set,1,1,1.0
from the set of,1,1,1.0
the set of nearest,1,1,1.0
set of nearest neighbors,1,1,1.0
nearest neighbors to determine,1,1,1.0
neighbors to determine how,1,1,1.0
to determine how greatly,1,1,1.0
determine how greatly it,1,1,1.0
how greatly it will,1,1,1.0
greatly it will beneﬁt,1,1,1.0
it will beneﬁt the,1,1,1.0
will beneﬁt the learning,1,1,1.0
beneﬁt the learning process,1,1,1.0
the learning process before,1,1,1.0
learning process before ofﬁcially,1,1,1.0
process before ofﬁcially describing,1,1,1.0
before ofﬁcially describing the,1,1,1.0
ofﬁcially describing the ramoboost,1,1,1.0
describing the ramoboost algorithm,1,1,1.0
ramoboost algorithm we would,1,1,1.0
algorithm we would like,1,1,1.0
would like to explain,1,1,1.0
like to explain several,1,1,1.0
to explain several terms,1,1,1.0
explain several terms for,1,1,1.0
several terms for improved,1,1,1.0
terms for improved readability,1,1,1.0
for improved readability in,1,1,1.0
improved readability in lieu,1,1,1.0
readability in lieu of,1,1,1.0
in lieu of directly,1,1,1.0
lieu of directly operating,1,1,1.0
of directly operating on,1,1,1.0
directly operating on the,1,1,1.0
operating on the original,1,1,1.0
on the original training,1,1,1.0
the original training dataset,1,1,1.0
original training dataset d,1,1,1.0
training dataset d ramoboost,1,1,1.0
dataset d ramoboost follows,1,1,1.0
d ramoboost follows the,1,1,1.0
ramoboost follows the classic,1,1,1.0
follows the classic procedure,1,1,1.0
the classic procedure to,1,1,1.0
classic procedure to apply,1,1,1.0
procedure to apply the,1,1,1.0
to apply the boosting,1,1,1.0
apply the boosting process,1,1,1.0
the boosting process on,1,1,1.0
boosting process on the,1,1,1.0
process on the misclassiﬁed,1,1,1.0
on the misclassiﬁed dataset,1,1,1.0
the misclassiﬁed dataset for,1,1,1.0
misclassiﬁed dataset for the,1,1,1.0
dataset for the classiﬁcation,1,1,1.0
the classiﬁcation problem b,1,1,1.0
classiﬁcation problem b is,1,1,1.0
problem b is deﬁned,1,1,1.0
b is deﬁned to,1,1,1.0
is deﬁned to be,1,1,1.0
deﬁned to be the,1,1,1.0
to be the set,1,1,1.0
be the set where,1,1,1.0
the set where each,1,1,1.0
set where each example,1,1,1.0
where each example in,1,1,1.0
each example in d,1,1,1.0
example in d is,1,1,1.0
in d is replicated,1,1,1.0
d is replicated n,1,1,1.0
is replicated n times,1,1,1.0
replicated n times with,1,1,1.0
n times with a,1,1,1.0
times with a different,1,1,1.0
with a different class,1,1,1.0
a different class label,1,1,1.0
different class label other,1,1,1.0
class label other than,1,1,1.0
label other than the,1,1,1.0
other than the true,1,1,1.0
than the true one,1,1,1.0
the true one ˆri,1,1,1.0
true one ˆri serves,1,1,1.0
one ˆri serves as,1,1,1.0
ˆri serves as the,1,1,1.0
serves as the distribution,1,1,1.0
as the distribution function,1,1,1.0
the distribution function determining,1,1,1.0
distribution function determining the,1,1,1.0
function determining the probability,1,1,1.0
determining the probability that,1,1,1.0
the probability that examples,1,1,1.0
probability that examples in,1,1,1.0
that examples in b,1,1,1.0
examples in b are,1,1,1.0
in b are chosen,1,1,1.0
b are chosen for,1,1,1.0
are chosen for generating,1,1,1.0
chosen for generating the,1,1,1.0
the synthetic instances it,1,1,1.0
synthetic instances it is,1,1,1.0
instances it is calculated,1,1,1.0
it is calculated by,1,1,1.0
is calculated by mapping,1,1,1.0
calculated by mapping the,1,1,1.0
by mapping the number,1,1,1.0
mapping the number of,1,1,1.0
of majority cases in,4,1,4.0
majority cases in the,3,1,3.0
cases in the nearest,1,1,1.0
in the nearest neighbors,1,1,1.0
neighbors of each example,1,1,1.0
of each example in,1,1,1.0
each example in b,1,1,1.0
example in b into,1,1,1.0
in b into the,1,1,1.0
b into the range,1,1,1.0
into the range the,1,1,1.0
the range the ramoboost,1,1,1.0
range the ramoboost learning,1,1,1.0
the ramoboost learning algorithm,1,1,1.0
ramoboost learning algorithm is,1,1,1.0
learning algorithm is presented,1,1,1.0
algorithm is presented presented,1,1,1.0
is presented presented in,1,1,1.0
presented presented in algorithm,1,1,1.0
presented in algorithm according,1,1,1.0
in algorithm according to,1,1,1.0
algorithm according to this,1,1,1.0
according to this description,1,1,1.0
to this description the,1,1,1.0
this description the ramoboost,1,1,1.0
description the ramoboost algorithm,1,1,1.0
the ramoboost algorithm includes,1,1,1.0
ramoboost algorithm includes two,1,1,1.0
algorithm includes two mechanisms,1,1,1.0
includes two mechanisms to,1,1,1.0
two mechanisms to facilitate,1,1,1.0
mechanisms to facilitate learning,1,1,1.0
facilitate learning from anced,1,1,1.0
learning from anced data,1,1,1.0
from anced data the,1,1,1.0
anced data the ﬁrst,1,1,1.0
data the ﬁrst consists,1,1,1.0
the ﬁrst consists of,1,1,1.0
ﬁrst consists of steps,1,1,1.0
consists of steps to,1,1,1.0
of steps to where,1,1,1.0
steps to where instances,1,1,1.0
to where instances are,1,1,1.0
where instances are adaptively,1,1,1.0
instances are adaptively generated,1,1,1.0
are adaptively generated accord,1,1,1.0
adaptively generated accord ing,1,1,1.0
generated accord ing to,1,1,1.0
accord ing to their,1,1,1.0
ing to their distributions,1,1,1.0
to their distributions in,1,1,1.0
their distributions in this,1,1,1.0
distributions in this way,1,1,1.0
in this way more,1,1,1.0
this way more synthetic,1,1,1.0
way more synthetic instances,1,1,1.0
more synthetic instances are,2,1,2.0
synthetic instances are created,2,1,2.0
instances are created for,2,1,2.0
are created for minority,1,1,1.0
created for minority examples,1,1,1.0
for minority examples th,1,1,1.0
minority examples th at,1,1,1.0
examples th at are,1,1,1.0
th at are more,1,1,1.0
at are more likely,1,1,1.0
likely to be classiﬁed,1,1,1.0
to be classiﬁed compared,1,1,1.0
be classiﬁed compared to,1,1,1.0
classiﬁed compared to minority,1,1,1.0
compared to minority examples,1,1,1.0
to minority examples this,1,1,1.0
minority examples this is,1,1,1.0
examples this is signiﬁcantly,1,1,1.0
this is signiﬁcantly different,1,1,1.0
is signiﬁcantly different from,1,1,1.0
signiﬁcantly different from the,1,1,1.0
different from the smote,1,1,1.0
from the smote algorithm,1,1,1.0
the smote algorithm where,1,1,1.0
smote algorithm where each,1,1,1.0
algorithm where each minority,1,1,1.0
where each minority example,1,1,1.0
minority example has equal,1,1,1.0
example has equal weight,1,1,1.0
has equal weight and,1,1,1.0
equal weight and therefore,1,1,1.0
weight and therefore the,1,1,1.0
and therefore the same,1,1,1.0
therefore the same numbers,1,1,1.0
the same numbers of,2,1,2.0
same numbers of synthetic,1,1,1.0
numbers of synthetic instances,1,1,1.0
of synthetic instances are,1,1,1.0
are created for each,1,1,1.0
created for each minority,2,1,2.0
minority example the second,1,1,1.0
example the second mechanism,1,1,1.0
the second mechanism steps,1,1,1.0
second mechanism steps to,1,1,1.0
mechanism steps to use,1,1,1.0
steps to use the,1,1,1.0
to use the of,1,1,1.0
use the of the,1,1,1.0
the of the current,1,1,1.0
of the current hypothesis,1,1,1.0
the current hypothesis ht,1,1,1.0
current hypothesis ht to,1,1,1.0
hypothesis ht to update,1,1,1.0
ht to update the,1,1,1.0
to update the sampling,1,1,1.0
update the sampling distribution,1,1,1.0
the sampling distribution dt,1,1,1.0
sampling distribution dt which,1,1,1.0
distribution dt which is,1,1,1.0
dt which is used,1,1,1.0
is used to sample,1,1,1.0
used to sample the,1,1,1.0
to sample the training,1,1,1.0
sample the training dataset,1,1,1.0
the training dataset in,2,1,2.0
training dataset in the,1,1,1.0
dataset in the next,1,1,1.0
the next iteration as,1,1,1.0
next iteration as shown,1,1,1.0
iteration as shown in,2,1,2.0
as shown in step,2,1,2.0
shown in step similar,1,1,1.0
in step similar to,1,1,1.0
step similar to the,1,1,1.0
similar to the algorithm,1,1,1.0
to the algorithm ram,1,1,1.0
the algorithm ram o,1,1,1.0
algorithm ram o boost,1,1,1.0
ram o boost n,1,1,1.0
o boost n t,1,1,1.0
boost n t α,1,1,1.0
n t α input,1,1,1.0
t α input training,1,1,1.0
α input training dataset,1,1,1.0
input training dataset with,1,1,1.0
training dataset with m,1,1,1.0
dataset with m class,1,1,1.0
with m class examples,1,1,1.0
m class examples xm,1,1,1.0
class examples xm ym,1,1,1.0
examples xm ym w,1,1,1.0
xm ym w h,1,1,1.0
ym w h e,1,1,1.0
w h e r,1,1,1.0
h e r e,1,1,1.0
e r e xi,1,1,1.0
r e xi i,1,1,1.0
e xi i m,1,1,1.0
xi i m is,1,1,1.0
i m is an,1,1,1.0
m is an instance,1,1,1.0
is an instance of,1,1,1.0
an instance of the,1,1,1.0
instance of the n,1,1,1.0
of the n dimensional,1,1,1.0
the n dimensional feature,1,1,1.0
n dimensional feature space,1,1,1.0
dimensional feature space x,1,1,1.0
feature space x and,1,1,1.0
space x and yi,1,1,1.0
x and yi y,1,1,1.0
and yi y major,1,1,1.0
yi y major minor,1,1,1.0
y major minor is,1,1,1.0
major minor is the,1,1,1.0
minor is the class,1,1,1.0
is the class identity,1,1,1.0
the class identity label,1,1,1.0
class identity label associated,1,1,1.0
identity label associated with,1,1,1.0
label associated with instance,1,1,1.0
associated with instance xi,1,1,1.0
with instance xi n,1,1,1.0
instance xi n number,1,1,1.0
xi n number of,1,1,1.0
n number of synthetic,1,1,1.0
of synthetic data samples,1,1,1.0
synthetic data samples to,1,1,1.0
data samples to be,1,1,1.0
samples to be generated,1,1,1.0
to be generated at,1,1,1.0
be generated at each,1,1,1.0
generated at each iteration,1,1,1.0
at each iteration t,1,1,1.0
each iteration t number,1,1,1.0
iteration t number of,1,1,1.0
t number of iterations,1,1,1.0
number of iterations the,1,1,1.0
of iterations the number,1,1,1.0
iterations the number of,1,1,1.0
the number of base,1,1,1.0
number of base classiﬁers,1,1,1.0
of base classiﬁers number,1,1,1.0
base classiﬁers number of,1,1,1.0
classiﬁers number of nearest,1,1,1.0
of nearest neighbors in,1,1,1.0
nearest neighbors in adjusting,1,1,1.0
neighbors in adjusting the,1,1,1.0
in adjusting the sampling,1,1,1.0
adjusting the sampling probability,1,1,1.0
the sampling probability of,1,1,1.0
sampling probability of the,1,1,1.0
probability of the minority,1,1,1.0
of the minority examples,1,1,1.0
the minority examples number,1,1,1.0
minority examples number of,1,1,1.0
examples number of nearest,1,1,1.0
used to generate the,1,1,1.0
to generate the synthetic,1,1,1.0
generate the synthetic data,1,1,1.0
the synthetic data instances,1,1,1.0
synthetic data instances α,1,1,1.0
data instances α the,1,1,1.0
instances α the scaling,1,1,1.0
α the scaling coefﬁcient,1,1,1.0
the scaling coefﬁcient let,1,1,1.0
scaling coefﬁcient let b,1,1,1.0
coefﬁcient let b i,1,1,1.0
let b i y,1,1,1.0
b i y i,1,1,1.0
i y i m,1,1,1.0
y i m y,1,1,1.0
i m y initialize,1,1,1.0
m y initialize i,1,1,1.0
y initialize i y,1,1,1.0
initialize i y for,1,1,1.0
i y for i,1,1,1.0
y for i y,1,1,1.0
for i y b,1,1,1.0
i y b for,1,1,1.0
y b for two,1,1,1.0
b for two class,1,1,1.0
for two class problems,1,1,1.0
two class problems m,1,1,1.0
class problems m do,1,1,1.0
problems m do for,1,1,1.0
m do for t,1,1,1.0
do for t t,1,1,1.0
for t t sample,1,1,1.0
t t sample the,1,1,1.0
t sample the mislabeled,1,1,1.0
sample the mislabeled training,1,1,1.0
the mislabeled training data,1,1,1.0
mislabeled training data with,1,1,1.0
training data with dt,1,1,1.0
data with dt and,1,1,1.0
with dt and get,2,1,2.0
dt and get back,2,1,2.0
and get back the,1,1,1.0
get back the sampled,1,1,1.0
back the sampled dataset,1,1,1.0
the sampled dataset se,1,1,1.0
sampled dataset se of,1,1,1.0
dataset se of identical,1,1,1.0
se of identical size,1,1,1.0
of identical size slice,1,1,1.0
identical size slice se,1,1,1.0
size slice se into,1,1,1.0
slice se into the,1,1,1.0
se into the majority,1,1,1.0
into the majority subset,1,1,1.0
the majority subset and,1,1,1.0
majority subset and the,1,1,1.0
subset and the minority,1,1,1.0
and the minority subset,1,1,1.0
the minority subset of,1,1,1.0
minority subset of size,1,1,1.0
subset of size mlt,1,1,1.0
of size mlt and,1,1,1.0
size mlt and mst,1,1,1.0
mlt and mst respectively,1,1,1.0
and mst respectively for,1,1,1.0
mst respectively for each,1,1,1.0
respectively for each example,1,1,1.0
for each example xi,2,1,2.0
each example xi ﬁnd,1,1,1.0
example xi ﬁnd its,1,1,1.0
xi ﬁnd its nearest,1,1,1.0
ﬁnd its nearest neighbors,2,1,2.0
its nearest neighbors in,2,1,2.0
nearest neighbors in the,1,1,1.0
neighbors in the dataset,1,1,1.0
in the dataset se,1,1,1.0
the dataset se according,1,1,1.0
dataset se according to,1,1,1.0
se according to the,1,1,1.0
according to the euclidean,2,1,2.0
to the euclidean distance,2,1,2.0
the euclidean distance in,2,1,2.0
euclidean distance in space,1,1,1.0
distance in space and,1,1,1.0
in space and calculate,1,1,1.0
space and calculate ri,1,1,1.0
and calculate ri deﬁned,1,1,1.0
calculate ri deﬁned as,1,1,1.0
ri deﬁned as ri,1,1,1.0
deﬁned as ri exp,1,1,1.0
as ri exp α,1,1,1.0
ri exp α δi,1,1,1.0
exp α δi i,1,1,1.0
α δi i mst,1,1,1.0
δi i mst where,1,1,1.0
i mst where δi,1,1,1.0
mst where δi is,1,1,1.0
where δi is the,1,1,1.0
δi is the number,1,1,1.0
majority cases in examples,1,1,1.0
cases in examples normalize,1,1,1.0
in examples normalize ri,1,1,1.0
examples normalize ri according,1,1,1.0
normalize ri according to,1,1,1.0
ri according to ˆri,1,1,1.0
according to ˆri ri,1,1,1.0
to ˆri ri ri,1,1,1.0
ˆri ri ri such,1,1,1.0
ri ri such that,1,1,1.0
ri such that ˆri,1,1,1.0
such that ˆri is,1,1,1.0
that ˆri is a,1,1,1.0
ˆri is a distribution,1,1,1.0
is a distribution function,1,1,1.0
a distribution function mst,1,1,1.0
distribution function mst ri,1,1,1.0
function mst ri deﬁne,1,1,1.0
mst ri deﬁne dt,1,1,1.0
ri deﬁne dt ˆri,1,1,1.0
deﬁne dt ˆri sample,1,1,1.0
dt ˆri sample with,1,1,1.0
ˆri sample with dt,1,1,1.0
sample with dt and,1,1,1.0
and get back a,1,1,1.0
get back a sampling,1,1,1.0
back a sampling minority,1,1,1.0
a sampling minority dataset,1,1,1.0
sampling minority dataset gt,1,1,1.0
minority dataset gt o,1,1,1.0
dataset gt o fs,1,1,1.0
gt o fs i,1,1,1.0
o fs i z,1,1,1.0
fs i z emst,1,1,1.0
i z emst for,1,1,1.0
z emst for each,1,1,1.0
emst for each example,1,1,1.0
each example xi gt,1,1,1.0
example xi gt ﬁnd,1,1,1.0
xi gt ﬁnd its,1,1,1.0
gt ﬁnd its nearest,1,1,1.0
nearest neighbors in according,1,1,1.0
neighbors in according to,1,1,1.0
in according to the,1,1,1.0
euclidean distance in n,1,1,1.0
distance in n dimensional,1,1,1.0
in n dimensional space,1,1,1.0
n dimensional space and,1,1,1.0
dimensional space and use,1,1,1.0
space and use linear,1,1,1.0
and use linear inter,1,1,1.0
use linear inter polation,1,1,1.0
linear inter polation to,1,1,1.0
inter polation to generate,1,1,1.0
polation to generate n,1,1,1.0
to generate n synthetic,1,1,1.0
generate n synthetic data,1,1,1.0
n synthetic data samples,2,1,2.0
synthetic data samples provide,1,1,1.0
data samples provide the,1,1,1.0
samples provide the base,1,1,1.0
provide the base classiﬁer,1,1,1.0
the base classiﬁer with,1,1,1.0
base classiﬁer with sampling,1,1,1.0
classiﬁer with sampling dataset,1,1,1.0
with sampling dataset se,1,1,1.0
sampling dataset se and,1,1,1.0
dataset se and the,1,1,1.0
se and the n,1,1,1.0
and the n synthetic,1,1,1.0
the n synthetic data,1,1,1.0
synthetic data samples get,1,1,1.0
data samples get back,1,1,1.0
samples get back a,1,1,1.0
get back a hypothesis,1,1,1.0
back a hypothesis ht,1,1,1.0
a hypothesis ht x,1,1,1.0
hypothesis ht x y,1,1,1.0
ht x y calculate,1,1,1.0
x y calculate the,1,1,1.0
y calculate the of,1,1,1.0
calculate the of ht,1,1,1.0
the of ht εt,1,1,1.0
of ht εt i,1,1,1.0
ht εt i y,1,1,1.0
εt i y b,1,1,1.0
i y b dt,1,1,1.0
y b dt i,1,1,1.0
b dt i y,1,1,1.0
dt i y ht,1,1,1.0
i y ht xi,1,1,1.0
y ht xi yi,1,1,1.0
ht xi yi ht,1,1,1.0
xi yi ht xi,2,1,2.0
yi ht xi y,2,1,2.0
ht xi y set,1,1,1.0
xi y set βt,1,1,1.0
y set βt εt,1,1,1.0
set βt εt εt,1,1,1.0
βt εt εt update,1,1,1.0
εt εt update dt,1,1,1.0
εt update dt i,1,1,1.0
update dt i y,1,1,1.0
dt i y dt,1,1,1.0
i y dt i,1,1,1.0
y dt i y,1,1,1.0
dt i y zt,1,1,1.0
i y zt β,1,1,1.0
y zt β xi,1,1,1.0
zt β xi yi,1,1,1.0
β xi yi ht,1,1,1.0
ht xi y t,1,1,1.0
xi y t where,1,1,1.0
y t where zt,1,1,1.0
t where zt is,1,1,1.0
where zt is a,1,1,1.0
zt is a normalization,1,1,1.0
is a normalization constant,1,1,1.0
a normalization constant end,1,1,1.0
normalization constant end ieee,1,1,1.0
constant end ieee transactions,1,1,1.0
end ieee transactions on,1,1,1.0
vol no october output,1,1,1.0
no october output the,1,1,1.0
october output the output,1,1,1.0
output the output hypothesis,1,1,1.0
the output hypothesis h,1,1,1.0
output hypothesis h f,1,1,1.0
hypothesis h f inal,1,1,1.0
h f inal x,2,1,2.0
f inal x is,1,1,1.0
inal x is calculated,1,1,1.0
x is calculated as,1,1,1.0
is calculated as follows,1,1,1.0
calculated as follows h,1,1,1.0
as follows h f,1,1,1.0
follows h f inal,1,1,1.0
f inal x arg,1,1,1.0
inal x arg max,1,1,1.0
x arg max log,1,1,1.0
arg max log βt,1,1,1.0
max log βt ht,1,1,1.0
log βt ht x,1,1,1.0
βt ht x y,1,1,1.0
ht x y algorithm,1,1,1.0
x y algorithm the,1,1,1.0
y algorithm the anism,1,1,1.0
algorithm the anism can,1,1,1.0
the anism can adaptively,1,1,1.0
anism can adaptively shift,1,1,1.0
adaptively shift the ﬁnal,1,1,1.0
shift the ﬁnal hypothesis,1,1,1.0
the ﬁnal hypothesis toward,1,1,1.0
ﬁnal hypothesis toward the,1,1,1.0
hypothesis toward the decision,1,1,1.0
toward the decision boundary,1,1,1.0
decision boundary to facilitate,1,1,1.0
boundary to facilitate the,1,1,1.0
to facilitate the learning,1,1,1.0
facilitate the learning process,1,1,1.0
the learning process the,1,1,1.0
learning process the key,1,1,1.0
process the key components,1,1,1.0
the key components of,1,1,1.0
key components of the,1,1,1.0
components of the two,1,1,1.0
of the two ramoboost,1,1,1.0
the two ramoboost mechanisms,1,1,1.0
two ramoboost mechanisms are,1,1,1.0
ramoboost mechanisms are steps,1,1,1.0
mechanisms are steps and,1,1,1.0
are steps and in,1,1,1.0
steps and in the,2,1,2.0
and in the by,1,1,1.0
in the by sampling,1,1,1.0
the by sampling the,1,1,1.0
by sampling the training,1,1,1.0
sampling the training dataset,1,1,1.0
the training dataset with,1,1,1.0
training dataset with the,1,1,1.0
dataset with the updated,1,1,1.0
with the updated weight,1,1,1.0
the updated weight distribution,1,1,1.0
updated weight distribution function,1,1,1.0
weight distribution function dt,1,1,1.0
distribution function dt at,1,1,1.0
function dt at the,1,1,1.0
dt at the tth,1,1,1.0
at the tth iteration,1,1,1.0
the tth iteration as,1,1,1.0
tth iteration as shown,1,1,1.0
shown in step ramoboost,1,1,1.0
in step ramoboost can,1,1,1.0
step ramoboost can sively,1,1,1.0
ramoboost can sively shift,1,1,1.0
can sively shift the,1,1,1.0
sively shift the decision,1,1,1.0
toward the examples meanwhile,1,1,1.0
the examples meanwhile in,1,1,1.0
examples meanwhile in step,1,1,1.0
meanwhile in step ramoboost,1,1,1.0
in step ramoboost generates,1,1,1.0
step ramoboost generates more,1,1,1.0
ramoboost generates more synthetic,1,1,1.0
generates more synthetic instances,1,1,1.0
more synthetic instances for,1,1,1.0
synthetic instances for those,1,1,1.0
instances for those minority,1,1,1.0
for those minority ples,1,1,1.0
those minority ples by,1,1,1.0
minority ples by sampling,1,1,1.0
ples by sampling the,1,1,1.0
by sampling the minority,1,1,1.0
sampling the minority dataset,1,1,1.0
the minority dataset with,1,1,1.0
minority dataset with the,1,1,1.0
dataset with the distribution,1,1,1.0
with the distribution function,1,1,1.0
the distribution function obtained,1,1,1.0
distribution function obtained from,1,1,1.0
function obtained from manipulating,1,1,1.0
obtained from manipulating the,1,1,1.0
from manipulating the number,1,1,1.0
manipulating the number of,2,1,2.0
of majority examples in,1,1,1.0
examples in the k,1,1,1.0
in the k nearest,1,1,1.0
each minority example through,1,1,1.0
minority example through and,1,1,1.0
example through and analysis,1,1,1.0
through and analysis of,1,1,1.0
and analysis of the,1,1,1.0
analysis of the ramoboost,1,1,1.0
of the ramoboost learning,1,1,1.0
the ramoboost learning methodology,1,1,1.0
ramoboost learning methodology data,1,1,1.0
learning methodology data generation,1,1,1.0
methodology data generation mechanism,1,1,1.0
data generation mechanism the,1,1,1.0
generation mechanism the proposed,1,1,1.0
mechanism the proposed boost,1,1,1.0
the proposed boost algorithm,1,1,1.0
proposed boost algorithm shares,1,1,1.0
boost algorithm shares some,1,1,1.0
algorithm shares some data,1,1,1.0
shares some data generation,1,1,1.0
some data generation aspects,1,1,1.0
data generation aspects with,1,1,1.0
generation aspects with smote,1,1,1.0
aspects with smote a,1,1,1.0
with smote a synthetic,1,1,1.0
smote a synthetic instance,1,1,1.0
a synthetic instance is,1,1,1.0
synthetic instance is generated,1,1,1.0
instance is generated by,1,1,1.0
is generated by adding,1,1,1.0
generated by adding the,1,1,1.0
by adding the feature,1,1,1.0
adding the feature vector,1,1,1.0
the feature vector of,2,1,2.0
feature vector of the,2,1,2.0
vector of the minority,2,1,2.0
of the minority example,4,1,4.0
under consideration and a,1,1,1.0
consideration and a randomized,1,1,1.0
and a randomized real,1,1,1.0
a randomized real number,1,1,1.0
randomized real number in,1,1,1.0
real number in multiplied,1,1,1.0
number in multiplied by,1,1,1.0
in multiplied by the,1,1,1.0
multiplied by the difference,1,1,1.0
by the difference between,1,1,1.0
the difference between the,2,1,2.0
difference between the feature,1,1,1.0
between the feature vector,1,1,1.0
of the minority ple,1,1,1.0
the minority ple under,1,1,1.0
minority ple under consideration,1,1,1.0
ple under consideration and,1,1,1.0
under consideration and one,1,1,1.0
consideration and one randomly,1,1,1.0
and one randomly chosen,1,1,1.0
one randomly chosen example,1,1,1.0
randomly chosen example within,1,1,1.0
chosen example within its,1,1,1.0
example within its k,1,1,1.0
within its k nearest,1,1,1.0
its k nearest neighbors,1,1,1.0
neighbors from the minority,1,1,1.0
from the minority sampling,1,1,1.0
the minority sampling dataset,1,1,1.0
minority sampling dataset step,1,1,1.0
sampling dataset step in,1,1,1.0
dataset step in the,1,1,1.0
step in the which,1,1,1.0
in the which can,1,1,1.0
the which can be,1,1,1.0
which can be formulated,1,1,1.0
can be formulated in,1,1,1.0
be formulated in the,1,1,1.0
formulated in the same,1,1,1.0
in the same way,1,1,1.0
the same way as,1,1,1.0
same way as the,1,1,1.0
way as the difference,1,1,1.0
as the difference between,1,1,1.0
difference between the two,1,1,1.0
between the two however,1,1,1.0
the two however is,1,1,1.0
two however is that,1,1,1.0
however is that ramoboost,1,1,1.0
is that ramoboost employs,1,1,1.0
that ramoboost employs a,1,1,1.0
ramoboost employs a systematic,1,1,1.0
employs a systematic method,1,1,1.0
a systematic method to,1,1,1.0
systematic method to adaptively,1,1,1.0
method to adaptively determine,1,1,1.0
to adaptively determine the,1,1,1.0
adaptively determine the number,1,1,1.0
determine the number of,1,1,1.0
of synthetic instances created,1,1,1.0
synthetic instances created for,1,1,1.0
instances created for each,1,1,1.0
each minority example in,2,1,2.0
minority example in the,1,1,1.0
example in the sampling,1,1,1.0
in the sampling dataset,1,1,1.0
the sampling dataset according,1,1,1.0
sampling dataset according to,1,1,1.0
dataset according to their,1,1,1.0
according to their learnability,1,1,1.0
to their learnability more,1,1,1.0
their learnability more synthetic,1,1,1.0
learnability more synthetic instances,1,1,1.0
more synthetic instances will,1,1,1.0
synthetic instances will be,1,1,1.0
instances will be generated,1,1,1.0
will be generated for,1,1,1.0
be generated for those,1,1,1.0
generated for those examples,1,1,1.0
for those examples therefore,1,1,1.0
those examples therefore the,1,1,1.0
examples therefore the ﬁnal,1,1,1.0
therefore the ﬁnal hypothesis,1,1,1.0
the ﬁnal hypothesis will,1,1,1.0
ﬁnal hypothesis will be,1,1,1.0
hypothesis will be more,1,1,1.0
will be more focused,1,1,1.0
more focused on those,1,1,1.0
focused on those difﬁcult,1,1,1.0
on those difﬁcult decision,1,1,1.0
those difﬁcult decision regions,1,1,1.0
difﬁcult decision regions concretely,1,1,1.0
decision regions concretely ramoboost,1,1,1.0
regions concretely ramoboost generates,1,1,1.0
concretely ramoboost generates synthetic,1,1,1.0
ramoboost generates synthetic instances,1,1,1.0
synthetic instances by manipulating,1,1,1.0
instances by manipulating the,1,1,1.0
by manipulating the number,1,1,1.0
cases in the neighbors,1,1,1.0
in the neighbors across,1,1,1.0
the neighbors across the,1,1,1.0
neighbors across the whole,1,1,1.0
across the whole sampling,1,1,1.0
the whole sampling dataset,1,1,1.0
whole sampling dataset steps,1,1,1.0
sampling dataset steps and,1,1,1.0
dataset steps and in,1,1,1.0
and in the while,1,1,1.0
in the while smote,1,1,1.0
the while smote universally,1,1,1.0
while smote universally generates,1,1,1.0
smote universally generates identical,1,1,1.0
universally generates identical number,1,1,1.0
generates identical number of,1,1,1.0
identical number of synthetic,1,1,1.0
each minority example similar,1,1,1.0
minority example similar to,1,1,1.0
example similar to ramoboost,1,1,1.0
similar to ramoboost adasyn,1,1,1.0
to ramoboost adasyn also,1,1,1.0
ramoboost adasyn also aims,1,1,1.0
adasyn also aims to,1,1,1.0
also aims to cally,1,1,1.0
aims to cally generate,1,1,1.0
to cally generate synthetic,1,1,1.0
cally generate synthetic minority,1,1,1.0
synthetic minority instances according,1,1,1.0
minority instances according to,1,1,1.0
instances according to the,1,1,1.0
according to the derlying,1,1,1.0
to the derlying data,1,1,1.0
the derlying data distribution,1,1,1.0
derlying data distribution instead,1,1,1.0
data distribution instead of,1,1,1.0
distribution instead of using,1,1,1.0
instead of using a,1,1,1.0
of using a uniform,1,1,1.0
using a uniform sampling,1,1,1.0
a uniform sampling distribution,1,1,1.0
uniform sampling distribution consequently,1,1,1.0
sampling distribution consequently adasyn,1,1,1.0
distribution consequently adasyn also,1,1,1.0
consequently adasyn also has,1,1,1.0
adasyn also has the,1,1,1.0
also has the ability,1,1,1.0
has the ability to,1,1,1.0
the ability to push,1,1,1.0
ability to push the,1,1,1.0
to push the learning,1,1,1.0
push the learning algorithm,1,1,1.0
algorithm to be more,1,1,1.0
on the difﬁcult regions,1,1,1.0
the difﬁcult regions of,1,1,1.0
difﬁcult regions of the,1,1,1.0
regions of the decision,1,1,1.0
the decision boundary however,1,1,1.0
decision boundary however adasyn,1,1,1.0
boundary however adasyn does,1,1,1.0
however adasyn does this,1,1,1.0
adasyn does this in,1,1,1.0
does this in an,1,1,1.0
this in an aggressive,1,1,1.0
in an aggressive manner,1,1,1.0
an aggressive manner almost,1,1,1.0
aggressive manner almost all,1,1,1.0
manner almost all of,1,1,1.0
almost all of the,2,1,2.0
all of the generated,1,1,1.0
of the generated synthetic,1,1,1.0
the generated synthetic minority,1,1,1.0
generated synthetic minority instances,1,1,1.0
synthetic minority instances are,1,1,1.0
minority instances are very,1,1,1.0
instances are very close,1,1,1.0
are very close to,1,1,1.0
very close to the,3,1,3.0
the decision boundary this,2,1,2.0
decision boundary this is,1,1,1.0
boundary this is because,1,1,1.0
this is because the,1,1,1.0
is because the data,1,1,1.0
because the data generation,1,1,1.0
the data generation mechanism,4,1,4.0
data generation mechanism of,4,1,4.0
generation mechanism of adasyn,1,1,1.0
mechanism of adasyn approves,1,1,1.0
of adasyn approves generation,1,1,1.0
adasyn approves generation of,1,1,1.0
approves generation of synthetic,1,1,1.0
generation of synthetic data,1,1,1.0
of synthetic data only,1,1,1.0
synthetic data only when,1,1,1.0
data only when there,1,1,1.0
only when there exists,1,1,1.0
when there exists at,1,1,1.0
there exists at least,1,1,1.0
exists at least one,1,1,1.0
at least one majority,1,1,1.0
least one majority case,1,1,1.0
one majority case in,1,1,1.0
majority case in the,1,1,1.0
case in the neighbor,1,1,1.0
in the neighbor of,2,1,2.0
the neighbor of the,2,1,2.0
neighbor of the minority,2,1,2.0
example under consideration speciﬁcally,1,1,1.0
under consideration speciﬁcally the,1,1,1.0
consideration speciﬁcally the number,1,1,1.0
speciﬁcally the number of,1,1,1.0
cases in the neighbor,1,1,1.0
example under consideration directly,1,1,1.0
under consideration directly dictates,1,1,1.0
consideration directly dictates the,1,1,1.0
directly dictates the number,1,1,1.0
dictates the number of,1,1,1.0
synthetic instances generated around,1,1,1.0
instances generated around it,1,1,1.0
generated around it in,1,1,1.0
around it in the,1,1,1.0
it in the extreme,1,1,1.0
in the extreme case,1,1,1.0
the extreme case noisy,1,1,1.0
extreme case noisy examples,1,1,1.0
case noisy examples in,1,1,1.0
noisy examples in the,1,1,1.0
minority class can have,1,1,1.0
class can have multiple,1,1,1.0
can have multiple synthetic,1,1,1.0
have multiple synthetic instances,1,1,1.0
multiple synthetic instances generated,1,1,1.0
synthetic instances generated while,1,1,1.0
instances generated while examples,1,1,1.0
generated while examples that,1,1,1.0
while examples that are,1,1,1.0
examples that are relatively,1,1,1.0
that are relatively far,1,1,1.0
are relatively far away,1,1,1.0
relatively far away from,1,1,1.0
far away from the,1,1,1.0
away from the class,1,1,1.0
from the class boundary,1,1,1.0
the class boundary but,1,1,1.0
class boundary but are,1,1,1.0
boundary but are tive,1,1,1.0
but are tive of,1,1,1.0
are tive of the,1,1,1.0
tive of the target,1,1,1.0
of the target concept,1,1,1.0
the target concept of,1,1,1.0
target concept of the,1,1,1.0
concept of the minority,1,1,1.0
minority class are not,1,1,1.0
class are not selected,1,1,1.0
are not selected for,1,1,1.0
not selected for synthetic,1,1,1.0
selected for synthetic data,1,1,1.0
for synthetic data generation,2,1,2.0
data generation in contrast,1,1,1.0
generation in contrast ramoboost,1,1,1.0
in contrast ramoboost ploys,1,1,1.0
contrast ramoboost ploys a,1,1,1.0
ramoboost ploys a logistic,1,1,1.0
ploys a logistic function,1,1,1.0
a logistic function to,1,1,1.0
logistic function to ﬁrstly,1,1,1.0
function to ﬁrstly map,1,1,1.0
to ﬁrstly map δ,1,1,1.0
ﬁrstly map δ the,1,1,1.0
map δ the number,1,1,1.0
δ the number of,1,1,1.0
cases within the k,1,1,1.0
example under consideration to,2,1,2.0
under consideration to a,1,1,1.0
consideration to a real,1,1,1.0
to a real number,1,1,1.0
a real number r,1,1,1.0
real number r between,1,1,1.0
number r between and,1,1,1.0
r between and which,1,1,1.0
between and which is,1,1,1.0
and which is then,1,1,1.0
which is then normalized,1,1,1.0
is then normalized to,1,1,1.0
then normalized to a,1,1,1.0
normalized to a distribution,1,1,1.0
to a distribution function,1,1,1.0
a distribution function for,1,1,1.0
distribution function for determining,1,1,1.0
function for determining the,1,1,1.0
for determining the probability,1,1,1.0
determining the probability of,1,1,1.0
the probability of each,2,1,2.0
probability of each minority,2,1,2.0
minority example in this,1,1,1.0
example in this way,1,1,1.0
in this way ramoboost,1,1,1.0
this way ramoboost considers,1,1,1.0
way ramoboost considers all,1,1,1.0
ramoboost considers all minority,1,1,1.0
considers all minority examples,1,1,1.0
all minority examples for,1,1,1.0
minority examples for synthetic,1,1,1.0
examples for synthetic generation,1,1,1.0
for synthetic generation albeit,1,1,1.0
synthetic generation albeit at,1,1,1.0
generation albeit at varied,1,1,1.0
albeit at varied levels,1,1,1.0
at varied levels adasyn,1,1,1.0
varied levels adasyn additionally,1,1,1.0
levels adasyn additionally introduces,1,1,1.0
adasyn additionally introduces complications,1,1,1.0
additionally introduces complications at,1,1,1.0
introduces complications at the,1,1,1.0
complications at the decision,1,1,1.0
at the decision boundary,1,1,1.0
the decision boundary since,1,1,1.0
decision boundary since almost,1,1,1.0
boundary since almost all,1,1,1.0
since almost all of,1,1,1.0
all of the synthetic,1,1,1.0
the synthetic instances are,1,1,1.0
synthetic instances are located,1,1,1.0
instances are located in,1,1,1.0
are located in the,1,1,1.0
located in the decision,1,1,1.0
in the decision boundary,1,1,1.0
the decision boundary region,2,1,2.0
decision boundary region which,1,1,1.0
boundary region which means,1,1,1.0
region which means that,1,1,1.0
which means that excessively,1,1,1.0
means that excessively more,1,1,1.0
that excessively more synthetic,1,1,1.0
excessively more synthetic instances,1,1,1.0
synthetic instances are probably,1,1,1.0
instances are probably generated,1,1,1.0
are probably generated for,1,1,1.0
probably generated for noisy,1,1,1.0
generated for noisy examples,1,1,1.0
for noisy examples with,1,1,1.0
noisy examples with the,1,1,1.0
examples with the minority,1,1,1.0
with the minority class,1,1,1.0
the minority class label,1,1,1.0
minority class label phasizing,1,1,1.0
class label phasizing the,1,1,1.0
label phasizing the decision,1,1,1.0
phasizing the decision boundary,1,1,1.0
decision boundary region may,1,1,1.0
boundary region may magnify,1,1,1.0
region may magnify the,1,1,1.0
may magnify the inﬂuence,1,1,1.0
magnify the inﬂuence of,1,1,1.0
the inﬂuence of noise,1,1,1.0
inﬂuence of noise within,1,1,1.0
of noise within the,1,1,1.0
noise within the training,1,1,1.0
within the training dataset,1,1,1.0
the training dataset thereby,1,1,1.0
training dataset thereby leading,1,1,1.0
dataset thereby leading to,1,1,1.0
thereby leading to performance,1,1,1.0
leading to performance depreciation,1,1,1.0
to performance depreciation ramoboost,1,1,1.0
performance depreciation ramoboost on,1,1,1.0
depreciation ramoboost on the,1,1,1.0
ramoboost on the other,1,1,1.0
the other hand assigns,1,1,1.0
other hand assigns high,1,1,1.0
hand assigns high probabilities,1,1,1.0
assigns high probabilities for,1,1,1.0
high probabilities for minority,1,1,1.0
probabilities for minority examples,1,1,1.0
for minority examples close,1,1,1.0
minority examples close to,1,1,1.0
examples close to the,1,1,1.0
the decision boundary which,1,1,1.0
decision boundary which means,1,1,1.0
boundary which means that,1,1,1.0
which means that synthetic,1,1,1.0
means that synthetic instances,1,1,1.0
that synthetic instances are,1,1,1.0
synthetic instances are generated,1,1,1.0
instances are generated near,1,1,1.0
are generated near the,1,1,1.0
generated near the decision,1,1,1.0
the decision boundary on,1,1,1.0
decision boundary on a,1,1,1.0
boundary on a relative,1,1,1.0
on a relative basis,1,1,1.0
a relative basis as,1,1,1.0
relative basis as opposed,1,1,1.0
basis as opposed to,1,1,1.0
as opposed to an,1,1,1.0
opposed to an absolute,1,1,1.0
to an absolute basis,1,1,1.0
an absolute basis as,1,1,1.0
absolute basis as a,1,1,1.0
basis as a result,1,1,1.0
as a result the,2,1,2.0
a result the negative,1,1,1.0
result the negative impact,1,1,1.0
the negative impact of,1,1,1.0
negative impact of noise,1,1,1.0
impact of noise is,1,1,1.0
of noise is attenuated,1,1,1.0
noise is attenuated in,1,1,1.0
is attenuated in ramoboost,1,1,1.0
attenuated in ramoboost as,1,1,1.0
in ramoboost as compared,1,1,1.0
ramoboost as compared to,1,1,1.0
as compared to adasyn,1,1,1.0
compared to adasyn in,1,1,1.0
to adasyn in order,1,1,1.0
adasyn in order to,1,1,1.0
in order to compare,1,1,1.0
order to compare the,1,1,1.0
to compare the data,1,1,1.0
compare the data generation,1,1,1.0
generation mechanism of ramoboost,3,1,3.0
mechanism of ramoboost with,1,1,1.0
of ramoboost with that,1,1,1.0
ramoboost with that of,1,1,1.0
with that of smote,1,1,1.0
that of smote and,2,1,2.0
smote and adasyn we,1,1,1.0
and adasyn we provide,1,1,1.0
adasyn we provide an,1,1,1.0
we provide an example,1,1,1.0
provide an example of,1,1,1.0
example of a dataset,1,1,1.0
of a dataset with,1,1,1.0
a dataset with majority,1,1,1.0
dataset with majority examples,1,1,1.0
with majority examples and,1,1,1.0
majority examples and minority,1,1,1.0
examples and minority examples,1,1,1.0
and minority examples fig,1,1,1.0
minority examples fig a,1,1,1.0
examples fig a shows,1,1,1.0
fig a shows the,1,1,1.0
a shows the original,1,1,1.0
shows the original imbalanced,1,1,1.0
the original imbalanced data,1,1,1.0
original imbalanced data distribution,1,1,1.0
imbalanced data distribution and,1,1,1.0
data distribution and fig,1,1,1.0
distribution and fig b,1,1,1.0
and fig b d,1,1,1.0
fig b d shows,1,1,1.0
b d shows the,1,1,1.0
d shows the data,1,1,1.0
shows the data distribution,1,1,1.0
the data distribution data,1,1,1.0
data distribution data distribution,1,1,1.0
distribution data distribution and,1,1,1.0
data distribution and the,1,1,1.0
distribution and the data,1,1,1.0
and the data distribution,1,1,1.0
the data distribution respectively,1,1,1.0
data distribution respectively in,1,1,1.0
distribution respectively in all,1,1,1.0
respectively in all of,1,1,1.0
in all of these,1,1,1.0
all of these ﬁgures,1,1,1.0
of these ﬁgures the,1,1,1.0
these ﬁgures the plus,1,1,1.0
ﬁgures the plus and,1,1,1.0
the plus and point,1,1,1.0
plus and point shapes,1,1,1.0
and point shapes represent,1,1,1.0
point shapes represent the,1,1,1.0
shapes represent the inal,1,1,1.0
represent the inal majority,1,1,1.0
the inal majority data,1,1,1.0
inal majority data original,1,1,1.0
majority data original minority,1,1,1.0
data original minority data,1,1,1.0
original minority data and,1,1,1.0
minority data and the,1,1,1.0
data and the generated,1,1,1.0
and the generated synthetic,1,1,1.0
the generated synthetic data,1,1,1.0
generated synthetic data respectively,1,1,1.0
synthetic data respectively furthermore,1,1,1.0
data respectively furthermore for,1,1,1.0
respectively furthermore for each,1,1,1.0
furthermore for each ﬁgure,1,1,1.0
for each ﬁgure we,1,1,1.0
each ﬁgure we also,1,1,1.0
ﬁgure we also illustrate,1,1,1.0
we also illustrate the,1,1,1.0
also illustrate the classiﬁcation,1,1,1.0
illustrate the classiﬁcation confusion,1,1,1.0
the classiﬁcation confusion matrix,1,1,1.0
classiﬁcation confusion matrix in,1,1,1.0
confusion matrix in terms,1,1,1.0
matrix in terms of,1,1,1.0
in terms of instant,1,1,1.0
terms of instant counts,1,1,1.0
of instant counts for,1,1,1.0
instant counts for performance,1,1,1.0
counts for performance assessment,1,1,1.0
for performance assessment here,1,1,1.0
performance assessment here we,1,1,1.0
assessment here we follow,1,1,1.0
here we follow the,1,1,1.0
we follow the suggestions,1,1,1.0
follow the suggestions of,1,1,1.0
the suggestions of and,1,1,1.0
suggestions of and and,1,1,1.0
of and and use,1,1,1.0
and and use the,1,1,1.0
and use the minority,1,1,1.0
use the minority class,1,1,1.0
the minority class as,1,1,1.0
minority class as the,1,1,1.0
class as the positive,1,1,1.0
as the positive class,1,1,1.0
the positive class and,1,1,1.0
positive class and majority,1,1,1.0
and majority class as,1,1,1.0
majority class as the,1,1,1.0
class as the negative,1,1,1.0
the negative class the,1,1,1.0
negative class the classiﬁer,1,1,1.0
class the classiﬁer used,1,1,1.0
the classiﬁer used to,1,1,1.0
classiﬁer used to make,1,1,1.0
used to make predictions,1,1,1.0
to make predictions on,1,1,1.0
make predictions on all,1,1,1.0
predictions on all datasets,1,1,1.0
on all datasets shown,1,1,1.0
all datasets shown in,1,1,1.0
datasets shown in fig,1,1,1.0
shown in fig is,1,1,1.0
in fig is classiﬁcation,1,1,1.0
fig is classiﬁcation and,1,1,1.0
is classiﬁcation and regression,1,1,1.0
classiﬁcation and regression tree,1,1,1.0
and regression tree comparing,1,1,1.0
regression tree comparing the,1,1,1.0
tree comparing the confusion,1,1,1.0
comparing the confusion matrix,1,1,1.0
the confusion matrix of,1,1,1.0
confusion matrix of each,1,1,1.0
matrix of each ﬁgure,1,1,1.0
of each ﬁgure we,1,1,1.0
each ﬁgure we see,1,1,1.0
ﬁgure we see that,1,1,1.0
we see that the,1,1,1.0
see that the proposed,1,1,1.0
that the proposed ramoboost,1,1,1.0
the proposed ramoboost method,2,1,2.0
proposed ramoboost method can,1,1,1.0
ramoboost method can improve,1,1,1.0
method can improve classiﬁcation,1,1,1.0
can improve classiﬁcation performance,1,1,1.0
improve classiﬁcation performance speciﬁcally,1,1,1.0
classiﬁcation performance speciﬁcally the,1,1,1.0
performance speciﬁcally the improvement,1,1,1.0
speciﬁcally the improvement of,1,1,1.0
the improvement of true,1,1,1.0
improvement of true negative,1,1,1.0
of true negative tn,1,1,1.0
true negative tn counts,1,1,1.0
negative tn counts for,1,1,1.0
tn counts for smote,1,1,1.0
counts for smote with,1,1,1.0
for smote with respect,1,1,1.0
smote with respect to,1,1,1.0
respect to the original,1,1,1.0
to the original dataset,1,1,1.0
the original dataset changes,1,1,1.0
original dataset changes from,1,1,1.0
dataset changes from to,1,1,1.0
changes from to while,1,1,1.0
from to while for,1,1,1.0
to while for ramoboost,1,1,1.0
while for ramoboost it,1,1,1.0
for ramoboost it increases,1,1,1.0
ramoboost it increases from,1,1,1.0
it increases from to,1,1,1.0
increases from to this,1,1,1.0
from to this is,1,1,1.0
to this is because,1,1,1.0
this is because in,1,1,1.0
is because in smote,1,1,1.0
because in smote the,1,1,1.0
in smote the same,1,1,1.0
smote the same numbers,1,1,1.0
same numbers of instances,1,1,1.0
numbers of instances are,1,1,1.0
of instances are generated,1,1,1.0
instances are generated for,1,1,1.0
are generated for each,1,1,1.0
each minority example while,1,1,1.0
minority example while in,1,1,1.0
example while in ramoboost,1,1,1.0
while in ramoboost the,1,1,1.0
in ramoboost the data,1,1,1.0
ramoboost the data generation,1,1,1.0
the data generation process,1,1,1.0
data generation process is,1,1,1.0
generation process is adaptive,1,1,1.0
process is adaptive according,1,1,1.0
is adaptive according to,1,1,1.0
adaptive according to the,1,1,1.0
according to the data,1,1,1.0
to the data distribution,1,1,1.0
the data distribution from,1,1,1.0
data distribution from fig,1,1,1.0
distribution from fig c,1,1,1.0
from fig c we,1,1,1.0
fig c we also,1,1,1.0
c we also see,1,1,1.0
we also see that,1,1,1.0
also see that adasyn,1,1,1.0
see that adasyn is,1,1,1.0
that adasyn is very,1,1,1.0
adasyn is very aggressive,1,1,1.0
is very aggressive in,1,1,1.0
very aggressive in learning,1,1,1.0
aggressive in learning from,1,1,1.0
in learning from the,1,1,1.0
learning from the boundary,1,1,1.0
from the boundary since,2,1,2.0
the boundary since it,2,1,2.0
boundary since it generates,2,1,2.0
since it generates synthetic,1,1,1.0
it generates synthetic data,1,1,1.0
generates synthetic data instances,1,1,1.0
synthetic data instances very,1,1,1.0
data instances very close,2,1,2.0
instances very close to,2,1,2.0
decision boundary this may,1,1,1.0
boundary this may have,1,1,1.0
this may have two,1,1,1.0
may have two effects,1,1,1.0
have two effects on,1,1,1.0
two effects on the,1,1,1.0
effects on the learning,1,1,1.0
on the learning performance,1,1,1.0
the learning performance it,1,1,1.0
learning performance it may,1,1,1.0
performance it may increasechen,1,1,1.0
it may increasechen et,1,1,1.0
may increasechen et al,1,1,1.0
increasechen et al ranked,1,1,1.0
oversampling in boosting a,1,1,1.0
in boosting a b,1,1,1.0
boosting a b c,1,1,1.0
a b c d,2,1,2.0
b c d hypothesis,1,1,1.0
c d hypothesis outputy,1,1,1.0
d hypothesis outputy p,1,1,1.0
hypothesis outputy p true,4,1,4.0
outputy p true class,4,1,4.0
p true class n,4,1,4.0
true class n n,4,1,4.0
class n n hypothesis,3,1,3.0
n n hypothesis outputy,3,1,3.0
n hypothesis outputy p,3,1,3.0
class n n fig,1,1,1.0
n n fig comparison,1,1,1.0
n fig comparison of,1,1,1.0
fig comparison of different,1,1,1.0
comparison of different synthetic,1,1,1.0
of different synthetic data,1,1,1.0
different synthetic data generation,1,1,1.0
synthetic data generation mechanisms,1,1,1.0
data generation mechanisms a,1,1,1.0
generation mechanisms a original,1,1,1.0
mechanisms a original imbalanced,1,1,1.0
a original imbalanced data,1,1,1.0
original imbalanced data distribu,1,1,1.0
imbalanced data distribu tion,1,1,1.0
data distribu tion majority,1,1,1.0
distribu tion majority examples,1,1,1.0
tion majority examples and,1,1,1.0
majority examples and mi,1,1,1.0
examples and mi nority,1,1,1.0
and mi nority examples,1,1,1.0
mi nority examples b,1,1,1.0
nority examples b data,1,1,1.0
examples b data distribution,1,1,1.0
b data distribution after,1,1,1.0
data distribution after smote,1,1,1.0
distribution after smote method,1,1,1.0
after smote method c,1,1,1.0
smote method c data,1,1,1.0
method c data distribu,1,1,1.0
c data distribu tion,1,1,1.0
data distribu tion after,1,1,1.0
distribu tion after adasyn,1,1,1.0
tion after adasyn method,1,1,1.0
after adasyn method d,1,1,1.0
adasyn method d data,1,1,1.0
method d data dis,1,1,1.0
d data dis tribution,1,1,1.0
data dis tribution after,1,1,1.0
dis tribution after ramoboost,1,1,1.0
tribution after ramoboost method,1,1,1.0
after ramoboost method the,1,1,1.0
ramoboost method the classiﬁcation,1,1,1.0
method the classiﬁcation accuracy,1,1,1.0
the classiﬁcation accuracy of,1,1,1.0
classiﬁcation accuracy of the,1,1,1.0
accuracy of the minority,1,1,1.0
the minority data as,1,1,1.0
minority data as it,1,1,1.0
data as it provides,1,1,1.0
as it provides a,1,1,1.0
it provides a good,1,1,1.0
provides a good representation,1,1,1.0
a good representation of,1,1,1.0
good representation of the,1,1,1.0
representation of the minority,1,1,1.0
the minority data distribution,1,1,1.0
minority data distribution close,1,1,1.0
data distribution close to,1,1,1.0
distribution close to the,1,1,1.0
to the boundary thereby,1,1,1.0
the boundary thereby improving,1,1,1.0
boundary thereby improving the,1,1,1.0
thereby improving the recall,1,1,1.0
improving the recall performance,1,1,1.0
the recall performance which,1,1,1.0
recall performance which will,1,1,1.0
performance which will be,1,1,1.0
which will be discussed,1,1,1.0
will be discussed in,1,1,1.0
be discussed in detail,1,1,1.0
discussed in detail in,1,1,1.0
in detail in section,1,1,1.0
detail in section however,1,1,1.0
in section however it,1,1,1.0
section however it may,1,1,1.0
however it may also,1,1,1.0
it may also decrease,1,1,1.0
may also decrease the,1,1,1.0
also decrease the classiﬁcation,1,1,1.0
decrease the classiﬁcation performance,1,1,1.0
the classiﬁcation performance of,1,1,1.0
classiﬁcation performance of the,1,1,1.0
performance of the jority,1,1,1.0
of the jority class,1,1,1.0
the jority class which,1,1,1.0
jority class which in,1,1,1.0
class which in turn,1,1,1.0
which in turn deteriorates,1,1,1.0
in turn deteriorates the,1,1,1.0
turn deteriorates the overall,1,1,1.0
deteriorates the overall classiﬁcation,1,1,1.0
the overall classiﬁcation performance,1,1,1.0
overall classiﬁcation performance one,1,1,1.0
classiﬁcation performance one can,1,1,1.0
performance one can observe,1,1,1.0
one can observe from,1,1,1.0
can observe from fig,1,1,1.0
observe from fig c,1,1,1.0
from fig c that,1,1,1.0
fig c that although,1,1,1.0
c that although the,1,1,1.0
that although the classiﬁcation,1,1,1.0
although the classiﬁcation accuracy,1,1,1.0
the classiﬁcation accuracy for,1,1,1.0
classiﬁcation accuracy for minority,1,1,1.0
accuracy for minority examples,1,1,1.0
for minority examples under,1,1,1.0
minority examples under the,1,1,1.0
examples under the adasyn,1,1,1.0
under the adasyn technique,1,1,1.0
the adasyn technique is,1,1,1.0
adasyn technique is the,1,1,1.0
technique is the best,1,1,1.0
is the best among,1,1,1.0
the best among all,1,1,1.0
best among all these,1,1,1.0
among all these methods,1,1,1.0
all these methods true,1,1,1.0
these methods true positive,1,1,1.0
methods true positive tp,1,1,1.0
true positive tp therefore,1,1,1.0
positive tp therefore recall,1,1,1.0
tp therefore recall the,1,1,1.0
therefore recall the tn,1,1,1.0
recall the tn counts,1,1,1.0
the tn counts of,1,1,1.0
tn counts of adasyn,1,1,1.0
counts of adasyn also,1,1,1.0
of adasyn also decreases,1,1,1.0
adasyn also decreases signiﬁcantly,1,1,1.0
also decreases signiﬁcantly the,1,1,1.0
decreases signiﬁcantly the lowest,1,1,1.0
signiﬁcantly the lowest of,1,1,1.0
the lowest of all,1,1,1.0
lowest of all in,1,1,1.0
of all in this,1,1,1.0
all in this case,1,1,1.0
in this case with,1,1,1.0
this case with tn,1,1,1.0
case with tn to,1,1,1.0
with tn to this,1,1,1.0
tn to this end,1,1,1.0
to this end ramoboost,1,1,1.0
this end ramoboost can,1,1,1.0
end ramoboost can be,1,1,1.0
ramoboost can be considered,1,1,1.0
be considered to take,1,1,1.0
considered to take advantage,1,1,1.0
to take advantage of,1,1,1.0
take advantage of both,1,1,1.0
advantage of both smote,1,1,1.0
of both smote and,1,1,1.0
both smote and adasyn,1,1,1.0
smote and adasyn to,1,1,1.0
and adasyn to improve,1,1,1.0
adasyn to improve the,1,1,1.0
to improve the overall,1,1,1.0
improve the overall learning,1,1,1.0
the overall learning performance,1,1,1.0
overall learning performance our,1,1,1.0
learning performance our simulation,1,1,1.0
performance our simulation analyses,1,1,1.0
our simulation analyses which,1,1,1.0
simulation analyses which are,1,1,1.0
analyses which are based,1,1,1.0
which are based on,1,1,1.0
are based on various,1,1,1.0
based on various datasets,1,1,1.0
on various datasets and,1,1,1.0
various datasets and various,1,1,1.0
datasets and various assessment,1,1,1.0
and various assessment metrics,1,1,1.0
various assessment metrics in,1,1,1.0
assessment metrics in section,1,1,1.0
metrics in section also,1,1,1.0
in section also conﬁrm,1,1,1.0
section also conﬁrm this,1,1,1.0
also conﬁrm this the,1,1,1.0
conﬁrm this the boosting,1,1,1.0
this the boosting procedure,1,1,1.0
the boosting procedure boosting,1,1,1.0
boosting procedure boosting has,1,1,1.0
procedure boosting has attracted,1,1,1.0
boosting has attracted cantly,1,1,1.0
has attracted cantly increased,1,1,1.0
attracted cantly increased attention,1,1,1.0
cantly increased attention recently,1,1,1.0
increased attention recently in,1,1,1.0
attention recently in the,1,1,1.0
recently in the computational,1,1,1.0
in the computational gence,1,1,1.0
the computational gence community,1,1,1.0
computational gence community in,1,1,1.0
gence community in our,1,1,1.0
community in our proposed,1,1,1.0
in our proposed ramoboost,1,1,1.0
our proposed ramoboost approach,1,1,1.0
proposed ramoboost approach the,1,1,1.0
ramoboost approach the boosting,1,1,1.0
approach the boosting algorithm,1,1,1.0
the boosting algorithm is,1,1,1.0
boosting algorithm is essentially,1,1,1.0
algorithm is essentially the,1,1,1.0
is essentially the same,1,1,1.0
essentially the same as,1,1,1.0
the same as the,1,1,1.0
same as the classic,1,1,1.0
as the classic rather,1,1,1.0
the classic rather than,1,1,1.0
classic rather than reducing,1,1,1.0
rather than reducing the,1,1,1.0
than reducing the prediction,1,1,1.0
reducing the prediction error,1,1,1.0
the prediction error on,1,1,1.0
prediction error on the,1,1,1.0
error on the training,1,1,1.0
on the training dataset,1,1,1.0
training dataset in each,1,1,1.0
dataset in each iteration,1,1,1.0
in each iteration loop,2,1,2.0
each iteration loop in,1,1,1.0
iteration loop in a,1,1,1.0
loop in a stepwise,1,1,1.0
in a stepwise manner,1,1,1.0
a stepwise manner the,1,1,1.0
stepwise manner the boosting,1,1,1.0
manner the boosting algorithm,1,1,1.0
the boosting algorithm of,1,1,1.0
boosting algorithm of can,1,1,1.0
algorithm of can focus,1,1,1.0
of can focus the,1,1,1.0
can focus the weak,1,1,1.0
focus the weak learner,1,1,1.0
the weak learner on,1,1,1.0
weak learner on the,1,1,1.0
learner on the labels,1,1,1.0
on the labels that,1,1,1.0
the labels that are,1,1,1.0
labels that are hardest,1,1,1.0
that are hardest to,1,1,1.0
are hardest to discriminate,1,1,1.0
hardest to discriminate by,1,1,1.0
to discriminate by manipulating,1,1,1.0
discriminate by manipulating the,1,1,1.0
by manipulating the as,1,1,1.0
manipulating the as deﬁned,1,1,1.0
the as deﬁned in,1,1,1.0
as deﬁned in in,1,1,1.0
deﬁned in in other,1,1,1.0
in in other words,1,1,1.0
in other words the,1,1,1.0
other words the emphasis,1,1,1.0
words the emphasis of,1,1,1.0
the emphasis of is,1,1,1.0
emphasis of is to,1,1,1.0
of is to make,1,1,1.0
is to make the,1,1,1.0
to make the incorrect,1,1,1.0
make the incorrect class,1,1,1.0
the incorrect class label,1,1,1.0
incorrect class label as,1,1,1.0
class label as distinguishable,1,1,1.0
label as distinguishable as,1,1,1.0
as distinguishable as possible,1,1,1.0
distinguishable as possible from,1,1,1.0
as possible from the,1,1,1.0
possible from the correct,1,1,1.0
from the correct class,1,1,1.0
the correct class label,1,1,1.0
correct class label belonging,1,1,1.0
class label belonging to,1,1,1.0
label belonging to the,1,1,1.0
belonging to the example,1,1,1.0
to the example under,1,1,1.0
example under consideration this,1,1,1.0
under consideration this is,1,1,1.0
consideration this is why,1,1,1.0
is why the mislabeled,1,1,1.0
why the mislabeled dataset,1,1,1.0
the mislabeled dataset is,1,1,1.0
mislabeled dataset is sampled,1,1,1.0
dataset is sampled in,1,1,1.0
is sampled in each,1,1,1.0
sampled in each iteration,1,1,1.0
each iteration loop instead,1,1,1.0
iteration loop instead of,1,1,1.0
loop instead of the,1,1,1.0
instead of the correct,1,1,1.0
of the correct one,1,1,1.0
the correct one compared,1,1,1.0
correct one compared to,1,1,1.0
one compared to enables,1,1,1.0
compared to enables the,1,1,1.0
to enables the weak,1,1,1.0
enables the weak learner,1,1,1.0
the weak learner to,1,1,1.0
weak learner to make,1,1,1.0
learner to make useful,1,1,1.0
to make useful contributions,1,1,1.0
make useful contributions to,1,1,1.0
useful contributions to the,1,1,1.0
contributions to the accuracy,1,1,1.0
to the accuracy of,1,1,1.0
accuracy of the ﬁnal,1,1,1.0
of the ﬁnal hypothesis,1,1,1.0
the ﬁnal hypothesis even,1,1,1.0
ﬁnal hypothesis even when,1,1,1.0
hypothesis even when the,1,1,1.0
even when the weak,1,1,1.0
when the weak hypothesis,1,1,1.0
the weak hypothesis does,1,1,1.0
weak hypothesis does not,1,1,1.0
hypothesis does not predict,1,1,1.0
does not predict the,1,1,1.0
not predict the correct,1,1,1.0
predict the correct label,1,1,1.0
the correct label with,1,1,1.0
correct label with a,1,1,1.0
label with a probability,1,1,1.0
with a probability greater,1,1,1.0
a probability greater than,1,1,1.0
probability greater than in,1,1,1.0
greater than in this,1,1,1.0
than in this way,1,1,1.0
in this way the,1,1,1.0
this way the iteration,1,1,1.0
way the iteration loop,1,1,1.0
the iteration loop is,1,1,1.0
iteration loop is not,1,1,1.0
loop is not broken,1,1,1.0
is not broken regardless,1,1,1.0
not broken regardless of,1,1,1.0
broken regardless of the,1,1,1.0
regardless of the performance,1,1,1.0
performance of the trained,1,1,1.0
of the trained hypothesis,1,1,1.0
the trained hypothesis which,1,1,1.0
trained hypothesis which is,1,1,1.0
hypothesis which is not,1,1,1.0
which is not the,1,1,1.0
not the case for,1,1,1.0
the case for other,1,1,1.0
case for other boosting,1,1,1.0
for other boosting algorithms,1,1,1.0
other boosting algorithms including,1,1,1.0
boosting algorithms including given,1,1,1.0
algorithms including given that,1,1,1.0
including given that it,1,1,1.0
given that it is,1,1,1.0
that it is generally,1,1,1.0
it is generally very,1,1,1.0
is generally very hard,1,1,1.0
generally very hard for,1,1,1.0
very hard for a,1,1,1.0
hard for a single,1,1,1.0
for a single weak,1,1,1.0
a single weak learner,1,1,1.0
single weak learner to,1,1,1.0
weak learner to extract,1,1,1.0
learner to extract sufﬁcient,1,1,1.0
to extract sufﬁcient knowledge,1,1,1.0
extract sufﬁcient knowledge from,1,1,1.0
sufﬁcient knowledge from the,1,1,1.0
knowledge from the imbalanced,1,1,1.0
from the imbalanced dataset,1,1,1.0
the imbalanced dataset at,1,1,1.0
imbalanced dataset at one,1,1,1.0
dataset at one instance,1,1,1.0
at one instance the,1,1,1.0
one instance the performance,1,1,1.0
instance the performance of,1,1,1.0
the performance of ramoboost,3,1,3.0
performance of ramoboost and,1,1,1.0
of ramoboost and most,1,1,1.0
ramoboost and most algorithms,1,1,1.0
and most algorithms may,1,1,1.0
most algorithms may suffer,1,1,1.0
algorithms may suffer from,1,1,1.0
may suffer from an,1,1,1.0
suffer from an insufﬁcient,1,1,1.0
from an insufﬁcient number,1,1,1.0
an insufﬁcient number of,1,1,1.0
insufﬁcient number of boosting,1,1,1.0
number of boosting iterations,1,1,1.0
of boosting iterations thus,1,1,1.0
boosting iterations thus for,1,1,1.0
iterations thus for an,1,1,1.0
thus for an imbalanced,1,1,1.0
for an imbalanced ieee,1,1,1.0
an imbalanced ieee transactions,1,1,1.0
imbalanced ieee transactions on,1,1,1.0
vol no october of,1,1,1.0
no october of any,1,1,1.0
october of any size,1,1,1.0
of any size whose,1,1,1.0
any size whose target,1,1,1.0
size whose target concept,1,1,1.0
whose target concept we,1,1,1.0
target concept we assume,1,1,1.0
concept we assume is,1,1,1.0
we assume is difﬁcult,1,1,1.0
assume is difﬁcult to,1,1,1.0
is difﬁcult to learn,1,1,1.0
difﬁcult to learn it,1,1,1.0
to learn it is,1,1,1.0
learn it is our,1,1,1.0
it is our belief,1,1,1.0
is our belief that,1,1,1.0
our belief that the,1,1,1.0
belief that the performance,1,1,1.0
performance of ramoboost can,1,1,1.0
of ramoboost can be,1,1,1.0
ramoboost can be guaranteed,1,1,1.0
can be guaranteed satisfactory,1,1,1.0
be guaranteed satisfactory if,1,1,1.0
guaranteed satisfactory if it,1,1,1.0
satisfactory if it can,1,1,1.0
if it can iterate,1,1,1.0
it can iterate for,1,1,1.0
can iterate for enough,1,1,1.0
iterate for enough epochs,1,1,1.0
for enough epochs this,1,1,1.0
enough epochs this is,1,1,1.0
epochs this is our,1,1,1.0
this is our motivation,1,1,1.0
is our motivation for,1,1,1.0
our motivation for employing,1,1,1.0
motivation for employing the,1,1,1.0
for employing the algorithm,1,1,1.0
employing the algorithm in,1,1,1.0
the algorithm in ramoboost,1,1,1.0
algorithm in ramoboost computational,1,1,1.0
in ramoboost computational complexity,1,1,1.0
ramoboost computational complexity analysis,1,1,1.0
computational complexity analysis in,1,1,1.0
complexity analysis in the,1,1,1.0
analysis in the training,1,1,1.0
in the training stage,1,1,1.0
the training stage the,1,1,1.0
training stage the computational,1,1,1.0
stage the computational complexity,1,1,1.0
the computational complexity of,2,1,2.0
computational complexity of ramoboost,1,1,1.0
complexity of ramoboost arises,1,1,1.0
of ramoboost arises from,1,1,1.0
ramoboost arises from the,1,1,1.0
arises from the construction,1,1,1.0
from the construction of,1,1,1.0
the construction of the,1,1,1.0
construction of the hypothesis,1,1,1.0
of the hypothesis at,1,1,1.0
the hypothesis at each,1,1,1.0
each iteration loop as,1,1,1.0
iteration loop as well,1,1,1.0
loop as well as,1,1,1.0
well as the boosting,1,1,1.0
as the boosting procedure,1,1,1.0
the boosting procedure we,1,1,1.0
boosting procedure we assume,1,1,1.0
procedure we assume the,1,1,1.0
we assume the following,1,1,1.0
assume the following in,1,1,1.0
the following in our,1,1,1.0
following in our analysis,1,1,1.0
in our analysis dimension,1,1,1.0
our analysis dimension of,1,1,1.0
analysis dimension of the,1,1,1.0
dimension of the feature,1,1,1.0
the feature space number,1,1,1.0
feature space number of,1,1,1.0
space number of training,1,1,1.0
number of training examples,1,1,1.0
of training examples ratio,1,1,1.0
training examples ratio of,1,1,1.0
examples ratio of minority,1,1,1.0
ratio of minority examples,1,1,1.0
of minority examples in,1,1,1.0
minority examples in the,2,1,2.0
the training dataset p,1,1,1.0
training dataset p t,1,1,1.0
dataset p t number,1,1,1.0
p t number of,1,1,1.0
t number of boosting,1,1,1.0
number of boosting epochs,1,1,1.0
of boosting epochs the,1,1,1.0
boosting epochs the procedure,1,1,1.0
epochs the procedure of,1,1,1.0
the procedure of generatin,1,1,1.0
procedure of generatin g,1,1,1.0
of generatin g synthetic,1,1,1.0
generatin g synthetic instances,1,1,1.0
g synthetic instances for,1,1,1.0
synthetic instances for the,1,1,1.0
instances for the training,1,1,1.0
for the training dataset,1,1,1.0
the training dataset is,1,1,1.0
training dataset is initialized,1,1,1.0
dataset is initialized with,1,1,1.0
is initialized with the,1,1,1.0
initialized with the calculation,1,1,1.0
with the calculation of,1,1,1.0
the calculation of the,1,1,1.0
calculation of the probability,1,1,1.0
of the probability of,1,1,1.0
example for generating synthetic,1,1,1.0
for generating synthetic instances,1,1,1.0
generating synthetic instances the,1,1,1.0
synthetic instances the time,1,1,1.0
instances the time complexity,1,1,1.0
the time complexity can,1,1,1.0
time complexity can be,1,1,1.0
complexity can be decomposed,1,1,1.0
can be decomposed into,1,1,1.0
be decomposed into three,1,1,1.0
decomposed into three steps,1,1,1.0
into three steps calculating,1,1,1.0
three steps calculating the,1,1,1.0
steps calculating the euc,1,1,1.0
calculating the euc lidean,1,1,1.0
the euc lidean distance,1,1,1.0
euc lidean distance from,1,1,1.0
lidean distance from the,1,1,1.0
distance from the minority,1,1,1.0
from the minority example,1,1,1.0
under consideration to all,1,1,1.0
consideration to all the,1,1,1.0
to all the other,1,1,1.0
all the other examples,1,1,1.0
the other examples in,1,1,1.0
other examples in the,1,1,1.0
in the training o,1,1,1.0
the training o mn,1,1,1.0
training o mn sorting,1,1,1.0
o mn sorting all,1,1,1.0
mn sorting all current,1,1,1.0
sorting all current euclidean,1,1,1.0
all current euclidean distance,1,1,1.0
current euclidean distance calculations,1,1,1.0
euclidean distance calculations in,1,1,1.0
distance calculations in ascending,1,1,1.0
calculations in ascending o,1,1,1.0
in ascending o n,1,1,1.0
ascending o n log,1,1,1.0
o n log n,3,1,3.0
n log n r,1,1,1.0
log n r e,1,1,1.0
n r e t,1,1,1.0
r e t r,1,1,1.0
e t r i,1,1,1.0
t r i e,1,1,1.0
r i e v,1,1,1.0
i e v i,1,1,1.0
e v i n,1,1,1.0
v i n gt,1,1,1.0
i n gt h,1,1,1.0
n gt h eﬁ,1,1,1.0
gt h eﬁ r,1,1,1.0
h eﬁ r s,1,1,1.0
eﬁ r s examples,1,1,1.0
r s examples corresponding,1,1,1.0
s examples corresponding to,1,1,1.0
examples corresponding to the,1,1,1.0
corresponding to the ﬁrst,1,1,1.0
to the ﬁrst items,1,1,1.0
the ﬁrst items in,1,1,1.0
ﬁrst items in the,1,1,1.0
items in the sorted,1,1,1.0
in the sorted euclidean,1,1,1.0
the sorted euclidean distance,1,1,1.0
sorted euclidean distance complexity,1,1,1.0
euclidean distance complexity o,1,1,1.0
distance complexity o thus,1,1,1.0
complexity o thus the,1,1,1.0
o thus the time,1,1,1.0
thus the time complexity,1,1,1.0
the time complexity for,2,1,2.0
time complexity for this,1,1,1.0
complexity for this step,1,1,1.0
for this step should,1,1,1.0
this step should be,1,1,1.0
step should be o,1,1,1.0
should be o mn,1,1,1.0
be o mn n,1,1,1.0
o mn n log,1,1,1.0
mn n log n,1,1,1.0
n log n in,1,1,1.0
log n in typical,1,1,1.0
n in typical situations,1,1,1.0
in typical situations and,1,1,1.0
typical situations and m,1,1,1.0
situations and m are,1,1,1.0
and m are both,1,1,1.0
m are both signiﬁcantly,1,1,1.0
are both signiﬁcantly smaller,1,1,1.0
both signiﬁcantly smaller than,1,1,1.0
signiﬁcantly smaller than n,1,1,1.0
smaller than n which,1,1,1.0
than n which simpliﬁes,1,1,1.0
n which simpliﬁes the,1,1,1.0
which simpliﬁes the time,1,1,1.0
simpliﬁes the time complexity,1,1,1.0
the time complexity to,1,1,1.0
time complexity to approximately,1,1,1.0
complexity to approximately o,1,1,1.0
to approximately o n,1,1,1.0
approximately o n log,1,1,1.0
n log n the,1,1,1.0
log n the next,1,1,1.0
n the next step,1,1,1.0
the next step is,1,1,1.0
next step is to,1,1,1.0
step is to ﬁnd,1,1,1.0
is to ﬁnd the,1,1,1.0
to ﬁnd the minority,1,1,1.0
ﬁnd the minority neighbors,1,1,1.0
the minority neighbors of,1,1,1.0
minority neighbors of each,1,1,1.0
minority example for synthetic,1,1,1.0
example for synthetic data,1,1,1.0
synthetic data generation the,1,1,1.0
data generation the time,1,1,1.0
generation the time complexity,1,1,1.0
the time complexity of,2,1,2.0
time complexity of this,1,1,1.0
complexity of this calculation,1,1,1.0
of this calculation is,1,1,1.0
this calculation is the,1,1,1.0
calculation is the same,1,1,1.0
is the same with,1,1,1.0
the same with the,1,1,1.0
same with the ﬁrst,1,1,1.0
with the ﬁrst step,1,1,1.0
the ﬁrst step except,1,1,1.0
ﬁrst step except that,1,1,1.0
step except that the,1,1,1.0
except that the euclidean,1,1,1.0
the euclidean distance calcula,1,1,1.0
euclidean distance calcula tion,1,1,1.0
distance calcula tion is,1,1,1.0
calcula tion is between,1,1,1.0
tion is between the,1,1,1.0
is between the minority,1,1,1.0
between the minority examples,1,1,1.0
the minority examples therefore,1,1,1.0
minority examples therefore the,1,1,1.0
examples therefore the time,1,1,1.0
therefore the time complexity,1,1,1.0
the time complexity is,1,1,1.0
time complexity is no,1,1,1.0
complexity is no greater,1,1,1.0
is no greater than,1,1,1.0
no greater than o,1,1,1.0
greater than o n,1,1,1.0
than o n log,1,1,1.0
n log n since,1,1,1.0
log n since there,1,1,1.0
n since there are,1,1,1.0
since there are altogether,1,1,1.0
there are altogether np,1,1,1.0
are altogether np minority,1,1,1.0
altogether np minority examples,1,1,1.0
np minority examples in,1,1,1.0
the training dataset the,1,1,1.0
training dataset the total,1,1,1.0
dataset the total time,1,1,1.0
the total time complexity,1,1,1.0
total time complexity should,1,1,1.0
time complexity should be,1,1,1.0
complexity should be o,1,1,1.0
should be o log,1,1,1.0
be o log n,1,1,1.0
o log n which,1,1,1.0
log n which can,1,1,1.0
n which can be,1,1,1.0
which can be simpliﬁed,1,1,1.0
can be simpliﬁed to,1,1,1.0
be simpliﬁed to o,1,1,1.0
simpliﬁed to o log,1,1,1.0
to o log n,1,1,1.0
o log n lastly,1,1,1.0
log n lastly since,1,1,1.0
n lastly since data,1,1,1.0
lastly since data generation,1,1,1.0
since data generation is,1,1,1.0
data generation is applied,1,1,1.0
generation is applied in,1,1,1.0
is applied in each,1,1,1.0
applied in each boosting,1,1,1.0
in each boosting iteration,1,1,1.0
each boosting iteration the,1,1,1.0
boosting iteration the time,1,1,1.0
iteration the time complexity,1,1,1.0
time complexity of synthetic,1,1,1.0
complexity of synthetic data,1,1,1.0
of synthetic data generation,1,1,1.0
synthetic data generation for,1,1,1.0
data generation for the,1,1,1.0
generation for the learning,1,1,1.0
for the learning process,1,1,1.0
the learning process of,1,1,1.0
learning process of ramoboost,1,1,1.0
process of ramoboost is,1,1,1.0
of ramoboost is o,1,1,1.0
ramoboost is o log,1,1,1.0
is o log n,2,1,2.0
o log n for,1,1,1.0
log n for a,1,1,1.0
n for a neural,1,1,1.0
for a neural network,1,1,1.0
a neural network with,1,1,1.0
neural network with multilayer,1,1,1.0
network with multilayer perceptron,1,1,1.0
with multilayer perceptron mlp,1,1,1.0
multilayer perceptron mlp the,1,1,1.0
perceptron mlp the time,1,1,1.0
mlp the time complexity,1,1,1.0
time complexity for the,1,1,1.0
complexity for the boosting,1,1,1.0
for the boosting process,1,1,1.0
the boosting process excluding,1,1,1.0
boosting process excluding thetic,1,1,1.0
process excluding thetic data,1,1,1.0
excluding thetic data generation,1,1,1.0
thetic data generation is,1,1,1.0
data generation is at,1,1,1.0
generation is at worse,1,1,1.0
is at worse o,1,1,1.0
at worse o therefore,1,1,1.0
worse o therefore we,1,1,1.0
o therefore we summarize,1,1,1.0
therefore we summarize that,1,1,1.0
we summarize that the,1,1,1.0
summarize that the training,1,1,1.0
that the training process,1,1,1.0
the training process time,1,1,1.0
training process time complexity,1,1,1.0
process time complexity for,1,1,1.0
time complexity for ramoboost,1,1,1.0
complexity for ramoboost with,1,1,1.0
for ramoboost with mlp,1,1,1.0
ramoboost with mlp as,1,1,1.0
with mlp as a,1,1,1.0
mlp as a base,1,1,1.0
as a base classiﬁer,1,1,1.0
a base classiﬁer is,1,1,1.0
base classiﬁer is o,1,1,1.0
classiﬁer is o log,1,1,1.0
o log n in,1,1,1.0
log n in the,1,1,1.0
n in the testing,1,1,1.0
in the testing stage,1,1,1.0
the testing stage the,1,1,1.0
testing stage the computational,1,1,1.0
stage the computational operation,1,1,1.0
the computational operation in,1,1,1.0
computational operation in each,1,1,1.0
operation in each hypothesis,1,1,1.0
in each hypothesis is,1,1,1.0
each hypothesis is just,1,1,1.0
hypothesis is just a,1,1,1.0
is just a comparison,1,1,1.0
just a comparison operation,1,1,1.0
a comparison operation the,1,1,1.0
comparison operation the time,1,1,1.0
operation the time tion,1,1,1.0
the time tion for,1,1,1.0
time tion for each,1,1,1.0
tion for each of,1,1,1.0
for each of them,1,1,1.0
each of them is,1,1,1.0
of them is very,1,1,1.0
them is very small,1,1,1.0
is very small since,1,1,1.0
very small since the,1,1,1.0
small since the ﬁnal,1,1,1.0
since the ﬁnal hypothesis,1,1,1.0
the ﬁnal hypothesis is,1,1,1.0
ﬁnal hypothesis is a,1,1,1.0
hypothesis is a weighted,1,1,1.0
is a weighted combination,1,1,1.0
a weighted combination of,1,1,1.0
weighted combination of all,1,1,1.0
combination of all trained,1,1,1.0
of all trained hypothesis,1,1,1.0
all trained hypothesis as,1,1,1.0
trained hypothesis as shown,1,1,1.0
hypothesis as shown in,1,1,1.0
as shown in the,1,1,1.0
shown in the computational,1,1,1.0
in the computational complexity,1,1,1.0
computational complexity of predicting,1,1,1.0
complexity of predicting the,1,1,1.0
of predicting the class,1,1,1.0
predicting the class label,1,1,1.0
class label of an,1,1,1.0
label of an instance,1,1,1.0
of an instance can,1,1,1.0
an instance can be,1,1,1.0
instance can be estimated,1,1,1.0
can be estimated as,1,1,1.0
be estimated as o,1,1,1.0
estimated as o t,1,1,1.0
as o t iv,1,1,1.0
o t iv simulation,1,1,1.0
t iv simulation and,1,1,1.0
iv simulation and discussion,1,1,1.0
simulation and discussion in,1,1,1.0
and discussion in this,1,1,1.0
discussion in this section,1,1,1.0
this section we conduct,1,1,1.0
section we conduct various,1,1,1.0
we conduct various simulations,1,1,1.0
conduct various simulations of,1,1,1.0
various simulations of the,1,1,1.0
simulations of the proposed,1,1,1.0
proposed ramoboost method and,1,1,1.0
ramoboost method and compare,1,1,1.0
method and compare its,1,1,1.0
and compare its performance,1,1,1.0
compare its performance table,1,1,1.0
its performance table i,1,1,1.0
performance table i summary,1,1,1.0
table i summary of,1,1,1.0
i summary of the,1,1,1.0
summary of the datas,1,1,1.0
of the datas et,1,1,1.0
the datas et characteristics,1,1,1.0
datas et characteristics sorted,1,1,1.0
et characteristics sorted in,1,1,1.0
characteristics sorted in the,1,1,1.0
sorted in the degree,1,1,1.0
in the degree of,1,1,1.0
the degree of class,1,1,1.0
degree of class skew,1,1,1.0
of class skew dataset,1,1,1.0
class skew dataset feature,1,1,1.0
skew dataset feature data,1,1,1.0
dataset feature data minority,1,1,1.0
feature data minority majority,1,1,1.0
data minority majority imbalanced,1,1,1.0
minority majority imbalanced instances,1,1,1.0
majority imbalanced instances instances,1,1,1.0
imbalanced instances instances ratio,1,1,1.0
instances instances ratio sonar,1,1,1.0
instances ratio sonar spambase,1,1,1.0
ratio sonar spambase ionosphere,1,1,1.0
sonar spambase ionosphere pid,5,1,5.0
spambase ionosphere pid wine,5,1,5.0
ionosphere pid wine german,5,1,5.0
pid wine german phoneme,5,1,5.0
wine german phoneme vehicle,5,1,5.0
german phoneme vehicle texture,5,1,5.0
phoneme vehicle texture segment,5,1,5.0
vehicle texture segment satimage,5,1,5.0
texture segment satimage vow,6,1,6.0
segment satimage vow e,6,1,6.0
satimage vow e l,6,1,6.0
vow e l abalone,6,1,6.0
e l abalone glass,6,1,6.0
l abalone glass yeast,6,1,6.0
abalone glass yeast letter,6,1,6.0
glass yeast letter shuttle,5,1,5.0
yeast letter shuttle with,1,1,1.0
letter shuttle with smoteboost,1,1,1.0
shuttle with smoteboost smote,1,1,1.0
with smoteboost smote adasyn,1,1,1.0
smoteboost smote adasyn adacost,37,1,37.0
smote adasyn adacost linesmote,1,1,1.0
adasyn adacost linesmote and,1,1,1.0
adacost linesmote and across,1,1,1.0
linesmote and across different,1,1,1.0
and across different datasets,1,1,1.0
across different datasets the,1,1,1.0
different datasets the neural,1,1,1.0
datasets the neural network,1,1,1.0
the neural network with,1,1,1.0
neural network with mlp,1,1,1.0
network with mlp is,1,1,1.0
with mlp is employed,1,1,1.0
mlp is employed as,1,1,1.0
is employed as the,2,1,2.0
employed as the base,1,1,1.0
as the base learner,2,1,2.0
the base learner the,2,1,2.0
base learner the mlp,1,1,1.0
learner the mlp is,1,1,1.0
the mlp is conﬁgured,1,1,1.0
mlp is conﬁgured as,1,1,1.0
is conﬁgured as follows,1,1,1.0
conﬁgured as follows the,1,1,1.0
the number of hidden,3,1,3.0
number of hidden layer,3,1,3.0
of hidden layer neurons,3,1,3.0
hidden layer neurons is,1,1,1.0
layer neurons is set,1,1,1.0
neurons is set to,2,1,2.0
is set to be,3,1,3.0
set to be four,1,1,1.0
to be four and,1,1,1.0
be four and the,1,1,1.0
four and the number,1,1,1.0
the number of input,1,1,1.0
number of input neurons,1,1,1.0
of input neurons is,1,1,1.0
input neurons is equal,1,1,1.0
neurons is equal to,1,1,1.0
is equal to the,1,1,1.0
the number of features,1,1,1.0
number of features for,1,1,1.0
of features for each,1,1,1.0
features for each dataset,1,1,1.0
for each dataset similar,1,1,1.0
each dataset similar to,1,1,1.0
dataset similar to most,1,1,1.0
similar to most of,1,1,1.0
to most of the,1,1,1.0
most of the existing,1,1,1.0
of the existing imbalanced,2,1,2.0
the existing imbalanced learning,2,1,2.0
existing imbalanced learning methods,1,1,1.0
imbalanced learning methods in,1,1,1.0
learning methods in literature,1,1,1.0
methods in literature we,1,1,1.0
in literature we also,1,1,1.0
literature we also consider,1,1,1.0
we also consider only,1,1,1.0
also consider only imbalanced,1,1,1.0
consider only imbalanced problems,1,1,1.0
only imbalanced problems in,1,1,1.0
imbalanced problems in our,1,1,1.0
problems in our current,1,1,1.0
in our current study,1,1,1.0
our current study therefore,1,1,1.0
current study therefore the,1,1,1.0
study therefore the number,1,1,1.0
the number of output,1,1,1.0
number of output neurons,1,1,1.0
of output neurons is,1,1,1.0
output neurons is set,1,1,1.0
is set to two,1,1,1.0
set to two for,1,1,1.0
to two for all,1,1,1.0
two for all simulations,1,1,1.0
for all simulations the,1,1,1.0
all simulations the sigmoid,1,1,1.0
simulations the sigmoid function,1,1,1.0
the sigmoid function is,1,1,1.0
sigmoid function is used,1,1,1.0
function is used as,1,1,1.0
is used as the,3,1,3.0
used as the activation,1,1,1.0
as the activation function,1,1,1.0
the activation function and,1,1,1.0
activation function and the,1,1,1.0
function and the inner,1,1,1.0
and the inner training,1,1,1.0
the inner training epochs,1,1,1.0
inner training epochs is,1,1,1.0
training epochs is set,1,1,1.0
epochs is set to,1,1,1.0
set to be with,1,1,1.0
to be with a,1,1,1.0
be with a learning,1,1,1.0
with a learning rate,1,1,1.0
a learning rate of,1,1,1.0
learning rate of due,1,1,1.0
rate of due to,1,1,1.0
of due to the,1,1,1.0
due to the concern,1,1,1.0
to the concern that,1,1,1.0
the concern that the,1,1,1.0
concern that the scattered,1,1,1.0
that the scattered feature,1,1,1.0
the scattered feature distribution,1,1,1.0
scattered feature distribution of,1,1,1.0
feature distribution of some,1,1,1.0
distribution of some datasets,1,1,1.0
of some datasets may,1,1,1.0
some datasets may hinder,1,1,1.0
datasets may hinder the,1,1,1.0
may hinder the neural,1,1,1.0
hinder the neural network,1,1,1.0
the neural network from,1,1,1.0
neural network from converging,1,1,1.0
network from converging fast,1,1,1.0
from converging fast enough,1,1,1.0
converging fast enough for,1,1,1.0
fast enough for the,1,1,1.0
enough for the parameter,1,1,1.0
for the parameter acceleration,1,1,1.0
the parameter acceleration process,1,1,1.0
parameter acceleration process before,1,1,1.0
acceleration process before all,1,1,1.0
process before all datasets,1,1,1.0
before all datasets are,1,1,1.0
all datasets are presented,1,1,1.0
datasets are presented to,1,1,1.0
are presented to t,1,1,1.0
presented to t he,1,1,1.0
to t he comparative,1,1,1.0
t he comparative algorithms,1,1,1.0
he comparative algorithms for,1,1,1.0
comparative algorithms for learning,1,1,1.0
algorithms for learning we,1,1,1.0
for learning we ﬁrst,1,1,1.0
learning we ﬁrst use,1,1,1.0
we ﬁrst use the,1,1,1.0
ﬁrst use the nonlinear,1,1,1.0
use the nonlinear normalization,1,1,1.0
the nonlinear normalization approach,1,1,1.0
nonlinear normalization approach to,1,1,1.0
normalization approach to normalize,1,1,1.0
approach to normalize the,1,1,1.0
to normalize the features,1,1,1.0
normalize the features of,1,1,1.0
the features of the,1,1,1.0
features of the datasets,1,1,1.0
of the datasets to,1,1,1.0
the datasets to reside,1,1,1.0
datasets to reside in,1,1,1.0
to reside in the,1,1,1.0
reside in the interval,1,1,1.0
in the interval dataset,1,1,1.0
the interval dataset description,1,1,1.0
interval dataset description the,1,1,1.0
dataset description the performance,1,1,1.0
description the performance of,1,1,1.0
performance of ramoboost is,1,1,1.0
of ramoboost is evaluated,1,1,1.0
ramoboost is evaluated on,1,1,1.0
is evaluated on datasets,1,1,1.0
evaluated on datasets from,1,1,1.0
on datasets from the,1,1,1.0
from the uci machine,1,1,1.0
the uci machine learning,1,1,1.0
machine learning repository and,1,1,1.0
learning repository and elena,1,1,1.0
repository and elena project,1,1,1.0
and elena project these,1,1,1.0
elena project these datasets,1,1,1.0
project these datasets vary,1,1,1.0
these datasets vary in,1,1,1.0
datasets vary in size,1,1,1.0
vary in size and,1,1,1.0
in size and class,1,1,1.0
size and class distributions,1,1,1.0
and class distributions to,1,1,1.0
class distributions to ensure,1,1,1.0
distributions to ensure a,1,1,1.0
to ensure a thorough,1,1,1.0
ensure a thorough assessment,1,1,1.0
a thorough assessment of,1,1,1.0
thorough assessment of performance,1,1,1.0
assessment of performance table,1,1,1.0
of performance table i,1,1,1.0
performance table i summarizes,1,1,1.0
table i summarizes the,1,1,1.0
i summarizes the characteristics,1,1,1.0
summarizes the characteristics of,1,1,1.0
the characteristics of the,1,1,1.0
of the datasets used,1,1,1.0
the datasets used in,1,1,1.0
used in our simulation,1,1,1.0
in our simulation since,1,1,1.0
our simulation since several,1,1,1.0
simulation since several of,1,1,1.0
since several of the,1,1,1.0
several of the original,1,1,1.0
of the original datasets,1,1,1.0
the original datasets are,1,1,1.0
original datasets are multiclass,1,1,1.0
datasets are multiclass data,1,1,1.0
are multiclass data we,1,1,1.0
multiclass data we modiﬁed,1,1,1.0
data we modiﬁed those,1,1,1.0
we modiﬁed those datasets,1,1,1.0
modiﬁed those datasets following,1,1,1.0
those datasets following suggestions,1,1,1.0
datasets following suggestions in,1,1,1.0
following suggestions in literature,1,1,1.0
suggestions in literature to,1,1,1.0
in literature to make,1,1,1.0
literature to make them,1,1,1.0
to make them into,1,1,1.0
make them into datasets,1,1,1.0
them into datasets table,1,1,1.0
into datasets table ii,1,1,1.0
datasets table ii shows,1,1,1.0
table ii shows the,1,1,1.0
ii shows the modiﬁcations,1,1,1.0
shows the modiﬁcations that,1,1,1.0
the modiﬁcations that we,1,1,1.0
modiﬁcations that we used,1,1,1.0
that we used in,1,1,1.0
we used in this,1,1,1.0
used in this paper,2,1,2.0
in this paper to,1,1,1.0
this paper to create,1,1,1.0
paper to create the,1,1,1.0
to create the minority,1,1,1.0
create the minority and,1,1,1.0
and majority classes assessment,1,1,1.0
majority classes assessment metrics,1,1,1.0
classes assessment metrics under,1,1,1.0
assessment metrics under the,1,1,1.0
metrics under the imbalanced,1,1,1.0
under the imbalanced learning,1,1,1.0
the imbalanced learning scenario,1,1,1.0
imbalanced learning scenario the,1,1,1.0
learning scenario the conventional,1,1,1.0
scenario the conventional assessment,1,1,1.0
the conventional assessment method,1,1,1.0
conventional assessment method of,1,1,1.0
assessment method of using,1,1,1.0
method of using a,1,1,1.0
of using a single,1,1,1.0
using a single criterion,1,1,1.0
a single criterion such,1,1,1.0
single criterion such as,1,1,1.0
criterion such as overallchen,1,1,1.0
such as overallchen et,1,1,1.0
as overallchen et al,1,1,1.0
overallchen et al ranked,1,1,1.0
oversampling in boosting table,4,1,4.0
in boosting table ii,1,1,1.0
boosting table ii description,1,1,1.0
table ii description of,1,1,1.0
ii description of imbalanced,1,1,1.0
description of imbalanced datasets,1,1,1.0
of imbalanced datasets dataset,1,1,1.0
imbalanced datasets dataset minority,1,1,1.0
datasets dataset minority class,1,1,1.0
dataset minority class majority,1,1,1.0
minority class majority class,1,1,1.0
class majority class sonar,1,1,1.0
majority class sonar class,1,1,1.0
class sonar class r,1,1,1.0
sonar class r rock,1,1,1.0
class r rock instances,1,1,1.0
r rock instances class,1,1,1.0
rock instances class m,1,1,1.0
instances class m metal,1,1,1.0
class m metal cylinder,1,1,1.0
m metal cylinder instances,1,1,1.0
metal cylinder instances spambase,1,1,1.0
cylinder instances spambase spam,1,1,1.0
instances spambase spam email,1,1,1.0
spambase spam email legitimate,1,1,1.0
spam email legitimate email,1,1,1.0
email legitimate email ionosphere,1,1,1.0
legitimate email ionosphere bad,1,1,1.0
email ionosphere bad radar,1,1,1.0
ionosphere bad radar class,1,1,1.0
bad radar class good,1,1,1.0
radar class good radar,1,1,1.0
class good radar class,1,1,1.0
good radar class pid,1,1,1.0
radar class pid positive,1,1,1.0
class pid positive class,1,1,1.0
pid positive class negative,1,1,1.0
positive class negative class,1,1,1.0
class negative class wine,1,1,1.0
negative class wine class,1,1,1.0
class wine class classes,1,1,1.0
wine class classes and,1,1,1.0
class classes and german,1,1,1.0
classes and german customers,1,1,1.0
and german customers with,1,1,1.0
german customers with bad,1,1,1.0
customers with bad credit,1,1,1.0
with bad credit customers,1,1,1.0
bad credit customers with,1,1,1.0
credit customers with good,1,1,1.0
customers with good credit,1,1,1.0
with good credit phoneme,1,1,1.0
good credit phoneme class,1,1,1.0
credit phoneme class of,1,1,1.0
phoneme class of oral,1,1,1.0
class of oral sounds,1,1,1.0
of oral sounds class,1,1,1.0
oral sounds class class,1,1,1.0
sounds class class of,1,1,1.0
class class of nasal,1,1,1.0
class of nasal sounds,1,1,1.0
of nasal sounds class,1,1,1.0
nasal sounds class vehicle,1,1,1.0
sounds class vehicle class,1,1,1.0
class vehicle class of,1,1,1.0
vehicle class of van,1,1,1.0
class of van classes,1,1,1.0
of van classes of,1,1,1.0
van classes of opel,1,1,1.0
classes of opel saas,1,1,1.0
of opel saas and,1,1,1.0
opel saas and bus,1,1,1.0
saas and bus texture,1,1,1.0
and bus texture classes,1,1,1.0
bus texture classes of,1,1,1.0
texture classes of and,1,1,1.0
classes of and classes,1,1,1.0
of and classes of,1,1,1.0
and classes of and,1,1,1.0
classes of and segment,1,1,1.0
of and segment class,1,1,1.0
and segment class of,1,1,1.0
segment class of brickface,1,1,1.0
class of brickface classes,1,1,1.0
of brickface classes of,1,1,1.0
brickface classes of sky,1,1,1.0
classes of sky foliage,1,1,1.0
of sky foliage cement,1,1,1.0
sky foliage cement window,1,1,1.0
foliage cement window path,1,1,1.0
cement window path and,1,1,1.0
window path and grass,1,1,1.0
path and grass classes,1,1,1.0
and grass classes of,1,1,1.0
grass classes of horizontal,1,1,1.0
classes of horizontal line,1,1,1.0
of horizontal line graphic,1,1,1.0
horizontal line graphic class,1,1,1.0
line graphic class of,1,1,1.0
graphic class of text,1,1,1.0
class of text vertical,1,1,1.0
of text vertical line,1,1,1.0
text vertical line and,1,1,1.0
vertical line and picture,1,1,1.0
line and picture satimage,1,1,1.0
and picture satimage class,1,1,1.0
picture satimage class of,1,1,1.0
satimage class of damp,1,1,1.0
class of damp grey,1,1,1.0
of damp grey soil,1,1,1.0
damp grey soil classes,1,1,1.0
grey soil classes of,1,1,1.0
soil classes of red,1,1,1.0
classes of red soil,1,1,1.0
of red soil cotton,1,1,1.0
red soil cotton crop,1,1,1.0
soil cotton crop grey,1,1,1.0
cotton crop grey soil,1,1,1.0
crop grey soil soil,1,1,1.0
grey soil soil with,1,1,1.0
soil soil with vegetation,1,1,1.0
soil with vegetation stubble,1,1,1.0
with vegetation stubble and,1,1,1.0
vegetation stubble and very,1,1,1.0
stubble and very damp,1,1,1.0
and very damp grey,1,1,1.0
very damp grey soil,1,1,1.0
damp grey soil class,1,1,1.0
grey soil class of,1,1,1.0
soil class of digit,1,1,1.0
class of digit classes,1,1,1.0
of digit classes of,1,1,1.0
digit classes of digits,1,1,1.0
classes of digits and,1,1,1.0
of digits and vow,1,1,1.0
digits and vow e,1,1,1.0
and vow e l,1,1,1.0
vow e l class,1,1,1.0
e l class classes,1,1,1.0
l class classes of,1,1,1.0
class classes of to,1,1,1.0
classes of to abalone,1,1,1.0
of to abalone class,1,1,1.0
to abalone class of,1,1,1.0
abalone class of class,1,1,1.0
class of class of,1,1,1.0
of class of glass,1,1,1.0
class of glass class,1,1,1.0
of glass class tableware,1,1,1.0
glass class tableware all,1,1,1.0
class tableware all other,1,1,1.0
tableware all other classes,1,1,1.0
all other classes yeast,1,1,1.0
other classes yeast class,1,1,1.0
classes yeast class of,1,1,1.0
yeast class of pox,1,1,1.0
class of pox class,1,1,1.0
of pox class of,1,1,1.0
pox class of cyt,1,1,1.0
class of cyt letter,1,1,1.0
of cyt letter class,1,1,1.0
cyt letter class of,1,1,1.0
letter class of letter,1,1,1.0
class of letter z,1,1,1.0
of letter z classes,1,1,1.0
letter z classes of,1,1,1.0
z classes of letters,1,1,1.0
classes of letters a,1,1,1.0
of letters a y,1,1,1.0
letters a y shuttle,1,1,1.0
a y shuttle class,1,1,1.0
y shuttle class of,1,1,1.0
shuttle class of fpv,1,1,1.0
class of fpv close,1,1,1.0
of fpv close classes,1,1,1.0
fpv close classes of,1,1,1.0
close classes of rad,1,1,1.0
classes of rad flow,1,1,1.0
of rad flow fpv,1,1,1.0
rad flow fpv open,1,1,1.0
flow fpv open high,1,1,1.0
fpv open high bypass,1,1,1.0
open high bypass bpv,1,1,1.0
high bypass bpv close,1,1,1.0
bypass bpv close and,1,1,1.0
bpv close and bpv,1,1,1.0
close and bpv open,1,1,1.0
and bpv open accuracy,1,1,1.0
bpv open accuracy oa,1,1,1.0
open accuracy oa may,1,1,1.0
accuracy oa may not,1,1,1.0
oa may not be,1,1,1.0
may not be able,1,1,1.0
not be able to,1,1,1.0
be able to provide,1,1,1.0
able to provide a,1,1,1.0
to provide a comprehensive,1,1,1.0
provide a comprehensive assessment,1,1,1.0
a comprehensive assessment of,1,1,1.0
comprehensive assessment of the,1,1,1.0
assessment of the learning,1,1,1.0
of the learning algorithm,1,1,1.0
the learning algorithm considering,1,1,1.0
learning algorithm considering a,1,1,1.0
algorithm considering a simple,1,1,1.0
considering a simple case,1,1,1.0
a simple case of,1,1,1.0
simple case of a,1,1,1.0
case of a given,1,1,1.0
of a given dataset,1,1,1.0
a given dataset with,1,1,1.0
given dataset with minority,1,1,1.0
dataset with minority class,1,1,1.0
with minority class examples,1,1,1.0
minority class examples and,1,1,1.0
class examples and majority,1,1,1.0
examples and majority class,1,1,1.0
and majority class examples,1,1,1.0
majority class examples a,1,1,1.0
class examples a naïve,1,1,1.0
examples a naïve approach,1,1,1.0
a naïve approach of,1,1,1.0
naïve approach of classifying,1,1,1.0
approach of classifying every,1,1,1.0
of classifying every example,1,1,1.0
classifying every example to,1,1,1.0
every example to be,1,1,1.0
example to be the,1,1,1.0
to be the majority,1,1,1.0
be the majority class,1,1,1.0
majority class can at,1,1,1.0
class can at best,1,1,1.0
can at best provide,1,1,1.0
at best provide an,1,1,1.0
best provide an oa,1,1,1.0
provide an oa of,1,1,1.0
an oa of over,1,1,1.0
oa of over the,1,1,1.0
of over the entire,1,1,1.0
over the entire dataset,1,1,1.0
the entire dataset however,1,1,1.0
entire dataset however in,1,1,1.0
dataset however in many,1,1,1.0
however in many applications,1,1,1.0
in many applications such,1,1,1.0
many applications such as,1,1,1.0
applications such as biomedical,1,1,1.0
such as biomedical data,1,1,1.0
as biomedical data analysis,1,1,1.0
biomedical data analysis suc,1,1,1.0
data analysis suc h,1,1,1.0
analysis suc h a,1,1,1.0
suc h a classiﬁcation,1,1,1.0
h a classiﬁcation performance,1,1,1.0
a classiﬁcation performance would,1,1,1.0
classiﬁcation performance would be,1,1,1.0
performance would be unacceptable,1,1,1.0
would be unacceptable as,1,1,1.0
be unacceptable as it,1,1,1.0
unacceptable as it misclassiﬁes,1,1,1.0
as it misclassiﬁes all,1,1,1.0
it misclassiﬁes all the,1,1,1.0
misclassiﬁes all the minority,1,1,1.0
all the minority cases,1,1,1.0
the minority cases which,1,1,1.0
minority cases which generally,1,1,1.0
cases which generally are,1,1,1.0
which generally are more,1,1,1.0
generally are more important,1,1,1.0
are more important in,1,1,1.0
more important in such,1,1,1.0
important in such situations,1,1,1.0
in such situations as,1,1,1.0
such situations as a,1,1,1.0
situations as a result,1,1,1.0
a result the oa,1,1,1.0
result the oa by,1,1,1.0
the oa by itself,1,1,1.0
oa by itself may,1,1,1.0
by itself may not,1,1,1.0
itself may not be,1,1,1.0
may not be sufﬁcient,1,1,1.0
not be sufﬁcient in,1,1,1.0
be sufﬁcient in evaluating,1,1,1.0
sufﬁcient in evaluating the,1,1,1.0
in evaluating the classiﬁcation,1,1,1.0
evaluating the classiﬁcation performance,1,1,1.0
the classiﬁcation performance fo,1,1,1.0
classiﬁcation performance fo r,1,1,1.0
performance fo r imbalanced,1,1,1.0
fo r imbalanced learning,1,1,1.0
r imbalanced learning problems,1,1,1.0
imbalanced learning problems in,1,1,1.0
learning problems in our,1,1,1.0
problems in our simulations,1,1,1.0
in our simulations we,1,1,1.0
our simulations we adopt,1,1,1.0
simulations we adopt ﬁve,1,1,1.0
we adopt ﬁve major,1,1,1.0
adopt ﬁve major assessment,1,1,1.0
ﬁve major assessment metrics,1,1,1.0
major assessment metrics related,1,1,1.0
assessment metrics related to,1,1,1.0
metrics related to the,1,1,1.0
related to the confusion,1,1,1.0
to the confusion matrix,1,1,1.0
the confusion matrix for,1,1,1.0
confusion matrix for analysis,1,1,1.0
matrix for analysis oa,1,1,1.0
for analysis oa precision,1,1,1.0
analysis oa precision recall,1,1,1.0
oa precision recall and,1,1,1.0
precision recall and the,1,1,1.0
recall and the detailed,1,1,1.0
and the detailed discussions,1,1,1.0
the detailed discussions on,1,1,1.0
detailed discussions on these,1,1,1.0
discussions on these metrics,1,1,1.0
on these metrics and,1,1,1.0
these metrics and their,1,1,1.0
metrics and their appli,1,1,1.0
and their appli cations,1,1,1.0
their appli cations for,1,1,1.0
appli cations for imbalanced,1,1,1.0
cations for imbalanced learning,1,1,1.0
for imbalanced learning can,1,1,1.0
imbalanced learning can be,1,1,1.0
learning can be found,1,1,1.0
be found in in,1,1,1.0
found in in addition,1,1,1.0
in in addition to,1,1,1.0
in addition to these,1,1,1.0
addition to these singular,1,1,1.0
to these singular assessment,1,1,1.0
these singular assessment metrics,1,1,1.0
singular assessment metrics we,1,1,1.0
assessment metrics we also,1,1,1.0
metrics we also adopted,1,1,1.0
we also adopted the,1,1,1.0
also adopted the roc,1,1,1.0
adopted the roc graph,1,1,1.0
the roc graph for,1,1,1.0
roc graph for evaluation,1,1,1.0
graph for evaluation in,1,1,1.0
for evaluation in this,1,1,1.0
evaluation in this paper,1,1,1.0
in this paper brieﬂy,1,1,1.0
this paper brieﬂy speaking,1,1,1.0
paper brieﬂy speaking the,1,1,1.0
brieﬂy speaking the roc,1,1,1.0
speaking the roc space,1,1,1.0
the roc space is,1,1,1.0
roc space is established,1,1,1.0
space is established by,1,1,1.0
is established by plotting,1,1,1.0
established by plotting the,1,1,1.0
by plotting the tps,1,1,1.0
plotting the tps rate,1,1,1.0
the tps rate tp,1,1,1.0
tps rate tp over,1,1,1.0
rate tp over false,1,1,1.0
tp over false positives,1,1,1.0
over false positives rate,1,1,1.0
false positives rate fp,1,1,1.0
positives rate fp h,1,1,1.0
rate fp h e,1,1,1.0
fp h e roc,1,1,1.0
h e roc curves,1,1,1.0
e roc curves are,1,1,1.0
roc curves are normally,1,1,1.0
curves are normally formulated,1,1,1.0
are normally formulated by,1,1,1.0
normally formulated by adjusting,1,1,1.0
formulated by adjusting the,1,1,1.0
by adjusting the decision,1,1,1.0
adjusting the decision threshold,1,1,1.0
the decision threshold to,1,1,1.0
decision threshold to generate,1,1,1.0
threshold to generate a,1,1,1.0
to generate a seri,1,1,1.0
generate a seri es,1,1,1.0
a seri es of,1,1,1.0
seri es of points,1,1,1.0
es of points in,1,1,1.0
points in the roc,2,1,2.0
in the roc space,2,1,2.0
the roc space in,1,1,1.0
roc space in order,1,1,1.0
in order to assess,1,1,1.0
order to assess different,1,1,1.0
to assess different classiﬁers,1,1,1.0
assess different classiﬁers performance,1,1,1.0
different classiﬁers performance in,1,1,1.0
classiﬁers performance in this,1,1,1.0
performance in this case,2,1,2.0
in this case one,2,1,2.0
this case one generally,1,1,1.0
case one generally uses,1,1,1.0
one generally uses the,1,1,1.0
generally uses the auc,1,1,1.0
uses the auc as,1,1,1.0
the auc as an,1,1,1.0
auc as an evaluation,1,1,1.0
as an evaluation criterion,1,1,1.0
an evaluation criterion a,1,1,1.0
evaluation criterion a detailed,1,1,1.0
criterion a detailed discussion,1,1,1.0
a detailed discussion of,1,1,1.0
detailed discussion of roc,1,1,1.0
discussion of roc analysis,1,1,1.0
of roc analysis and,1,1,1.0
roc analysis and its,1,1,1.0
analysis and its assessment,1,1,1.0
and its assessment for,1,1,1.0
its assessment for classiﬁer,1,1,1.0
assessment for classiﬁer performances,1,1,1.0
for classiﬁer performances can,1,1,1.0
classiﬁer performances can be,1,1,1.0
performances can be found,1,1,1.0
be found in and,1,1,1.0
found in and we,1,1,1.0
in and we would,1,1,1.0
and we would also,1,1,1.0
we would also like,1,1,1.0
also like to note,1,1,1.0
like to note that,2,1,2.0
to note that there,1,1,1.0
note that there are,1,1,1.0
that there are other,1,1,1.0
there are other metrics,1,1,1.0
are other metrics that,1,1,1.0
other metrics that can,1,1,1.0
metrics that can potentially,1,1,1.0
that can potentially be,1,1,1.0
can potentially be used,1,1,1.0
potentially be used to,1,1,1.0
to assess the imbalanced,1,1,1.0
assess the imbalanced learning,1,1,1.0
the imbalanced learning performance,1,1,1.0
imbalanced learning performance for,1,1,1.0
learning performance for instance,1,1,1.0
performance for instance it,1,1,1.0
for instance it was,1,1,1.0
instance it was recently,1,1,1.0
it was recently presented,1,1,1.0
was recently presented in,1,1,1.0
recently presented in that,1,1,1.0
presented in that h,1,1,1.0
in that h could,1,1,1.0
that h could be,1,1,1.0
h could be a,1,1,1.0
could be a qualiﬁed,1,1,1.0
be a qualiﬁed alternative,1,1,1.0
a qualiﬁed alternative metric,1,1,1.0
qualiﬁed alternative metric of,1,1,1.0
alternative metric of auc,1,1,1.0
metric of auc the,1,1,1.0
of auc the major,1,1,1.0
auc the major motivation,1,1,1.0
the major motivation of,1,1,1.0
major motivation of h,1,1,1.0
motivation of h is,1,1,1.0
of h is based,1,1,1.0
h is based on,1,1,1.0
based on the fact,1,1,1.0
on the fact that,1,1,1.0
the fact that auc,1,1,1.0
fact that auc is,1,1,1.0
that auc is equivalent,1,1,1.0
auc is equivalent to,1,1,1.0
is equivalent to averaging,1,1,1.0
equivalent to averaging the,1,1,1.0
to averaging the misclassiﬁcation,1,1,1.0
averaging the misclassiﬁcation loss,1,1,1.0
the misclassiﬁcation loss over,1,1,1.0
misclassiﬁcation loss over a,1,1,1.0
loss over a cost,1,1,1.0
over a cost ratio,1,1,1.0
a cost ratio distribution,1,1,1.0
cost ratio distribution dependent,1,1,1.0
ratio distribution dependent on,1,1,1.0
distribution dependent on the,1,1,1.0
dependent on the score,1,1,1.0
on the score distributions,1,1,1.0
the score distributions which,1,1,1.0
score distributions which are,1,1,1.0
distributions which are decided,1,1,1.0
which are decided by,1,1,1.0
are decided by the,1,1,1.0
decided by the classiﬁer,1,1,1.0
by the classiﬁer itself,1,1,1.0
the classiﬁer itself rather,1,1,1.0
classiﬁer itself rather than,1,1,1.0
itself rather than the,1,1,1.0
rather than the roc,1,1,1.0
than the roc curve,1,1,1.0
the roc curve averaged,1,1,1.0
roc curve averaged roc,1,1,1.0
curve averaged roc curve,1,1,1.0
averaged roc curve roc,1,1,1.0
roc curve roc curve,1,1,1.0
curve roc curve point,1,1,1.0
roc curve point on,2,1,2.0
curve point on roc,1,1,1.0
point on roc curve,2,1,2.0
on roc curve l,1,1,1.0
roc curve l point,1,1,1.0
curve l point on,1,1,1.0
l point on roc,1,1,1.0
on roc curve point,1,1,1.0
curve point on averaged,1,1,1.0
point on averaged roc,1,1,1.0
on averaged roc curve,1,1,1.0
averaged roc curve x,1,1,1.0
roc curve x fig,1,1,1.0
curve x fig vertical,1,1,1.0
x fig vertical averaging,1,1,1.0
fig vertical averaging approach,1,1,1.0
vertical averaging approach of,1,1,1.0
averaging approach of roc,1,1,1.0
approach of roc curves,1,1,1.0
of roc curves target,1,1,1.0
roc curves target dataset,1,1,1.0
curves target dataset this,1,1,1.0
target dataset this may,1,1,1.0
dataset this may introduce,1,1,1.0
this may introduce undesired,1,1,1.0
may introduce undesired subjectivity,1,1,1.0
introduce undesired subjectivity into,1,1,1.0
undesired subjectivity into performance,1,1,1.0
subjectivity into performance evaluation,1,1,1.0
into performance evaluation h,1,1,1.0
performance evaluation h targets,1,1,1.0
evaluation h targets this,1,1,1.0
h targets this ﬂaw,1,1,1.0
targets this ﬂaw by,1,1,1.0
this ﬂaw by decoupling,1,1,1.0
ﬂaw by decoupling the,1,1,1.0
by decoupling the weight,1,1,1.0
decoupling the weight function,1,1,1.0
the weight function for,1,1,1.0
weight function for loss,1,1,1.0
function for loss calculation,1,1,1.0
for loss calculation from,1,1,1.0
loss calculation from score,1,1,1.0
calculation from score distributions,1,1,1.0
from score distributions for,1,1,1.0
score distributions for instance,1,1,1.0
distributions for instance it,1,1,1.0
for instance it can,1,1,1.0
instance it can apply,1,1,1.0
it can apply a,1,1,1.0
can apply a beta,1,1,1.0
apply a beta distribution,1,1,1.0
a beta distribution to,1,1,1.0
beta distribution to simulate,1,1,1.0
distribution to simulate the,1,1,1.0
to simulate the weight,1,1,1.0
simulate the weight function,1,1,1.0
the weight function which,1,1,1.0
weight function which ensures,1,1,1.0
function which ensures objectivity,1,1,1.0
which ensures objectivity across,1,1,1.0
ensures objectivity across all,1,1,1.0
objectivity across all algorithms,1,1,1.0
across all algorithms under,1,1,1.0
all algorithms under comparison,1,1,1.0
algorithms under comparison the,1,1,1.0
under comparison the interested,1,1,1.0
comparison the interested reader,1,1,1.0
the interested reader can,1,1,1.0
interested reader can refer,1,1,1.0
reader can refer to,1,1,1.0
can refer to for,1,1,1.0
refer to for further,1,1,1.0
to for further details,1,1,1.0
for further details on,1,1,1.0
further details on h,1,1,1.0
details on h and,1,1,1.0
on h and for,1,1,1.0
h and for a,1,1,1.0
and for a ical,1,1,1.0
for a ical review,1,1,1.0
a ical review of,1,1,1.0
ical review of the,1,1,1.0
review of the assessment,1,1,1.0
of the assessment met,1,1,1.0
the assessment met rics,1,1,1.0
assessment met rics for,1,1,1.0
met rics for imbalanced,1,1,1.0
rics for imbalanced learning,1,1,1.0
imbalanced learning in order,1,1,1.0
learning in order to,1,1,1.0
in order to reﬂect,1,1,1.0
order to reﬂect the,1,1,1.0
to reﬂect the roc,1,1,1.0
reﬂect the roc curve,1,1,1.0
the roc curve characteristics,1,1,1.0
roc curve characteristics for,1,1,1.0
curve characteristics for all,1,1,1.0
characteristics for all of,1,1,1.0
all of the random,1,1,1.0
of the random runs,2,1,2.0
the random runs we,1,1,1.0
random runs we adopt,1,1,1.0
runs we adopt the,1,1,1.0
we adopt the vertical,1,1,1.0
adopt the vertical averaging,1,1,1.0
the vertical averaging approach,1,1,1.0
vertical averaging approach in,1,1,1.0
averaging approach in to,1,1,1.0
approach in to plot,1,1,1.0
in to plot the,1,1,1.0
to plot the averaged,1,1,1.0
plot the averaged roc,1,1,1.0
the averaged roc curves,1,1,1.0
averaged roc curves our,1,1,1.0
roc curves our implementation,1,1,1.0
curves our implementation of,1,1,1.0
our implementation of the,1,1,1.0
implementation of the vertical,1,1,1.0
of the vertical averaging,1,1,1.0
the vertical averaging method,1,1,1.0
vertical averaging method is,1,1,1.0
averaging method is illustrated,1,1,1.0
method is illustrated in,1,1,1.0
is illustrated in fig,1,1,1.0
illustrated in fig assume,1,1,1.0
in fig assume one,1,1,1.0
fig assume one would,1,1,1.0
assume one would like,1,1,1.0
one would like to,1,1,1.0
would like to average,1,1,1.0
like to average two,1,1,1.0
to average two roc,1,1,1.0
average two roc curves,1,1,1.0
two roc curves and,1,1,1.0
roc curves and each,1,1,1.0
curves and each of,1,1,1.0
and each of which,1,1,1.0
each of which is,1,1,1.0
of which is formed,1,1,1.0
which is formed by,1,1,1.0
is formed by a,1,1,1.0
formed by a series,1,1,1.0
by a series of,1,1,1.0
a series of points,1,1,1.0
series of points in,1,1,1.0
the roc space the,1,1,1.0
roc space the ﬁrst,1,1,1.0
space the ﬁrst step,1,1,1.0
the ﬁrst step is,1,1,1.0
ﬁrst step is to,1,1,1.0
step is to evenly,1,1,1.0
is to evenly divide,1,1,1.0
to evenly divide the,1,1,1.0
evenly divide the range,1,1,1.0
divide the range of,1,1,1.0
the range of fp,1,1,1.0
range of fp into,1,1,1.0
of fp into a,1,1,1.0
fp into a set,1,1,1.0
into a set of,1,1,1.0
a set of intervals,1,1,1.0
set of intervals then,1,1,1.0
of intervals then at,1,1,1.0
intervals then at each,1,1,1.0
then at each interval,1,1,1.0
at each interval ﬁnd,1,1,1.0
each interval ﬁnd the,1,1,1.0
interval ﬁnd the ieee,1,1,1.0
ﬁnd the ieee transactions,1,1,1.0
the ieee transactions on,1,1,1.0
vol no october table,3,1,3.0
no october table iii,1,1,1.0
october table iii eva,1,1,1.0
table iii eva l,1,1,1.0
iii eva l uat,1,1,1.0
eva l uat i,1,1,1.0
l uat i o,1,1,1.0
uat i o nmetrics,1,1,1.0
i o nmetrics and,1,1,1.0
o nmetrics and performance,1,1,1.0
nmetrics and performance comparison,1,1,1.0
and performance comparison dataset,1,1,1.0
performance comparison dataset methods,1,1,1.0
comparison dataset methods oa,1,1,1.0
dataset methods oa precision,2,1,2.0
methods oa precision recall,2,1,2.0
oa precision recall auc,2,1,2.0
precision recall auc sonar,1,1,1.0
recall auc sonar ramoboost,1,1,1.0
auc sonar ramoboost smoteboost,1,1,1.0
sonar ramoboost smoteboost smote,1,1,1.0
ramoboost smoteboost smote adasyn,33,1,33.0
smote adasyn adacost borderlinesmote,36,1,36.0
adasyn adacost borderlinesmote spambase,1,1,1.0
adacost borderlinesmote spambase ramoboost,1,1,1.0
borderlinesmote spambase ramoboost smoteboost,1,1,1.0
spambase ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote ionosphere,1,1,1.0
adacost borderlinesmote ionosphere ramoboost,1,1,1.0
borderlinesmote ionosphere ramoboost smoteboost,1,1,1.0
ionosphere ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote pid,1,1,1.0
adacost borderlinesmote pid ramoboost,1,1,1.0
borderlinesmote pid ramoboost smoteboost,1,1,1.0
pid ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote wine,1,1,1.0
adacost borderlinesmote wine ramoboost,1,1,1.0
borderlinesmote wine ramoboost smoteboost,1,1,1.0
wine ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote german,1,1,1.0
adacost borderlinesmote german ramoboost,1,1,1.0
borderlinesmote german ramoboost smoteboost,1,1,1.0
german ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote phoneme,1,1,1.0
adacost borderlinesmote phoneme ramoboost,1,1,1.0
borderlinesmote phoneme ramoboost smoteboost,1,1,1.0
phoneme ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote vehicle,1,1,1.0
adacost borderlinesmote vehicle ramoboost,1,1,1.0
borderlinesmote vehicle ramoboost smoteboost,1,1,1.0
vehicle ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote texture,2,1,2.0
adacost borderlinesmote texture ramoboost,1,1,1.0
borderlinesmote texture ramoboost smoteboost,1,1,1.0
texture ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote segment,1,1,1.0
adacost borderlinesmote segment ramoboost,1,1,1.0
borderlinesmote segment ramoboost smoteboost,1,1,1.0
segment ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote et,1,1,1.0
adacost borderlinesmote et al,1,1,1.0
borderlinesmote et al ranked,1,1,1.0
oversampling in boosting dataset,1,1,1.0
in boosting dataset methods,1,1,1.0
boosting dataset methods oa,1,1,1.0
precision recall auc ramoboost,1,1,1.0
recall auc ramoboost smoteboost,1,1,1.0
auc ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote satimage,1,1,1.0
adacost borderlinesmote satimage ramoboost,1,1,1.0
borderlinesmote satimage ramoboost smoteboost,1,1,1.0
satimage ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote ramoboost,1,1,1.0
adacost borderlinesmote ramoboost smoteboost,1,1,1.0
borderlinesmote ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote vowel,1,1,1.0
adacost borderlinesmote vowel ramoboost,1,1,1.0
borderlinesmote vowel ramoboost smoteboost,1,1,1.0
vowel ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote abalone,1,1,1.0
adacost borderlinesmote abalone ramoboost,1,1,1.0
borderlinesmote abalone ramoboost smoteboost,1,1,1.0
abalone ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote glass,1,1,1.0
adacost borderlinesmote glass ramoboost,1,1,1.0
borderlinesmote glass ramoboost smoteboost,1,1,1.0
glass ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote yeast,1,1,1.0
adacost borderlinesmote yeast ramoboost,1,1,1.0
borderlinesmote yeast ramoboost smoteboost,1,1,1.0
yeast ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote letter,1,1,1.0
adacost borderlinesmote letter ramoboost,1,1,1.0
borderlinesmote letter ramoboost smoteboost,1,1,1.0
letter ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote shuttle,1,1,1.0
adacost borderlinesmote shuttle ramoboost,1,1,1.0
borderlinesmote shuttle ramoboost smoteboost,1,1,1.0
shuttle ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote tp,1,1,1.0
adacost borderlinesmote tp values,1,1,1.0
borderlinesmote tp values of,1,1,1.0
tp values of each,1,1,1.0
values of each roc,1,1,1.0
of each roc curve,1,1,1.0
each roc curve and,1,1,1.0
roc curve and average,1,1,1.0
curve and average them,1,1,1.0
and average them in,1,1,1.0
average them in fig,1,1,1.0
them in fig and,1,1,1.0
in fig and are,1,1,1.0
fig and are the,1,1,1.0
and are the points,1,1,1.0
are the points from,1,1,1.0
the points from and,1,1,1.0
points from and corresponding,1,1,1.0
from and corresponding to,1,1,1.0
and corresponding to the,1,1,1.0
corresponding to the interval,1,1,1.0
to the interval fp,1,1,1.0
the interval fp by,1,1,1.0
interval fp by averaging,1,1,1.0
fp by averaging their,1,1,1.0
by averaging their tp,1,1,1.0
averaging their tp values,1,1,1.0
their tp values the,1,1,1.0
tp values the corresponding,1,1,1.0
values the corresponding roc,1,1,1.0
the corresponding roc point,1,1,1.0
corresponding roc point on,1,1,1.0
roc point on the,1,1,1.0
point on the averaged,2,1,2.0
on the averaged roc,2,1,2.0
the averaged roc curve,2,1,2.0
averaged roc curve is,1,1,1.0
roc curve is obtained,1,1,1.0
curve is obtained however,1,1,1.0
is obtained however there,1,1,1.0
obtained however there exist,1,1,1.0
however there exist some,1,1,1.0
there exist some roc,1,1,1.0
exist some roc curves,1,1,1.0
some roc curves that,1,1,1.0
roc curves that do,1,1,1.0
curves that do not,1,1,1.0
that do not have,1,1,1.0
do not have corresponding,1,1,1.0
not have corresponding points,1,1,1.0
have corresponding points on,1,1,1.0
corresponding points on certain,1,1,1.0
points on certain intervals,1,1,1.0
on certain intervals in,1,1,1.0
certain intervals in this,1,1,1.0
intervals in this case,1,1,1.0
this case one can,1,1,1.0
case one can use,1,1,1.0
one can use the,1,1,1.0
can use the linear,1,1,1.0
use the linear interpolation,1,1,1.0
the linear interpolation method,1,1,1.0
linear interpolation method to,1,1,1.0
interpolation method to obtain,1,1,1.0
method to obtain the,1,1,1.0
to obtain the averaged,1,1,1.0
obtain the averaged roc,1,1,1.0
the averaged roc points,1,1,1.0
averaged roc points for,1,1,1.0
roc points for instance,1,1,1.0
points for instance in,1,1,1.0
for instance in fig,1,1,1.0
instance in fig the,1,1,1.0
in fig the point,1,1,1.0
fig the point corresponding,1,1,1.0
the point corresponding to,1,1,1.0
point corresponding to fp,1,1,1.0
corresponding to fp is,1,1,1.0
to fp is calculated,1,1,1.0
fp is calculated based,1,1,1.0
is calculated based on,1,1,1.0
calculated based on the,1,1,1.0
based on the linear,1,1,1.0
on the linear interpolation,1,1,1.0
the linear interpolation of,1,1,1.0
linear interpolation of the,1,1,1.0
interpolation of the two,1,1,1.0
of the two neighboring,1,1,1.0
the two neighboring points,1,1,1.0
two neighboring points and,1,1,1.0
neighboring points and n,1,1,1.0
points and n c,1,1,1.0
and n c is,1,1,1.0
n c is obtained,1,1,1.0
c is obtained it,1,1,1.0
is obtained it can,1,1,1.0
it can be averaged,1,1,1.0
can be averaged with,1,1,1.0
be averaged with to,1,1,1.0
averaged with to get,1,1,1.0
with to get the,1,1,1.0
to get the corresponding,1,1,1.0
get the corresponding point,1,1,1.0
the corresponding point on,1,1,1.0
corresponding point on the,1,1,1.0
averaged roc curve our,1,1,1.0
roc curve our auc,1,1,1.0
curve our auc results,1,1,1.0
our auc results presented,1,1,1.0
auc results presented in,1,1,1.0
results presented in this,2,1,2.0
in this section are,1,1,1.0
this section are based,1,1,1.0
section are based on,1,1,1.0
based on the average,2,1,2.0
on the average of,2,1,2.0
the average of all,1,1,1.0
average of all random,1,1,1.0
of all random runs,1,1,1.0
all random runs according,1,1,1.0
random runs according to,1,1,1.0
runs according to the,1,1,1.0
according to the vertical,1,1,1.0
to the vertical aver,1,1,1.0
the vertical aver aging,1,1,1.0
vertical aver aging ieee,1,1,1.0
aver aging ieee transactions,1,1,1.0
aging ieee transactions on,1,1,1.0
b c d e,1,1,1.0
c d e f,1,1,1.0
d e f ramoboost,1,1,1.0
e f ramoboost smoteboost,1,1,1.0
f ramoboost smoteboost smote,1,1,1.0
adasyn adacost borderlinesmote smote,2,1,2.0
adacost borderlinesmote smote adasyn,1,1,1.0
borderlinesmote smote adasyn adacost,1,1,1.0
smote adasyn adacost borerlinesmote,1,1,1.0
adasyn adacost borerlinesmote ramoboost,1,1,1.0
adacost borerlinesmote ramoboost smoteboost,1,1,1.0
borerlinesmote ramoboost smoteboost smoteboost,1,1,1.0
ramoboost smoteboost smoteboost smote,1,1,1.0
smoteboost smoteboost smote adasyn,1,1,1.0
smoteboost smote adasyn borderlinesmote,3,1,3.0
smote adasyn borderlinesmote ramoboost,1,1,1.0
adasyn borderlinesmote ramoboost adacost,1,1,1.0
borderlinesmote ramoboost adacost ramoboost,1,1,1.0
ramoboost adacost ramoboost smoteboost,1,1,1.0
adacost ramoboost smoteboost smote,1,1,1.0
adacost borderlinesmote smote borderlinesmote,1,1,1.0
borderlinesmote smote borderlinesmote adasyn,1,1,1.0
smote borderlinesmote adasyn adacost,1,1,1.0
borderlinesmote adasyn adacost smoteboost,1,1,1.0
adasyn adacost smoteboost ramoboost,1,1,1.0
adacost smoteboost ramoboost smoteboost,1,1,1.0
smoteboost ramoboost smoteboost adacostsmote,1,1,1.0
ramoboost smoteboost adacostsmote adasyn,1,1,1.0
smoteboost adacostsmote adasyn borderlinesmote,1,1,1.0
adacostsmote adasyn borderlinesmote ramoboost,1,1,1.0
adasyn borderlinesmote ramoboost fig,1,1,1.0
borderlinesmote ramoboost fig averaged,1,1,1.0
ramoboost fig averaged roc,1,1,1.0
fig averaged roc curves,1,1,1.0
averaged roc curves for,7,1,7.0
roc curves for ramoboost,1,1,1.0
curves for ramoboost smoteboost,1,1,1.0
for ramoboost smoteboost smote,1,1,1.0
ramoboost smoteboost smote adas,1,1,1.0
smoteboost smote adas yn,1,1,1.0
smote adas yn adacost,1,1,1.0
adas yn adacost borderlinesmote,1,1,1.0
yn adacost borderlinesmote and,1,1,1.0
adacost borderlinesmote and methods,1,1,1.0
borderlinesmote and methods a,1,1,1.0
and methods a averaged,1,1,1.0
methods a averaged roc,1,1,1.0
a averaged roc curves,1,1,1.0
roc curves for german,1,1,1.0
curves for german dataset,1,1,1.0
for german dataset b,1,1,1.0
german dataset b averaged,1,1,1.0
dataset b averaged roc,1,1,1.0
b averaged roc curves,1,1,1.0
roc curves for ionosphere,1,1,1.0
curves for ionosphere dataset,1,1,1.0
for ionosphere dataset c,1,1,1.0
ionosphere dataset c averaged,1,1,1.0
dataset c averaged roc,1,1,1.0
c averaged roc curves,1,1,1.0
roc curves for dataset,1,1,1.0
curves for dataset d,1,1,1.0
for dataset d averaged,1,1,1.0
dataset d averaged roc,1,1,1.0
d averaged roc curves,1,1,1.0
roc curves for phoneme,1,1,1.0
curves for phoneme dataset,1,1,1.0
for phoneme dataset e,1,1,1.0
phoneme dataset e averaged,1,1,1.0
dataset e averaged roc,1,1,1.0
e averaged roc curves,1,1,1.0
roc curves for satim,1,1,1.0
curves for satim age,1,1,1.0
for satim age dataset,1,1,1.0
satim age dataset f,1,1,1.0
age dataset f averaged,1,1,1.0
dataset f averaged roc,1,1,1.0
f averaged roc curves,1,1,1.0
roc curves for abalone,1,1,1.0
curves for abalone dataset,1,1,1.0
for abalone dataset table,1,1,1.0
abalone dataset table iv,1,1,1.0
dataset table iv simula,1,1,1.0
table iv simula tion,1,1,1.0
iv simula tion s,1,1,1.0
simula tion s ignificance,5,1,5.0
tion s ignificance test,5,1,5.0
s ignificance test of,12,1,12.0
ignificance test of averaged,8,1,8.0
test of averaged auc,8,1,8.0
of averaged auc b,8,1,8.0
averaged auc b etween,8,1,8.0
auc b etween ramob,8,1,8.0
b etween ramob oost,8,1,8.0
etween ramob oost and,8,1,8.0
ramob oost and smoteb,1,1,1.0
oost and smoteb oost,1,1,1.0
and smoteb oost dataset,1,1,1.0
smoteb oost dataset ramoboost,1,1,1.0
oost dataset ramoboost smoteboost,1,1,1.0
dataset ramoboost smoteboost difference,1,1,1.0
ramoboost smoteboost difference rank,1,1,1.0
smoteboost difference rank sonar,1,1,1.0
difference rank sonar spambase,2,1,2.0
rank sonar spambase ionosphere,2,1,2.0
yeast letter shuttle n,1,1,1.0
letter shuttle n d,1,1,1.0
shuttle n d in,1,1,1.0
n d in order,1,1,1.0
d in order to,1,1,1.0
in order to evaluate,1,1,1.0
order to evaluate the,1,1,1.0
to evaluate the signiﬁcance,1,1,1.0
evaluate the signiﬁcance of,1,1,1.0
the signiﬁcance of the,1,1,1.0
signiﬁcance of the simulation,1,1,1.0
of the simulation results,1,1,1.0
the simulation results of,2,1,2.0
simulation results of the,1,1,1.0
results of the comparative,1,1,1.0
of the comparative algorithms,5,1,5.0
the comparative algorithms wilcoxon,1,1,1.0
comparative algorithms wilcoxon test,1,1,1.0
algorithms wilcoxon test is,1,1,1.0
wilcoxon test is used,1,1,1.0
test is used in,1,1,1.0
is used in this,1,1,1.0
in this paper wilcoxon,1,1,1.0
this paper wilcoxon test,1,1,1.0
paper wilcoxon test is,1,1,1.0
wilcoxon test is a,1,1,1.0
test is a metric,1,1,1.0
is a metric statistical,1,1,1.0
a metric statistical procedure,1,1,1.0
metric statistical procedure for,1,1,1.0
statistical procedure for comparing,1,1,1.0
procedure for comparing two,1,1,1.0
for comparing two samples,1,1,1.0
comparing two samples that,1,1,1.0
two samples that are,1,1,1.0
samples that are paired,1,1,1.0
that are paired or,1,1,1.0
are paired or related,1,1,1.0
paired or related it,1,1,1.0
or related it assumes,1,1,1.0
related it assumes commensurability,1,1,1.0
it assumes commensurability of,1,1,1.0
assumes commensurability of ferences,1,1,1.0
commensurability of ferences but,1,1,1.0
of ferences but only,1,1,1.0
ferences but only qualitatively,1,1,1.0
but only qualitatively greater,1,1,1.0
only qualitatively greater differences,1,1,1.0
qualitatively greater differences still,1,1,1.0
greater differences still count,1,1,1.0
differences still count more,1,1,1.0
still count more which,1,1,1.0
count more which is,1,1,1.0
more which is probably,1,1,1.0
which is probably desired,1,1,1.0
is probably desired but,1,1,1.0
probably desired but the,1,1,1.0
desired but the absolute,1,1,1.0
but the absolute magnitudes,1,1,1.0
the absolute magnitudes are,1,1,1.0
absolute magnitudes are ignored,1,1,1.0
magnitudes are ignored from,1,1,1.0
are ignored from a,1,1,1.0
ignored from a statistical,1,1,1.0
from a statistical point,2,1,2.0
a statistical point of,2,1,2.0
statistical point of view,2,1,2.0
point of view the,1,1,1.0
of view the test,1,1,1.0
view the test is,1,1,1.0
the test is safer,1,1,1.0
test is safer since,1,1,1.0
is safer since it,1,1,1.0
safer since it does,1,1,1.0
since it does not,1,1,1.0
it does not assume,1,1,1.0
does not assume normal,1,1,1.0
not assume normal distributions,1,1,1.0
assume normal distributions also,1,1,1.0
normal distributions also outliers,1,1,1.0
distributions also outliers exceptionally,1,1,1.0
also outliers exceptionally performances,1,1,1.0
outliers exceptionally performances on,1,1,1.0
exceptionally performances on a,1,1,1.0
performances on a few,1,1,1.0
on a few datasets,1,1,1.0
a few datasets have,1,1,1.0
few datasets have less,1,1,1.0
datasets have less effect,1,1,1.0
have less effect on,1,1,1.0
less effect on the,1,1,1.0
effect on the wilcoxon,1,1,1.0
on the wilcoxon than,1,1,1.0
the wilcoxon than on,1,1,1.0
wilcoxon than on the,1,1,1.0
than on the suppose,1,1,1.0
on the suppose there,1,1,1.0
the suppose there are,1,1,1.0
suppose there are n,1,1,1.0
there are n objects,1,1,1.0
are n objects to,1,1,1.0
n objects to be,1,1,1.0
objects to be observed,1,1,1.0
to be observed by,1,1,1.0
be observed by two,1,1,1.0
observed by two gorithms,1,1,1.0
by two gorithms let,1,1,1.0
two gorithms let us,1,1,1.0
gorithms let us denote,1,1,1.0
let us denote the,1,1,1.0
us denote the difference,1,1,1.0
denote the difference value,1,1,1.0
the difference value of,2,1,2.0
difference value of the,2,1,2.0
value of the two,2,1,2.0
of the two table,1,1,1.0
the two table v,1,1,1.0
two table v simula,1,1,1.0
table v simula tion,1,1,1.0
v simula tion s,1,1,1.0
ramob oost and adacost,1,1,1.0
oost and adacost dataset,1,1,1.0
and adacost dataset ramoboost,1,1,1.0
adacost dataset ramoboost adacost,1,1,1.0
dataset ramoboost adacost difference,1,1,1.0
ramoboost adacost difference rank,1,1,1.0
adacost difference rank sonar,1,1,1.0
yeast letter shuttle and,1,1,1.0
letter shuttle and algorithms,1,1,1.0
shuttle and algorithms observation,1,1,1.0
and algorithms observation on,1,1,1.0
algorithms observation on the,1,1,1.0
observation on the ith,1,1,1.0
on the ith objects,1,1,1.0
the ith objects to,1,1,1.0
ith objects to be,1,1,1.0
objects to be di,1,1,1.0
to be di i,1,1,1.0
be di i the,1,1,1.0
di i the differences,1,1,1.0
i the differences are,1,1,1.0
the differences are ranked,1,1,1.0
differences are ranked according,1,1,1.0
are ranked according to,1,1,1.0
ranked according to their,1,1,1.0
according to their solute,1,1,1.0
to their solute values,1,1,1.0
their solute values ranks,1,1,1.0
solute values ranks of,1,1,1.0
values ranks of the,1,1,1.0
ranks of the tied,1,1,1.0
of the tied values,1,1,1.0
the tied values are,1,1,1.0
tied values are averaged,1,1,1.0
values are averaged let,1,1,1.0
are averaged let stand,1,1,1.0
averaged let stand for,1,1,1.0
let stand for the,1,1,1.0
stand for the sum,1,1,1.0
for the sum of,1,1,1.0
the sum of the,2,1,2.0
sum of the ranks,1,1,1.0
of the ranks of,1,1,1.0
the ranks of the,1,1,1.0
ranks of the objects,1,1,1.0
of the objects on,1,1,1.0
the objects on which,1,1,1.0
objects on which the,1,1,1.0
on which the difference,1,1,1.0
which the difference value,1,1,1.0
of the two algorithms,1,1,1.0
the two algorithms observations,1,1,1.0
two algorithms observations are,1,1,1.0
algorithms observations are greater,1,1,1.0
observations are greater than,1,1,1.0
are greater than zero,1,1,1.0
greater than zero and,1,1,1.0
than zero and denote,1,1,1.0
zero and denote the,1,1,1.0
and denote the sum,1,1,1.0
denote the sum of,1,1,1.0
sum of the opposite,1,1,1.0
of the opposite ranks,1,1,1.0
the opposite ranks of,1,1,1.0
opposite ranks of di,1,1,1.0
ranks of di are,1,1,1.0
of di are evenly,1,1,1.0
di are evenly split,1,1,1.0
are evenly split between,1,1,1.0
evenly split between and,1,1,1.0
split between and equations,1,1,1.0
between and equations and,1,1,1.0
and equations and conclude,1,1,1.0
equations and conclude the,1,1,1.0
and conclude the calculations,1,1,1.0
conclude the calculations of,1,1,1.0
the calculations of and,1,1,1.0
calculations of and di,1,1,1.0
of and di rank,1,1,1.0
and di rank di,1,1,1.0
di rank di rank,3,1,3.0
rank di rank di,2,1,2.0
rank di rank chen,1,1,1.0
di rank chen et,1,1,1.0
rank chen et al,1,1,1.0
chen et al ranked,1,1,1.0
in boosting table vi,1,1,1.0
boosting table vi simula,1,1,1.0
table vi simula tion,1,1,1.0
vi simula tion s,1,1,1.0
ramob oost and each,10,1,10.0
oost and each of,10,1,10.0
and each of smoteb,10,1,10.0
each of smoteb oost,10,1,10.0
of smoteb oost smote,10,1,10.0
smoteb oost smote adasyn,10,1,10.0
oost smote adasyn a,9,1,9.0
smote adasyn a dacost,9,1,9.0
adasyn a dacost b,9,1,9.0
a dacost b orderline,9,1,9.0
dacost b orderline smote,9,1,9.0
b orderline smote and,10,1,10.0
orderline smote and omek,10,1,10.0
smote and omek ramoboost,5,1,5.0
and omek ramoboost smoteboost,5,1,5.0
omek ramoboost smoteboost smote,5,1,5.0
adasyn adacost borderlinesmote t,5,1,5.0
adacost borderlinesmote t table,4,1,4.0
borderlinesmote t table vii,1,1,1.0
t table vii simula,1,1,1.0
table vii simula tion,1,1,1.0
vii simula tion auc,1,1,1.0
simula tion auc p,2,1,2.0
tion auc p erformance,2,1,2.0
auc p erformance characteristics,2,1,2.0
p erformance characteristics dataset,2,1,2.0
erformance characteristics dataset ramo,1,1,1.0
characteristics dataset ramo smote,1,1,1.0
dataset ramo smote adasyn,1,1,1.0
ramo smote adasyn borderlinesmote,2,1,2.0
smote adasyn borderlinesmote sonar,1,1,1.0
adasyn borderlinesmote sonar spambase,1,1,1.0
borderlinesmote sonar spambase ionosphere,2,1,2.0
yeast letter shuttle if,1,1,1.0
letter shuttle if we,1,1,1.0
shuttle if we set,1,1,1.0
if we set t,1,1,1.0
we set t min,1,1,1.0
set t min with,1,1,1.0
t min with a,1,1,1.0
min with a signiﬁcance,1,1,1.0
with a signiﬁcance level,1,1,1.0
level of α and,1,1,1.0
of α and the,1,1,1.0
α and the number,1,1,1.0
the number of observed,1,1,1.0
number of observed objects,1,1,1.0
of observed objects being,1,1,1.0
observed objects being n,1,1,1.0
objects being n the,1,1,1.0
being n the signiﬁcance,1,1,1.0
n the signiﬁcance value,1,1,1.0
the signiﬁcance value n,2,1,2.0
signiﬁcance value n that,1,1,1.0
value n that t,1,1,1.0
n that t should,1,1,1.0
that t should be,1,1,1.0
t should be equal,1,1,1.0
should be equal or,1,1,1.0
be equal or less,1,1,1.0
equal or less than,1,1,1.0
or less than for,1,1,1.0
less than for rejection,1,1,1.0
than for rejection of,1,1,1.0
for rejection of a,1,1,1.0
rejection of a null,1,1,1.0
of a null hypothesis,1,1,1.0
a null hypothesis can,1,1,1.0
null hypothesis can be,1,1,1.0
hypothesis can be retrieved,1,1,1.0
can be retrieved by,1,1,1.0
be retrieved by querying,1,1,1.0
retrieved by querying the,1,1,1.0
by querying the critical,1,1,1.0
querying the critical value,1,1,1.0
the critical value table,2,1,2.0
critical value table which,1,1,1.0
value table which can,1,1,1.0
table which can be,1,1,1.0
which can be accessed,1,1,1.0
can be accessed in,1,1,1.0
be accessed in in,1,1,1.0
accessed in in the,1,1,1.0
in in the rest,1,1,1.0
in the rest of,2,1,2.0
the rest of this,2,1,2.0
rest of this section,2,1,2.0
of this section wilcoxon,1,1,1.0
this section wilcoxon test,1,1,1.0
section wilcoxon test is,1,1,1.0
wilcoxon test is conducted,1,1,1.0
test is conducted between,1,1,1.0
is conducted between ramoboost,1,1,1.0
conducted between ramoboost and,1,1,1.0
between ramoboost and each,1,1,1.0
ramoboost and each of,1,1,1.0
and each of other,1,1,1.0
each of other comparative,1,1,1.0
of other comparative algorithms,1,1,1.0
other comparative algorithms ramoboost,1,1,1.0
comparative algorithms ramoboost smoteboost,1,1,1.0
algorithms ramoboost smoteboost ramoboost,1,1,1.0
ramoboost smoteboost ramoboost adacost,1,1,1.0
smoteboost ramoboost adacost ramo,1,1,1.0
ramoboost adacost ramo smote,1,1,1.0
adacost ramo smote etc,1,1,1.0
ramo smote etc in,1,1,1.0
smote etc in all,1,1,1.0
etc in all tables,1,1,1.0
in all tables presenting,1,1,1.0
all tables presenting the,1,1,1.0
tables presenting the results,1,1,1.0
presenting the results of,1,1,1.0
the results of signiﬁcance,1,1,1.0
results of signiﬁcance test,1,1,1.0
of signiﬁcance test the,1,1,1.0
signiﬁcance test the symbol,1,1,1.0
test the symbol signiﬁes,1,1,1.0
the symbol signiﬁes that,1,1,1.0
symbol signiﬁes that ramoboost,1,1,1.0
signiﬁes that ramoboost is,1,1,1.0
that ramoboost is quantitatively,1,1,1.0
ramoboost is quantitatively better,1,1,1.0
is quantitatively better than,1,1,1.0
quantitatively better than the,1,1,1.0
better than the comparative,1,1,1.0
than the comparative algorithm,1,1,1.0
the comparative algorithm under,1,1,1.0
comparative algorithm under consideration,1,1,1.0
algorithm under consideration in,1,1,1.0
under consideration in terms,1,1,1.0
consideration in terms of,1,1,1.0
terms of the speciﬁed,1,1,1.0
of the speciﬁed assessment,1,1,1.0
the speciﬁed assessment metric,1,1,1.0
speciﬁed assessment metric and,1,1,1.0
assessment metric and denotes,1,1,1.0
metric and denotes the,1,1,1.0
and denotes the opposite,1,1,1.0
denotes the opposite whenever,1,1,1.0
the opposite whenever there,1,1,1.0
opposite whenever there is,1,1,1.0
whenever there is a,1,1,1.0
there is a signiﬁcance,1,1,1.0
is a signiﬁcance existing,1,1,1.0
a signiﬁcance existing we,1,1,1.0
signiﬁcance existing we highlight,1,1,1.0
existing we highlight the,1,1,1.0
we highlight the corresponding,1,1,1.0
highlight the corresponding result,1,1,1.0
the corresponding result by,1,1,1.0
corresponding result by underscoring,1,1,1.0
result by underscoring it,1,1,1.0
by underscoring it simulation,1,1,1.0
underscoring it simulation results,1,1,1.0
it simulation results in,1,1,1.0
simulation results in our,1,1,1.0
results in our simulation,1,1,1.0
in our simulation we,1,1,1.0
our simulation we use,1,1,1.0
simulation we use boosting,1,1,1.0
we use boosting iterations,1,1,1.0
use boosting iterations t,1,1,1.0
boosting iterations t in,1,1,1.0
iterations t in the,1,1,1.0
t in the algorithm,1,1,1.0
in the algorithm as,1,1,1.0
the algorithm as suggested,1,1,1.0
algorithm as suggested in,1,1,1.0
as suggested in for,1,1,1.0
suggested in for ensemble,1,1,1.0
in for ensemble learning,1,1,1.0
for ensemble learning the,1,1,1.0
ensemble learning the number,1,1,1.0
learning the number of,1,1,1.0
of synthetic data generated,1,1,1.0
synthetic data generated at,1,1,1.0
data generated at each,1,1,1.0
generated at each boosting,1,1,1.0
each boosting iteration is,1,1,1.0
boosting iteration is set,1,1,1.0
iteration is set to,1,1,1.0
is set to of,1,1,1.0
set to of the,1,1,1.0
to of the number,1,1,1.0
the number of the,1,1,1.0
number of the minority,1,1,1.0
of the minority instances,1,1,1.0
the minority instances the,1,1,1.0
minority instances the parameters,1,1,1.0
instances the parameters and,1,1,1.0
the parameters and are,1,1,1.0
parameters and are set,1,1,1.0
and are set to,1,1,1.0
are set to be,1,1,1.0
set to be and,1,1,1.0
to be and respectively,1,1,1.0
be and respectively the,1,1,1.0
and respectively the scaling,1,1,1.0
respectively the scaling coefﬁcient,1,1,1.0
the scaling coefﬁcient α,1,1,1.0
scaling coefﬁcient α is,1,1,1.0
coefﬁcient α is set,1,1,1.0
α is set to,1,1,1.0
is set to which,1,1,1.0
set to which was,1,1,1.0
to which was sen,1,1,1.0
which was sen using,1,1,1.0
was sen using techniques,1,1,1.0
sen using techniques for,1,1,1.0
using techniques for optimizing,1,1,1.0
techniques for optimizing boost,1,1,1.0
for optimizing boost s,1,1,1.0
optimizing boost s performance,1,1,1.0
boost s performance for,1,1,1.0
s performance for smoteboost,1,1,1.0
performance for smoteboost smote,1,1,1.0
for smoteboost smote adasyn,1,1,1.0
smote adasyn borderlinesmote and,4,1,4.0
adasyn borderlinesmote and the,1,1,1.0
borderlinesmote and the number,1,1,1.0
the number of est,1,1,1.0
number of est neighbors,1,1,1.0
of est neighbors is,1,1,1.0
est neighbors is set,1,1,1.0
neighbors is set to,1,1,1.0
is set to ﬁve,1,1,1.0
set to ﬁve the,1,1,1.0
to ﬁve the cost,1,1,1.0
ﬁve the cost factor,1,1,1.0
the cost factor c,1,1,1.0
cost factor c for,1,1,1.0
factor c for adacost,1,1,1.0
c for adacost is,1,1,1.0
for adacost is set,1,1,1.0
adacost is set to,1,1,1.0
is set to three,1,1,1.0
set to three according,1,1,1.0
to three according to,1,1,1.0
three according to the,1,1,1.0
according to the suggestion,1,1,1.0
to the suggestion of,1,1,1.0
the suggestion of c,1,1,1.0
suggestion of c should,1,1,1.0
of c should be,1,1,1.0
c should be an,1,1,1.0
should be an integer,1,1,1.0
be an integer between,1,1,1.0
an integer between and,1,1,1.0
integer between and following,1,1,1.0
between and following the,1,1,1.0
and following the suggestion,1,1,1.0
following the suggestion of,1,1,1.0
the suggestion of the,1,1,1.0
suggestion of the signiﬁcance,1,1,1.0
of the signiﬁcance test,1,1,1.0
the signiﬁcance test are,1,1,1.0
signiﬁcance test are conducted,1,1,1.0
test are conducted on,1,1,1.0
are conducted on the,1,1,1.0
conducted on the averaged,1,1,1.0
on the averaged auc,4,1,4.0
the averaged auc of,2,1,2.0
averaged auc of all,1,1,1.0
auc of all algorithms,1,1,1.0
of all algorithms in,1,1,1.0
all algorithms in a,1,1,1.0
algorithms in a pairwise,1,1,1.0
in a pairwise table,1,1,1.0
a pairwise table viii,1,1,1.0
pairwise table viii simula,1,1,1.0
table viii simula tion,1,1,1.0
viii simula tion s,1,1,1.0
oost smote adasyn adacost,1,1,1.0
smote adasyn adacost b,1,1,1.0
adasyn adacost b orderline,1,1,1.0
adacost b orderline smote,1,1,1.0
smote and omek ramo,1,1,1.0
and omek ramo smote,1,1,1.0
omek ramo smote adasyn,1,1,1.0
smote adasyn borderlinesmote t,1,1,1.0
adasyn borderlinesmote t manner,1,1,1.0
borderlinesmote t manner for,1,1,1.0
t manner for all,1,1,1.0
manner for all simulations,1,1,1.0
for all simulations introduced,1,1,1.0
all simulations introduced in,1,1,1.0
simulations introduced in the,1,1,1.0
introduced in the rest,1,1,1.0
of this section simulation,1,1,1.0
this section simulation in,1,1,1.0
section simulation in this,1,1,1.0
simulation in this simulation,1,1,1.0
in this simulation we,1,1,1.0
this simulation we apply,1,1,1.0
simulation we apply all,1,1,1.0
we apply all ative,1,1,1.0
apply all ative algorithms,1,1,1.0
all ative algorithms to,1,1,1.0
ative algorithms to the,1,1,1.0
algorithms to the datasets,1,1,1.0
to the datasets described,1,1,1.0
the datasets described in,2,1,2.0
datasets described in table,2,1,2.0
described in table the,1,1,1.0
in table the simulation,1,1,1.0
table the simulation results,1,1,1.0
the simulation results are,2,1,2.0
simulation results are based,1,1,1.0
results are based on,1,1,1.0
the average of ten,1,1,1.0
average of ten runs,1,1,1.0
of ten runs at,1,1,1.0
ten runs at each,1,1,1.0
runs at each run,1,1,1.0
at each run we,1,1,1.0
each run we randomly,1,1,1.0
run we randomly select,1,1,1.0
we randomly select half,1,1,1.0
randomly select half of,1,1,1.0
select half of the,1,1,1.0
half of the dataset,1,1,1.0
of the dataset as,1,1,1.0
the dataset as training,1,1,1.0
dataset as training data,1,1,1.0
as training data and,1,1,1.0
training data and use,1,1,1.0
and use the remaining,1,1,1.0
use the remaining half,1,1,1.0
the remaining half as,1,1,1.0
remaining half as testing,1,1,1.0
half as testing data,1,1,1.0
as testing data fig,1,1,1.0
testing data fig gives,1,1,1.0
data fig gives several,1,1,1.0
fig gives several snapshots,1,1,1.0
gives several snapshots of,1,1,1.0
several snapshots of the,1,1,1.0
snapshots of the averaged,1,1,1.0
of the averaged roc,1,1,1.0
the averaged roc graphs,1,1,1.0
averaged roc graphs of,1,1,1.0
roc graphs of the,1,1,1.0
graphs of the ramoboost,1,1,1.0
of the ramoboost smoteboost,1,1,1.0
the ramoboost smoteboost smote,1,1,1.0
smoteboost smote adasyn cost,1,1,1.0
smote adasyn cost borderlinesmote,1,1,1.0
adasyn cost borderlinesmote and,1,1,1.0
cost borderlinesmote and methods,1,1,1.0
borderlinesmote and methods here,1,1,1.0
and methods here fig,1,1,1.0
methods here fig a,1,1,1.0
here fig a f,1,1,1.0
fig a f represents,1,1,1.0
a f represents the,1,1,1.0
f represents the results,1,1,1.0
represents the results for,1,1,1.0
results for the german,1,1,1.0
for the german ionosphere,1,1,1.0
the german ionosphere phoneme,1,1,1.0
german ionosphere phoneme satimage,1,1,1.0
ionosphere phoneme satimage and,1,1,1.0
phoneme satimage and abalone,1,1,1.0
satimage and abalone datasets,1,1,1.0
and abalone datasets respectively,1,1,1.0
abalone datasets respectively this,1,1,1.0
datasets respectively this ﬁgure,1,1,1.0
respectively this ﬁgure indicates,1,1,1.0
this ﬁgure indicates that,1,1,1.0
ﬁgure indicates that the,1,1,1.0
indicates that the ramoboost,1,1,1.0
that the ramoboost method,1,1,1.0
the ramoboost method is,1,1,1.0
ramoboost method is competitive,1,1,1.0
method is competitive when,1,1,1.0
is competitive when compared,1,1,1.0
competitive when compared to,1,1,1.0
to other methods in,1,1,1.0
other methods in roc,1,1,1.0
methods in roc space,1,1,1.0
in roc space table,1,1,1.0
roc space table iii,1,1,1.0
space table iii summarizes,1,1,1.0
table iii summarizes the,1,1,1.0
iii summarizes the performance,1,1,1.0
performance of the comparative,1,1,1.0
the comparative algorithms in,2,1,2.0
comparative algorithms in which,1,1,1.0
algorithms in which the,1,1,1.0
in which the best,3,1,3.0
which the best performance,3,1,3.0
the best performance of,1,1,1.0
best performance of each,1,1,1.0
performance of each algorithm,1,1,1.0
of each algorithm across,1,1,1.0
each algorithm across each,1,1,1.0
algorithm across each evaluation,1,1,1.0
across each evaluation criteria,1,1,1.0
each evaluation criteria i,1,1,1.0
evaluation criteria i s,1,1,1.0
criteria i s highlighted,1,1,1.0
i s highlighted from,1,1,1.0
s highlighted from table,1,1,1.0
highlighted from table iii,1,1,1.0
from table iii we,1,1,1.0
table iii we ﬁnd,1,1,1.0
iii we ﬁnd that,1,1,1.0
we ﬁnd that ramoboost,1,1,1.0
ﬁnd that ramoboost can,1,1,1.0
that ramoboost can provide,1,1,1.0
ramoboost can provide competitive,1,1,1.0
can provide competitive simulation,1,1,1.0
provide competitive simulation results,1,1,1.0
competitive simulation results on,1,1,1.0
simulation results on most,1,1,1.0
results on most of,1,1,1.0
on most of the,2,1,2.0
most of the datasets,2,1,2.0
of the datasets when,1,1,1.0
the datasets when compared,1,1,1.0
datasets when compared to,1,1,1.0
compared to other comparative,1,1,1.0
to other comparative algorithms,1,1,1.0
other comparative algorithms except,1,1,1.0
comparative algorithms except for,1,1,1.0
algorithms except for recall,1,1,1.0
except for recall performance,1,1,1.0
for recall performance ieee,1,1,1.0
recall performance ieee transactions,1,1,1.0
performance ieee transactions on,1,1,1.0
no october table ix,1,1,1.0
october table ix simula,1,1,1.0
table ix simula tion,1,1,1.0
ix simula tion auc,1,1,1.0
erformance characteristics dataset ramoboost,1,1,1.0
characteristics dataset ramoboost smoteboost,1,1,1.0
dataset ramoboost smoteboost smote,2,1,2.0
adacost borderlinesmote texture segment,1,1,1.0
borderlinesmote texture segment satimage,1,1,1.0
glass yeast letter table,1,1,1.0
yeast letter table x,1,1,1.0
letter table x simula,1,1,1.0
table x simula tion,1,1,1.0
x simula tion s,1,1,1.0
borderlinesmote t table xi,1,1,1.0
t table xi simula,1,1,1.0
table xi simula tion,1,1,1.0
xi simula tion time,1,1,1.0
simula tion time in,1,1,1.0
tion time in seconds,1,1,1.0
time in seconds of,2,1,2.0
in seconds of comparative,1,1,1.0
seconds of comparative algorithms,1,1,1.0
of comparative algorithms across,1,1,1.0
comparative algorithms across all,1,1,1.0
algorithms across all datasets,1,1,1.0
across all datasets dataset,1,1,1.0
all datasets dataset ramoboost,1,1,1.0
datasets dataset ramoboost smoteboost,1,1,1.0
adasyn adacost borderlinesmote sonar,1,1,1.0
adacost borderlinesmote sonar spambase,1,1,1.0
yeast letter shuttle table,1,1,1.0
letter shuttle table xii,1,1,1.0
shuttle table xii simula,1,1,1.0
table xii simula tion,1,1,1.0
xii simula tion of,1,1,1.0
simula tion of tuning,12,1,12.0
tion of tuning the,12,1,12.0
of tuning the oversampling,2,1,2.0
tuning the oversampling rati,2,1,2.0
the oversampling rati o,2,1,2.0
oversampling rati o a,1,1,1.0
rati o a u,2,1,2.0
o a u cp,2,1,2.0
a u cp erformance,4,1,4.0
u cp erformance charateristics,1,1,1.0
cp erformance charateristics oversampling,1,1,1.0
erformance charateristics oversampling ratio,1,1,1.0
charateristics oversampling ratio ramoboost,1,1,1.0
oversampling ratio ramoboost smoteboost,1,1,1.0
ratio ramoboost smoteboost smote,2,1,2.0
adasyn adacost borderlinesmote see,1,1,1.0
adacost borderlinesmote see that,1,1,1.0
borderlinesmote see that adasyn,1,1,1.0
see that adasyn seems,1,1,1.0
that adasyn seems to,1,1,1.0
adasyn seems to provide,1,1,1.0
seems to provide a,1,1,1.0
to provide a better,1,1,1.0
provide a better recall,1,1,1.0
a better recall rate,1,1,1.0
better recall rate on,1,1,1.0
recall rate on most,1,1,1.0
rate on most of,1,1,1.0
on most of these,1,1,1.0
most of these datasets,1,1,1.0
of these datasets this,1,1,1.0
these datasets this is,1,1,1.0
datasets this is because,1,1,1.0
this is because adasyn,1,1,1.0
is because adasyn can,1,1,1.0
because adasyn can learn,1,1,1.0
adasyn can learn very,1,1,1.0
can learn very aggressively,1,1,1.0
learn very aggressively from,1,1,1.0
very aggressively from the,1,1,1.0
aggressively from the boundary,1,1,1.0
since it generates thetic,1,1,1.0
it generates thetic data,1,1,1.0
generates thetic data instances,1,1,1.0
thetic data instances very,1,1,1.0
the decision boundary see,1,1,1.0
decision boundary see fig,1,1,1.0
boundary see fig c,1,1,1.0
see fig c this,1,1,1.0
fig c this means,1,1,1.0
c this means that,1,1,1.0
this means that adasyn,1,1,1.0
means that adasyn may,1,1,1.0
that adasyn may push,1,1,1.0
adasyn may push the,1,1,1.0
may push the algorithm,1,1,1.0
push the algorithm to,1,1,1.0
the algorithm to focus,1,1,1.0
algorithm to focus on,1,1,1.0
focus on the minority,1,1,1.0
on the minority positive,1,1,1.0
the minority positive class,1,1,1.0
minority positive class data,1,1,1.0
positive class data to,1,1,1.0
class data to improve,1,1,1.0
data to improve the,1,1,1.0
to improve the recall,1,1,1.0
improve the recall criteria,1,1,1.0
the recall criteria while,1,1,1.0
recall criteria while the,1,1,1.0
criteria while the overall,1,1,1.0
while the overall performance,1,1,1.0
the overall performance may,1,1,1.0
overall performance may not,1,1,1.0
performance may not prove,1,1,1.0
may not prove signiﬁcantly,1,1,1.0
not prove signiﬁcantly in,1,1,1.0
prove signiﬁcantly in other,1,1,1.0
signiﬁcantly in other words,1,1,1.0
in other words if,1,1,1.0
other words if one,1,1,1.0
words if one algorithm,1,1,1.0
if one algorithm classiﬁes,1,1,1.0
one algorithm classiﬁes all,1,1,1.0
algorithm classiﬁes all testing,1,1,1.0
classiﬁes all testing data,1,1,1.0
all testing data as,1,1,1.0
testing data as positive,1,1,1.0
data as positive minority,1,1,1.0
as positive minority class,1,1,1.0
positive minority class its,1,1,1.0
minority class its recall,1,1,1.0
class its recall r,1,1,1.0
its recall r a,1,1,1.0
recall r a t,1,1,1.0
r a t e,1,1,1.0
a t e will,1,1,1.0
t e will be,1,1,1.0
e will be maximized,1,1,1.0
will be maximized even,1,1,1.0
be maximized even if,1,1,1.0
maximized even if the,1,1,1.0
even if the overall,1,1,1.0
if the overall performance,1,1,1.0
the overall performance is,1,1,1.0
overall performance is low,1,1,1.0
performance is low the,1,1,1.0
is low the results,1,1,1.0
low the results in,1,1,1.0
the results in table,3,1,3.0
results in table iii,1,1,1.0
in table iii shows,1,1,1.0
table iii shows that,1,1,1.0
iii shows that adasyn,1,1,1.0
shows that adasyn performs,1,1,1.0
that adasyn performs better,1,1,1.0
adasyn performs better than,1,1,1.0
performs better than other,1,1,1.0
better than other comparative,1,1,1.0
than other comparative algorithms,2,1,2.0
other comparative algorithms in,2,1,2.0
comparative algorithms in terms,1,1,1.0
algorithms in terms of,1,1,1.0
in terms of recall,1,1,1.0
terms of recall w,1,1,1.0
of recall w h,1,1,1.0
recall w h i,1,1,1.0
w h i c,1,1,1.0
h i c h,1,1,1.0
i c h only,1,1,1.0
c h only stands,1,1,1.0
h only stands for,1,1,1.0
only stands for the,1,1,1.0
stands for the number,1,1,1.0
the number of correctly,1,1,1.0
number of correctly classiﬁed,1,1,1.0
of correctly classiﬁed minority,1,1,1.0
correctly classiﬁed minority instances,1,1,1.0
classiﬁed minority instances but,1,1,1.0
minority instances but performs,1,1,1.0
instances but performs worse,1,1,1.0
but performs worse in,1,1,1.0
performs worse in all,1,1,1.0
worse in all other,1,1,1.0
in all other assessment,1,1,1.0
all other assessment metrics,1,1,1.0
other assessment metrics such,1,1,1.0
assessment metrics such as,1,1,1.0
metrics such as and,1,1,1.0
such as and which,1,1,1.0
as and which represent,1,1,1.0
and which represent the,1,1,1.0
which represent the rithm,1,1,1.0
represent the rithm s,1,1,1.0
the rithm s overall,1,1,1.0
rithm s overall performance,1,1,1.0
s overall performance on,1,1,1.0
overall performance on most,1,1,1.0
performance on most of,1,1,1.0
of the datasets these,1,1,1.0
the datasets these results,1,1,1.0
datasets these results conﬁrm,1,1,1.0
these results conﬁrm our,1,1,1.0
results conﬁrm our discussions,1,1,1.0
conﬁrm our discussions in,1,1,1.0
our discussions in section,1,1,1.0
discussions in section regarding,1,1,1.0
in section regarding the,1,1,1.0
section regarding the different,1,1,1.0
regarding the different characteristics,1,1,1.0
the different characteristics of,1,1,1.0
different characteristics of these,1,1,1.0
characteristics of these algorithms,1,1,1.0
of these algorithms the,1,1,1.0
these algorithms the signiﬁcance,1,1,1.0
algorithms the signiﬁcance test,1,1,1.0
the signiﬁcance test is,1,1,1.0
signiﬁcance test is applied,2,1,2.0
test is applied on,1,1,1.0
is applied on the,1,1,1.0
applied on the simulation,1,1,1.0
on the simulation results,1,1,1.0
the simulation results to,1,1,1.0
simulation results to evaluate,1,1,1.0
results to evaluate whether,1,1,1.0
to evaluate whether ramoboost,2,1,2.0
evaluate whether ramoboost can,2,1,2.0
whether ramoboost can statistically,2,1,2.0
ramoboost can statistically outperformchen,1,1,1.0
can statistically outperformchen et,1,1,1.0
statistically outperformchen et al,1,1,1.0
outperformchen et al ranked,1,1,1.0
in boosting table xiii,1,1,1.0
boosting table xiii simula,1,1,1.0
table xiii simula tion,1,1,1.0
xiii simula tion of,1,1,1.0
oversampling rati o s,1,1,1.0
rati o s ignificance,3,1,3.0
o s ignificance test,3,1,3.0
ignificance test of auc,4,1,4.0
test of auc based,3,1,3.0
of auc based on,3,1,3.0
auc based on random,3,1,3.0
based on random runs,4,1,4.0
on random runs between,4,1,4.0
random runs between ramob,4,1,4.0
runs between ramob oost,4,1,4.0
between ramob oost and,4,1,4.0
smote and omek oversampling,1,1,1.0
and omek oversampling ramoboost,1,1,1.0
omek oversampling ramoboost ratio,1,1,1.0
oversampling ramoboost ratio smoteboost,1,1,1.0
ramoboost ratio smoteboost smote,2,1,2.0
ratio smoteboost smote adasyn,2,1,2.0
adasyn adacost borderlinesmote table,5,1,5.0
adacost borderlinesmote table xiv,1,1,1.0
borderlinesmote table xiv simula,1,1,1.0
table xiv simula tion,1,1,1.0
xiv simula tion of,1,1,1.0
of tuning the imbalanced,4,1,4.0
tuning the imbalanced rati,4,1,4.0
the imbalanced rati o,4,1,4.0
imbalanced rati o p,1,1,1.0
rati o p olicy,1,1,1.0
o p olicy of,1,1,1.0
p olicy of combination,1,1,1.0
olicy of combination of,1,1,1.0
of combination of classes,1,1,1.0
combination of classes in,1,1,1.0
of classes in a,1,1,1.0
classes in a balone,1,1,1.0
in a balone d,1,1,1.0
a balone d atas,1,1,1.0
balone d atas et,1,1,1.0
d atas et index,1,1,1.0
atas et index minority,1,1,1.0
et index minority combination,1,1,1.0
index minority combination majority,1,1,1.0
minority combination majority combination,1,1,1.0
combination majority combination minority,1,1,1.0
majority combination minority majority,1,1,1.0
combination minority majority imbalanced,1,1,1.0
minority majority imbalanced ratio,1,1,1.0
majority imbalanced ratio i,1,1,1.0
imbalanced ratio i ii,1,1,1.0
ratio i ii iii,1,1,1.0
i ii iii iv,1,1,1.0
ii iii iv v,1,1,1.0
iii iv v vi,1,1,1.0
iv v vi vii,1,1,1.0
v vi vii viii,1,1,1.0
vi vii viii ix,1,1,1.0
vii viii ix x,1,1,1.0
viii ix x table,1,1,1.0
ix x table xv,1,1,1.0
x table xv simula,1,1,1.0
table xv simula tion,1,1,1.0
xv simula tion of,1,1,1.0
imbalanced rati o a,1,1,1.0
u cp erformance characteristics,3,1,3.0
cp erformance characteristics imbalanced,1,1,1.0
erformance characteristics imbalanced ratio,1,1,1.0
characteristics imbalanced ratio ramoboost,1,1,1.0
imbalanced ratio ramoboost smoteboost,1,1,1.0
adacost borderlinesmote table xvi,1,1,1.0
borderlinesmote table xvi simula,1,1,1.0
table xvi simula tion,1,1,1.0
xvi simula tion of,1,1,1.0
imbalanced rati o s,2,1,2.0
adacost borderlinesmote t other,1,1,1.0
borderlinesmote t other comparative,1,1,1.0
t other comparative algorithms,1,1,1.0
other comparative algorithms since,1,1,1.0
comparative algorithms since there,1,1,1.0
algorithms since there are,1,1,1.0
since there are datasets,1,1,1.0
there are datasets t,1,1,1.0
are datasets t should,1,1,1.0
datasets t should be,2,1,2.0
t should be less,2,1,2.0
should be less than,2,1,2.0
be less than or,2,1,2.0
less than or equal,2,1,2.0
than or equal to,2,1,2.0
or equal to to,1,1,1.0
equal to to reject,1,1,1.0
to to reject a,1,1,1.0
to reject a null,2,1,2.0
reject a null pothesis,1,1,1.0
a null pothesis in,1,1,1.0
null pothesis in the,1,1,1.0
pothesis in the signiﬁcance,1,1,1.0
in the signiﬁcance level,1,1,1.0
the signiﬁcance level of,1,1,1.0
signiﬁcance level of according,1,1,1.0
level of according to,1,1,1.0
of according to the,1,1,1.0
according to the critical,1,1,1.0
to the critical value,1,1,1.0
critical value table table,1,1,1.0
value table table iv,1,1,1.0
table table iv shows,1,1,1.0
table iv shows the,1,1,1.0
iv shows the signiﬁcance,1,1,1.0
shows the signiﬁcance test,1,1,1.0
the signiﬁcance test result,1,1,1.0
signiﬁcance test result of,1,1,1.0
test result of averaged,1,1,1.0
result of averaged auc,1,1,1.0
of averaged auc for,1,1,1.0
averaged auc for ramoboost,1,1,1.0
auc for ramoboost smoteboost,1,1,1.0
for ramoboost smoteboost one,1,1,1.0
ramoboost smoteboost one can,1,1,1.0
smoteboost one can conclude,1,1,1.0
one can conclude that,1,1,1.0
can conclude that ramoboost,1,1,1.0
conclude that ramoboost can,1,1,1.0
that ramoboost can statistically,1,1,1.0
ramoboost can statistically outperform,2,1,2.0
can statistically outperform smoteboost,1,1,1.0
statistically outperform smoteboost t,1,1,1.0
outperform smoteboost t min,1,1,1.0
smoteboost t min it,1,1,1.0
t min it proves,1,1,1.0
min it proves that,1,1,1.0
it proves that although,1,1,1.0
proves that although ramoboost,1,1,1.0
that although ramoboost shares,1,1,1.0
although ramoboost shares the,1,1,1.0
ramoboost shares the same,1,1,1.0
shares the same boosting,1,1,1.0
the same boosting procedure,1,1,1.0
same boosting procedure and,1,1,1.0
boosting procedure and data,1,1,1.0
procedure and data generation,1,1,1.0
and data generation technique,1,1,1.0
data generation technique with,1,1,1.0
generation technique with smoteboost,1,1,1.0
technique with smoteboost the,1,1,1.0
with smoteboost the adaptive,1,1,1.0
smoteboost the adaptive ranking,1,1,1.0
the adaptive ranking mechanism,1,1,1.0
adaptive ranking mechanism for,1,1,1.0
ranking mechanism for determining,1,1,1.0
mechanism for determining the,1,1,1.0
for determining the number,1,1,1.0
determining the number of,1,1,1.0
each minority example makes,1,1,1.0
minority example makes ramoboost,1,1,1.0
example makes ramoboost perform,1,1,1.0
makes ramoboost perform better,1,1,1.0
ramoboost perform better than,1,1,1.0
perform better than smoteboost,1,1,1.0
better than smoteboost table,1,1,1.0
than smoteboost table v,1,1,1.0
smoteboost table v shows,1,1,1.0
table v shows the,1,1,1.0
v shows the similar,1,1,1.0
shows the similar result,1,1,1.0
the similar result with,1,1,1.0
similar result with that,1,1,1.0
result with that of,1,1,1.0
with that of table,1,1,1.0
that of table iv,1,1,1.0
of table iv for,1,1,1.0
table iv for boost,1,1,1.0
iv for boost adacost,1,1,1.0
for boost adacost from,1,1,1.0
boost adacost from which,1,1,1.0
adacost from which however,1,1,1.0
from which however one,1,1,1.0
which however one can,1,1,1.0
however one can see,1,1,1.0
can see that ramoboost,3,1,3.0
see that ramoboost can,2,1,2.0
that ramoboost can not,1,1,1.0
ramoboost can not statistically,1,1,1.0
can not statistically outperform,1,1,1.0
not statistically outperform adacost,1,1,1.0
statistically outperform adacost t,1,1,1.0
outperform adacost t min,1,1,1.0
adacost t min for,1,1,1.0
t min for space,1,1,1.0
min for space consideration,1,1,1.0
for space consideration the,1,1,1.0
space consideration the detailed,1,1,1.0
consideration the detailed statistical,1,1,1.0
the detailed statistical analysis,1,1,1.0
detailed statistical analysis for,1,1,1.0
statistical analysis for ramoboost,1,1,1.0
analysis for ramoboost against,1,1,1.0
for ramoboost against the,1,1,1.0
ramoboost against the remaining,1,1,1.0
against the remaining comparative,1,1,1.0
the remaining comparative algorithms,1,1,1.0
remaining comparative algorithms is,1,1,1.0
comparative algorithms is omitted,1,1,1.0
algorithms is omitted instead,1,1,1.0
is omitted instead we,1,1,1.0
omitted instead we provide,1,1,1.0
instead we provide in,1,1,1.0
we provide in table,1,1,1.0
provide in table vi,1,1,1.0
in table vi the,1,1,1.0
table vi the t,1,1,1.0
vi the t of,1,1,1.0
the t of ramoboost,1,1,1.0
t of ramoboost against,1,1,1.0
of ramoboost against all,1,1,1.0
ramoboost against all comparative,1,1,1.0
against all comparative algorithms,1,1,1.0
all comparative algorithms one,1,1,1.0
comparative algorithms one can,2,1,2.0
algorithms one can see,2,1,2.0
see that ramoboost also,1,1,1.0
that ramoboost also ieee,1,1,1.0
ramoboost also ieee transactions,1,1,1.0
also ieee transactions on,1,1,1.0
vol no october outperforms,1,1,1.0
no october outperforms smote,1,1,1.0
october outperforms smote adasyn,1,1,1.0
outperforms smote adasyn borderlinesmote,1,1,1.0
adasyn borderlinesmote and we,1,1,1.0
borderlinesmote and we have,1,1,1.0
and we have also,1,1,1.0
we have also conducted,1,1,1.0
have also conducted the,1,1,1.0
also conducted the simulations,1,1,1.0
conducted the simulations of,1,1,1.0
the simulations of ramoboost,1,1,1.0
simulations of ramoboost on,1,1,1.0
of ramoboost on all,2,1,2.0
ramoboost on all datasets,2,1,2.0
on all datasets when,1,1,1.0
all datasets when the,1,1,1.0
datasets when the number,1,1,1.0
hidden layer neurons for,1,1,1.0
layer neurons for the,1,1,1.0
neurons for the base,1,1,1.0
for the base classiﬁer,1,1,1.0
the base classiﬁer mlp,1,1,1.0
base classiﬁer mlp is,1,1,1.0
classiﬁer mlp is set,1,1,1.0
mlp is set to,1,1,1.0
set to be ten,1,1,1.0
to be ten our,1,1,1.0
be ten our simulation,1,1,1.0
ten our simulation results,1,1,1.0
our simulation results indicate,1,1,1.0
simulation results indicate that,1,1,1.0
results indicate that increasing,1,1,1.0
indicate that increasing the,1,1,1.0
that increasing the number,1,1,1.0
increasing the number of,1,1,1.0
hidden layer neurons does,1,1,1.0
layer neurons does not,1,1,1.0
neurons does not necessarily,1,1,1.0
does not necessarily improve,1,1,1.0
not necessarily improve the,1,1,1.0
necessarily improve the learning,1,1,1.0
improve the learning performance,1,1,1.0
the learning performance in,1,1,1.0
learning performance in this,1,1,1.0
this case we feel,1,1,1.0
case we feel there,1,1,1.0
we feel there might,1,1,1.0
feel there might be,1,1,1.0
there might be several,1,1,1.0
might be several reasons,1,1,1.0
be several reasons for,1,1,1.0
several reasons for this,1,1,1.0
reasons for this such,1,1,1.0
for this such as,1,1,1.0
this such as the,1,1,1.0
such as the potential,1,1,1.0
as the potential overﬁtting,1,1,1.0
the potential overﬁtting issue,1,1,1.0
potential overﬁtting issue furthermore,1,1,1.0
overﬁtting issue furthermore as,1,1,1.0
issue furthermore as suggested,1,1,1.0
furthermore as suggested in,1,1,1.0
as suggested in using,1,1,1.0
suggested in using a,1,1,1.0
in using a strong,1,1,1.0
using a strong base,1,1,1.0
a strong base classiﬁer,1,1,1.0
strong base classiﬁer in,1,1,1.0
base classiﬁer in the,1,1,1.0
classiﬁer in the ensemble,1,1,1.0
in the ensemble approach,1,1,1.0
the ensemble approach may,1,1,1.0
ensemble approach may not,1,1,1.0
approach may not beneﬁt,1,1,1.0
may not beneﬁt the,1,1,1.0
not beneﬁt the ﬁnal,1,1,1.0
beneﬁt the ﬁnal learning,1,1,1.0
the ﬁnal learning performance,1,1,1.0
ﬁnal learning performance due,1,1,1.0
learning performance due to,1,1,1.0
performance due to increased,1,1,1.0
due to increased bias,1,1,1.0
to increased bias of,1,1,1.0
increased bias of such,1,1,1.0
bias of such classiﬁers,1,1,1.0
of such classiﬁers due,1,1,1.0
such classiﬁers due to,1,1,1.0
classiﬁers due to space,1,1,1.0
due to space consideration,1,1,1.0
to space consideration we,1,1,1.0
space consideration we refrain,1,1,1.0
consideration we refrain from,1,1,1.0
we refrain from providing,1,1,1.0
refrain from providing the,1,1,1.0
from providing the detailed,1,1,1.0
providing the detailed results,1,1,1.0
the detailed results for,1,1,1.0
detailed results for all,1,1,1.0
results for all these,1,1,1.0
for all these experiments,1,1,1.0
all these experiments simulation,1,1,1.0
these experiments simulation in,1,1,1.0
experiments simulation in section,1,1,1.0
simulation in section we,1,1,1.0
in section we investigated,1,1,1.0
section we investigated the,1,1,1.0
we investigated the data,1,1,1.0
investigated the data generation,1,1,1.0
mechanism of ramoboost compared,1,1,1.0
of ramoboost compared to,1,1,1.0
ramoboost compared to that,1,1,1.0
compared to that of,1,1,1.0
to that of smote,1,1,1.0
smote and adasyn on,1,1,1.0
and adasyn on a,1,1,1.0
adasyn on a synthetic,1,1,1.0
on a synthetic dataset,1,1,1.0
a synthetic dataset shown,1,1,1.0
synthetic dataset shown in,1,1,1.0
dataset shown in fig,1,1,1.0
shown in fig one,1,1,1.0
in fig one interesting,1,1,1.0
fig one interesting question,1,1,1.0
one interesting question that,1,1,1.0
interesting question that arises,1,1,1.0
question that arises is,1,1,1.0
that arises is if,1,1,1.0
arises is if the,1,1,1.0
is if the data,1,1,1.0
if the data generation,1,1,1.0
mechanism of ramoboost is,1,1,1.0
of ramoboost is extracted,1,1,1.0
ramoboost is extracted and,1,1,1.0
is extracted and wrapped,1,1,1.0
extracted and wrapped up,1,1,1.0
and wrapped up with,1,1,1.0
wrapped up with other,1,1,1.0
up with other classiﬁer,1,1,1.0
with other classiﬁer in,1,1,1.0
other classiﬁer in the,1,1,1.0
classiﬁer in the way,1,1,1.0
in the way that,1,1,1.0
the way that smote,1,1,1.0
way that smote and,1,1,1.0
that smote and adasyn,1,1,1.0
smote and adasyn is,1,1,1.0
and adasyn is used,1,1,1.0
adasyn is used which,1,1,1.0
is used which can,1,1,1.0
used which can be,1,1,1.0
which can be named,1,1,1.0
can be named as,1,1,1.0
be named as ramo,1,1,1.0
named as ramo how,1,1,1.0
as ramo how will,1,1,1.0
ramo how will the,1,1,1.0
how will the learning,1,1,1.0
will the learning formance,1,1,1.0
the learning formance of,1,1,1.0
learning formance of ramo,1,1,1.0
formance of ramo be,1,1,1.0
of ramo be when,1,1,1.0
ramo be when it,1,1,1.0
be when it is,1,1,1.0
when it is compared,1,1,1.0
it is compared to,1,1,1.0
is compared to other,1,1,1.0
compared to other sampling,1,1,1.0
to other sampling approaches,1,1,1.0
other sampling approaches to,1,1,1.0
sampling approaches to this,1,1,1.0
approaches to this end,1,1,1.0
to this end we,1,1,1.0
this end we have,1,1,1.0
end we have conducted,1,1,1.0
we have conducted simulations,1,1,1.0
have conducted simulations for,1,1,1.0
conducted simulations for ramo,1,1,1.0
simulations for ramo against,1,1,1.0
for ramo against smote,1,1,1.0
ramo against smote adasyn,1,1,1.0
against smote adasyn borderlinesmote,1,1,1.0
adasyn borderlinesmote and on,1,1,1.0
borderlinesmote and on the,1,1,1.0
and on the datasets,1,1,1.0
on the datasets described,1,1,1.0
described in table for,1,1,1.0
in table for space,1,1,1.0
table for space considerations,1,1,1.0
for space considerations we,1,1,1.0
space considerations we only,1,1,1.0
considerations we only provide,1,1,1.0
we only provide simulation,1,1,1.0
only provide simulation results,1,1,1.0
provide simulation results of,1,1,1.0
simulation results of averaged,1,1,1.0
results of averaged auc,1,1,1.0
of averaged auc of,1,1,1.0
averaged auc of the,2,1,2.0
auc of the comparative,2,1,2.0
comparative algorithms in table,1,1,1.0
algorithms in table vii,1,1,1.0
in table vii the,1,1,1.0
table vii the auc,1,1,1.0
vii the auc value,1,1,1.0
the auc value of,2,1,2.0
auc value of the,1,1,1.0
value of the corresponding,1,1,1.0
of the corresponding winning,1,1,1.0
the corresponding winning approach,1,1,1.0
corresponding winning approach for,1,1,1.0
winning approach for each,1,1,1.0
approach for each dataset,1,1,1.0
for each dataset is,1,1,1.0
each dataset is highlighted,1,1,1.0
dataset is highlighted based,1,1,1.0
is highlighted based on,2,1,2.0
highlighted based on the,3,1,3.0
on the results in,2,1,2.0
results in table vii,1,1,1.0
in table vii signiﬁcance,1,1,1.0
table vii signiﬁcance test,1,1,1.0
vii signiﬁcance test is,1,1,1.0
test is applied to,2,1,2.0
is applied to evaluate,2,1,2.0
applied to evaluate whether,2,1,2.0
to evaluate whether ramo,1,1,1.0
evaluate whether ramo can,1,1,1.0
whether ramo can statistically,1,1,1.0
ramo can statistically outperform,2,1,2.0
can statistically outperform other,2,1,2.0
statistically outperform other existing,2,1,2.0
outperform other existing approaches,2,1,2.0
other existing approaches since,1,1,1.0
existing approaches since the,1,1,1.0
approaches since the number,1,1,1.0
since the number of,2,1,2.0
number of datasets used,1,1,1.0
of datasets used in,1,1,1.0
datasets used in simulation,1,1,1.0
used in simulation is,1,1,1.0
in simulation is the,1,1,1.0
simulation is the same,1,1,1.0
is the same as,1,1,1.0
the same as in,1,1,1.0
same as in simulation,1,1,1.0
as in simulation the,1,1,1.0
in simulation the signiﬁcance,1,1,1.0
simulation the signiﬁcance value,1,1,1.0
signiﬁcance value n is,1,1,1.0
value n is also,1,1,1.0
n is also table,1,1,1.0
is also table viii,1,1,1.0
also table viii demonstrates,1,1,1.0
table viii demonstrates the,1,1,1.0
viii demonstrates the signiﬁcance,1,1,1.0
demonstrates the signiﬁcance test,1,1,1.0
the signiﬁcance test results,2,1,2.0
signiﬁcance test results the,1,1,1.0
test results the t,1,1,1.0
results the t value,1,1,1.0
the t value of,2,1,2.0
t value of each,1,1,1.0
value of each comparative,1,1,1.0
of each comparative algorithms,1,1,1.0
each comparative algorithms one,1,1,1.0
can see that ramo,1,1,1.0
see that ramo can,1,1,1.0
that ramo can statistically,1,1,1.0
can statistically outperform adasyn,1,1,1.0
statistically outperform adasyn and,1,1,1.0
outperform adasyn and but,1,1,1.0
adasyn and but it,1,1,1.0
and but it can,1,1,1.0
but it can not,1,1,1.0
it can not outperform,1,1,1.0
can not outperform smote,2,1,2.0
not outperform smote in,1,1,1.0
outperform smote in this,1,1,1.0
smote in this case,1,1,1.0
in this case simulation,1,1,1.0
this case simulation another,1,1,1.0
case simulation another interesting,1,1,1.0
simulation another interesting simulation,1,1,1.0
another interesting simulation we,1,1,1.0
interesting simulation we conducted,1,1,1.0
simulation we conducted is,1,1,1.0
we conducted is to,1,1,1.0
conducted is to compare,1,1,1.0
is to compare ramoboost,1,1,1.0
to compare ramoboost with,1,1,1.0
compare ramoboost with other,1,1,1.0
ramoboost with other comparative,1,1,1.0
with other comparative algorithms,1,1,1.0
other comparative algorithms when,1,1,1.0
comparative algorithms when both,1,1,1.0
algorithms when both and,1,1,1.0
when both and are,2,1,2.0
both and are ten,1,1,1.0
and are ten for,1,1,1.0
are ten for ramoboost,1,1,1.0
ten for ramoboost we,1,1,1.0
for ramoboost we also,1,1,1.0
ramoboost we also conﬁgured,1,1,1.0
we also conﬁgured the,1,1,1.0
also conﬁgured the k,1,1,1.0
conﬁgured the k value,1,1,1.0
the k value of,1,1,1.0
k value of smoteboost,1,1,1.0
value of smoteboost smote,1,1,1.0
of smoteboost smote adasyn,1,1,1.0
adasyn borderlinesmote and to,1,1,1.0
borderlinesmote and to be,1,1,1.0
and to be ten,1,1,1.0
to be ten to,1,1,1.0
be ten to provide,1,1,1.0
ten to provide a,1,1,1.0
to provide a fair,1,1,1.0
provide a fair comparison,1,1,1.0
a fair comparison all,1,1,1.0
fair comparison all other,1,1,1.0
comparison all other parameters,1,1,1.0
all other parameters remained,1,1,1.0
other parameters remained the,1,1,1.0
parameters remained the same,1,1,1.0
remained the same we,1,1,1.0
the same we compared,1,1,1.0
same we compared the,1,1,1.0
we compared the algorithms,1,1,1.0
compared the algorithms against,1,1,1.0
the algorithms against the,1,1,1.0
algorithms against the ten,1,1,1.0
against the ten datasets,1,1,1.0
the ten datasets with,1,1,1.0
ten datasets with the,1,1,1.0
datasets with the largest,1,1,1.0
with the largest skew,1,1,1.0
the largest skew ratio,1,1,1.0
largest skew ratio since,1,1,1.0
skew ratio since we,1,1,1.0
ratio since we are,1,1,1.0
since we are more,1,1,1.0
we are more interested,1,1,1.0
are more interested in,1,1,1.0
more interested in investigating,1,1,1.0
interested in investigating how,1,1,1.0
in investigating how ramoboost,1,1,1.0
investigating how ramoboost performs,1,1,1.0
how ramoboost performs with,1,1,1.0
ramoboost performs with highly,1,1,1.0
performs with highly imbalanced,1,1,1.0
with highly imbalanced datasets,1,1,1.0
highly imbalanced datasets in,1,1,1.0
imbalanced datasets in order,1,1,1.0
datasets in order to,1,1,1.0
in order to retain,1,1,1.0
order to retain these,1,1,1.0
to retain these severely,1,1,1.0
retain these severely anced,1,1,1.0
these severely anced ratios,1,1,1.0
severely anced ratios we,1,1,1.0
anced ratios we adopted,1,1,1.0
ratios we adopted a,1,1,1.0
we adopted a different,1,1,1.0
adopted a different way,1,1,1.0
a different way of,1,1,1.0
different way of generating,1,1,1.0
way of generating the,1,1,1.0
of generating the training,1,1,1.0
generating the training and,1,1,1.0
the training and testing,2,1,2.0
training and testing datasets,2,1,2.0
and testing datasets sp,1,1,1.0
testing datasets sp eciﬁcally,1,1,1.0
datasets sp eciﬁcally the,1,1,1.0
sp eciﬁcally the training,1,1,1.0
eciﬁcally the training dataset,1,1,1.0
the training dataset was,1,1,1.0
training dataset was created,1,1,1.0
dataset was created by,1,1,1.0
was created by consolidating,1,1,1.0
created by consolidating half,1,1,1.0
by consolidating half of,1,1,1.0
consolidating half of the,1,1,1.0
half of the randomly,2,1,2.0
of the randomly selected,2,1,2.0
the randomly selected majority,1,1,1.0
randomly selected majority class,1,1,1.0
selected majority class examples,1,1,1.0
majority class examples and,1,1,1.0
class examples and half,1,1,1.0
examples and half of,1,1,1.0
and half of the,1,1,1.0
the randomly selected minority,1,1,1.0
randomly selected minority class,1,1,1.0
selected minority class examples,1,1,1.0
minority class examples the,1,1,1.0
class examples the r,1,1,1.0
examples the r emaining,1,1,1.0
the r emaining examples,1,1,1.0
r emaining examples were,1,1,1.0
emaining examples were used,1,1,1.0
examples were used as,1,1,1.0
were used as testing,1,1,1.0
used as testing dataset,1,1,1.0
as testing dataset one,1,1,1.0
testing dataset one can,1,1,1.0
dataset one can easily,1,1,1.0
one can easily verify,1,1,1.0
can easily verify that,1,1,1.0
easily verify that the,1,1,1.0
verify that the training,1,1,1.0
that the training and,1,1,1.0
and testing datasets generated,1,1,1.0
testing datasets generated th,1,1,1.0
datasets generated th is,1,1,1.0
generated th is way,1,1,1.0
th is way bear,1,1,1.0
is way bear the,1,1,1.0
way bear the same,1,1,1.0
bear the same imbalance,1,1,1.0
the same imbalance ratio,1,1,1.0
same imbalance ratio as,1,1,1.0
imbalance ratio as the,1,1,1.0
ratio as the original,1,1,1.0
as the original dataset,1,1,1.0
the original dataset for,1,1,1.0
original dataset for space,1,1,1.0
dataset for space considerations,1,1,1.0
for space considerations only,1,1,1.0
space considerations only the,1,1,1.0
considerations only the simulation,1,1,1.0
only the simulation results,1,1,1.0
simulation results of auc,1,1,1.0
results of auc values,1,1,1.0
of auc values are,1,1,1.0
auc values are provided,1,1,1.0
values are provided in,1,1,1.0
are provided in table,1,1,1.0
provided in table ix,1,1,1.0
in table ix with,1,1,1.0
table ix with the,1,1,1.0
ix with the winning,1,1,1.0
with the winning value,1,1,1.0
the winning value across,1,1,1.0
winning value across all,1,1,1.0
value across all comparative,1,1,1.0
across all comparative algorithms,1,1,1.0
all comparative algorithms for,1,1,1.0
comparative algorithms for each,1,1,1.0
algorithms for each dataset,1,1,1.0
for each dataset highlighted,1,1,1.0
each dataset highlighted based,1,1,1.0
dataset highlighted based on,1,1,1.0
results in table ix,1,1,1.0
in table ix niﬁcance,1,1,1.0
table ix niﬁcance test,1,1,1.0
ix niﬁcance test is,1,1,1.0
niﬁcance test is applied,1,1,1.0
other existing approaches when,1,1,1.0
existing approaches when both,1,1,1.0
approaches when both and,1,1,1.0
both and are equal,1,1,1.0
and are equal to,1,1,1.0
are equal to ten,1,1,1.0
equal to ten since,1,1,1.0
to ten since there,1,1,1.0
ten since there are,1,1,1.0
since there are just,1,1,1.0
there are just ten,1,1,1.0
are just ten datasets,1,1,1.0
just ten datasets t,1,1,1.0
ten datasets t should,1,1,1.0
or equal to eight,1,1,1.0
equal to eight to,1,1,1.0
to eight to reject,1,1,1.0
eight to reject a,1,1,1.0
reject a null hypothesis,1,1,1.0
a null hypothesis between,1,1,1.0
null hypothesis between two,1,1,1.0
hypothesis between two comparative,1,1,1.0
between two comparative algorithms,1,1,1.0
two comparative algorithms table,1,1,1.0
comparative algorithms table x,1,1,1.0
algorithms table x presents,1,1,1.0
table x presents the,1,1,1.0
x presents the t,1,1,1.0
presents the t value,1,1,1.0
t value of ramoboost,1,1,1.0
value of ramoboost against,1,1,1.0
of ramoboost against other,3,1,3.0
ramoboost against other comparative,3,1,3.0
against other comparative algorithms,3,1,3.0
other comparative algorithms from,2,1,2.0
comparative algorithms from which,1,1,1.0
algorithms from which one,1,1,1.0
from which one can,1,1,1.0
which one can see,1,1,1.0
that ramoboost can also,1,1,1.0
ramoboost can also statistically,1,1,1.0
can also statistically outperform,1,1,1.0
also statistically outperform smoteboost,1,1,1.0
statistically outperform smoteboost adasyn,1,1,1.0
outperform smoteboost adasyn borderlinesmote,1,1,1.0
smoteboost adasyn borderlinesmote and,1,1,1.0
adasyn borderlinesmote and but,1,1,1.0
borderlinesmote and but can,1,1,1.0
and but can not,1,1,1.0
but can not outperform,1,1,1.0
not outperform smote and,1,1,1.0
outperform smote and adacost,1,1,1.0
smote and adacost in,1,1,1.0
and adacost in this,1,1,1.0
adacost in this case,1,1,1.0
in this case computational,1,1,1.0
this case computational time,1,1,1.0
case computational time for,1,1,1.0
computational time for simulation,1,1,1.0
time for simulation table,1,1,1.0
for simulation table xi,1,1,1.0
simulation table xi shows,1,1,1.0
table xi shows the,1,1,1.0
xi shows the computational,1,1,1.0
shows the computational time,1,1,1.0
the computational time in,1,1,1.0
computational time in seconds,1,1,1.0
in seconds of ramoboost,1,1,1.0
seconds of ramoboost on,1,1,1.0
on all datasets based,1,1,1.0
all datasets based on,1,1,1.0
datasets based on the,1,1,1.0
based on the simulation,1,1,1.0
on the simulation ronment,1,1,1.0
the simulation ronment of,1,1,1.0
simulation ronment of intel,1,1,1.0
ronment of intel core,1,1,1.0
of intel core duo,1,1,1.0
intel core duo cpu,1,1,1.0
core duo cpu ghz,1,1,1.0
duo cpu ghz gb,1,1,1.0
cpu ghz gb ram,1,1,1.0
ghz gb ram and,1,1,1.0
gb ram and matlab,1,1,1.0
ram and matlab version,1,1,1.0
and matlab version from,1,1,1.0
matlab version from table,1,1,1.0
version from table xi,1,1,1.0
from table xi one,1,1,1.0
table xi one can,1,1,1.0
xi one can see,1,1,1.0
can see that the,1,1,1.0
see that the computational,1,1,1.0
the computational time cost,1,1,1.0
computational time cost of,1,1,1.0
time cost of ramoboost,1,1,1.0
cost of ramoboost is,1,1,1.0
of ramoboost is similar,1,1,1.0
ramoboost is similar to,1,1,1.0
is similar to that,1,1,1.0
similar to that of,1,1,1.0
to that of the,1,1,1.0
that of the existing,1,1,1.0
of the existing approaches,1,1,1.0
the existing approaches the,1,1,1.0
existing approaches the runtime,1,1,1.0
approaches the runtime cost,1,1,1.0
the runtime cost for,1,1,1.0
runtime cost for is,1,1,1.0
cost for is generally,1,1,1.0
for is generally higher,1,1,1.0
is generally higher than,1,1,1.0
generally higher than other,1,1,1.0
higher than other comparative,1,1,1.0
other comparative algorithms especially,1,1,1.0
comparative algorithms especially when,1,1,1.0
algorithms especially when the,1,1,1.0
especially when the size,1,1,1.0
when the size of,1,1,1.0
size of the dataset,1,1,1.0
of the dataset is,1,1,1.0
dataset is very large,1,1,1.0
is very large letter,1,1,1.0
very large letter and,1,1,1.0
large letter and this,1,1,1.0
letter and this is,1,1,1.0
and this is probably,1,1,1.0
this is probably because,1,1,1.0
is probably because tomek,1,1,1.0
probably because tomek iterates,1,1,1.0
because tomek iterates across,1,1,1.0
tomek iterates across the,1,1,1.0
iterates across the entire,1,1,1.0
across the entire d,1,1,1.0
the entire d ata,1,1,1.0
entire d ata space,1,1,1.0
d ata space repeatedly,1,1,1.0
ata space repeatedly until,1,1,1.0
space repeatedly until all,1,1,1.0
repeatedly until all tomek,1,1,1.0
until all tomek links,1,1,1.0
all tomek links have,1,1,1.0
tomek links have been,1,1,1.0
links have been cleared,1,1,1.0
have been cleared simulation,1,1,1.0
been cleared simulation results,1,1,1.0
cleared simulation results on,1,1,1.0
simulation results on tuning,1,1,1.0
results on tuning parameters,1,1,1.0
on tuning parameters to,1,1,1.0
tuning parameters to evaluate,1,1,1.0
parameters to evaluate the,1,1,1.0
to evaluate the robustness,1,1,1.0
evaluate the robustness of,1,1,1.0
the robustness of ramoboost,2,1,2.0
robustness of ramoboost against,1,1,1.0
comparative algorithms in diff,1,1,1.0
algorithms in diff erent,1,1,1.0
in diff erent parameter,1,1,1.0
diff erent parameter conﬁgurations,1,1,1.0
erent parameter conﬁgurations and,1,1,1.0
parameter conﬁgurations and scenarios,1,1,1.0
conﬁgurations and scenarios simulations,1,1,1.0
and scenarios simulations on,1,1,1.0
scenarios simulations on tuning,1,1,1.0
simulations on tuning the,1,1,1.0
on tuning the minority,1,1,1.0
tuning the minority pling,1,1,1.0
the minority pling ratios,1,1,1.0
minority pling ratios the,1,1,1.0
pling ratios the imbalanced,1,1,1.0
ratios the imbalanced ratio,1,1,1.0
the imbalanced ratio and,1,1,1.0
imbalanced ratio and the,1,1,1.0
ratio and the class,1,1,1.0
the class label noise,6,1,6.0
class label noise and,2,1,2.0
label noise and the,1,1,1.0
noise and the attribute,1,1,1.0
and the attribute noise,1,1,1.0
the attribute noise of,1,1,1.0
attribute noise of the,1,1,1.0
noise of the datasets,1,1,1.0
of the datasets have,1,1,1.0
the datasets have been,1,1,1.0
datasets have been conducted,1,1,1.0
have been conducted for,1,1,1.0
been conducted for space,1,1,1.0
conducted for space consideration,1,1,1.0
for space consideration we,1,1,1.0
space consideration we only,1,1,1.0
consideration we only present,1,1,1.0
we only present the,1,1,1.0
only present the results,1,1,1.0
present the results on,1,1,1.0
results on the abalone,1,1,1.0
on the abalone dataset,1,1,1.0
the abalone dataset again,1,1,1.0
abalone dataset again mlp,1,1,1.0
dataset again mlp with,1,1,1.0
again mlp with the,1,1,1.0
mlp with the conﬁguration,1,1,1.0
with the conﬁguration scribed,1,1,1.0
the conﬁguration scribed at,1,1,1.0
conﬁguration scribed at the,1,1,1.0
scribed at the beginning,1,1,1.0
the beginning of section,1,1,1.0
beginning of section is,1,1,1.0
of section is used,1,1,1.0
section is used as,1,1,1.0
used as the base,1,1,1.0
base learner the simulation,1,1,1.0
learner the simulation results,1,1,1.0
simulation results are also,1,1,1.0
results are also based,1,1,1.0
are also based on,1,1,1.0
also based on ten,1,1,1.0
based on ten random,1,1,1.0
on ten random runs,2,1,2.0
ten random runs in,1,1,1.0
random runs in each,1,1,1.0
runs in each of,1,1,1.0
each of these random,1,1,1.0
of these random runs,1,1,1.0
these random runs half,1,1,1.0
random runs half of,1,1,1.0
runs half of the,1,1,1.0
half of the original,1,1,1.0
of the original minority,1,1,1.0
the original minority and,1,1,1.0
original minority and majority,1,1,1.0
minority and majority datasets,1,1,1.0
and majority datasets are,1,1,1.0
majority datasets are randomly,1,1,1.0
datasets are randomly chosen,1,1,1.0
are randomly chosen and,1,1,1.0
randomly chosen and merged,1,1,1.0
chosen and merged to,1,1,1.0
and merged to be,1,1,1.0
merged to be the,1,1,1.0
to be the training,1,1,1.0
be the training dataset,1,1,1.0
the training dataset and,1,1,1.0
training dataset and the,1,1,1.0
dataset and the remaining,1,1,1.0
and the remaining part,1,1,1.0
the remaining part is,1,1,1.0
remaining part is used,1,1,1.0
part is used as,1,1,1.0
used as the testing,1,1,1.0
as the testing dataset,1,1,1.0
the testing dataset this,1,1,1.0
testing dataset this experiment,1,1,1.0
dataset this experiment is,1,1,1.0
this experiment is motivated,1,1,1.0
experiment is motivated by,1,1,1.0
is motivated by which,1,1,1.0
motivated by which suggested,1,1,1.0
by which suggested that,1,1,1.0
which suggested that the,1,1,1.0
suggested that the oversampling,1,1,1.0
that the oversampling ratio,1,1,1.0
the oversampling ratio could,1,1,1.0
oversampling ratio could play,1,1,1.0
ratio could play a,1,1,1.0
could play a critical,1,1,1.0
play a critical role,1,1,1.0
a critical role for,1,1,1.0
critical role for imbalanced,1,1,1.0
role for imbalanced learning,1,1,1.0
for imbalanced learning problems,3,1,3.0
imbalanced learning problems here,1,1,1.0
learning problems here we,1,1,1.0
problems here we use,1,1,1.0
here we use the,1,1,1.0
we use the abalone,1,1,1.0
use the abalone dataset,1,1,1.0
the abalone dataset described,1,1,1.0
abalone dataset described in,1,1,1.0
dataset described in section,1,1,1.0
described in section as,1,1,1.0
in section as an,1,1,1.0
section as an example,1,1,1.0
as an example to,1,1,1.0
an example to show,1,1,1.0
example to show the,1,1,1.0
to show the performance,1,1,1.0
show the performance by,1,1,1.0
the performance by tuning,1,1,1.0
performance by tuning the,1,1,1.0
by tuning the overs,1,1,1.0
tuning the overs ampling,1,1,1.0
the overs ampling ratio,1,1,1.0
overs ampling ratio speciﬁcally,1,1,1.0
ampling ratio speciﬁcally the,1,1,1.0
ratio speciﬁcally the oversampling,1,1,1.0
speciﬁcally the oversampling ratio,1,1,1.0
the oversampling ratio for,1,1,1.0
oversampling ratio for t,1,1,1.0
ratio for t he,1,1,1.0
for t he minority,1,1,1.0
t he minority class,1,1,1.0
he minority class is,1,1,1.0
minority class is increased,1,1,1.0
class is increased progressively,1,1,1.0
is increased progressively from,1,1,1.0
increased progressively from to,1,1,1.0
progressively from to with,1,1,1.0
from to with an,1,1,1.0
to with an interval,1,1,1.0
with an interval of,1,1,1.0
an interval of table,1,1,1.0
interval of table xii,1,1,1.0
of table xii displays,1,1,1.0
table xii displays the,1,1,1.0
xii displays the simulation,1,1,1.0
displays the simulation results,1,1,1.0
the simulation results on,1,1,1.0
simulation results on ten,1,1,1.0
results on ten random,1,1,1.0
ten random runs using,1,1,1.0
random runs using the,1,1,1.0
runs using the averaged,1,1,1.0
using the averaged auc,1,1,1.0
the comparative algorithms for,1,1,1.0
comparative algorithms for this,1,1,1.0
algorithms for this dataset,1,1,1.0
for this dataset in,1,1,1.0
this dataset in which,1,1,1.0
dataset in which the,1,1,1.0
the best performance is,2,1,2.0
best performance is highlighted,2,1,2.0
performance is highlighted for,1,1,1.0
is highlighted for the,1,1,1.0
highlighted for the signiﬁcance,1,1,1.0
for the signiﬁcance test,1,1,1.0
the signiﬁcance test if,1,1,1.0
signiﬁcance test if we,1,1,1.0
test if we consider,1,1,1.0
if we consider only,1,1,1.0
we consider only the,1,1,1.0
consider only the averaged,1,1,1.0
only the averaged auc,1,1,1.0
the averaged auc since,1,1,1.0
averaged auc since t,1,1,1.0
auc since t ramoboost,1,1,1.0
since t ramoboost is,1,1,1.0
t ramoboost is undoubtedly,1,1,1.0
ramoboost is undoubtedly signiﬁcantly,1,1,1.0
is undoubtedly signiﬁcantly better,1,1,1.0
undoubtedly signiﬁcantly better than,1,1,1.0
signiﬁcantly better than all,1,1,1.0
better than all comparative,1,1,1.0
than all comparative algorithms,1,1,1.0
all comparative algorithms in,1,1,1.0
comparative algorithms in all,1,1,1.0
algorithms in all simulation,1,1,1.0
in all simulation scenarios,1,1,1.0
all simulation scenarios we,1,1,1.0
simulation scenarios we also,1,1,1.0
scenarios we also conducted,1,1,1.0
we also conducted the,1,1,1.0
also conducted the signiﬁcance,1,1,1.0
conducted the signiﬁcance test,1,1,1.0
the signiﬁcance test based,2,1,2.0
signiﬁcance test based on,2,1,2.0
test based on the,2,1,2.0
based on the auc,1,1,1.0
on the auc of,1,1,1.0
the auc of the,1,1,1.0
auc of the random,1,1,1.0
the random runs in,1,1,1.0
random runs in this,1,1,1.0
runs in this case,1,1,1.0
in this case n,1,1,1.0
this case n is,1,1,1.0
case n is equal,1,1,1.0
n is equal to,1,1,1.0
is equal to eight,1,1,1.0
equal to eight since,1,1,1.0
to eight since the,1,1,1.0
eight since the number,1,1,1.0
the number of random,1,1,1.0
number of random runs,1,1,1.0
of random runs is,1,1,1.0
random runs is equal,1,1,1.0
runs is equal to,1,1,1.0
is equal to ten,1,1,1.0
equal to ten table,1,1,1.0
to ten table xiii,1,1,1.0
ten table xiii shows,1,1,1.0
table xiii shows the,1,1,1.0
xiii shows the t,1,1,1.0
shows the t value,1,1,1.0
the t value for,1,1,1.0
t value for the,1,1,1.0
value for the comparison,1,1,1.0
for the comparison between,1,1,1.0
the comparison between ramoboost,1,1,1.0
comparison between ramoboost and,1,1,1.0
between ramoboost and other,1,1,1.0
ramoboost and other comparative,2,1,2.0
and other comparative algorithms,1,1,1.0
other comparative algorithms based,1,1,1.0
comparative algorithms based on,1,1,1.0
algorithms based on random,1,1,1.0
on random runs the,1,1,1.0
random runs the original,1,1,1.0
runs the original abalone,1,1,1.0
the original abalone dataset,2,1,2.0
original abalone dataset has,1,1,1.0
abalone dataset has classes,1,1,1.0
dataset has classes and,1,1,1.0
has classes and examples,1,1,1.0
classes and examples in,1,1,1.0
and examples in which,1,1,1.0
examples in which we,1,1,1.0
in which we employed,1,1,1.0
which we employed only,1,1,1.0
we employed only two,1,1,1.0
employed only two classes,1,1,1.0
only two classes to,1,1,1.0
two classes to evaluatechen,1,1,1.0
classes to evaluatechen et,1,1,1.0
to evaluatechen et al,1,1,1.0
evaluatechen et al ranked,1,1,1.0
in boosting table xvii,1,1,1.0
boosting table xvii simula,1,1,1.0
table xvii simula tion,1,1,1.0
xvii simula tion of,1,1,1.0
smote and omek imbalanced,1,1,1.0
and omek imbalanced ramoboost,1,1,1.0
omek imbalanced ramoboost ratio,1,1,1.0
imbalanced ramoboost ratio smoteboost,1,1,1.0
adacost borderlinesmote table xviii,1,1,1.0
borderlinesmote table xviii simula,1,1,1.0
table xviii simula tion,1,1,1.0
xviii simula tion of,1,1,1.0
of tuning the class,5,1,5.0
tuning the class label,5,1,5.0
class label noise level,5,1,5.0
label noise level a,1,1,1.0
noise level a u,2,1,2.0
level a u cp,2,1,2.0
cp erformance characteristics noise,2,1,2.0
erformance characteristics noise level,2,1,2.0
characteristics noise level ramoboost,2,1,2.0
noise level ramoboost smoteboost,2,1,2.0
level ramoboost smoteboost smote,2,1,2.0
adacost borderlinesmote table xix,1,1,1.0
borderlinesmote table xix simula,1,1,1.0
table xix simula tion,1,1,1.0
xix simula tion of,1,1,1.0
label noise level s,4,1,4.0
noise level s ignificance,4,1,4.0
level s ignificance test,4,1,4.0
borderlinesmote t table xx,1,1,1.0
t table xx simula,1,1,1.0
table xx simula tion,1,1,1.0
xx simula tion of,1,1,1.0
test of auc b,1,1,1.0
of auc b ased,1,1,1.0
auc b ased on,1,1,1.0
b ased on random,1,1,1.0
ased on random runs,1,1,1.0
smote and omek noise,2,1,2.0
and omek noise ramoboost,2,1,2.0
omek noise ramoboost level,2,1,2.0
noise ramoboost level smoteboost,2,1,2.0
ramoboost level smoteboost smote,2,1,2.0
level smoteboost smote adasyn,2,1,2.0
adasyn adacost borderlinesmote the,1,1,1.0
adacost borderlinesmote the comparative,1,1,1.0
borderlinesmote the comparative algorithms,1,1,1.0
the comparative algorithms on,1,1,1.0
comparative algorithms on versatile,1,1,1.0
algorithms on versatile datasets,1,1,1.0
on versatile datasets as,1,1,1.0
versatile datasets as described,1,1,1.0
datasets as described in,1,1,1.0
as described in section,1,1,1.0
described in section in,1,1,1.0
in section in order,1,1,1.0
section in order to,1,1,1.0
order to obtain versatile,1,1,1.0
to obtain versatile imbalanced,1,1,1.0
obtain versatile imbalanced ratio,1,1,1.0
versatile imbalanced ratio we,1,1,1.0
imbalanced ratio we manipulate,1,1,1.0
ratio we manipulate the,1,1,1.0
we manipulate the classes,1,1,1.0
manipulate the classes combination,1,1,1.0
the classes combination of,1,1,1.0
classes combination of the,1,1,1.0
of the original abalone,1,1,1.0
original abalone dataset to,1,1,1.0
abalone dataset to form,1,1,1.0
dataset to form minority,1,1,1.0
to form minority class,1,1,1.0
form minority class and,1,1,1.0
and majority class table,1,1,1.0
majority class table xiv,1,1,1.0
class table xiv concludes,1,1,1.0
table xiv concludes the,1,1,1.0
xiv concludes the details,1,1,1.0
concludes the details for,1,1,1.0
the details for such,1,1,1.0
details for such combination,1,1,1.0
for such combination and,1,1,1.0
such combination and the,1,1,1.0
combination and the corresponding,1,1,1.0
and the corresponding imbalanced,1,1,1.0
the corresponding imbalanced ratio,1,1,1.0
corresponding imbalanced ratio table,1,1,1.0
imbalanced ratio table xv,1,1,1.0
ratio table xv presents,1,1,1.0
table xv presents simulation,1,1,1.0
xv presents simulation results,1,1,1.0
presents simulation results of,1,1,1.0
simulation results of ten,1,1,1.0
results of ten random,2,1,2.0
of ten random runs,5,1,5.0
ten random runs of,1,1,1.0
random runs of experiments,1,1,1.0
runs of experiments on,1,1,1.0
of experiments on tuning,1,1,1.0
experiments on tuning the,1,1,1.0
on tuning the imbalanced,1,1,1.0
tuning the imbalanced ratio,1,1,1.0
the imbalanced ratio in,1,1,1.0
imbalanced ratio in which,1,1,1.0
ratio in which the,1,1,1.0
performance is highlighted based,1,1,1.0
based on the averaged,3,1,3.0
the averaged auc results,2,1,2.0
averaged auc results in,1,1,1.0
auc results in table,1,1,1.0
results in table xv,1,1,1.0
in table xv wilcoxon,1,1,1.0
table xv wilcoxon test,1,1,1.0
xv wilcoxon test tells,1,1,1.0
wilcoxon test tells us,1,1,1.0
test tells us whether,1,1,1.0
tells us whether any,1,1,1.0
us whether any signiﬁcance,1,1,1.0
whether any signiﬁcance exists,1,1,1.0
any signiﬁcance exists between,1,1,1.0
signiﬁcance exists between ramoboost,1,1,1.0
exists between ramoboost and,1,1,1.0
between ramoboost and any,1,1,1.0
ramoboost and any of,1,1,1.0
and any of the,1,1,1.0
any of the comparative,1,1,1.0
the comparative algorithms which,1,1,1.0
comparative algorithms which is,1,1,1.0
algorithms which is shown,1,1,1.0
which is shown in,1,1,1.0
is shown in table,1,1,1.0
shown in table xvi,1,1,1.0
in table xvi using,1,1,1.0
table xvi using the,1,1,1.0
xvi using the abalone,1,1,1.0
using the abalone dataset,1,1,1.0
the abalone dataset we,1,1,1.0
abalone dataset we conducted,1,1,1.0
dataset we conducted signiﬁcance,1,1,1.0
we conducted signiﬁcance tests,1,1,1.0
conducted signiﬁcance tests on,1,1,1.0
signiﬁcance tests on the,1,1,1.0
tests on the auc,1,1,1.0
on the auc values,1,1,1.0
the auc values of,1,1,1.0
auc values of ten,2,1,2.0
values of ten random,2,1,2.0
ten random runs the,1,1,1.0
random runs the results,1,1,1.0
runs the results are,1,1,1.0
the results are given,1,1,1.0
results are given in,1,1,1.0
are given in table,1,1,1.0
given in table xvii,1,1,1.0
in table xvii noise,1,1,1.0
table xvii noise in,1,1,1.0
xvii noise in imbalanced,1,1,1.0
noise in imbalanced datasets,2,1,2.0
in imbalanced datasets may,1,1,1.0
imbalanced datasets may exhibit,1,1,1.0
datasets may exhibit unpredictably,1,1,1.0
may exhibit unpredictably negative,1,1,1.0
exhibit unpredictably negative effects,1,1,1.0
unpredictably negative effects on,1,1,1.0
negative effects on the,1,1,1.0
effects on the performance,1,1,1.0
the performance of learning,1,1,1.0
of learning algorithms in,1,1,1.0
learning algorithms in order,1,1,1.0
algorithms in order to,1,1,1.0
in order to systematically,1,1,1.0
order to systematically investigate,1,1,1.0
to systematically investigate the,1,1,1.0
systematically investigate the robustness,1,1,1.0
investigate the robustness of,1,1,1.0
the robustness of boost,1,1,1.0
robustness of boost we,1,1,1.0
of boost we manually,1,1,1.0
boost we manually introduce,1,1,1.0
we manually introduce class,1,1,1.0
manually introduce class label,1,1,1.0
introduce class label noise,1,1,1.0
label noise and attribute,1,1,1.0
noise and attribute noise,1,1,1.0
and attribute noise of,1,1,1.0
attribute noise of different,1,1,1.0
noise of different levels,1,1,1.0
of different levels into,1,1,1.0
different levels into the,1,1,1.0
levels into the abalone,1,1,1.0
into the abalone dataset,1,1,1.0
the abalone dataset and,1,1,1.0
abalone dataset and let,1,1,1.0
dataset and let ramoboost,1,1,1.0
and let ramoboost as,1,1,1.0
let ramoboost as well,1,1,1.0
ramoboost as well as,1,1,1.0
as well as other,1,1,1.0
well as other comparative,1,1,1.0
as other comparative algorithms,1,1,1.0
other comparative algorithms ieee,1,1,1.0
comparative algorithms ieee transactions,1,1,1.0
algorithms ieee transactions on,1,1,1.0
no october table xxi,1,1,1.0
october table xxi simula,1,1,1.0
table xxi simula tion,1,1,1.0
xxi simula tion of,1,1,1.0
of tuning the attribute,1,1,1.0
tuning the attribute noise,1,1,1.0
the attribute noise level,1,1,1.0
attribute noise level a,1,1,1.0
adacost borderlinesmote table xxii,1,1,1.0
borderlinesmote table xxii simula,1,1,1.0
table xxii simula tion,1,1,1.0
xxii simula tion of,1,1,1.0
borderlinesmote t table xxiii,1,1,1.0
t table xxiii simula,1,1,1.0
table xxiii simula tion,1,1,1.0
xxiii simula tion of,1,1,1.0
adasyn adacost borderlinesmote from,1,1,1.0
adacost borderlinesmote from it,1,1,1.0
borderlinesmote from it for,1,1,1.0
from it for adding,1,1,1.0
it for adding class,1,1,1.0
for adding class label,1,1,1.0
adding class label noise,1,1,1.0
class label noise we,1,1,1.0
label noise we adopted,1,1,1.0
noise we adopted the,1,1,1.0
we adopted the procedure,1,1,1.0
adopted the procedure of,1,1,1.0
the procedure of speciﬁcally,1,1,1.0
procedure of speciﬁcally given,1,1,1.0
of speciﬁcally given a,1,1,1.0
speciﬁcally given a pair,1,1,1.0
given a pair of,1,1,1.0
a pair of classes,1,1,1.0
pair of classes x,1,1,1.0
of classes x y,1,1,1.0
classes x y and,1,1,1.0
x y and a,1,1,1.0
y and a noise,1,1,1.0
and a noise level,1,1,1.0
a noise level x,2,1,2.0
noise level x an,1,1,1.0
level x an instance,1,1,1.0
x an instance with,1,1,1.0
an instance with its,1,1,1.0
instance with its label,1,1,1.0
with its label x,1,1,1.0
its label x has,1,1,1.0
label x has an,1,1,1.0
x has an x,1,1,1.0
has an x chance,1,1,1.0
an x chance to,1,1,1.0
x chance to be,1,1,1.0
chance to be corrupted,1,1,1.0
to be corrupted and,1,1,1.0
be corrupted and mislabeled,1,1,1.0
corrupted and mislabeled as,1,1,1.0
and mislabeled as y,1,1,1.0
mislabeled as y and,1,1,1.0
as y and so,1,1,1.0
y and so does,1,1,1.0
and so does an,1,1,1.0
so does an instance,1,1,1.0
does an instance of,1,1,1.0
an instance of class,1,1,1.0
instance of class y,1,1,1.0
of class y table,1,1,1.0
class y table xviii,1,1,1.0
y table xviii shows,1,1,1.0
table xviii shows the,1,1,1.0
xviii shows the auc,1,1,1.0
shows the auc value,1,1,1.0
auc value of ramoboost,1,1,1.0
value of ramoboost and,1,1,1.0
of ramoboost and other,1,1,1.0
and other comparative learning,1,1,1.0
other comparative learning algorithms,1,1,1.0
comparative learning algorithms under,1,1,1.0
learning algorithms under different,1,1,1.0
algorithms under different class,1,1,1.0
under different class label,1,1,1.0
different class label noise,1,1,1.0
class label noise levels,1,1,1.0
label noise levels tables,1,1,1.0
noise levels tables xix,1,1,1.0
levels tables xix and,1,1,1.0
tables xix and xx,1,1,1.0
xix and xx show,1,1,1.0
and xx show the,1,1,1.0
xx show the signiﬁcance,1,1,1.0
show the signiﬁcance test,1,1,1.0
the averaged auc and,2,1,2.0
averaged auc and auc,1,1,1.0
auc and auc values,1,1,1.0
and auc values of,1,1,1.0
ten random runs attribute,1,1,1.0
random runs attribute noise,1,1,1.0
runs attribute noise was,1,1,1.0
attribute noise was manually,1,1,1.0
noise was manually added,1,1,1.0
was manually added in,1,1,1.0
manually added in accordance,1,1,1.0
added in accordance with,1,1,1.0
in accordance with the,1,1,1.0
accordance with the procedure,1,1,1.0
with the procedure in,1,1,1.0
the procedure in to,1,1,1.0
procedure in to corrupt,1,1,1.0
in to corrupt each,1,1,1.0
to corrupt each attribute,1,1,1.0
corrupt each attribute ai,1,1,1.0
each attribute ai with,1,1,1.0
attribute ai with a,1,1,1.0
ai with a noise,1,1,1.0
with a noise level,1,1,1.0
noise level x the,1,1,1.0
level x the value,1,1,1.0
x the value of,1,1,1.0
the value of ai,1,1,1.0
value of ai is,1,1,1.0
of ai is assigned,1,1,1.0
ai is assigned a,1,1,1.0
is assigned a random,1,1,1.0
assigned a random value,1,1,1.0
a random value approximately,1,1,1.0
random value approximately x,1,1,1.0
value approximately x of,1,1,1.0
approximately x of the,1,1,1.0
x of the time,1,1,1.0
of the time with,1,1,1.0
the time with each,1,1,1.0
time with each tive,1,1,1.0
with each tive value,1,1,1.0
each tive value being,1,1,1.0
tive value being approximately,1,1,1.0
value being approximately equally,1,1,1.0
being approximately equally likely,1,1,1.0
approximately equally likely to,1,1,1.0
likely to be selected,1,1,1.0
to be selected table,1,1,1.0
be selected table xxi,1,1,1.0
selected table xxi shows,1,1,1.0
table xxi shows the,1,1,1.0
xxi shows the averaged,1,1,1.0
shows the averaged auc,1,1,1.0
averaged auc results of,1,1,1.0
auc results of ten,1,1,1.0
ten random runs tables,1,1,1.0
random runs tables xxii,1,1,1.0
runs tables xxii and,1,1,1.0
tables xxii and xxiii,1,1,1.0
xxii and xxiii present,1,1,1.0
and xxiii present the,1,1,1.0
xxiii present the signiﬁcance,1,1,1.0
present the signiﬁcance test,1,1,1.0
signiﬁcance test results based,1,1,1.0
test results based on,1,1,1.0
results based on the,1,1,1.0
averaged auc and the,1,1,1.0
auc and the auc,1,1,1.0
and the auc of,1,1,1.0
the auc of ten,1,1,1.0
auc of ten random,1,1,1.0
ten random runs all,1,1,1.0
random runs all simulation,1,1,1.0
runs all simulation results,1,1,1.0
all simulation results presented,1,1,1.0
simulation results presented in,1,1,1.0
in this section illustrate,1,1,1.0
this section illustrate the,1,1,1.0
section illustrate the robustness,1,1,1.0
illustrate the robustness of,1,1,1.0
robustness of ramoboost when,1,1,1.0
of ramoboost when exposed,1,1,1.0
ramoboost when exposed to,1,1,1.0
when exposed to different,1,1,1.0
exposed to different internal,1,1,1.0
to different internal is,1,1,1.0
different internal is signiﬁcantly,1,1,1.0
internal is signiﬁcantly better,1,1,1.0
is signiﬁcantly better tha,1,1,1.0
signiﬁcantly better tha n,1,1,1.0
better tha n the,1,1,1.0
tha n the comparative,1,1,1.0
n the comparative algorithm,1,1,1.0
the comparative algorithm only,1,1,1.0
comparative algorithm only if,1,1,1.0
algorithm only if the,1,1,1.0
only if the corresponding,1,1,1.0
if the corresponding table,1,1,1.0
the corresponding table cell,1,1,1.0
corresponding table cell is,1,1,1.0
table cell is hi,1,1,1.0
cell is hi ghlighted,1,1,1.0
is hi ghlighted and,1,1,1.0
hi ghlighted and underscored,1,1,1.0
ghlighted and underscored with,1,1,1.0
and underscored with symbol,1,1,1.0
underscored with symbol symbol,1,1,1.0
with symbol symbol represents,1,1,1.0
symbol symbol represents the,1,1,1.0
symbol represents the opposite,1,1,1.0
represents the opposite oversampling,1,1,1.0
the opposite oversampling ratio,1,1,1.0
opposite oversampling ratio and,1,1,1.0
oversampling ratio and exte,1,1,1.0
ratio and exte rnal,1,1,1.0
and exte rnal imbalanced,1,1,1.0
exte rnal imbalanced class,1,1,1.0
rnal imbalanced class ratio,1,1,1.0
imbalanced class ratio noises,1,1,1.0
class ratio noises conﬁgurations,1,1,1.0
ratio noises conﬁgurations the,1,1,1.0
noises conﬁgurations the signiﬁcance,1,1,1.0
conﬁgurations the signiﬁcance tests,1,1,1.0
the signiﬁcance tests also,1,1,1.0
signiﬁcance tests also demonstrate,1,1,1.0
tests also demonstrate the,1,1,1.0
also demonstrate the competitiveness,1,1,1.0
demonstrate the competitiveness of,1,1,1.0
the competitiveness of ramoboost,1,1,1.0
competitiveness of ramoboost against,1,1,1.0
comparative algorithms from a,1,1,1.0
algorithms from a statistical,1,1,1.0
point of view c,1,1,1.0
of view c onclusion,1,1,1.0
view c onclusion in,1,1,1.0
c onclusion in this,1,1,1.0
onclusion in this paper,1,1,1.0
this paper we presented,1,1,1.0
paper we presented the,1,1,1.0
we presented the ramoboost,1,1,1.0
presented the ramoboost method,1,1,1.0
the ramoboost method for,1,1,1.0
ramoboost method for balanced,1,1,1.0
method for balanced data,1,1,1.0
for balanced data classiﬁcation,1,1,1.0
balanced data classiﬁcation problem,1,1,1.0
data classiﬁcation problem the,1,1,1.0
classiﬁcation problem the key,1,1,1.0
problem the key characteristics,1,1,1.0
the key characteristics of,1,1,1.0
key characteristics of ramoboost,1,1,1.0
characteristics of ramoboost are,1,1,1.0
of ramoboost are adaptive,1,1,1.0
ramoboost are adaptive learning,1,1,1.0
are adaptive learning and,1,1,1.0
adaptive learning and reduction,1,1,1.0
learning and reduction of,1,1,1.0
and reduction of bias,1,1,1.0
reduction of bias this,1,1,1.0
of bias this is,1,1,1.0
bias this is accomplished,1,1,1.0
is accomplished by adaptively,1,1,1.0
accomplished by adaptively shifting,1,1,1.0
by adaptively shifting the,1,1,1.0
adaptively shifting the decision,1,1,1.0
shifting the decision boundary,1,1,1.0
decision boundary toward those,1,1,1.0
boundary toward those difﬁcult,1,1,1.0
toward those difﬁcult examples,1,1,1.0
those difﬁcult examples in,1,1,1.0
difﬁcult examples in both,1,1,1.0
examples in both minority,1,1,1.0
in both minority and,1,1,1.0
both minority and majority,1,1,1.0
and majority examples and,1,1,1.0
majority examples and systematically,1,1,1.0
examples and systematically creating,1,1,1.0
and systematically creating minority,1,1,1.0
systematically creating minority synthetic,1,1,1.0
creating minority synthetic stances,1,1,1.0
minority synthetic stances based,1,1,1.0
synthetic stances based on,1,1,1.0
stances based on the,1,1,1.0
based on the distribution,1,1,1.0
on the distribution function,1,1,1.0
the distribution function simulation,1,1,1.0
distribution function simulation results,1,1,1.0
function simulation results on,1,1,1.0
simulation results on datasets,1,1,1.0
results on datasets across,1,1,1.0
on datasets across various,1,1,1.0
datasets across various assessment,1,1,1.0
across various assessment metrics,1,1,1.0
various assessment metrics including,1,1,1.0
assessment metrics including oa,1,1,1.0
metrics including oa precision,1,1,1.0
including oa precision recall,1,1,1.0
oa precision recall roc,1,1,1.0
precision recall roc graphs,1,1,1.0
recall roc graphs and,1,1,1.0
roc graphs and auc,1,1,1.0
graphs and auc demonstrate,1,1,1.0
and auc demonstrate the,1,1,1.0
auc demonstrate the effectiveness,1,1,1.0
demonstrate the effectiveness and,1,1,1.0
the effectiveness and robustness,1,1,1.0
effectiveness and robustness of,1,1,1.0
and robustness of the,1,1,1.0
robustness of the proposed,1,1,1.0
the proposed method as,1,1,1.0
proposed method as a,1,1,1.0
method as a new,1,1,1.0
as a new method,1,1,1.0
a new method for,1,1,1.0
new method for imbalanced,1,1,1.0
method for imbalanced learning,1,1,1.0
imbalanced learning problems there,1,1,1.0
learning problems there are,1,1,1.0
problems there are several,1,1,1.0
there are several interesting,1,1,1.0
are several interesting future,1,1,1.0
several interesting future research,1,1,1.0
interesting future research directions,1,1,1.0
future research directions for,1,1,1.0
research directions for boost,1,1,1.0
directions for boost for,1,1,1.0
for boost for instance,1,1,1.0
boost for instance our,1,1,1.0
for instance our current,1,1,1.0
instance our current study,1,1,1.0
our current study is,1,1,1.0
current study is focused,1,1,1.0
study is focused on,1,1,1.0
is focused on handling,1,1,1.0
focused on handling datasets,1,1,1.0
on handling datasets with,1,1,1.0
handling datasets with continuous,1,1,1.0
datasets with continuous features,1,1,1.0
with continuous features it,1,1,1.0
continuous features it is,1,1,1.0
features it is possible,1,1,1.0
it is possible to,1,1,1.0
is possible to extend,1,1,1.0
possible to extend ramoboost,1,1,1.0
to extend ramoboost to,1,1,1.0
extend ramoboost to handle,1,1,1.0
ramoboost to handle datasets,1,1,1.0
to handle datasets with,1,1,1.0
handle datasets with nominal,1,1,1.0
datasets with nominal features,1,1,1.0
with nominal features by,1,1,1.0
nominal features by adopting,1,1,1.0
features by adopting the,1,1,1.0
by adopting the techniques,1,1,1.0
adopting the techniques used,1,1,1.0
the techniques used in,1,1,1.0
techniques used in the,1,1,1.0
used in the method,1,1,1.0
in the method et,1,1,1.0
the method et al,1,1,1.0
method et al ranked,1,1,1.0
oversampling in boosting second,1,1,1.0
in boosting second in,1,1,1.0
boosting second in this,1,1,1.0
second in this paper,1,1,1.0
in this paper ramoboost,1,1,1.0
this paper ramoboost is,1,1,1.0
paper ramoboost is only,1,1,1.0
ramoboost is only evaluated,1,1,1.0
is only evaluated on,1,1,1.0
only evaluated on class,1,1,1.0
evaluated on class imbalanced,1,1,1.0
on class imbalanced problems,1,1,1.0
class imbalanced problems it,1,1,1.0
imbalanced problems it can,1,1,1.0
problems it can be,1,1,1.0
it can be generalized,1,1,1.0
can be generalized to,1,1,1.0
be generalized to handle,1,1,1.0
generalized to handle multiclass,1,1,1.0
to handle multiclass imbalanced,1,1,1.0
handle multiclass imbalanced learning,1,1,1.0
multiclass imbalanced learning problems,1,1,1.0
learning problems to improve,1,1,1.0
problems to improve its,1,1,1.0
to improve its plicability,1,1,1.0
improve its plicability in,1,1,1.0
its plicability in practice,1,1,1.0
plicability in practice third,1,1,1.0
in practice third in,1,1,1.0
practice third in ramoboost,1,1,1.0
third in ramoboost the,1,1,1.0
in ramoboost the euclidean,1,1,1.0
ramoboost the euclidean distance,1,1,1.0
the euclidean distance is,1,1,1.0
euclidean distance is employed,1,1,1.0
distance is employed as,1,1,1.0
employed as the distance,1,1,1.0
as the distance measure,1,1,1.0
the distance measure however,1,1,1.0
distance measure however there,1,1,1.0
measure however there are,1,1,1.0
however there are other,1,1,1.0
there are other alternatives,1,1,1.0
are other alternatives that,1,1,1.0
other alternatives that are,1,1,1.0
alternatives that are also,1,1,1.0
that are also eligible,1,1,1.0
are also eligible and,1,1,1.0
also eligible and worthy,1,1,1.0
eligible and worthy of,1,1,1.0
and worthy of trying,1,1,1.0
worthy of trying and,1,1,1.0
of trying and may,1,1,1.0
trying and may show,1,1,1.0
and may show improved,1,1,1.0
may show improved performance,1,1,1.0
show improved performance for,1,1,1.0
improved performance for the,1,1,1.0
performance for the ramoboost,1,1,1.0
for the ramoboost framework,1,1,1.0
the ramoboost framework finally,1,1,1.0
ramoboost framework finally similar,1,1,1.0
framework finally similar to,1,1,1.0
finally similar to many,1,1,1.0
similar to many of,1,1,1.0
to many of the,1,1,1.0
many of the existing,1,1,1.0
existing imbalanced learning algorithms,1,1,1.0
imbalanced learning algorithms there,1,1,1.0
learning algorithms there are,1,1,1.0
algorithms there are several,1,1,1.0
there are several parameters,1,1,1.0
are several parameters that,1,1,1.0
several parameters that need,1,1,1.0
parameters that need to,1,1,1.0
need to be determined,1,1,1.0
to be determined for,1,1,1.0
be determined for ramoboost,1,1,1.0
determined for ramoboost we,1,1,1.0
for ramoboost we have,1,1,1.0
ramoboost we have shown,1,1,1.0
we have shown some,1,1,1.0
have shown some empirical,1,1,1.0
shown some empirical results,1,1,1.0
some empirical results regarding,1,1,1.0
empirical results regarding this,1,1,1.0
results regarding this issue,1,1,1.0
regarding this issue in,1,1,1.0
this issue in this,1,1,1.0
issue in this paper,1,1,1.0
this paper and we,1,1,1.0
paper and we also,1,1,1.0
and we also would,1,1,1.0
we also would like,1,1,1.0
also would like to,1,1,1.0
would like to note,1,1,1.0
to note that a,1,1,1.0
note that a systematic,1,1,1.0
that a systematic and,1,1,1.0
a systematic and adaptive,1,1,1.0
systematic and adaptive way,1,1,1.0
and adaptive way to,1,1,1.0
adaptive way to adjust,1,1,1.0
way to adjust those,1,1,1.0
to adjust those parameters,1,1,1.0
adjust those parameters could,1,1,1.0
those parameters could be,1,1,1.0
parameters could be a,1,1,1.0
could be a challenging,1,1,1.0
be a challenging but,1,1,1.0
a challenging but important,1,1,1.0
challenging but important issue,1,1,1.0
but important issue for,1,1,1.0
important issue for this,1,1,1.0
issue for this method,1,1,1.0
for this method to,1,1,1.0
this method to be,1,1,1.0
method to be applied,1,1,1.0
to be applied across,1,1,1.0
be applied across different,1,1,1.0
applied across different application,1,1,1.0
across different application domains,1,1,1.0
different application domains our,1,1,1.0
application domains our group,1,1,1.0
domains our group is,1,1,1.0
our group is currently,1,1,1.0
group is currently investigating,1,1,1.0
is currently investigating all,1,1,1.0
currently investigating all these,1,1,1.0
investigating all these issues,1,1,1.0
all these issues motivated,1,1,1.0
these issues motivated by,1,1,1.0
issues motivated by our,1,1,1.0
motivated by our initial,1,1,1.0
by our initial results,1,1,1.0
our initial results in,1,1,1.0
initial results in this,1,1,1.0
results in this paper,1,1,1.0
this paper we believe,1,1,1.0
paper we believe that,1,1,1.0
we believe that ramoboost,1,1,1.0
believe that ramoboost may,1,1,1.0
that ramoboost may provide,1,1,1.0
ramoboost may provide new,1,1,1.0
may provide new insights,1,1,1.0
provide new insights for,1,1,1.0
new insights for imbalanced,1,1,1.0
insights for imbalanced learning,1,1,1.0
imbalanced learning problems and,1,1,1.0
learning problems and have,1,1,1.0
problems and have the,1,1,1.0
and have the potential,1,1,1.0
have the potential to,1,1,1.0
the potential to be,1,1,1.0
potential to be a,1,1,1.0
to be a powerful,1,1,1.0
be a powerful tool,1,1,1.0
a powerful tool in,1,1,1.0
powerful tool in many,1,1,1.0
tool in many application,1,1,1.0
many application domains references,1,1,1.0
application domains references he,1,1,1.0
domains references he and,1,1,1.0
references he and garcia,1,1,1.0
knowl data eng vol,3,1,3.0
data eng vol no,3,1,3.0
eng vol no pp,3,1,3.0
vol no pp provost,1,1,1.0
no pp provost learning,1,1,1.0
pp provost learning with,1,1,1.0
provost learning with imbalanced,1,1,1.0
data sets in learning,1,1,1.0
sets in learning from,1,1,1.0
imbalanced data sets japkowicz,1,1,1.0
data sets japkowicz ed,1,1,1.0
sets japkowicz ed menlo,1,1,1.0
japkowicz ed menlo park,1,1,1.0
ed menlo park ca,1,1,1.0
ca aaai press v,1,1,1.0
aaai press v chawla,1,1,1.0
press v chawla japkowicz,1,1,1.0
v chawla japkowicz and,2,1,2.0
chawla japkowicz and kołcz,2,1,2.0
japkowicz and kołcz editorial,1,1,1.0
and kołcz editorial special,1,1,1.0
kołcz editorial special issue,1,1,1.0
editorial special issue on,1,1,1.0
special issue on learning,1,1,1.0
issue on learning from,1,1,1.0
imbalanced data sets acm,1,1,1.0
data sets acm sigkdd,1,1,1.0
sets acm sigkdd explorations,1,1,1.0
acm sigkdd explorations vol,1,1,1.0
sigkdd explorations vol no,1,1,1.0
explorations vol no pp,1,1,1.0
no pp jun v,2,1,2.0
pp jun v chawla,2,1,2.0
jun v chawla japkowicz,1,1,1.0
japkowicz and kołcz uncertainty,1,1,1.0
and kołcz uncertainty sampling,1,1,1.0
kołcz uncertainty sampling for,1,1,1.0
uncertainty sampling for classiﬁers,1,1,1.0
sampling for classiﬁers in,1,1,1.0
for classiﬁers in proc,1,1,1.0
classiﬁers in proc int,1,1,1.0
in proc int conf,11,1,11.0
proc int conf mach,8,1,8.0
int conf mach workshop,4,1,4.0
conf mach workshop learn,4,1,4.0
mach workshop learn imbalanced,4,1,4.0
workshop learn imbalanced data,4,1,4.0
learn imbalanced data sets,5,1,5.0
sets ii washington pp,2,1,2.0
ii washington pp japkowicz,1,1,1.0
washington pp japkowicz learning,1,1,1.0
pp japkowicz learning from,1,1,1.0
of various strategies in,1,1,1.0
various strategies in proc,1,1,1.0
strategies in proc learn,1,1,1.0
in proc learn imbalanced,1,1,1.0
proc learn imbalanced data,1,1,1.0
imbalanced data sets papers,1,1,1.0
data sets papers aaai,1,1,1.0
sets papers aaai workshop,1,1,1.0
papers aaai workshop menlo,1,1,1.0
aaai workshop menlo park,1,1,1.0
workshop menlo park ca,1,1,1.0
menlo park ca pp,1,1,1.0
park ca pp provost,1,1,1.0
ca pp provost and,1,1,1.0
and fawcett robust classiﬁcation,1,1,1.0
fawcett robust classiﬁcation for,1,1,1.0
robust classiﬁcation for imprecise,1,1,1.0
classiﬁcation for imprecise ments,1,1,1.0
for imprecise ments mach,1,1,1.0
imprecise ments mach vol,1,1,1.0
ments mach vol no,1,1,1.0
mach vol no pp,1,1,1.0
vol no pp mar,1,1,1.0
no pp mar clearwater,1,1,1.0
pp mar clearwater and,1,1,1.0
mar clearwater and stern,1,1,1.0
clearwater and stern a,1,1,1.0
and stern a program,1,1,1.0
stern a program in,1,1,1.0
a program in high,1,1,1.0
program in high energy,1,1,1.0
in high energy physics,1,1,1.0
high energy physics event,1,1,1.0
energy physics event classiﬁcation,1,1,1.0
physics event classiﬁcation comput,1,1,1.0
event classiﬁcation comput phys,1,1,1.0
classiﬁcation comput phys commun,1,1,1.0
comput phys commun vol,1,1,1.0
phys commun vol no,1,1,1.0
commun vol no pp,1,1,1.0
vol no pp weiss,1,1,1.0
no pp weiss mining,1,1,1.0
a unifying framework acm,1,1,1.0
unifying framework acm sigkdd,1,1,1.0
framework acm sigkdd explorations,1,1,1.0
acm sigkdd explorations newslett,4,1,4.0
sigkdd explorations newslett vol,4,1,4.0
explorations newslett vol no,4,1,4.0
newslett vol no pp,4,1,4.0
no pp jun g,1,1,1.0
pp jun g a,1,1,1.0
jun g a t,1,1,1.0
g a t i,1,1,1.0
a t i s,1,1,1.0
t i s t,1,1,1.0
i s t a,1,1,1.0
s t a r,1,1,1.0
t a r r,1,1,1.0
a r r a,1,1,1.0
r r a t,1,1,1.0
r a t i,1,1,1.0
a t i a,1,1,1.0
t i a n,1,1,1.0
i a n dm,1,1,1.0
a n dm o,1,1,1.0
n dm o n,1,1,1.0
dm o n a,1,1,1.0
o n a r,1,1,1.0
n a r d,1,1,1.0
a r d as,1,1,1.0
r d as t,1,1,1.0
d as t u,1,1,1.0
as t u d,1,1,1.0
t u d yo,1,1,1.0
u d yo f,1,1,1.0
d yo f the,1,1,1.0
yo f the behavior,1,1,1.0
f the behavior of,1,1,1.0
several methods for b,1,1,1.0
methods for b alancing,1,1,1.0
for b alancing machine,1,1,1.0
b alancing machine learning,1,1,1.0
alancing machine learning training,1,1,1.0
learning training data acm,1,1,1.0
training data acm sigkdd,1,1,1.0
jun v chawla and,1,1,1.0
v chawla and imbalanced,1,1,1.0
chawla and imbalanced datasets,1,1,1.0
and imbalanced datasets investigating,1,1,1.0
imbalanced datasets investigating the,1,1,1.0
datasets investigating the effect,1,1,1.0
investigating the effect of,1,1,1.0
the effect of sampling,1,1,1.0
effect of sampling method,1,1,1.0
of sampling method probabilistic,1,1,1.0
sampling method probabilistic estimate,1,1,1.0
method probabilistic estimate and,1,1,1.0
probabilistic estimate and decision,1,1,1.0
estimate and decision tree,1,1,1.0
and decision tree structure,1,1,1.0
decision tree structure in,1,1,1.0
tree structure in proc,1,1,1.0
structure in proc int,1,1,1.0
sets ii pp jo,1,1,1.0
ii pp jo and,1,1,1.0
pp jo and japkowicz,1,1,1.0
japkowicz class imbalances small,1,1,1.0
class imbalances small disjuncts,1,1,1.0
imbalances small disjuncts acm,1,1,1.0
small disjuncts acm sigkdd,1,1,1.0
disjuncts acm sigkdd explorations,1,1,1.0
no pp jun weiss,1,1,1.0
pp jun weiss and,1,1,1.0
jun weiss and provost,1,1,1.0
on tree induction artiﬁcial,1,1,1.0
tree induction artiﬁcial intell,1,1,1.0
induction artiﬁcial intell vol,1,1,1.0
artiﬁcial intell vol no,2,1,2.0
intell vol no pp,2,1,2.0
vol no pp jul,1,1,1.0
no pp jul japkowicz,1,1,1.0
pp jul japkowicz class,1,1,1.0
jul japkowicz class imbalances,1,1,1.0
the right issue in,1,1,1.0
right issue in proc,1,1,1.0
issue in proc int,1,1,1.0
sets ii pp prati,1,1,1.0
ii pp prati batista,1,1,1.0
monard class imbalances class,1,1,1.0
class imbalances class overlapping,1,1,1.0
imbalances class overlapping an,1,1,1.0
system behavior in proc,1,1,1.0
behavior in proc mexican,1,1,1.0
in proc mexican int,1,1,1.0
proc mexican int conf,1,1,1.0
mexican int conf artiﬁcial,1,1,1.0
int conf artiﬁcial adv,1,1,1.0
conf artiﬁcial adv artiﬁcial,1,1,1.0
artiﬁcial adv artiﬁcial intell,1,1,1.0
adv artiﬁcial intell pp,1,1,1.0
artiﬁcial intell pp v,1,1,1.0
intell pp v chawla,1,1,1.0
pp v chawla bowyer,1,1,1.0
v chawla bowyer hall,1,1,1.0
chawla bowyer hall and,1,1,1.0
bowyer hall and kegelmeyer,1,1,1.0
hall and kegelmeyer smote,1,1,1.0
synthetic minority technique artiﬁcial,1,1,1.0
minority technique artiﬁcial intell,1,1,1.0
technique artiﬁcial intell vol,1,1,1.0
vol no pp han,1,1,1.0
no pp han wang,1,1,1.0
wang and mao a,1,1,1.0
and mao a new,1,1,1.0
mao a new sampling,1,1,1.0
a new sampling method,1,1,1.0
new sampling method in,1,1,1.0
sets learning in proc,1,1,1.0
learning in proc int,2,1,2.0
proc int conf intell,1,1,1.0
int conf intell adv,1,1,1.0
conf intell adv intell,1,1,1.0
intell adv intell comput,1,1,1.0
adv intell comput pp,1,1,1.0
intell comput pp guo,1,1,1.0
comput pp guo and,1,1,1.0
pp guo and viktor,1,1,1.0
guo and viktor learning,1,1,1.0
and viktor learning from,1,1,1.0
viktor learning from imbalanced,1,1,1.0
data generation the approach,1,1,1.0
generation the approach acm,1,1,1.0
the approach acm sigkdd,1,1,1.0
approach acm sigkdd explorations,1,1,1.0
no pp jun mease,1,1,1.0
pp jun mease wyner,1,1,1.0
jun mease wyner and,1,1,1.0
mease wyner and buja,1,1,1.0
wyner and buja boosted,1,1,1.0
and buja boosted classiﬁcation,1,1,1.0
buja boosted classiﬁcation trees,1,1,1.0
boosted classiﬁcation trees and,1,1,1.0
classiﬁcation trees and class,1,1,1.0
trees and class estimation,1,1,1.0
and class estimation mach,1,1,1.0
class estimation mach learn,1,1,1.0
estimation mach learn res,1,1,1.0
mach learn res vol,2,1,2.0
learn res vol pp,1,1,1.0
res vol pp may,1,1,1.0
vol pp may yuan,1,1,1.0
pp may yuan li,1,1,1.0
may yuan li and,1,1,1.0
yuan li and zhang,1,1,1.0
cluster machines in proc,1,1,1.0
machines in proc annu,1,1,1.0
in proc annu acm,1,1,1.0
proc annu acm int,1,1,1.0
annu acm int conf,1,1,1.0
acm int conf multimedia,1,1,1.0
int conf multimedia santa,1,1,1.0
conf multimedia santa barbara,1,1,1.0
multimedia santa barbara ca,1,1,1.0
santa barbara ca pp,1,1,1.0
barbara ca pp ting,1,1,1.0
ca pp ting an,1,1,1.0
pp ting an method,1,1,1.0
ting an method to,1,1,1.0
an method to induce,1,1,1.0
method to induce trees,1,1,1.0
to induce trees ieee,1,1,1.0
induce trees ieee trans,1,1,1.0
trees ieee trans knowl,1,1,1.0
vol no pp and,1,1,1.0
no pp and vasconcelos,1,1,1.0
pp and vasconcelos asymmetric,1,1,1.0
and vasconcelos asymmetric boosting,1,1,1.0
vasconcelos asymmetric boosting in,1,1,1.0
asymmetric boosting in proc,1,1,1.0
boosting in proc int,2,1,2.0
int conf mach learn,4,1,4.0
conf mach learn corvallis,1,1,1.0
mach learn corvallis or,1,1,1.0
learn corvallis or pp,1,1,1.0
corvallis or pp viola,1,1,1.0
or pp viola and,1,1,1.0
pp viola and jones,1,1,1.0
viola and jones fast,1,1,1.0
and jones fast and,1,1,1.0
jones fast and robust,1,1,1.0
fast and robust classiﬁcation,1,1,1.0
and robust classiﬁcation using,1,1,1.0
robust classiﬁcation using asymmetric,1,1,1.0
classiﬁcation using asymmetric adaboost,1,1,1.0
using asymmetric adaboost and,1,1,1.0
asymmetric adaboost and a,1,1,1.0
adaboost and a detector,1,1,1.0
and a detector cascade,1,1,1.0
a detector cascade in,1,1,1.0
detector cascade in advances,1,1,1.0
cascade in advances in,1,1,1.0
in advances in neural,1,1,1.0
neural information processing system,1,1,1.0
information processing system cambridge,1,1,1.0
processing system cambridge ma,1,1,1.0
system cambridge ma mit,1,1,1.0
cambridge ma mit press,2,1,2.0
ma mit press pp,2,1,2.0
mit press pp fan,1,1,1.0
press pp fan stolfo,1,1,1.0
pp fan stolfo zhang,1,1,1.0
fan stolfo zhang an,1,1,1.0
stolfo zhang an d,1,1,1.0
zhang an d chan,1,1,1.0
an d chan adacost,1,1,1.0
d chan adacost cation,1,1,1.0
chan adacost cation boosting,1,1,1.0
adacost cation boosting in,1,1,1.0
cation boosting in proc,1,1,1.0
conf mach learn pp,2,1,2.0
mach learn pp domingos,1,1,1.0
learn pp domingos metacost,1,1,1.0
pp domingos metacost a,1,1,1.0
domingos metacost a genera,1,1,1.0
metacost a genera l,1,1,1.0
a genera l method,1,1,1.0
genera l method for,1,1,1.0
l method for making,1,1,1.0
method for making classiﬁers,1,1,1.0
for making classiﬁers sensitive,1,1,1.0
making classiﬁers sensitive in,1,1,1.0
classiﬁers sensitive in proc,1,1,1.0
sensitive in proc acm,1,1,1.0
in proc acm sigkdd,1,1,1.0
proc acm sigkdd int,1,1,1.0
acm sigkdd int conf,1,1,1.0
sigkdd int conf knowl,1,1,1.0
int conf knowl discovery,2,1,2.0
conf knowl discovery data,2,1,2.0
knowl discovery data mining,2,1,2.0
discovery data mining pp,1,1,1.0
data mining pp liu,1,1,1.0
mining pp liu and,1,1,1.0
pp liu and zhou,1,1,1.0
liu and zhou training,1,1,1.0
and zhou training neural,1,1,1.0
zhou training neural networks,1,1,1.0
training neural networks with,1,1,1.0
neural networks with methods,1,1,1.0
networks with methods addressing,1,1,1.0
with methods addressing the,1,1,1.0
methods addressing the class,1,1,1.0
addressing the class imbalance,2,1,2.0
class imbalance problem ieee,1,1,1.0
imbalance problem ieee trans,1,1,1.0
problem ieee trans knowl,1,1,1.0
trans knowl data vol,1,1,1.0
knowl data vol no,1,1,1.0
data vol no pp,1,1,1.0
no pp y liu,1,1,1.0
pp y liu and,1,1,1.0
y liu and y,1,1,1.0
liu and y chen,1,1,1.0
and y chen face,1,1,1.0
y chen face recognition,1,1,1.0
chen face recognition using,1,1,1.0
face recognition using total,1,1,1.0
recognition using total adaptive,1,1,1.0
using total adaptive fuzzy,1,1,1.0
total adaptive fuzzy support,1,1,1.0
adaptive fuzzy support vector,1,1,1.0
fuzzy support vector machines,1,1,1.0
support vector machines ieee,1,1,1.0
vector machines ieee trans,1,1,1.0
machines ieee trans neural,2,1,2.0
ieee trans neural v,1,1,1.0
trans neural v o,1,1,1.0
neural v o l,1,1,1.0
v o l no,1,1,1.0
o l no pp,1,1,1.0
l no pp wu,1,1,1.0
no pp wu and,1,1,1.0
kernel boundary alignment considering,1,1,1.0
boundary alignment considering imbalanced,1,1,1.0
alignment considering imbalanced data,1,1,1.0
considering imbalanced data distribution,1,1,1.0
data distribution ieee trans,1,1,1.0
distribution ieee trans knowl,1,1,1.0
no pp jun wu,1,1,1.0
pp jun wu and,1,1,1.0
jun wu and y,1,1,1.0
and y chang aligning,1,1,1.0
y chang aligning boundary,1,1,1.0
chang aligning boundary in,1,1,1.0
aligning boundary in kernel,1,1,1.0
boundary in kernel space,1,1,1.0
in kernel space for,1,1,1.0
kernel space for learning,1,1,1.0
space for learning imbalanced,1,1,1.0
for learning imbalanced dataset,1,1,1.0
learning imbalanced dataset in,1,1,1.0
imbalanced dataset in proc,1,1,1.0
dataset in proc ieee,1,1,1.0
in proc ieee int,1,1,1.0
proc ieee int conf,1,1,1.0
ieee int conf data,1,1,1.0
int conf data mining,1,1,1.0
conf data mining brighton,1,1,1.0
data mining brighton pp,1,1,1.0
mining brighton pp hong,1,1,1.0
brighton pp hong chen,1,1,1.0
data sets ieee trans,1,1,1.0
sets ieee trans neural,1,1,1.0
ieee trans neural vol,4,1,4.0
trans neural vol no,4,1,4.0
neural vol no pp,4,1,4.0
vol no pp ertekin,1,1,1.0
no pp ertekin huang,1,1,1.0
pp ertekin huang bottou,1,1,1.0
ertekin huang bottou and,1,1,1.0
huang bottou and giles,1,1,1.0
bottou and giles learning,1,1,1.0
and giles learning on,1,1,1.0
giles learning on the,1,1,1.0
learning on the border,1,1,1.0
on the border active,1,1,1.0
the border active learning,1,1,1.0
border active learning in,1,1,1.0
active learning in imbalanced,1,1,1.0
learning in imbalanced data,1,1,1.0
in imbalanced data classiﬁcation,1,1,1.0
imbalanced data classiﬁcation in,1,1,1.0
data classiﬁcation in proc,2,1,2.0
classiﬁcation in proc acm,1,1,1.0
in proc acm conf,1,1,1.0
proc acm conf inform,1,1,1.0
acm conf inform knowl,1,1,1.0
conf inform knowl manage,1,1,1.0
inform knowl manage lisbon,1,1,1.0
knowl manage lisbon portugal,1,1,1.0
manage lisbon portugal pp,1,1,1.0
lisbon portugal pp ertekin,1,1,1.0
portugal pp ertekin huang,1,1,1.0
pp ertekin huang and,1,1,1.0
ertekin huang and giles,1,1,1.0
huang and giles active,1,1,1.0
and giles active learning,1,1,1.0
giles active learning for,1,1,1.0
active learning for class,1,1,1.0
learning for class imbalance,1,1,1.0
for class imbalance problem,1,1,1.0
imbalance problem in proc,2,1,2.0
problem in proc annu,1,1,1.0
in proc annu int,1,1,1.0
proc annu int acm,1,1,1.0
annu int acm sigir,1,1,1.0
int acm sigir conf,1,1,1.0
acm sigir conf res,1,1,1.0
sigir conf res develop,1,1,1.0
conf res develop inform,1,1,1.0
res develop inform retrieval,1,1,1.0
develop inform retrieval amsterdam,1,1,1.0
inform retrieval amsterdam the,1,1,1.0
retrieval amsterdam the netherlands,1,1,1.0
amsterdam the netherlands pp,1,1,1.0
the netherlands pp zhu,1,1,1.0
netherlands pp zhu and,1,1,1.0
pp zhu and hovy,1,1,1.0
zhu and hovy active,1,1,1.0
and hovy active learning,1,1,1.0
hovy active learning for,1,1,1.0
active learning for word,1,1,1.0
learning for word sense,1,1,1.0
for word sense disambiguation,1,1,1.0
word sense disambiguation with,1,1,1.0
sense disambiguation with methods,1,1,1.0
disambiguation with methods for,1,1,1.0
with methods for addressing,1,1,1.0
methods for addressing the,1,1,1.0
for addressing the class,1,1,1.0
problem in proc joint,1,1,1.0
in proc joint conf,1,1,1.0
proc joint conf empirical,1,1,1.0
joint conf empirical methods,1,1,1.0
conf empirical methods natural,1,1,1.0
empirical methods natural l,1,1,1.0
methods natural l ang,1,1,1.0
natural l ang process,1,1,1.0
l ang process computat,1,1,1.0
ang process computat natural,1,1,1.0
process computat natural lang,1,1,1.0
computat natural lang prague,1,1,1.0
natural lang prague czech,1,1,1.0
lang prague czech republic,1,1,1.0
prague czech republic pp,1,1,1.0
czech republic pp he,1,1,1.0
republic pp he y,1,1,1.0
imbalanced learning in proc,1,1,1.0
in proc int joint,1,1,1.0
proc int joint conf,1,1,1.0
int joint conf neural,1,1,1.0
joint conf neural jun,1,1,1.0
conf neural jun pp,1,1,1.0
neural jun pp v,1,1,1.0
jun pp v chawla,1,1,1.0
pp v chawla lazarevic,1,1,1.0
v chawla lazarevic hall,1,1,1.0
hall and bowyer boost,1,1,1.0
and bowyer boost improving,1,1,1.0
bowyer boost improving prediction,1,1,1.0
boost improving prediction of,1,1,1.0
in boosting in proc,1,1,1.0
boosting in proc principles,1,1,1.0
in proc principles knowl,1,1,1.0
proc principles knowl discovery,1,1,1.0
principles knowl discovery databases,1,1,1.0
knowl discovery databases croatia,1,1,1.0
discovery databases croatia pp,1,1,1.0
databases croatia pp y,1,1,1.0
croatia pp y freund,1,1,1.0
pp y freund and,2,1,2.0
y freund and schapire,2,1,2.0
freund and schapire experiments,1,1,1.0
and schapire experiments with,1,1,1.0
schapire experiments with a,1,1,1.0
experiments with a new,1,1,1.0
with a new boosting,1,1,1.0
a new boosting algorithm,1,1,1.0
new boosting algorithm in,1,1,1.0
boosting algorithm in proc,1,1,1.0
algorithm in proc int,1,1,1.0
mach learn pp y,1,1,1.0
learn pp y freund,1,1,1.0
application to boosting comput,1,1,1.0
to boosting comput syst,1,1,1.0
boosting comput syst sci,1,1,1.0
comput syst sci vol,1,1,1.0
syst sci vol no,1,1,1.0
sci vol no pp,1,1,1.0
vol no pp kubat,1,1,1.0
no pp kubat and,1,1,1.0
kubat and matwin addressi,1,1,1.0
and matwin addressi ng,1,1,1.0
matwin addressi ng the,1,1,1.0
addressi ng the curse,1,1,1.0
ng the curse of,1,1,1.0
sets selection in proc,1,1,1.0
selection in proc int,1,1,1.0
conf mach learn nashville,1,1,1.0
mach learn nashville tn,1,1,1.0
learn nashville tn pp,1,1,1.0
nashville tn pp caballero,1,1,1.0
tn pp caballero martinze,1,1,1.0
pp caballero martinze hervas,1,1,1.0
caballero martinze hervas and,1,1,1.0
martinze hervas and gutierrez,1,1,1.0
hervas and gutierrez sensitivity,1,1,1.0
and gutierrez sensitivity accuracy,1,1,1.0
gutierrez sensitivity accuracy in,1,1,1.0
sensitivity accuracy in multiclass,1,1,1.0
accuracy in multiclass problems,1,1,1.0
in multiclass problems using,1,1,1.0
multiclass problems using memetic,1,1,1.0
problems using memetic pareto,1,1,1.0
using memetic pareto evolutionary,1,1,1.0
memetic pareto evolutionary neural,1,1,1.0
pareto evolutionary neural networks,1,1,1.0
evolutionary neural networks ieee,1,1,1.0
neural networks ieee trans,1,1,1.0
networks ieee trans neural,1,1,1.0
ieee trans neural netw,2,1,2.0
trans neural netw vol,2,1,2.0
neural netw vol no,2,1,2.0
netw vol no pp,2,1,2.0
vol no pp may,1,1,1.0
no pp may constructing,1,1,1.0
pp may constructing ensembles,1,1,1.0
may constructing ensembles of,1,1,1.0
constructing ensembles of classiﬁers,1,1,1.0
ensembles of classiﬁers by,1,1,1.0
of classiﬁers by means,1,1,1.0
classiﬁers by means of,1,1,1.0
by means of weighted,1,1,1.0
means of weighted instance,1,1,1.0
of weighted instance selection,1,1,1.0
weighted instance selection ieee,1,1,1.0
instance selection ieee trans,1,1,1.0
selection ieee trans neural,1,1,1.0
vol no pp muhlbaier,1,1,1.0
no pp muhlbaier topalis,1,1,1.0
pp muhlbaier topalis and,1,1,1.0
muhlbaier topalis and polikar,1,1,1.0
topalis and polikar learn,1,1,1.0
and polikar learn combining,1,1,1.0
polikar learn combining ensemble,1,1,1.0
learn combining ensemble of,1,1,1.0
combining ensemble of classiﬁers,1,1,1.0
ensemble of classiﬁers with,1,1,1.0
of classiﬁers with dynamically,1,1,1.0
classiﬁers with dynamically weighted,1,1,1.0
with dynamically weighted for,1,1,1.0
dynamically weighted for efﬁcient,1,1,1.0
weighted for efﬁcient incremental,1,1,1.0
for efﬁcient incremental learning,1,1,1.0
efﬁcient incremental learning of,1,1,1.0
incremental learning of new,1,1,1.0
learning of new classes,1,1,1.0
of new classes ieee,1,1,1.0
new classes ieee trans,1,1,1.0
classes ieee trans neural,1,1,1.0
vol no pp ieee,1,1,1.0
no pp ieee transactions,1,1,1.0
pp ieee transactions on,1,1,1.0
vol no october shen,1,1,1.0
no october shen and,1,1,1.0
october shen and li,1,1,1.0
shen and li boosting,1,1,1.0
and li boosting through,1,1,1.0
li boosting through optimization,1,1,1.0
boosting through optimization of,1,1,1.0
through optimization of margin,1,1,1.0
optimization of margin utions,1,1,1.0
of margin utions ieee,1,1,1.0
margin utions ieee trans,1,1,1.0
utions ieee trans neural,1,1,1.0
vol no pp apr,2,1,2.0
no pp apr sun,1,1,1.0
pp apr sun and,1,1,1.0
apr sun and yao,1,1,1.0
sun and yao sparse,1,1,1.0
and yao sparse approximation,1,1,1.0
yao sparse approximation through,1,1,1.0
sparse approximation through boosting,1,1,1.0
approximation through boosting for,1,1,1.0
through boosting for learning,1,1,1.0
boosting for learning large,1,1,1.0
for learning large scale,1,1,1.0
learning large scale kernel,1,1,1.0
large scale kernel machines,1,1,1.0
scale kernel machines ieee,1,1,1.0
kernel machines ieee trans,1,1,1.0
no pp jun hu,1,1,1.0
pp jun hu hu,1,1,1.0
jun hu hu and,1,1,1.0
hu hu and maybank,1,1,1.0
hu and maybank algorithm,1,1,1.0
and maybank algorithm for,1,1,1.0
maybank algorithm for network,1,1,1.0
algorithm for network intrusion,1,1,1.0
for network intrusion detection,1,1,1.0
network intrusion detection ieee,1,1,1.0
intrusion detection ieee trans,1,1,1.0
detection ieee trans man,1,1,1.0
ieee trans man part,1,1,1.0
trans man part b,1,1,1.0
man part b vol,1,1,1.0
part b vol no,1,1,1.0
b vol no pp,1,1,1.0
no pp apr he,1,1,1.0
pp apr he and,1,1,1.0
apr he and shen,1,1,1.0
he and shen a,1,1,1.0
and shen a ranked,1,1,1.0
shen a ranked subspace,1,1,1.0
a ranked subspace learning,1,1,1.0
ranked subspace learning method,1,1,1.0
subspace learning method for,1,1,1.0
learning method for gene,1,1,1.0
method for gene expression,1,1,1.0
for gene expression data,1,1,1.0
gene expression data classiﬁcation,1,1,1.0
expression data classiﬁcation in,1,1,1.0
classiﬁcation in proc int,1,1,1.0
proc int conf artiﬁcial,1,1,1.0
int conf artiﬁcial intell,1,1,1.0
conf artiﬁcial intell pp,1,1,1.0
artiﬁcial intell pp asuncion,1,1,1.0
intell pp asuncion and,1,1,1.0
pp asuncion and newman,1,1,1.0
asuncion and newman uci,1,1,1.0
and newman uci machine,1,1,1.0
learning repository online available,1,1,1.0
repository online available http,1,1,1.0
online available http elena,1,1,1.0
available http elena project,1,1,1.0
http elena project online,1,1,1.0
elena project online available,1,1,1.0
project online available ftp,1,1,1.0
online available ftp fawcett,1,1,1.0
available ftp fawcett roc,1,1,1.0
ftp fawcett roc graphs,1,1,1.0
fawcett roc graphs notes,1,1,1.0
roc graphs notes and,1,1,1.0
graphs notes and practical,1,1,1.0
notes and practical considerations,1,1,1.0
and practical considerations for,1,1,1.0
practical considerations for data,1,1,1.0
considerations for data mining,1,1,1.0
for data mining researchers,1,1,1.0
data mining researchers hp,1,1,1.0
mining researchers hp palo,1,1,1.0
researchers hp palo alto,1,1,1.0
hp palo alto ca,1,1,1.0
palo alto ca tech,1,1,1.0
alto ca tech kubat,1,1,1.0
ca tech kubat holte,1,1,1.0
tech kubat holte and,1,1,1.0
holte and matwin machine,1,1,1.0
and matwin machine learning,1,1,1.0
satellite radar images mach,1,1,1.0
radar images mach vol,1,1,1.0
images mach vol nos,1,1,1.0
mach vol nos pp,1,1,1.0
vol nos pp maloof,1,1,1.0
nos pp maloof learning,1,1,1.0
and unknown in proc,1,1,1.0
unknown in proc int,1,1,1.0
ii washington pp provost,1,1,1.0
washington pp provost and,1,1,1.0
provost and fawcett analysis,1,1,1.0
and fawcett analysis and,1,1,1.0
fawcett analysis and visualization,1,1,1.0
analysis and visualization of,1,1,1.0
and visualization of classiﬁer,1,1,1.0
visualization of classiﬁer performance,1,1,1.0
of classiﬁer performance comparison,1,1,1.0
classiﬁer performance comparison under,1,1,1.0
performance comparison under imprecise,1,1,1.0
comparison under imprecise class,1,1,1.0
under imprecise class and,1,1,1.0
imprecise class and cost,1,1,1.0
and cost distributions in,1,1,1.0
cost distributions in proc,1,1,1.0
distributions in proc int,1,1,1.0
proc int conf knowl,1,1,1.0
discovery data mining newport,1,1,1.0
data mining newport beach,1,1,1.0
mining newport beach ca,1,1,1.0
newport beach ca pp,1,1,1.0
beach ca pp hand,1,1,1.0
ca pp hand measuring,1,1,1.0
pp hand measuring classiﬁer,1,1,1.0
hand measuring classiﬁer performance,1,1,1.0
measuring classiﬁer performance a,1,1,1.0
classiﬁer performance a coherent,1,1,1.0
performance a coherent alternative,1,1,1.0
a coherent alternative to,1,1,1.0
coherent alternative to the,1,1,1.0
alternative to the area,1,1,1.0
to the area under,1,1,1.0
the roc curve mach,1,1,1.0
roc curve mach learn,1,1,1.0
curve mach learn vol,1,1,1.0
mach learn vol no,1,1,1.0
learn vol no pp,1,1,1.0
vol no pp corder,1,1,1.0
no pp corder and,1,1,1.0
pp corder and foreman,1,1,1.0
corder and foreman nonparametric,1,1,1.0
and foreman nonparametric statistics,1,1,1.0
foreman nonparametric statistics for,1,1,1.0
nonparametric statistics for a,1,1,1.0
statistics for a approach,1,1,1.0
for a approach new,1,1,1.0
a approach new york,1,1,1.0
approach new york wiley,1,1,1.0
new york wiley demšar,1,1,1.0
york wiley demšar statistical,1,1,1.0
wiley demšar statistical comparisons,1,1,1.0
demšar statistical comparisons of,1,1,1.0
multiple data sets mach,1,1,1.0
data sets mach learn,1,1,1.0
sets mach learn res,1,1,1.0
learn res vol no,1,1,1.0
res vol no pp,1,1,1.0
vol no pp critical,1,1,1.0
no pp critical value,1,1,1.0
pp critical value table,1,1,1.0
critical value table of,1,1,1.0
value table of wilcoxon,1,1,1.0
table of wilcoxon test,1,1,1.0
of wilcoxon test online,1,1,1.0
wilcoxon test online available,1,1,1.0
test online available http,1,1,1.0
online available http opitz,1,1,1.0
available http opitz and,1,1,1.0
http opitz and maclin,1,1,1.0
opitz and maclin popular,1,1,1.0
and maclin popular ensemble,1,1,1.0
maclin popular ensemble methods,1,1,1.0
popular ensemble methods an,1,1,1.0
ensemble methods an empirical,1,1,1.0
methods an empirical study,1,1,1.0
an empirical study artiﬁcial,1,1,1.0
empirical study artiﬁcial intell,1,1,1.0
study artiﬁcial intell res,1,1,1.0
artiﬁcial intell res vol,1,1,1.0
intell res vol pp,1,1,1.0
res vol pp breiman,1,1,1.0
vol pp breiman arcing,1,1,1.0
pp breiman arcing classiﬁers,1,1,1.0
breiman arcing classiﬁers ann,1,1,1.0
arcing classiﬁers ann vol,1,1,1.0
classiﬁers ann vol no,1,1,1.0
ann vol no pp,1,1,1.0
pp v chawla cieslak,1,1,1.0
v chawla cieslak hall,1,1,1.0
chawla cieslak hall and,1,1,1.0
cieslak hall and joshi,1,1,1.0
hall and joshi ically,1,1,1.0
and joshi ically countering,1,1,1.0
joshi ically countering imbalance,1,1,1.0
ically countering imbalance and,1,1,1.0
countering imbalance and its,1,1,1.0
imbalance and its empirical,1,1,1.0
and its empirical relationship,1,1,1.0
its empirical relationship to,1,1,1.0
empirical relationship to cost,1,1,1.0
relationship to cost data,1,1,1.0
to cost data mining,1,1,1.0
cost data mining knowl,1,1,1.0
data mining knowl discovery,1,1,1.0
mining knowl discovery vol,1,1,1.0
knowl discovery vol no,1,1,1.0
discovery vol no pp,1,1,1.0
vol no pp anyfantis,1,1,1.0
no pp anyfantis karagiannopoulos,1,1,1.0
pp anyfantis karagiannopoulos kotsiantis,1,1,1.0
anyfantis karagiannopoulos kotsiantis and,1,1,1.0
karagiannopoulos kotsiantis and pintelas,1,1,1.0
kotsiantis and pintelas robustness,1,1,1.0
and pintelas robustness of,1,1,1.0
pintelas robustness of learning,1,1,1.0
robustness of learning techniques,1,1,1.0
of learning techniques in,1,1,1.0
learning techniques in handling,1,1,1.0
techniques in handling class,1,1,1.0
in handling class noise,1,1,1.0
handling class noise in,1,1,1.0
class noise in imbalanced,1,1,1.0
in imbalanced datasets in,1,1,1.0
imbalanced datasets in proc,1,1,1.0
datasets in proc ifip,1,1,1.0
in proc ifip int,1,1,1.0
proc ifip int federation,1,1,1.0
ifip int federation inform,1,1,1.0
int federation inform vol,1,1,1.0
federation inform vol pp,1,1,1.0
inform vol pp zhu,1,1,1.0
vol pp zhu wu,1,1,1.0
pp zhu wu and,1,1,1.0
zhu wu and y,1,1,1.0
wu and y yang,1,1,1.0
and y yang error,1,1,1.0
y yang error detection,1,1,1.0
yang error detection and,1,1,1.0
error detection and sensitive,1,1,1.0
detection and sensitive instance,1,1,1.0
and sensitive instance ranking,1,1,1.0
sensitive instance ranking in,1,1,1.0
instance ranking in noisy,1,1,1.0
ranking in noisy datasets,1,1,1.0
in noisy datasets in,1,1,1.0
noisy datasets in american,1,1,1.0
datasets in american tion,1,1,1.0
in american tion for,1,1,1.0
american tion for artiﬁcial,1,1,1.0
tion for artiﬁcial intelligence,1,1,1.0
for artiﬁcial intelligence cambridge,1,1,1.0
artiﬁcial intelligence cambridge ma,1,1,1.0
intelligence cambridge ma mit,1,1,1.0
mit press pp sheng,1,1,1.0
press pp sheng chen,1,1,1.0
pp sheng chen s,1,1,1.0
sheng chen s received,1,1,1.0
chen s received the,1,1,1.0
s received the and,1,1,1.0
received the and degrees,1,1,1.0
the and degrees in,1,1,1.0
and degrees in control,1,1,1.0
degrees in control science,1,1,1.0
in control science and,1,1,1.0
control science and engineering,1,1,1.0
science and engineering from,1,1,1.0
and engineering from huazhong,1,1,1.0
engineering from huazhong university,1,1,1.0
from huazhong university of,1,1,1.0
huazhong university of science,1,1,1.0
university of science and,1,1,1.0
of science and technology,2,1,2.0
science and technology wuhan,2,1,2.0
and technology wuhan china,2,1,2.0
technology wuhan china in,2,1,2.0
wuhan china in and,2,1,2.0
china in and respectively,2,1,2.0
in and respectively he,1,1,1.0
and respectively he is,1,1,1.0
respectively he is currently,1,1,1.0
he is currently pursuing,1,1,1.0
is currently pursuing the,1,1,1.0
currently pursuing the degree,1,1,1.0
pursuing the degree in,1,1,1.0
the degree in the,1,1,1.0
degree in the partment,1,1,1.0
in the partment of,1,1,1.0
the partment of electrical,2,1,2.0
partment of electrical and,1,1,1.0
technology hoboken nj his,2,1,2.0
hoboken nj his current,2,1,2.0
nj his current research,2,1,2.0
his current research interests,3,1,3.0
current research interests include,3,1,3.0
research interests include machine,2,1,2.0
interests include machine learning,2,1,2.0
include machine learning data,1,1,1.0
machine learning data mining,2,1,2.0
learning data mining and,1,1,1.0
data mining and computational,1,1,1.0
mining and computational intelligent,1,1,1.0
and computational intelligent systems,1,1,1.0
computational intelligent systems haibo,1,1,1.0
intelligent systems haibo he,1,1,1.0
systems haibo he m,1,1,1.0
haibo he m received,1,1,1.0
he m received the,1,1,1.0
m received the and,1,1,1.0
received the and grees,1,1,1.0
the and grees in,1,1,1.0
and grees in electrical,1,1,1.0
grees in electrical engineering,1,1,1.0
in electrical engineering from,2,1,2.0
electrical engineering from huazhong,1,1,1.0
engineering from huazhong versity,1,1,1.0
from huazhong versity of,1,1,1.0
huazhong versity of science,1,1,1.0
versity of science and,1,1,1.0
in and respectively and,1,1,1.0
and respectively and the,1,1,1.0
respectively and the degree,1,1,1.0
and the degree in,2,1,2.0
the degree in electrical,1,1,1.0
degree in electrical engineering,1,1,1.0
electrical engineering from ohio,1,1,1.0
engineering from ohio university,1,1,1.0
from ohio university athens,1,1,1.0
ohio university athens in,1,1,1.0
university athens in he,1,1,1.0
athens in he is,1,1,1.0
in he is currently,2,1,2.0
he is currently an,1,1,1.0
is currently an assistant,1,1,1.0
currently an assistant professor,1,1,1.0
an assistant professor at,2,1,2.0
assistant professor at the,2,1,2.0
professor at the partment,1,1,1.0
at the partment of,1,1,1.0
partment of electrical computer,1,1,1.0
rhode island kingston from,1,1,1.0
island kingston from to,1,1,1.0
kingston from to he,1,1,1.0
from to he was,1,1,1.0
to he was an,1,1,1.0
he was an assistant,1,1,1.0
was an assistant professor,1,1,1.0
professor at the department,1,1,1.0
at the department of,1,1,1.0
electrical and computer neering,1,1,1.0
and computer neering stevens,1,1,1.0
computer neering stevens institute,1,1,1.0
neering stevens institute of,1,1,1.0
research interests include sys,1,1,1.0
interests include sys tems,1,1,1.0
include sys tems machine,1,1,1.0
sys tems machine learning,1,1,1.0
tems machine learning data,1,1,1.0
learning data mining putational,1,1,1.0
data mining putational intelligence,1,1,1.0
mining putational intelligence and,1,1,1.0
putational intelligence and applications,1,1,1.0
intelligence and applications in,1,1,1.0
and applications in critical,1,1,1.0
applications in critical engineering,1,1,1.0
in critical engineering ﬁelds,1,1,1.0
critical engineering ﬁelds such,1,1,1.0
engineering ﬁelds such as,1,1,1.0
ﬁelds such as smart,1,1,1.0
such as smart grid,1,1,1.0
as smart grid and,1,1,1.0
smart grid and sensor,1,1,1.0
grid and sensor networks,1,1,1.0
and sensor networks very,1,1,1.0
sensor networks very large,1,1,1.0
networks very large scale,1,1,1.0
very large scale integration,1,1,1.0
large scale integration and,1,1,1.0
scale integration and programmable,1,1,1.0
integration and programmable gate,1,1,1.0
and programmable gate array,1,1,1.0
programmable gate array design,1,1,1.0
gate array design he,1,1,1.0
array design he has,1,1,1.0
design he has served,1,1,1.0
he has served regularly,1,1,1.0
has served regularly on,1,1,1.0
served regularly on the,1,1,1.0
regularly on the organizing,1,1,1.0
on the organizing committees,1,1,1.0
the organizing committees of,1,1,1.0
organizing committees of numerous,1,1,1.0
committees of numerous international,1,1,1.0
of numerous international conferences,1,1,1.0
numerous international conferences and,1,1,1.0
international conferences and also,1,1,1.0
conferences and also served,1,1,1.0
and also served as,1,1,1.0
also served as a,1,1,1.0
served as a guest,1,1,1.0
as a guest editor,1,1,1.0
a guest editor for,1,1,1.0
guest editor for several,1,1,1.0
editor for several journals,1,1,1.0
for several journals including,1,1,1.0
several journals including applied,1,1,1.0
journals including applied mathematics,1,1,1.0
including applied mathematics and,1,1,1.0
applied mathematics and computation,1,1,1.0
mathematics and computation soft,1,1,1.0
and computation soft computing,1,1,1.0
computation soft computing and,1,1,1.0
soft computing and journal,1,1,1.0
computing and journal of,1,1,1.0
and journal of experimental,1,1,1.0
journal of experimental theoretical,1,1,1.0
of experimental theoretical artiﬁcial,1,1,1.0
experimental theoretical artiﬁcial intelligence,1,1,1.0
theoretical artiﬁcial intelligence ei,1,1,1.0
artiﬁcial intelligence ei s,1,1,1.0
intelligence ei s currently,1,1,1.0
ei s currently the,1,1,1.0
s currently the editor,1,1,1.0
currently the editor of,1,1,1.0
the editor of the,1,1,1.0
editor of the ieee,1,1,1.0
of the ieee computational,1,1,1.0
the ieee computational intelligence,1,1,1.0
ieee computational intelligence society,1,1,1.0
computational intelligence society electronic,1,1,1.0
intelligence society electronic letter,1,1,1.0
society electronic letter an,1,1,1.0
electronic letter an editorial,1,1,1.0
letter an editorial board,1,1,1.0
an editorial board member,1,1,1.0
editorial board member of,1,1,1.0
board member of cognitive,1,1,1.0
member of cognitive computation,1,1,1.0
of cognitive computation and,1,1,1.0
cognitive computation and an,1,1,1.0
computation and an ciate,1,1,1.0
and an ciate editor,1,1,1.0
an ciate editor of,1,1,1.0
ciate editor of ieee,1,1,1.0
editor of ieee t,1,1,1.0
of ieee t ransactions,1,1,1.0
ieee t ransactions on,1,1,1.0
t ransactions on neural,1,1,1.0
ransactions on neural networks,1,1,1.0
neural networks and ieee,1,1,1.0
networks and ieee transactions,1,1,1.0
and ieee transactions on,1,1,1.0
ieee transactions on smart,1,1,1.0
transactions on smart grid,1,1,1.0
on smart grid edwardo,1,1,1.0
smart grid edwardo garcia,1,1,1.0
grid edwardo garcia received,1,1,1.0
edwardo garcia received the,1,1,1.0
garcia received the degree,1,1,1.0
received the degree in,1,1,1.0
the degree in mathematics,1,1,1.0
degree in mathematics from,1,1,1.0
in mathematics from new,1,1,1.0
mathematics from new york,1,1,1.0
from new york university,1,1,1.0
new york university new,1,1,1.0
york university new york,1,1,1.0
university new york and,1,1,1.0
new york and the,1,1,1.0
york and the degree,1,1,1.0
the degree in computer,1,1,1.0
degree in computer engineering,1,1,1.0
in computer engineering from,1,1,1.0
computer engineering from stevens,1,1,1.0
engineering from stevens institute,1,1,1.0
from stevens institute of,1,1,1.0
technology hoboken nj both,1,1,1.0
hoboken nj both in,1,1,1.0
nj both in he,1,1,1.0
both in he is,1,1,1.0
he is currently with,1,1,1.0
is currently with the,1,1,1.0
currently with the stevens,1,1,1.0
with the stevens institute,1,1,1.0
the stevens institute of,1,1,1.0
institute of technology his,1,1,1.0
of technology his current,1,1,1.0
technology his current research,1,1,1.0
include machine learning biologically,1,1,1.0
machine learning biologically inspired,1,1,1.0
learning biologically inspired intelligence,1,1,1.0
biologically inspired intelligence cognitive,1,1,1.0
inspired intelligence cognitive neuroscience,1,1,1.0
intelligence cognitive neuroscience data,1,1,1.0
cognitive neuroscience data mining,1,1,1.0
neuroscience data mining for,1,1,1.0
data mining for medical,1,1,1.0
mining for medical agnostics,1,1,1.0
for medical agnostics and,1,1,1.0
medical agnostics and mathematical,1,1,1.0
agnostics and mathematical methods,1,1,1.0
and mathematical methods for,1,1,1.0
mathematical methods for magnetic,1,1,1.0
methods for magnetic resonance,1,1,1.0
for magnetic resonance imaging,1,1,1.0
smote these methods erate,1,1,1.0
these methods erate synthetic,1,1,1.0
methods erate synthetic instances,1,1,1.0
erate synthetic instances in,1,1,1.0
method adaptively combines isting,1,1,1.0
adaptively combines isting instances,1,1,1.0
combines isting instances from,1,1,1.0
isting instances from the,1,1,1.0
minority class the experi,1,1,1.0
class the experi ments,1,1,1.0
the experi ments also,1,1,1.0
experi ments also show,1,1,1.0
ments also show that,1,1,1.0
classification tasks when m,1,1,1.0
tasks when m ost,1,1,1.0
when m ost of,1,1,1.0
m ost of data,1,1,1.0
ost of data comes,1,1,1.0
such as medical dia,1,1,1.0
as medical dia gnosis,1,1,1.0
medical dia gnosis fraud,1,1,1.0
dia gnosis fraud detection,1,1,1.0
gnosis fraud detection text,1,1,1.0
solve the class imbal,1,1,1.0
the class imbal ance,1,1,1.0
class imbal ance problem,1,1,1.0
imbal ance problem have,1,1,1.0
ance problem have also,1,1,1.0
by the original d,1,1,1.0
the original d ata,1,1,1.0
original d ata typically,1,1,1.0
d ata typically these,1,1,1.0
ata typically these techniques,1,1,1.0
method for solving ance,1,1,1.0
for solving ance issues,1,1,1.0
solving ance issues in,1,1,1.0
ance issues in traditional,1,1,1.0
to the data augm,1,1,1.0
the data augm entation,1,1,1.0
data augm entation problem,1,1,1.0
augm entation problem in,1,1,1.0
entation problem in deep,1,1,1.0
class imbalance and dat,1,1,1.0
imbalance and dat a,1,1,1.0
and dat a augmentation,1,1,1.0
dat a augmentation problems,1,1,1.0
a augmentation problems using,1,1,1.0
previously developed to sol,1,1,1.0
developed to sol ve,1,1,1.0
to sol ve problems,1,1,1.0
sol ve problems in,1,1,1.0
ve problems in explainable,1,1,1.0
close to existing stances,1,1,1.0
to existing stances thus,1,1,1.0
existing stances thus populating,1,1,1.0
stances thus populating the,1,1,1.0
plausible adaptations of e,1,1,1.0
adaptations of e xisting,1,1,1.0
of e xisting data,1,1,1.0
e xisting data if,1,1,1.0
xisting data if this,1,1,1.0
generated by this counterfac,1,1,1.0
by this counterfac tual,1,1,1.0
this counterfac tual method,1,1,1.0
counterfac tual method should,1,1,1.0
tual method should improve,1,1,1.0
and synthetic minority sampling,1,1,1.0
synthetic minority sampling technique,1,1,1.0
minority sampling technique smote,1,1,1.0
sampling technique smote in,1,1,1.0
they have some backs,1,1,1.0
have some backs since,1,1,1.0
some backs since ros,1,1,1.0
backs since ros merely,1,1,1.0
added to the taset,1,1,1.0
to the taset and,1,1,1.0
the taset and hence,1,1,1.0
taset and hence it,1,1,1.0
since rus randomly re,1,1,1.0
rus randomly re moves,1,1,1.0
randomly re moves examples,1,1,1.0
re moves examples from,1,1,1.0
moves examples from the,1,1,1.0
oversampling from the ity,1,1,1.0
from the ity class,1,1,1.0
the ity class as,1,1,1.0
ity class as smote,1,1,1.0
and data augmentation lems,1,1,1.0
data augmentation lems see,1,1,1.0
augmentation lems see section,1,1,1.0
lems see section temraz,1,1,1.0
minority class is c,1,1,1.0
class is c reated,1,1,1.0
is c reated by,1,1,1.0
c reated by lating,1,1,1.0
reated by lating between,1,1,1.0
by lating between several,1,1,1.0
lating between several minority,1,1,1.0
the problem and creat,1,1,1.0
problem and creat es,1,1,1.0
and creat es new,1,1,1.0
creat es new synthetic,1,1,1.0
es new synthetic instances,1,1,1.0
𝑃 and then determine,1,1,1.0
and then determine s,1,1,1.0
then determine s 𝑚,1,1,1.0
determine s 𝑚 as,1,1,1.0
s 𝑚 as the,1,1,1.0
𝑚 where 𝑚 nally,1,1,1.0
where 𝑚 nally smote,1,1,1.0
𝑚 nally smote creates,1,1,1.0
nally smote creates a,1,1,1.0
𝑝 𝑝 𝑚 𝑝,1,1,1.0
𝑝 𝑚 𝑝 𝛿,1,1,1.0
𝑚 𝑝 𝛿 𝛿,1,1,1.0
𝑝 𝛿 𝛿 where,1,1,1.0
𝛿 𝛿 where 𝛿,1,1,1.0
𝛿 where 𝛿 is,1,1,1.0
or indeed any consi,1,1,1.0
indeed any consi deration,1,1,1.0
any consi deration that,1,1,1.0
consi deration that some,1,1,1.0
deration that some minority,1,1,1.0
that do not exi,1,1,1.0
do not exi st,1,1,1.0
not exi st in,1,1,1.0
exi st in the,1,1,1.0
st in the domain,1,1,1.0
often hinge on fying,1,1,1.0
hinge on fying regions,1,1,1.0
on fying regions in,1,1,1.0
fying regions in the,1,1,1.0
they emphasise the importanc,1,1,1.0
emphasise the importanc e,1,1,1.0
the importanc e of,1,1,1.0
importanc e of focusing,1,1,1.0
e of focusing on,1,1,1.0
classes and sometimes ana,1,1,1.0
and sometimes ana lyze,1,1,1.0
sometimes ana lyze the,1,1,1.0
ana lyze the majority,1,1,1.0
lyze the majority class,1,1,1.0
may be more portant,1,1,1.0
be more portant or,1,1,1.0
more portant or safer,1,1,1.0
portant or safer than,1,1,1.0
smote for instance ns,1,1,1.0
for instance ns smote,1,1,1.0
instance ns smote clusters,1,1,1.0
ns smote clusters minority,1,1,1.0
clusters and then oversample,1,1,1.0
and then oversample s,1,1,1.0
then oversample s from,1,1,1.0
oversample s from clusters,1,1,1.0
s from clusters with,1,1,1.0
with the most stances,1,1,1.0
the most stances the,1,1,1.0
most stances the assumption,1,1,1.0
stances the assumption being,1,1,1.0
representative points within ters,1,1,1.0
points within ters to,1,1,1.0
within ters to guide,1,1,1.0
ters to guide smote,1,1,1.0
datasets into a spac,1,1,1.0
into a spac e,1,1,1.0
a spac e and,1,1,1.0
spac e and uses,1,1,1.0
e and uses a,1,1,1.0
others such as smote,1,1,1.0
such as smote and,1,1,1.0
as smote and adasyn,1,1,1.0
smote and adasyn explore,1,1,1.0
according to their butions,1,1,1.0
to their butions generating,1,1,1.0
their butions generating more,1,1,1.0
butions generating more synthetic,1,1,1.0
close to the c,1,1,1.0
to the c lass,1,1,1.0
the c lass boundary,1,1,1.0
c lass boundary are,1,1,1.0
lass boundary are particularly,1,1,1.0
need to be dled,1,1,1.0
to be dled differently,1,1,1.0
be dled differently owes,1,1,1.0
dled differently owes a,1,1,1.0
important to successful classificati,1,1,1.0
to successful classificati on,1,1,1.0
successful classificati on so,1,1,1.0
classificati on so generating,1,1,1.0
on so generating minority,1,1,1.0
so generating minority stances,1,1,1.0
generating minority stances in,1,1,1.0
minority stances in this,1,1,1.0
stances in this boundary,1,1,1.0
that the minority c,1,1,1.0
the minority c lass,1,1,1.0
minority c lass is,1,1,1.0
c lass is 𝑃,1,1,1.0
lass is 𝑃 and,1,1,1.0
called as 𝑚 𝑚,1,1,1.0
as 𝑚 𝑚 𝑚,1,1,1.0
𝑚 𝑚 𝑚 in,1,1,1.0
𝑚 𝑚 in step,1,1,1.0
are majority instances 𝑚,1,1,1.0
majority instances 𝑚 𝑚,1,1,1.0
instances 𝑚 𝑚 𝑝,1,1,1.0
𝑚 𝑚 𝑝 is,1,1,1.0
𝑚 𝑝 is considered,1,1,1.0
minority ones 𝑚 𝑚,1,1,1.0
ones 𝑚 𝑚 𝑝,1,1,1.0
𝑚 𝑚 𝑝 will,1,1,1.0
will be easily sified,1,1,1.0
be easily sified and,1,1,1.0
easily sified and put,1,1,1.0
sified and put into,1,1,1.0
danger set if 𝑚,1,1,1.0
set if 𝑚 then,1,1,1.0
if 𝑚 then 𝑝,1,1,1.0
𝑚 then 𝑝 is,1,1,1.0
them to generate synthet,1,1,1.0
to generate synthet ic,1,1,1.0
generate synthet ic instances,1,1,1.0
synthet ic instances in,1,1,1.0
ic instances in the,1,1,1.0
approximate the decision ary,1,1,1.0
the decision ary and,1,1,1.0
decision ary and then,1,1,1.0
ary and then generates,1,1,1.0
techniques in a s,1,1,1.0
in a s imilar,1,1,1.0
a s imilar vein,1,1,1.0
s imilar vein smote,1,1,1.0
imilar vein smote divides,1,1,1.0
vein smote divides minority,1,1,1.0
smote divides minority instances,1,1,1.0
preponderance of majority stances,1,1,1.0
of majority stances see,1,1,1.0
majority stances see also,1,1,1.0
stances see also adysyn,1,1,1.0
into account other ods,1,1,1.0
account other ods explore,1,1,1.0
other ods explore the,1,1,1.0
ods explore the relationship,1,1,1.0
minority class for insta,1,1,1.0
class for insta nce,1,1,1.0
for insta nce tomek,1,1,1.0
insta nce tomek finds,1,1,1.0
nce tomek finds pairs,1,1,1.0
tomek finds pairs of,1,1,1.0
minority instance generation smote,1,1,1.0
instance generation smote does,1,1,1.0
generation smote does this,1,1,1.0
smote does this by,1,1,1.0
of its neighbours smote,1,1,1.0
its neighbours smote s,1,1,1.0
neighbours smote s finer,1,1,1.0
smote s finer analysis,1,1,1.0
been shown to prove,1,1,1.0
shown to prove performance,1,1,1.0
to prove performance over,1,1,1.0
prove performance over sampling,1,1,1.0
swim adopt a differe,1,1,1.0
adopt a differe nt,1,1,1.0
a differe nt approach,1,1,1.0
differe nt approach leveraging,1,1,1.0
nt approach leveraging information,1,1,1.0
using the mahalanobis distance,1,1,1.0
the mahalanobis distance s,1,1,1.0
mahalanobis distance s requiring,1,1,1.0
distance s requiring generated,1,1,1.0
s requiring generated minority,1,1,1.0
instances see and smote,1,1,1.0
see and smote for,1,1,1.0
and smote for related,1,1,1.0
smote for related proaches,1,1,1.0
for related proaches finally,1,1,1.0
related proaches finally is,1,1,1.0
proaches finally is another,1,1,1.0
similarities to majority stances,1,1,1.0
to majority stances into,1,1,1.0
majority stances into account,1,1,1.0
stances into account in,1,1,1.0
minority class after sm,1,1,1.0
class after sm ote,1,1,1.0
after sm ote has,1,1,1.0
sm ote has been,1,1,1.0
ote has been applied,1,1,1.0
a step to move,1,1,1.0
step to move generated,1,1,1.0
to move generated instances,1,1,1.0
move generated instances that,1,1,1.0
the classic counterfactual planation,1,1,1.0
classic counterfactual planation is,1,1,1.0
counterfactual planation is one,1,1,1.0
planation is one that,1,1,1.0
that if you quested,1,1,1.0
if you quested a,1,1,1.0
you quested a loan,1,1,1.0
quested a loan for,1,1,1.0
conditions under which out,1,1,1.0
under which out come,1,1,1.0
which out come would,1,1,1.0
out come would change,1,1,1.0
come would change the,1,1,1.0
ai under diverse nam,1,1,1.0
under diverse nam es,1,1,1.0
diverse nam es for,1,1,1.0
nam es for stance,1,1,1.0
es for stance in,1,1,1.0
for stance in the,1,1,1.0
stance in the past,1,1,1.0
topic in xai bec,1,1,1.0
in xai bec ause,1,1,1.0
xai bec ause they,1,1,1.0
bec ause they appear,1,1,1.0
ause they appear to,1,1,1.0
one encoding the inal,1,1,1.0
encoding the inal loan,1,1,1.0
the inal loan refusal,1,1,1.0
inal loan refusal these,1,1,1.0
search a sometimes randoml,1,1,1.0
a sometimes randoml y,1,1,1.0
sometimes randoml y generated,1,1,1.0
randoml y generated space,1,1,1.0
y generated space of,1,1,1.0
loss function that bala,1,1,1.0
function that bala nces,1,1,1.0
that bala nces proximity,1,1,1.0
bala nces proximity to,1,1,1.0
nces proximity to the,1,1,1.0
the best counterfactual instanc,1,1,1.0
best counterfactual instanc e,1,1,1.0
counterfactual instanc e for,1,1,1.0
instanc e for a,1,1,1.0
e for a query,1,1,1.0
a set of verse,1,1,1.0
set of verse counterfactual,1,1,1.0
of verse counterfactual candidates,1,1,1.0
verse counterfactual candidates avoiding,1,1,1.0
with these optimization m,1,1,1.0
these optimization m ethods,1,1,1.0
optimization m ethods is,1,1,1.0
m ethods is that,1,1,1.0
ethods is that given,1,1,1.0
use in the ance,1,1,1.0
in the ance problem,1,1,1.0
the ance problem as,1,1,1.0
ance problem as it,1,1,1.0
the minority class w,1,1,1.0
minority class w ith,1,1,1.0
class w ith noise,1,1,1.0
w ith noise with,1,1,1.0
ith noise with quential,1,1,1.0
noise with quential negative,1,1,1.0
with quential negative effects,1,1,1.0
quential negative effects on,1,1,1.0
captures a counterfactual tion,1,1,1.0
a counterfactual tion between,1,1,1.0
counterfactual tion between existing,1,1,1.0
tion between existing instances,1,1,1.0
at most two fea,1,1,1.0
most two fea for,1,1,1.0
two fea for example,1,1,1.0
fea for example the,1,1,1.0
namely a old male,1,1,1.0
a old male accountant,1,1,1.0
old male accountant earning,1,1,1.0
male accountant earning who,1,1,1.0
then the loan sion,1,1,1.0
the loan sion is,1,1,1.0
loan sion is likely,1,1,1.0
sion is likely to,1,1,1.0
that arise in zation,1,1,1.0
arise in zation techniques,1,1,1.0
in zation techniques see,1,1,1.0
zation techniques see from,1,1,1.0
role in data tion,1,1,1.0
in data tion to,1,1,1.0
data tion to solve,1,1,1.0
tion to solve problems,1,1,1.0
that generated synthetic counterfact,1,1,1.0
generated synthetic counterfact ual,1,1,1.0
synthetic counterfact ual cases,1,1,1.0
counterfact ual cases could,1,1,1.0
ual cases could prove,1,1,1.0
cases could prove the,1,1,1.0
could prove the predictive,1,1,1.0
prove the predictive accuracy,1,1,1.0
of papers on counte,1,1,1.0
papers on counte factuals,1,1,1.0
on counte factuals in,1,1,1.0
counte factuals in xai,1,1,1.0
factuals in xai only,1,1,1.0
dataset calling it ity,1,1,1.0
calling it ity that,1,1,1.0
it ity that is,1,1,1.0
ity that is if,1,1,1.0
the original dataset howe,1,1,1.0
original dataset howe ver,1,1,1.0
dataset howe ver mothilal,1,1,1.0
howe ver mothilal et,1,1,1.0
ver mothilal et al,1,1,1.0
dataset based on generat,1,1,1.0
based on generat ed,1,1,1.0
on generat ed factuals,1,1,1.0
generat ed factuals could,1,1,1.0
ed factuals could act,1,1,1.0
factuals could act as,1,1,1.0
a model was t,1,1,1.0
model was t rained,1,1,1.0
was t rained and,1,1,1.0
t rained and tested,1,1,1.0
rained and tested they,1,1,1.0
counterfactual examples for tual,1,1,1.0
examples for tual data,1,1,1.0
for tual data and,1,1,1.0
tual data and found,1,1,1.0
pitis et al propos,1,1,1.0
et al propos ed,1,1,1.0
al propos ed counterfactual,1,1,1.0
propos ed counterfactual data,1,1,1.0
ed counterfactual data augmentation,1,1,1.0
examples by stitching gether,1,1,1.0
by stitching gether subsamples,1,1,1.0
stitching gether subsamples from,1,1,1.0
gether subsamples from the,1,1,1.0
use bespoke counterfactual ods,1,1,1.0
bespoke counterfactual ods developed,1,1,1.0
counterfactual ods developed for,1,1,1.0
ods developed for specific,1,1,1.0
tried and tested technique,1,1,1.0
and tested technique s,1,1,1.0
tested technique s from,1,1,1.0
technique s from the,1,1,1.0
s from the xai,1,1,1.0
their problem domain invol,1,1,1.0
problem domain invol ved,1,1,1.0
domain invol ved a,1,1,1.0
invol ved a model,1,1,1.0
ved a model for,1,1,1.0
significantly from the narios,1,1,1.0
from the narios recorded,1,1,1.0
the narios recorded in,1,1,1.0
narios recorded in the,1,1,1.0
for key weather variabl,1,1,1.0
key weather variabl es,1,1,1.0
weather variabl es like,1,1,1.0
variabl es like solar,1,1,1.0
es like solar diation,1,1,1.0
like solar diation or,1,1,1.0
solar diation or soil,1,1,1.0
diation or soil moisture,1,1,1.0
will burn grass cordingly,1,1,1.0
burn grass cordingly the,1,1,1.0
grass cordingly the model,1,1,1.0
cordingly the model does,1,1,1.0
growth for these disrupted,1,1,1.0
for these disrupted months,1,1,1.0
these disrupted months of,1,1,1.0
disrupted months of because,1,1,1.0
al defined a based,1,1,1.0
defined a based class,1,1,1.0
a based class boundary,1,1,1.0
based class boundary in,1,1,1.0
means from a cation,1,1,1.0
from a cation perspective,1,1,1.0
a cation perspective these,1,1,1.0
cation perspective these normal,1,1,1.0
used the counterfactual m,1,1,1.0
the counterfactual m ethod,1,1,1.0
counterfactual m ethod to,1,1,1.0
m ethod to create,1,1,1.0
ethod to create new,1,1,1.0
in using these mi,1,1,1.0
using these mi nority,1,1,1.0
these mi nority interestingly,1,1,1.0
mi nority interestingly temraz,1,1,1.0
nority interestingly temraz et,1,1,1.0
that that the ods,1,1,1.0
that the ods did,1,1,1.0
the ods did better,1,1,1.0
ods did better than,1,1,1.0
problem domain specifically t,1,1,1.0
domain specifically t he,1,1,1.0
specifically t he dice,1,1,1.0
t he dice method,1,1,1.0
he dice method however,1,1,1.0
generalizes to other probl,1,1,1.0
to other probl em,1,1,1.0
other probl em domains,1,1,1.0
probl em domains classifiers,1,1,1.0
em domains classifiers and,1,1,1.0
a solution to ulating,1,1,1.0
solution to ulating the,1,1,1.0
to ulating the minority,1,1,1.0
ulating the minority class,1,1,1.0
solve problems the cation,1,1,1.0
problems the cation of,1,1,1.0
the cation of this,1,1,1.0
cation of this xai,1,1,1.0
motivated by the observati,1,1,1.0
by the observati on,1,1,1.0
the observati on that,1,1,1.0
observati on that it,1,1,1.0
on that it seemed,1,1,1.0
purposes furthermore the eva,1,1,1.0
furthermore the eva luation,1,1,1.0
the eva luation metrics,1,1,1.0
eva luation metrics in,1,1,1.0
luation metrics in xai,1,1,1.0
method using a d,1,1,1.0
using a d reasoning,1,1,1.0
a d reasoning proach,1,1,1.0
d reasoning proach to,1,1,1.0
reasoning proach to generating,1,1,1.0
proach to generating synthetic,1,1,1.0
applied to binary fication,1,1,1.0
to binary fication problems,1,1,1.0
binary fication problems consider,1,1,1.0
fication problems consider a,1,1,1.0
breed age and history,1,1,1.0
age and history that,1,1,1.0
and history that is,1,1,1.0
history that is classed,1,1,1.0
with a different ory,1,1,1.0
a different ory they,1,1,1.0
different ory they have,1,1,1.0
ory they have ill,1,1,1.0
in this dataset usi,1,1,1.0
this dataset usi ng,1,1,1.0
dataset usi ng our,1,1,1.0
usi ng our counterfactual,1,1,1.0
ng our counterfactual method,1,1,1.0
minority instance by thi,1,1,1.0
instance by thi s,1,1,1.0
by thi s known,1,1,1.0
thi s known counterfactual,1,1,1.0
s known counterfactual relationship,1,1,1.0
new synthetic minority instanc,1,1,1.0
synthetic minority instanc e,1,1,1.0
minority instanc e using,1,1,1.0
instanc e using the,1,1,1.0
e using the features,1,1,1.0
using the features of,1,1,1.0
the features of and,1,1,1.0
features of and the,1,1,1.0
and and the ature,1,1,1.0
and the ature from,1,1,1.0
the ature from for,1,1,1.0
ature from for along,1,1,1.0
instance that is counterfac,1,1,1.0
that is counterfac tually,1,1,1.0
is counterfac tually related,1,1,1.0
counterfac tually related to,1,1,1.0
tually related to where,1,1,1.0
we do this i,1,1,1.0
do this i teratively,1,1,1.0
this i teratively for,1,1,1.0
i teratively for all,1,1,1.0
teratively for all those,1,1,1.0
counterfactual augmentation cfa a,1,1,1.0
augmentation cfa a n,1,1,1.0
cfa a n unpaired,1,1,1.0
a n unpaired instance,1,1,1.0
n unpaired instance 𝒙,1,1,1.0
of the 𝒑 low,1,1,1.0
the 𝒑 low box,1,1,1.0
𝒑 low box to,1,1,1.0
low box to generate,1,1,1.0
new synthetic counterfactual 𝒑,1,1,1.0
synthetic counterfactual 𝒑 green,1,1,1.0
counterfactual 𝒑 green box,1,1,1.0
minority class 𝑐𝑙𝑎𝑠𝑠e 𝑋,1,1,1.0
class 𝑐𝑙𝑎𝑠𝑠e 𝑋 𝑥,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑋 𝑥 𝑥,1,1,1.0
𝑋 𝑥 𝑥 where,1,1,1.0
in 𝑐𝑙𝑎𝑠𝑠efg 𝑥 𝑥,1,1,1.0
𝑐𝑙𝑎𝑠𝑠efg 𝑥 𝑥 𝑥j,1,1,1.0
𝑥 𝑥 𝑥j 𝑥k,1,1,1.0
𝑥 where 𝑥 𝑐𝑓,1,1,1.0
where 𝑥 𝑐𝑓 𝑥,2,1,2.0
𝑥 𝑐𝑓 𝑥 𝑝,2,1,2.0
𝑐𝑓 𝑥 𝑝 𝑥,1,1,1.0
𝑐𝑙𝑎𝑠𝑠efg where 𝑥 𝑐𝑓,1,1,1.0
𝑐𝑓 𝑥 𝑝 𝑃,1,1,1.0
𝑥 𝑝 𝑃 𝑝,1,1,1.0
𝑝 𝑃 𝑝 𝑝,1,1,1.0
𝑃 𝑝 𝑝 where,1,1,1.0
instance in 𝑐𝑙𝑎𝑠𝑠e 𝑝,1,1,1.0
in 𝑐𝑙𝑎𝑠𝑠e 𝑝 𝑝,1,1,1.0
𝑐𝑙𝑎𝑠𝑠e 𝑝 𝑝 𝑝j,1,1,1.0
𝑝 𝑝 𝑝j 𝑝k,1,1,1.0
𝑝 𝑝j 𝑝k 𝑝l,2,1,2.0
𝑝l 𝑝 where 𝑝,1,1,1.0
𝑝 where 𝑝 𝑐𝑓,1,1,1.0
where 𝑝 𝑐𝑓 𝑥,1,1,1.0
𝑝 𝑐𝑓 𝑥 𝑝,1,1,1.0
𝑐𝑓 𝑥 𝑝 𝑝,1,1,1.0
𝑝 𝑡𝑎𝑟𝑔𝑒𝑡 𝑥 𝑡𝑎𝑟𝑔𝑒𝑡,1,1,1.0
𝑡𝑎𝑟𝑔𝑒𝑡 𝑥 𝑡𝑎𝑟𝑔𝑒𝑡 𝑝,1,1,1.0
𝑥 𝑡𝑎𝑟𝑔𝑒𝑡 𝑝 neighbors,1,1,1.0
𝑡𝑎𝑟𝑔𝑒𝑡 𝑝 neighbors assume,1,1,1.0
minority class 𝑃 𝑥,1,1,1.0
class 𝑃 𝑥 𝑥j,1,1,1.0
𝑃 𝑥 𝑥j 𝑥k,1,1,1.0
𝑥k 𝑥l 𝑥 𝑝,1,1,1.0
𝑥l 𝑥 𝑝 𝑝j,1,1,1.0
𝑥 𝑝 𝑝j 𝑝k,1,1,1.0
exist in a taset,1,1,1.0
in a taset 𝑇,1,1,1.0
a taset 𝑇 these,1,1,1.0
taset 𝑇 these native,1,1,1.0
are called native cause,1,1,1.0
called native cause in,1,1,1.0
native cause in one,1,1,1.0
cause in one sense,1,1,1.0
𝒑 for each paired,1,1,1.0
for each paired instance,1,1,1.0
each paired instance 𝑥,1,1,1.0
paired instance 𝑥 cfa,1,1,1.0
𝑥 a paired stance,1,1,1.0
a paired stance involved,1,1,1.0
paired stance involved in,1,1,1.0
stance involved in a,1,1,1.0
counterfactual pairs notably t,1,1,1.0
pairs notably t his,1,1,1.0
notably t his means,1,1,1.0
t his means that,1,1,1.0
his means that all,1,1,1.0
euclidean distance ed de,1,1,1.0
distance ed de 𝑝,1,1,1.0
ed de 𝑝 𝑞,1,1,1.0
de 𝑝 𝑞 k,1,1,1.0
𝑝 𝑞 k e,1,1,1.0
𝑞 k e temraz,1,1,1.0
k e temraz keane,1,1,1.0
e temraz keane counterfactual,1,1,1.0
having identified a didate,1,1,1.0
identified a didate native,1,1,1.0
a didate native counterfactual,1,1,1.0
didate native counterfactual 𝑐𝑓,1,1,1.0
generates a synthetic tual,1,1,1.0
a synthetic tual instance,1,1,1.0
synthetic tual instance in,1,1,1.0
tual instance in the,1,1,1.0
diminish in finding matc,1,1,1.0
in finding matc and,1,1,1.0
finding matc and between,1,1,1.0
matc and between two,1,1,1.0
native counterfactual cfa c,1,1,1.0
counterfactual cfa c omputes,1,1,1.0
cfa c omputes a,1,1,1.0
c omputes a ance,1,1,1.0
omputes a ance by,1,1,1.0
a ance by finding,1,1,1.0
ance by finding the,1,1,1.0
deviation from the mea,1,1,1.0
from the mea n,1,1,1.0
the mea n all,1,1,1.0
mea n all the,1,1,1.0
n all the values,1,1,1.0
version of the al,1,1,1.0
of the al gorithm,1,1,1.0
the al gorithm from,1,1,1.0
al gorithm from its,1,1,1.0
gorithm from its xai,1,1,1.0
of a good factual,1,1,1.0
a good factual pairing,1,1,1.0
good factual pairing the,1,1,1.0
factual pairing the do,1,1,1.0
grounds keane and sm,1,1,1.0
keane and sm yth,1,1,1.0
and sm yth defined,1,1,1.0
sm yth defined a,1,1,1.0
yth defined a good,1,1,1.0
features are more temraz,1,1,1.0
are more temraz keane,1,1,1.0
more temraz keane counterfactual,1,1,1.0
hence generated synthetic us,1,1,1.0
generated synthetic us ing,1,1,1.0
synthetic us ing these,1,1,1.0
us ing these sparse,1,1,1.0
ing these sparse pairs,1,1,1.0
is a critical ence,1,1,1.0
a critical ence between,1,1,1.0
critical ence between the,1,1,1.0
ence between the xai,1,1,1.0
part in native counterfactual,1,1,1.0
in native counterfactual s,1,1,1.0
native counterfactual s this,1,1,1.0
counterfactual s this is,1,1,1.0
s this is why,1,1,1.0
insights from the class,1,1,1.0
from the class literature,1,1,1.0
the class literature first,1,1,1.0
class literature first by,1,1,1.0
relies on native terfactuals,1,1,1.0
on native terfactuals in,1,1,1.0
native terfactuals in the,2,1,2.0
terfactuals in the dataset,2,1,2.0
majority and minority i,1,1,1.0
and minority i nstances,1,1,1.0
minority i nstances and,1,1,1.0
i nstances and as,1,1,1.0
nstances and as such,1,1,1.0
work of those invol,1,1,1.0
of those invol ved,1,1,1.0
those invol ved in,1,1,1.0
invol ved in known,1,1,1.0
ved in known counterfactuals,1,1,1.0
however this counterfactual me,1,1,1.0
this counterfactual me thod,1,1,1.0
counterfactual me thod is,1,1,1.0
me thod is quite,1,1,1.0
thod is quite keane,1,1,1.0
not use interpolation be,1,1,1.0
use interpolation be tween,1,1,1.0
interpolation be tween nority,1,1,1.0
be tween nority instances,1,1,1.0
tween nority instances but,1,1,1.0
nority instances but rather,1,1,1.0
literature using six oversam,1,1,1.0
using six oversam pling,1,1,1.0
six oversam pling methods,1,1,1.0
oversam pling methods smote,1,1,1.0
pling methods smote adasyn,1,1,1.0
methods smote adasyn smote,1,1,1.0
smote adasyn smote rsb,1,1,1.0
adasyn smote rsb these,1,1,1.0
smote rsb these specific,1,1,1.0
rsb these specific methods,1,1,1.0
and their as tions,1,1,1.0
their as tions the,1,1,1.0
as tions the six,1,1,1.0
tions the six techniques,1,1,1.0
on a representative selecti,1,1,1.0
a representative selecti on,1,1,1.0
representative selecti on of,1,1,1.0
selecti on of keel,1,1,1.0
on of keel datasets,1,1,1.0
were used because ferent,1,1,1.0
used because ferent models,1,1,1.0
because ferent models find,1,1,1.0
ferent models find different,1,1,1.0
to assess the mance,1,1,1.0
assess the mance of,1,1,1.0
the mance of the,1,1,1.0
mance of the four,1,1,1.0
some of these dataset,1,1,1.0
of these dataset s,1,1,1.0
these dataset s are,1,1,1.0
dataset s are they,1,1,1.0
s are they were,1,1,1.0
ovo or and rest,1,1,1.0
or and rest ovr,1,1,1.0
and rest ovr or,1,1,1.0
rest ovr or methods,1,1,1.0
and predicts it a,1,1,1.0
predicts it a gainst,1,1,1.0
it a gainst all,1,1,1.0
a gainst all other,1,1,1.0
gainst all other classes,1,1,1.0
datasets were modified usi,1,1,1.0
were modified usi ng,1,1,1.0
modified usi ng both,1,1,1.0
usi ng both methods,1,1,1.0
ng both methods one,1,1,1.0
on the chemical ysis,1,1,1.0
the chemical ysis consisting,1,1,1.0
chemical ysis consisting of,1,1,1.0
ysis consisting of classes,1,1,1.0
a dataset with classe,1,1,1.0
dataset with classe the,1,1,1.0
with classe the problem,1,1,1.0
classe the problem is,1,1,1.0
set of features tracted,1,1,1.0
of features tracted from,1,1,1.0
features tracted from the,1,1,1.0
tracted from the silhouette,1,1,1.0
subsets where each subs,1,1,1.0
where each subs et,1,1,1.0
each subs et included,1,1,1.0
subs et included mately,1,1,1.0
et included mately equal,1,1,1.0
included mately equal size,1,1,1.0
mately equal size of,1,1,1.0
we split each data,1,1,1.0
split each data set,1,1,1.0
each data set into,1,1,1.0
data set into training,1,1,1.0
set into training and,1,1,1.0
cfa the native terfactuals,1,1,1.0
the native terfactuals in,1,1,1.0
terfactuals in the training,1,1,1.0
testing these generated dataset,1,1,1.0
these generated dataset s,1,1,1.0
generated dataset s from,1,1,1.0
dataset s from cfa,1,1,1.0
s from cfa were,1,1,1.0
augmentation and the datase,1,1,1.0
and the datase ts,1,1,1.0
the datase ts generated,1,1,1.0
datase ts generated by,1,1,1.0
ts generated by temraz,1,1,1.0
and for our ments,1,1,1.0
for our ments we,1,1,1.0
our ments we oversample,1,1,1.0
ments we oversample the,1,1,1.0
of the data augmentati,1,1,1.0
the data augmentati on,1,1,1.0
data augmentati on methods,1,1,1.0
augmentati on methods until,1,1,1.0
on methods until we,1,1,1.0
imbalanced dataset was als,1,1,1.0
dataset was als o,1,1,1.0
was als o run,1,1,1.0
als o run as,1,1,1.0
o run as a,1,1,1.0
parameters for each tion,1,1,1.0
for each tion method,1,1,1.0
each tion method rf,1,1,1.0
tion method rf lr,1,1,1.0
random forest rf 𝑛𝑡𝑟𝑒𝑒,1,1,1.0
forest rf 𝑛𝑡𝑟𝑒𝑒 𝑚𝑎𝑥r,1,1,1.0
rf 𝑛𝑡𝑟𝑒𝑒 𝑚𝑎𝑥r stu,1,1,1.0
𝑛𝑡𝑟𝑒𝑒 𝑚𝑎𝑥r stu neighbors,1,1,1.0
𝑚𝑎𝑥r stu neighbors 𝑛,1,1,1.0
stu neighbors 𝑛 xuyz,1,1,1.0
lr 𝑚𝑎𝑥 t 𝐶,1,1,1.0
𝑚𝑎𝑥 t 𝐶 𝑠𝑜𝑙𝑣𝑒𝑟,1,1,1.0
t 𝐶 𝑠𝑜𝑙𝑣𝑒𝑟 multilayer,1,1,1.0
𝐶 𝑠𝑜𝑙𝑣𝑒𝑟 multilayer perceptron,1,1,1.0
𝑠𝑜𝑙𝑣𝑒𝑟 multilayer perceptron mlp,1,1,1.0
multilayer perceptron mlp 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛,1,1,1.0
perceptron mlp 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 𝑎𝑙𝑝ℎ𝑎,1,1,1.0
mlp 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 𝑎𝑙𝑝ℎ𝑎 𝑠𝑜𝑙𝑣𝑒𝑟,1,1,1.0
𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 𝑎𝑙𝑝ℎ𝑎 𝑠𝑜𝑙𝑣𝑒𝑟 oversampling,1,1,1.0
𝑎𝑙𝑝ℎ𝑎 𝑠𝑜𝑙𝑣𝑒𝑟 oversampling methods,1,1,1.0
𝑠𝑜𝑙𝑣𝑒𝑟 oversampling methods smote,1,1,1.0
either positive or negati,1,1,1.0
positive or negati ve,1,1,1.0
or negati ve so,1,1,1.0
negati ve so the,1,1,1.0
ve so the prediction,1,1,1.0
of a given fication,1,1,1.0
a given fication a,1,1,1.0
given fication a true,1,1,1.0
fication a true positive,1,1,1.0
false negative fn curacy,1,1,1.0
negative fn curacy was,1,1,1.0
fn curacy was not,1,1,1.0
curacy was not used,1,1,1.0
discussed earlier it ca,1,1,1.0
earlier it ca n,1,1,1.0
it ca n be,1,1,1.0
ca n be spuriously,1,1,1.0
n be spuriously high,1,1,1.0
spuriously high for i,1,1,1.0
high for i balanced,1,1,1.0
for i balanced datasets,1,1,1.0
i balanced datasets it,1,1,1.0
balanced datasets it should,1,1,1.0
used in our experime,1,1,1.0
in our experime nts,1,1,1.0
our experime nts were,1,1,1.0
experime nts were converted,1,1,1.0
nts were converted to,1,1,1.0
defined as follows 𝑅𝑒𝑐𝑎𝑙𝑙,1,1,1.0
as follows 𝑅𝑒𝑐𝑎𝑙𝑙 𝑇𝑃,1,1,1.0
follows 𝑅𝑒𝑐𝑎𝑙𝑙 𝑇𝑃 𝑇𝑃,1,1,1.0
𝑅𝑒𝑐𝑎𝑙𝑙 𝑇𝑃 𝑇𝑃 𝐹𝑁,1,1,1.0
𝑇𝑃 𝑇𝑃 𝐹𝑁 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝑇𝑃 𝐹𝑁 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑃,1,1,1.0
𝐹𝑁 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑃 𝑇𝑃,1,1,1.0
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑃 𝑇𝑃 𝐹𝑃,1,1,1.0
𝑇𝑃 𝑇𝑃 𝐹𝑃 𝐹j,1,1,1.0
𝑇𝑃 𝐹𝑃 𝐹j 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝐹𝑃 𝐹j 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙,1,1,1.0
𝐹j 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛,1,1,1.0
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙,1,1,1.0
𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝐴𝑈𝐶,1,1,1.0
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑟𝑒𝑐𝑎𝑙𝑙 𝐴𝑈𝐶 𝑇𝑃,1,1,1.0
𝑟𝑒𝑐𝑎𝑙𝑙 𝐴𝑈𝐶 𝑇𝑃 𝑇𝑁,1,1,1.0
𝐴𝑈𝐶 𝑇𝑃 𝑇𝑁 table,1,1,1.0
𝑇𝑃 𝑇𝑁 table confusion,1,1,1.0
𝑇𝑁 table confusion matrix,1,1,1.0
of the classifiers test,1,1,1.0
the classifiers test ed,1,1,1.0
classifiers test ed see,1,1,1.0
test ed see bles,1,1,1.0
ed see bles recall,1,1,1.0
see bles recall the,1,1,1.0
bles recall the datasets,1,1,1.0
augmentation smote adasyn mote,1,1,1.0
smote adasyn mote smote,1,1,1.0
adasyn mote smote and,1,1,1.0
mote smote and cfa,1,1,1.0
smote and cfa tables,1,1,1.0
main metric for e,1,1,1.0
metric for e ach,1,1,1.0
for e ach sifier,1,1,1.0
e ach sifier on,1,1,1.0
ach sifier on the,1,1,1.0
sifier on the datasets,1,1,1.0
out of with smote,1,1,1.0
of with smote being,1,1,1.0
with smote being the,1,1,1.0
smote being the next,1,1,1.0
achieved a greater provement,1,1,1.0
a greater provement in,1,1,1.0
greater provement in the,1,1,1.0
provement in the results,1,1,1.0
cfa does better tha,1,1,1.0
does better tha n,1,1,1.0
better tha n all,1,1,1.0
tha n all the,1,1,1.0
n all the other,1,1,1.0
adasyn being the ne,1,1,1.0
being the ne xt,1,1,1.0
the ne xt best,1,1,1.0
ne xt best with,1,1,1.0
xt best with datasets,1,1,1.0
had the highest au,1,1,1.0
the highest au in,1,1,1.0
highest au in dataset,1,1,1.0
au in dataset for,1,1,1.0
datasets with the rsb,1,1,1.0
with the rsb being,1,1,1.0
the rsb being the,1,1,1.0
rsb being the next,1,1,1.0
only datasets with e,1,1,1.0
datasets with e being,1,1,1.0
with e being the,1,1,1.0
e being the next,1,1,1.0
adasyn had the highes,1,1,1.0
had the highes t,1,1,1.0
the highes t in,1,1,1.0
highes t in temraz,1,1,1.0
t in temraz keane,1,1,1.0
make a contribution i,1,1,1.0
a contribution i t,1,1,1.0
contribution i t seems,1,1,1.0
i t seems to,1,1,1.0
t seems to be,1,1,1.0
dataset baseline smote smote,4,1,4.0
baseline smote smote adasyn,4,1,4.0
smote smote adasyn smote,4,1,4.0
method when the sion,1,1,1.0
when the sion score,1,1,1.0
the sion score is,1,1,1.0
sion score is highest,1,1,1.0
best see table i,1,1,1.0
see table i n,1,1,1.0
table i n of,1,1,1.0
i n of cases,1,1,1.0
n of cases it,1,1,1.0
to for for line,1,1,1.0
for for line for,1,1,1.0
for line for for,1,1,1.0
line for for adasyn,1,1,1.0
adasyn for smote smote,1,1,1.0
for smote smote and,1,1,1.0
smote smote and it,1,1,1.0
of cases it ha,1,1,1.0
cases it ha s,1,1,1.0
it ha s the,1,1,1.0
ha s the highest,1,1,1.0
s the highest recall,1,1,1.0
smote smote cfa rf,2,1,2.0
data augmentation methods smot,1,1,1.0
augmentation methods smot e,1,1,1.0
methods smot e smote,1,1,1.0
smot e smote adasyn,1,1,1.0
e smote adasyn and,1,1,1.0
and and these rithms,1,1,1.0
and these rithms achieved,1,1,1.0
these rithms achieved lower,1,1,1.0
rithms achieved lower values,1,1,1.0
perhaps the most i,1,1,1.0
the most i nteresting,1,1,1.0
most i nteresting result,1,1,1.0
i nteresting result is,1,1,1.0
nteresting result is that,1,1,1.0
with no data tion,1,1,1.0
no data tion and,1,1,1.0
data tion and smote,1,1,1.0
tion and smote variants,1,1,1.0
achieved the best mance,1,1,1.0
the best mance in,1,1,1.0
best mance in out,1,1,1.0
mance in out of,1,1,1.0
with different data s,1,1,1.0
different data s ets,1,1,1.0
data s ets finally,1,1,1.0
s ets finally in,1,1,1.0
ets finally in the,1,1,1.0
between sensitivity and s,1,1,1.0
sensitivity and s pecificity,1,1,1.0
and s pecificity are,1,1,1.0
s pecificity are presented,1,1,1.0
pecificity are presented in,1,1,1.0
curves where cfa formed,1,1,1.0
where cfa formed methods,1,1,1.0
cfa formed methods obtained,1,1,1.0
formed methods obtained for,1,1,1.0
four classifiers on ent,1,1,1.0
classifiers on ent data,1,1,1.0
on ent data sets,1,1,1.0
ent data sets according,1,1,1.0
outperformed other data augmenta,1,1,1.0
other data augmenta tion,1,1,1.0
data augmenta tion methods,1,1,1.0
augmenta tion methods smote,1,1,1.0
tion methods smote adasyn,1,1,1.0
curve figure these re,1,1,1.0
figure these re sults,1,1,1.0
these re sults support,1,1,1.0
re sults support our,1,1,1.0
sults support our previous,1,1,1.0
the other hand fi,1,1,1.0
other hand fi gure,1,1,1.0
hand fi gure shows,1,1,1.0
fi gure shows several,1,1,1.0
gure shows several examples,1,1,1.0
cfa outperformed methods tained,1,1,1.0
outperformed methods tained for,1,1,1.0
methods tained for the,1,1,1.0
tained for the six,1,1,1.0
synthetic datapoints for planatory,1,1,1.0
datapoints for planatory purposes,1,1,1.0
for planatory purposes indeed,1,1,1.0
planatory purposes indeed the,1,1,1.0
that these explanatory tuals,1,1,1.0
these explanatory tuals are,1,1,1.0
explanatory tuals are generally,1,1,1.0
tuals are generally valid,1,1,1.0
to the data tion,1,1,1.0
the data tion as,1,1,1.0
data tion as we,1,1,1.0
tion as we saw,1,1,1.0
a prediction problem s,1,1,1.0
prediction problem s howed,1,1,1.0
problem s howed that,1,1,1.0
s howed that generated,1,1,1.0
howed that generated counterfactuals,1,1,1.0
in the minority cla,1,1,1.0
the minority cla ss,1,1,1.0
minority cla ss improved,1,1,1.0
cla ss improved performance,1,1,1.0
ss improved performance specifically,1,1,1.0
well because it ates,1,1,1.0
because it ates minority,1,1,1.0
it ates minority instances,1,1,1.0
ates minority instances that,1,1,1.0
offsets from known minority,1,1,1.0
from known minority but,1,1,1.0
known minority but this,1,1,1.0
minority but this account,1,1,1.0
are solved by ing,1,1,1.0
solved by ing similar,1,1,1.0
by ing similar cases,1,1,1.0
ing similar cases and,1,1,1.0
presented with a apa,1,1,1.0
with a apa rtment,1,1,1.0
a apa rtment with,1,1,1.0
apa rtment with bathrooms,1,1,1.0
rtment with bathrooms and,1,1,1.0
with bathrooms and the,1,1,1.0
bathrooms and the closest,1,1,1.0
a apartment with hrooms,1,1,1.0
apartment with hrooms the,1,1,1.0
with hrooms the system,1,1,1.0
hrooms the system could,1,1,1.0
between the historical c,1,1,1.0
the historical c ase,1,1,1.0
historical c ase and,1,1,1.0
c ase and the,1,1,1.0
ase and the target,1,1,1.0
be learned from number,1,1,1.0
learned from number differences,1,1,1.0
from number differences found,1,1,1.0
number differences found between,1,1,1.0
they capture the ke,1,1,1.0
capture the ke y,1,1,1.0
the ke y that,1,1,1.0
ke y that lead,1,1,1.0
y that lead to,1,1,1.0
create synthetic minority insta,1,1,1.0
synthetic minority insta nces,1,1,1.0
minority insta nces they,1,1,1.0
insta nces they stand,1,1,1.0
nces they stand a,1,1,1.0
local transformations though t,1,1,1.0
transformations though t hey,1,1,1.0
though t hey lack,1,1,1.0
t hey lack generality,1,1,1.0
hey lack generality they,1,1,1.0
of the same rences,1,1,1.0
the same rences as,1,1,1.0
same rences as is,1,1,1.0
rences as is the,1,1,1.0
constrained and local member,1,1,1.0
and local member cfa,1,1,1.0
local member cfa only,1,1,1.0
member cfa only considers,1,1,1.0
differences so the ship,1,1,1.0
so the ship is,1,1,1.0
the ship is highly,1,1,1.0
ship is highly constrained,1,1,1.0
that are already ve,1,1,1.0
are already ve ry,1,1,1.0
already ve ry similar,1,1,1.0
ve ry similar all,1,1,1.0
ry similar all other,1,1,1.0
cfa to fail howe,1,1,1.0
to fail howe ver,1,1,1.0
fail howe ver there,1,1,1.0
howe ver there are,1,1,1.0
ver there are several,1,1,1.0
respect to i t,1,1,1.0
to i t he,1,1,1.0
i t he quality,1,1,1.0
t he quality of,1,1,1.0
he quality of datas,1,1,1.0
quality of datas et,1,1,1.0
of datas et differences,1,1,1.0
datas et differences ii,1,1,1.0
et differences ii the,1,1,1.0
to generate synthetic dat,1,1,1.0
generate synthetic dat apoints,1,1,1.0
synthetic dat apoints will,1,1,1.0
dat apoints will be,1,1,1.0
apoints will be verely,1,1,1.0
will be verely hampered,1,1,1.0
be verely hampered current,1,1,1.0
verely hampered current indications,1,1,1.0
have not systematically tes,1,1,1.0
not systematically tes ted,1,1,1.0
systematically tes ted how,1,1,1.0
tes ted how changes,1,1,1.0
ted how changes in,1,1,1.0
apply to data augmenta,1,1,1.0
to data augmenta tion,1,1,1.0
data augmenta tion in,1,1,1.0
augmenta tion in data,1,1,1.0
tion in data augmentation,1,1,1.0
it produces very minim,1,1,1.0
produces very minim ferent,1,1,1.0
very minim ferent counterfactual,1,1,1.0
minim ferent counterfactual pairs,1,1,1.0
ferent counterfactual pairs so,1,1,1.0
experiments they explored us,1,1,1.0
they explored us ing,1,1,1.0
explored us ing and,1,1,1.0
us ing and counterfactuals,1,1,1.0
ing and counterfactuals but,1,1,1.0
not significantly improve tive,1,1,1.0
significantly improve tive importance,1,1,1.0
improve tive importance that,1,1,1.0
tive importance that is,1,1,1.0
to generate useful minori,1,1,1.0
generate useful minori ty,1,1,1.0
useful minori ty instances,1,1,1.0
minori ty instances we,1,1,1.0
ty instances we do,1,1,1.0
higher numbers of differences,1,1,1.0
numbers of differences were,1,1,1.0
of differences were used,1,1,1.0
differences were used in,1,1,1.0
values for that fe,1,1,1.0
for that fe ature,1,1,1.0
that fe ature this,1,1,1.0
fe ature this ance,1,1,1.0
ature this ance was,1,1,1.0
this ance was applied,1,1,1.0
ance was applied uniformly,1,1,1.0
used a more cated,1,1,1.0
a more cated tolerance,1,1,1.0
more cated tolerance scheme,1,1,1.0
cated tolerance scheme that,1,1,1.0
without tolerance fewer tuals,1,1,1.0
tolerance fewer tuals would,1,1,1.0
fewer tuals would be,1,1,1.0
tuals would be found,1,1,1.0
decision boundary so clearl,1,1,1.0
boundary so clearl y,1,1,1.0
so clearl y the,1,1,1.0
clearl y the definition,1,1,1.0
y the definition and,1,1,1.0
cfa is likely t,1,1,1.0
is likely t o,1,1,1.0
likely t o disimprove,1,1,1.0
t o disimprove in,1,1,1.0
o disimprove in this,1,1,1.0
cfa uses a soning,1,1,1.0
uses a soning approach,1,1,1.0
a soning approach to,1,1,1.0
soning approach to generating,1,1,1.0
performance of the m,1,1,1.0
of the m l,1,1,1.0
the m l models,1,1,1.0
m l models ii,1,1,1.0
l models ii this,1,1,1.0
examples by leveraging know,1,1,1.0
by leveraging know n,1,1,1.0
leveraging know n terfactuals,1,1,1.0
know n terfactuals in,1,1,1.0
n terfactuals in the,1,1,1.0
key benchmark smote ants,1,1,1.0
benchmark smote ants on,1,1,1.0
smote ants on a,1,1,1.0
ants on a wide,1,1,1.0
repository integration of rithms,1,1,1.0
integration of rithms and,1,1,1.0
of rithms and experimental,1,1,1.0
rithms and experimental analysis,1,1,1.0
class knowledge and formation,1,1,1.0
knowledge and formation systems,1,1,1.0
and formation systems bishop,1,1,1.0
formation systems bishop pattern,1,1,1.0
bunkhumpornpat sinapiromsaran lursinsap smote,1,1,1.0
sinapiromsaran lursinsap smote minority,1,1,1.0
lursinsap smote minority technique,1,1,1.0
smote minority technique for,1,1,1.0
handling the class imbal,1,1,1.0
the class imbal anced,1,1,1.0
class imbal anced problem,1,1,1.0
imbal anced problem in,1,1,1.0
anced problem in proceedings,1,1,1.0
sinapiromsaran lursinsap dbsmote based,1,1,1.0
lursinsap dbsmote based synthetic,1,1,1.0
dbsmote based synthetic minority,1,1,1.0
based synthetic minority technique,1,1,1.0
binder bischl b counterfactua,1,1,1.0
bischl b counterfactua l,1,1,1.0
b counterfactua l explanations,1,1,1.0
counterfactua l explanations in,1,1,1.0
l explanations in international,1,1,1.0
of the twentieth national,1,1,1.0
the twentieth national joint,1,1,1.0
twentieth national joint conference,1,1,1.0
identification for optimal neare,1,1,1.0
for optimal neare st,1,1,1.0
optimal neare st neighbor,1,1,1.0
neare st neighbor decision,1,1,1.0
st neighbor decision systems,1,1,1.0
greene keane counterfactual tions,1,1,1.0
keane counterfactual tions for,1,1,1.0
counterfactual tions for time,1,1,1.0
tions for time series,1,1,1.0
oversampling somo for anced,1,1,1.0
somo for anced data,1,1,1.0
for anced data set,1,1,1.0
anced data set learning,1,1,1.0
learning through a tic,1,1,1.0
through a tic oversampling,1,1,1.0
a tic oversampling method,1,1,1.0
tic oversampling method based,1,1,1.0
from a in ceedings,1,1,1.0
a in ceedings of,1,1,1.0
in ceedings of the,1,1,1.0
ceedings of the third,1,1,1.0
adasyn adaptive synthetic sampl,1,1,1.0
adaptive synthetic sampl ing,1,1,1.0
synthetic sampl ing approach,1,1,1.0
sampl ing approach for,1,1,1.0
ing approach for imbalanced,1,1,1.0
multiclass support vector chines,1,1,1.0
support vector chines ieee,1,1,1.0
vector chines ieee transactions,1,1,1.0
chines ieee transactions on,1,1,1.0
msmote improving classification mance,1,1,1.0
improving classification mance when,1,1,1.0
classification mance when training,1,1,1.0
mance when training data,1,1,1.0
for imbalance data classific,1,1,1.0
imbalance data classific ation,1,1,1.0
data classific ation based,1,1,1.0
classific ation based on,1,1,1.0
ation based on genetic,1,1,1.0
of oversampling and evolutional,1,1,1.0
oversampling and evolutional ly,1,1,1.0
and evolutional ly underdamping,1,1,1.0
evolutional ly underdamping soft,1,1,1.0
ly underdamping soft computing,1,1,1.0
algorithm for feature lection,1,1,1.0
for feature lection and,1,1,1.0
feature lection and parameter,1,1,1.0
lection and parameter optimization,1,1,1.0
kamei borderline for anced,1,1,1.0
borderline for anced data,1,1,1.0
for anced data classification,1,1,1.0
anced data classification international,1,1,1.0
by partially guided brid,1,1,1.0
partially guided brid sampling,1,1,1.0
guided brid sampling for,1,1,1.0
brid sampling for pattern,1,1,1.0
counterfactuals in data augmentati,1,1,1.0
in data augmentati on,1,1,1.0
data augmentati on to,1,1,1.0
augmentati on to predict,1,1,1.0
on to predict crop,1,1,1.0
international conference on based,1,1,1.0
conference on based reasoning,1,1,1.0
on based reasoning springer,1,1,1.0
based reasoning springer germany,1,1,1.0
error costs in ings,1,1,1.0
costs in ings of,1,1,1.0
